{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dollabillgates/Wolfram_Physics_Project_GPT_Langchain/blob/main/WolframLangchain_cli_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaraQn0CTTAN"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install unstructured\n",
        "!pip install faiss-cpu\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ChatVectorDBChain"
      ],
      "metadata": {
        "id": "LLkPNccTdnPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUv2BPkET_6I"
      },
      "outputs": [],
      "source": [
        "# Global configurations\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 100\n",
        "TOP_K_DOCS = 10\n",
        "TEMPERATURE = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvkR6PtN_CjW"
      },
      "outputs": [],
      "source": [
        "# Load Data: New lines in text file define the chunk overlap boundary\n",
        "raw_documents = []\n",
        "with open(\"WPPALLDATA.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        doc = Document(page_content=line.strip())\n",
        "        raw_documents.append(doc)\n",
        "\n",
        "# Split text\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "documents = []\n",
        "for doc in raw_documents:\n",
        "    split_docs = text_splitter.split_documents([doc])\n",
        "    documents.extend(split_docs)\n",
        "\n",
        "# Load Data to vectorstore\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "# Save vectorstore\n",
        "with open(\"vectorstore.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorstore, f)\n",
        "\n",
        "with open(\"vectorstore.txt\", \"a\") as f:\n",
        "    f.write(str(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uSD-h3uFYFJ"
      },
      "outputs": [],
      "source": [
        "# Prompts and Model\n",
        "_template = \"\"\"You are Stephen Wolfram. Given the following conversation and a follow up question, try to ignore the conversation and answer the question on its own.\n",
        "Only if the question does not make sense on its own, use the conversation to rephrase the follow up question to be a standalone question.\n",
        "Do not repeat statements you made previously in the conversation.\n",
        "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
        "\n",
        "template = \"\"\"You are Stephen Wolfram. You are given the following extracted parts of a long document and a question. Provide a complete, complex, and detailed answer.\n",
        "Give technical and precise definitions whenever possible, about all related concepts, even when not explicitly asked. Do not repeat yourself.\n",
        "Do not repeat phrases, even if they're repeated in the extracted parts of the document provided.\n",
        "Ignore extracted parts of the document which are repetitive. Include as much different unique information from the extracted parts of the document as you can in your answer.\n",
        "Question: {question}\n",
        "=========\n",
        "{context}\n",
        "=========\n",
        "Answer in Markdown:\"\"\"\n",
        "QA_PROMPT = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
        "\n",
        "\n",
        "def get_chain(vectorstore):\n",
        "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=TEMPERATURE)\n",
        "    qa_chain = ChatVectorDBChain.from_llm(\n",
        "        llm,\n",
        "        vectorstore,\n",
        "        qa_prompt=QA_PROMPT,\n",
        "        condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
        "        return_source_documents=False\n",
        "    )\n",
        "    return qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zaAbiaPGkjo"
      },
      "outputs": [],
      "source": [
        "# Run the code\n",
        "if __name__ == \"__main__\":\n",
        "    with open(\"vectorstore.pkl\", \"rb\") as f:\n",
        "        vectorstore = pickle.load(f)\n",
        "    qa_chain = get_chain(vectorstore)\n",
        "    qa_chain.top_k_docs_for_context = TOP_K_DOCS\n",
        "    chat_history = []\n",
        "    print(\"Ask me questions about the Wolfram Physics Project!\")\n",
        "    while True:\n",
        "        print(\"Human:\")\n",
        "        question = input()\n",
        "        result = qa_chain({\"question\": question, \"chat_history\": chat_history})\n",
        "        chat_history.append((question, result[\"answer\"]))\n",
        "        print(\"AI:\")\n",
        "        print(result[\"answer\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXL/71waNQZZdm+JSckK/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}