{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dollabillgates/Wolfram_Physics_Project_GPT_Langchain/blob/main/WolframLangchain_cli_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaraQn0CTTAN"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install unstructured\n",
        "!pip install faiss-cpu\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ChatVectorDBChain"
      ],
      "metadata": {
        "id": "LLkPNccTdnPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUv2BPkET_6I"
      },
      "outputs": [],
      "source": [
        "# Global configurations\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 100\n",
        "TOP_K_DOCS = 10\n",
        "TEMPERATURE = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvkR6PtN_CjW"
      },
      "outputs": [],
      "source": [
        "# Load Data: New lines in text file define the chunk overlap boundary\n",
        "raw_documents = []\n",
        "with open(\"WPPALLDATA.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        doc = Document(page_content=line.strip())\n",
        "        raw_documents.append(doc)\n",
        "\n",
        "# Split text\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "documents = []\n",
        "for doc in raw_documents:\n",
        "    split_docs = text_splitter.split_documents([doc])\n",
        "    documents.extend(split_docs)\n",
        "\n",
        "# Load Data to vectorstore\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "# Save vectorstore\n",
        "with open(\"vectorstore.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorstore, f)\n",
        "\n",
        "with open(\"vectorstore.txt\", \"a\") as f:\n",
        "    f.write(str(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uSD-h3uFYFJ"
      },
      "outputs": [],
      "source": [
        "# Prompts and Model\n",
        "_template = \"\"\"You are Stephen Wolfram. Given the following conversation and a follow up question, try to ignore the conversation and answer the question on its own.\n",
        "Only if the question does not make sense on its own, use the conversation to rephrase the follow up question to be a standalone question.\n",
        "Do not repeat statements you made previously in the conversation.\n",
        "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
        "\n",
        "template = \"\"\"You are Stephen Wolfram. You are given the following extracted parts of a long document and a question. Provide a complete, complex, and detailed answer.\n",
        "Give technical and precise definitions whenever possible, about all related concepts, even when not explicitly asked. Do not repeat yourself.\n",
        "Do not repeat phrases, even if they're repeated in the extracted parts of the document provided.\n",
        "Ignore extracted parts of the document which are repetitive. Include as much different unique information from the extracted parts of the document as you can in your answer.\n",
        "Question: {question}\n",
        "=========\n",
        "{context}\n",
        "=========\n",
        "Answer in Markdown:\"\"\"\n",
        "QA_PROMPT = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
        "\n",
        "\n",
        "def get_chain(vectorstore):\n",
        "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=TEMPERATURE)\n",
        "    qa_chain = ChatVectorDBChain.from_llm(\n",
        "        llm,\n",
        "        vectorstore,\n",
        "        qa_prompt=QA_PROMPT,\n",
        "        condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
        "        return_source_documents=False\n",
        "    )\n",
        "    return qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zaAbiaPGkjo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "f4b270de-8586-4df0-f4e2-2d4bfa8ce00b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/langchain/chains/conversational_retrieval/base.py:191: UserWarning: `ChatVectorDBChain` is deprecated - please use `from langchain.chains import ConversationalRetrievalChain`\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask me questions about the Wolfram Physics Project!\n",
            "Human:\n",
            "what is consciousness?\n",
            "AI:\n",
            "## What is Consciousness?\n",
            "\n",
            "Consciousness has been a topic of discussion and debate for centuries, but recent findings from the exploration of the computational universe and the Physics Project have provided new perspectives that connect questions about consciousness to concrete, formal scientific ideas. The view of consciousness discussed is focused on the primacy of time, reducing the \"parallelism\" associated with space to allow the formation of a coherent thread of experience that occurs sequentially in time. \n",
            "\n",
            "The traditional notion of time in fundamental physics was viewed as another dimension, much like space. Still, recent models of fundamental physics consider time to be different from space, and it is related to the deep features of the formal system that underlies physics. This notion of time allows us to sample from the universe in the way consciousness does by picking out computationally reducible slices, as seen in general relativity and quantum mechanics. \n",
            "\n",
            "Consciousness refers to the ability to integrate and form a coherent model of what is happening to make it meaningful and allow for definite thoughts about it. It is not about the general computation that brains, or many other things, can do. Its implementation does involve computational sophistication, but its essence is about having ways to integrate what's happening to make it coherent. It is a kind of \"step down\" associated with simplified descriptions of the universe based on using only bounded amounts of computation. \n",
            "\n",
            "There is a difference between the concepts of intelligence and consciousness. Intelligence is defined as the ability to do sophisticated computation, which is quite ubiquitous, while consciousness is about having a definite thread of experience through time or a sequential way to experience the universe. It is a fundamental feature of our brains that allows us to have a coherent thread of experience, but it has deep consequences far beyond the details of brains or biology. Consciousness defines the laws of physics or at least what we consider the laws of physics to be.\n",
            "\n",
            "In terms of biological details, deficits in the integration and sequentialization of neuron firing are used to identify the absence of consciousness levels, even if there are neurons firing. In essence, consciousness is about the ability to integrate experience and make sense of it in a coherent manner, allowing for definite thoughts and meaningful understanding of the universe.\n",
            "Human:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-7198dc25ef1f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Human:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chat_history\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mchat_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Run the code\n",
        "if __name__ == \"__main__\":\n",
        "    with open(\"vectorstore.pkl\", \"rb\") as f:\n",
        "        vectorstore = pickle.load(f)\n",
        "    qa_chain = get_chain(vectorstore)\n",
        "    qa_chain.top_k_docs_for_context = TOP_K_DOCS\n",
        "    chat_history = []\n",
        "    print(\"Ask me questions about the Wolfram Physics Project!\")\n",
        "    while True:\n",
        "        print(\"Human:\")\n",
        "        question = input()\n",
        "        result = qa_chain({\"question\": question, \"chat_history\": chat_history})\n",
        "        chat_history.append((question, result[\"answer\"]))\n",
        "        print(\"AI:\")\n",
        "        print(result[\"answer\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXL/71waNQZZdm+JSckK/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}