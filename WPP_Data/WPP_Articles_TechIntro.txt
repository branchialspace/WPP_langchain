How We Got Here: The Backstory of the Wolfram Physics Project 1.1 “Someday…”I’ve been saying it for decades: “Someday I’m going to mount a serious effort to find the fundamental theory of physics.” Well, I’m thrilled that today “someday” has come, and we’re launching the Wolfram Physics Project. And getting ready to launch this project over the past few months might be the single most intellectually exciting time I’ve ever had. So many things I’d wondered about for so long getting solved. So many exciting moments of “Surely it can’t be that simple?” And the dawning realization, “Oh my gosh, it’s actually going to work!” Physics was my first great intellectual passion. And I got started young, publishing my first paper when I was 15. I was lucky enough to be involved in physics in one of its golden ages, in the late 1970s. Not that I was trying to find a fundamental theory of physics back then. Like essentially all physicists, I spent my time on the hard work of figuring out the consequences of the theories we already had. But doing that got me progressively more involved with computers. And then I realized: computation is its own paradigm. There’s a whole way of thinking about the world using the idea of computation. And it’s very powerful, and fundamental. Maybe even more fundamental than physics can ever be. And so it was that I left physics, and began to explore the computational universe: in a sense the universe of all possible universes. That was forty years ago, and much has happened since then. My science led me to develop technology. The technology led me to more science. I did big science projects. I did big technology projects. And between the science and the technology, I felt like I was gradually building a tower that let me see and do more and more. I never forgot physics, though. And as I studied the computational universe I couldn’t help wondering whether maybe somewhere, out in this abstract computational world, might be our physical universe, just waiting to be discovered. Thirty years ago I had my first idea about how this might work. And over the decade that followed I figured out quite a bit—found some encouraging signs—and eventually started to tell the world about it. I kept on thinking about really pushing it further. I’d talk about it when I could, sometimes in very public venues. But I was off doing other, very satisfying things. It so happened that technology I’d built became very widely used by physicists. But to most of the physics community I was basically an ex-physicist, who sometimes said strange and alien things about fundamental physics. Meanwhile, two decades went by. I always hoped that one day I’d get to do my physics project. But I didn’t know when, and my hopes were dimming. But then, a bit more than a year ago, I had a little idea that solved a nagging problem I’d had with my approach. And when I talked about it with two young physicists at our annual Summer School they were so enthusiastic. And I realized, “Yes, there are people who really want to see this problem solved.” And after everything I’ve built and thought about, I have a responsibility to see if it can be done. Oh, and by the way, I really want to do it! It just seems like such a fun and fascinating thing. So why not just do it? We got started in earnest late last fall. I started doing lots of new computer experiments. New ideas started flowing. And it was incredible. We started to figure out so much. My plan had been that we’d mostly just describe as clearly as possible what I basically already knew, then launch it as a project for other people to get involved. But it was just too easy and too fun to figure things out. We had a new paradigm and things just started tumbling out. In all my years of doing science and technology, I’ve never seen anything like it. It’s been wonderful. But the plan was always to share the fun, and now we’re ready to do that. We’re publishing everything we’ve done so far (including all the tools, archives, even working-session videos), and we’re looking forward to seeing if this is the time in history when we finally get to figure out the fundamental theory for our universe.  Oh, and I finally get to bring to closure something I’ve wanted to do for more than half my life, and that in some ways I’ve spent half a century preparing for.
How We Got Here: The Backstory of the Wolfram Physics Project 1.2 Why Wasn’t This Already Figured Out? : People have thought about what we’d now call the fundamental theory of physics throughout recorded history. From creation myths, to philosophy, to science, it’s been a long story. And most of the time, it’s actually seemed as if the answer was not far away, at least to the standards of explanation of the day. But it never quite got solved. And if—as I believe—our project is finally on the right track, we kind of now know why. We just didn’t have the modern paradigm of computation before, and so we didn’t have the right way of thinking about things. Looking back, though, there were an awful lot of good ideas, that were very much in the right direction. And particularly in recent times, there was an awful lot of mathematical methodology developed that’s very relevant and on target. What does it matter what the fundamental theory of physics is? It’d certainly be an impressive achievement for science to figure it out. And my guess is that knowing it is eventually going to have some far-reaching long-term consequences for our general ways of thinking about things. Conceivably the theory will have near-term applications too. But in terms of what’s done year after year in developing technology, doing science or even understanding theological questions, knowing the fundamental theory of physics isn’t directly relevant; it’s more like an ultimate “background question”. And that’s realistically pretty much how it’s been treated throughout most of history. Back in ancient Greek times, almost every serious Greek philosopher seems to have had a theory. The details were different. But there was a theme. That somehow everything in the universe consists of the same thing or things repeated over and over again. Maybe it was all water. Maybe it was four elements. Maybe Platonic solids. Maybe atoms. Maybe the world is assembled like sentences from a grammar. To us today these seem quite vague and almost allegorical. But there was an important idea: that everything we see in the world might actually be the result of something simple and formalizable underneath. As the centuries went by, the idea of “natural laws” sharpened, sometimes with an almost computational feel. “God can only run the world by natural laws”, or “The universe is the thoughts of God actualized”. The 1600s brought the whole idea of describing the world using what amount to mathematical models. But while this had a huge effect on what could be studied and computed in physics, it didn’t immediately change the thinking that much about what the universe might ultimately be made of. It was still just tiny corpuscles (AKA atoms), though now presumed to be bound by gravitational forces. But what did begin to change was the whole idea that there should be any kind of “explicit explanation” for the universe that one could reason about: maybe there were just equations that were true about the universe, and that was all that could be said, a bit like Euclid’s axioms for geometry. But around the same time, systematic experimental science began to rise—and there implicitly emerged the picture (charmingly resonant with modern debates about machine learning) that physics should consist of finding equations that would represent theories that could fit experimental data. In the 1800s, as mathematical physics reached the point where it could deal with partial differential equations, and the notion of fields became popular, there started to be ideas about the universe being “all fields”. First there was the ether (along with the rather good idea that atoms might be knotted vortices in the ether). Later people wondered if the electromagnetic field could underlie everything. When the electron was discovered people wondered if perhaps everything was in fact made of electrons. And so on. But a key theme was that to figure out things about the universe, you should either just do experiments, or you should take known equations and compute: you weren’t expected to be able to reason about the universe. That made special relativity in 1905 quite a shock: because, once again, one was figuring out physics by abstract reasoning. But somehow that just reinforced the “trust the mathematics” idea, and for example—in what I consider to be one of the most important wrong turns in the history of physics—there emerged the idea of a mathematical notion of “spacetime”, in which (despite our strong intuitive sense to the contrary) space and time are treated as “the same kind of thing”. The introduction of general relativity in 1915—in addition to giving us the theory of gravity—brought with it the notion that “high-end” modern mathematics (in this case tensors and differential geometry) could inform physics. And that was an important piece of methodological input when quantum mechanics, and soon quantum field theory, were developed in the 1920s. Yes, it was difficult to “understand” the theory. But really the mathematics was the intellectual meat of what was going on, and should guide it. And that was what let one calculate things anyway. “Interpretation” was more philosophy than physics. The question of what space “is” had been discussed by philosophers since antiquity. Euclid had implicitly made a pretty definitive statement with his very first definition “a point is that which has no part”: i.e. there is no discreteness to points, or, in other words, space is continuous. And by the time calculus arose in the late 1600s, it was pretty much taken for granted that space was continuous, and position was a continuous variable. At different times, Descartes, Riemann and Einstein all had their doubts. But the force of the mathematical methodology provided by calculus was just too great. Still, in the 1930s there started to be problems with infinities in quantum calculations—and with quantization all the rage it started almost being assumed that space must be quantized too. But with the calculus-based thinking of the time nobody managed to make that work. (Graph theory was presumably too immature—and basically unknown—for the idea of space as a graph to arise.) Then in the 1940s the mathematical problems with infinities were avoided (by the idea of renormalization), and—with some important exceptions—the notion that space might be discrete basically disappeared from physics. Meanwhile, mathematical methods based on calculus were doing great in advancing physics. Quantum electrodynamics (QED) and general relativity were both particularly successful, and it started to seem as if figuring out everything in physics was just a question of doing the math well enough. But then there were the particles. The muon. The pion. The hyperons. For a while it had seemed that electrons, photons, protons and neutrons were what everything was made of. But by the 1950s particle accelerators were starting to discover many tens of new “elementary” particles. What were all these things? Oh, and they were all tied up with the strong nuclear force that held nuclei together. And despite all its success in QED, quantum field theory and its “just-work-everything-out-step-by-step” mathematics just didn’t seem to apply.  So a different approach developed: S-matrix theory. It was mathematically elaborate (functions of many complex variables), in some ways elegant, but in a sense very formal. Instead of saying “this is how things are built up from something underneath” it basically just said “Here are some mathematical constraints. Whatever solutions they have are what will happen. Don’t ask why.” And when it came to the particles, there were two approaches. One—roughly allied with quantum field theory—said that inside all these particles was something more fundamental, a new kind of particle called a quark. The other approach—allied with S-matrix theory—imagined something more “democratic” with different particles all just related by some kind of consistency condition. Through the 1960s these two approaches duked it out. S-matrix theory was definitely ahead—notably spawning Regge theory and what later became string theory. There were quantum-field-theory ideas like what became QCD, but they didn’t look promising. But by the early 1970s it began to be clear that quarks were something real, and in 1973 the phenomenon of asymptotic freedom was discovered, and quantum field theory was saved. In 1974 came the surprise discovery of a new kind of quark, and physics entered a golden age of rapid progress, essentially powered by quantum field theory. (And, yes, I was involved in that, and it was a lot of fun.) Soon the Standard Model emerged, and everything seemed to be fitting together, and it seemed once again that it was just a matter of calculating, and everything could be figured out. There were still mysteries: for example, why these particular particles, with these particular masses? But there was a methodology, and there was a sense that somehow this would all work out. An important piece of the story was the use of the theory of Lie groups (a piece of “high-end math” that made its way into physics in the 1950s). Which group was the one for the universe? The Standard Model involved three groups: SU(3), SU(2) and U(1). But could these all be combined into a single, bigger group, perhaps SU(5) or SO(10)—a single “grand unified” model? Around 1980 it all looked very promising. But there was one key prediction: the proton should be unstable, decaying, albeit very slowly. But then the experiments started coming in: no proton decay. What was particle physics to do? There were new theories, with new particles. But no new particles showed up. Meanwhile, people kept computing more and more with the Standard Model. And everything kept on working. One decimal place, two, three, four. It was difficult—but somehow routine—to do these calculations. And it seemed like particle physics had entered a phase like atomic physics and nuclear physics before it, where it was really just a question of calculating what one needed. But there was a crack in all of this. And it was gravity. Yes, quantum field theory had worked well in particle physics. But when it was applied to gravity, it really didn’t work at all. That wasn’t important for computing things about the Standard Model. But it showed there was something else that had to be figured out in physics. Meanwhile, the theory of gravity had steadily been developing, based on general relativity, which was unchanged since 1915. Until about the 1950s, there had been hopes of generalizing general relativity to make a “unified field theory” that could encompass “matter” as well as gravity. (And in fact, once again, there were good ideas here, about “everything being made of space”.) But it hadn’t worked out—though for example Einstein remarked that perhaps that was because it was being incorrectly assumed that space is continuous. General relativity is a difficult mathematical theory, fraught with issues of what is real, and what is “just math”. It didn’t get nearly as much attention as quantum field theory, but by the 1960s it was becoming better understood, there were starting to be sensitive experimental tests, and it was pretty clear that things like black holes were real predictions of the theory. And the discovery of the cosmic microwave background heightened interest in cosmology in general, and in the early universe in particular. In a way, particle physics had been propelled at the end of World War II by the success of the Manhattan Project. But a generation had passed, and by the end of the 1980s it no longer seemed compelling to spend billions of dollars to build the next particle accelerator. But right around then, there started to be more and more done in cosmology. More and more details about the early universe. More and more mystery about the dark matter that seems to exist around galaxies. And somehow this progress in cosmology just emphasized the importance of figuring out how particle physics (and quantum field theory) could be brought together with general relativity. But what was to be done? At the tail end of the 1970s-golden-age of particle physics there was another injection of “fancy math”, this time around fiber bundles and algebraic topology. The original application (instanton solutions to the equations of QCD) didn’t work out. But there began to develop a new kind of interchange between the front lines of pure mathematics and theoretical physics. And as traditional particle physics plateaued, there was more and more emphasis on quantum gravity. First there was supergravity—a kind of extension of the quark model and group theory “let’s just figure out more particles” tradition. But soon the focus turned to something new: string theory. Well, actually, it wasn’t new at all. String theory had been developed, and rejected, as part of the S-matrix initiative in the 1960s. But now it was retooled, and directed at quantum gravity with enough vigor that by the end of the 1980s a large fraction of all particle physicists were working on it. It didn’t really connect to anything experimentally visible. And it also had all sorts of weird problems—like implying that the universe should really be 26-dimensional, or maybe 10-dimensional. But the physics community was committed to it, and the theory kept on getting patched, all the while becoming more complicated. But even though the physics wasn’t terribly compelling, there were starting to be some spinoffs in math, that made elegant—and important—connections across different areas of high-end pure mathematics. And in the mid-1990s the high-end math paid back to physics again, bringing M-theory—which seemed to miraculously weave together disparate directions in string theory. For a while there were claims that M theory would be “it”—the fundamental theory of physics. But gradually the hopes faded, with rather little to show. There was another frontier, though. In the 1970s an initially rather rickety calculation had suggested that black holes—instead of being “completely black”—should emit particles as a result of quantum mechanics. For a while this was basically a curiosity, but slowly the calculations became more streamlined—and it began to look as if whole black holes in a sense had enough mathematical perfection that they could actually be studied a bit like little particles. And in the late 1990s from the mathematics of string theory there emerged the so-called AdS/CFT correspondence—an elaborate mathematical connection between a limiting case of general relativity and a limiting case of quantum field theory. I don’t think anyone would claim that AdS/CFT is itself anything like a fundamental theory of physics. But in the past 20 years it’s steadily grown to become perhaps the central hope for fundamental physics, mathematically hinting at a variety of deep connections—which, as it happens, look like they may actually dovetail quite beautifully with what we’ve recently figured out. The last time there was a widespread “we’re almost there” feeling about the fundamental theory of physics was probably around 1980, with another blip in the mid-1990s. And since then the focus of physics has definitely turned elsewhere. But there have been some initiatives—many actually dating from the 1970s and before—that have still continued in various pockets of the physics and math communities. Twistor theory. Causal set theory. Loop quantum gravity. Spin networks. Non-commutative geometry. Quantum relativity. Typically these have seemed like increasingly disconnected—and sometimes almost quixotic—efforts. But one of the wonderful things that’s come out of our project so far is that actually the core formalisms of a surprising number of these initiatives look to be directly and richly relevant. But what about other approaches to finding a fundamental theory of physics? Realistically I think the landscape has been quite barren of late. There’s a steady stream of people outside the physics community making proposals. But most of them are deeply handicapped by not connecting to quantum field theory and general relativity. Yes, these are mathematically sophisticated theories that pretty much take a physics PhD’s worth of study to understand. But they’re the best operational summaries we have right now of what’s known in physics, and if one doesn’t connect to them, one’s basically throwing away everything that was achieved in 20th-century physics. Is it surprising that a fundamental theory of physics hasn’t been found yet? If—as I think—we’re now finally on the right track, then no, not particularly. Because it requires ideas and paradigms that just hadn’t been developed until quite recently. Of course, to find a fundamental theory of physics you have to go to the effort of trying to do it, and you have to believe it’s possible. And here perhaps the biggest impediment has been the sheer size of physics as an enterprise. After its successes in the mid-20th century, physics became big—and being a physicist became a top aspiration. But with size came institutionalization and inertia. About a hundred years ago it was a small collection of people who originally invented the two great theories (general relativity and quantum field theory) that have basically defined fundamental physics for the past century. I don’t think it would have been a great surprise to any of them that to make further progress one would need new ideas and new directions.  But to most modern physicists—perhaps six or seven academic generations removed from the founders of these fields, and embedded in a large structure with particular ways of doing things—the existing ideas and directions, and the kinds of things that can be done with them, just seem like the only way it can be. So if in the normal course of science, a fundamental theory of physics does not appear—as it did not—there ends up being almost a collective conclusion that to find it must be too hard, or even impossible. And not really what physics, or physicists, should be doing. So who will do it? There’s a pretty thin set of possibilities. It pretty much has to be someone who knows the methods and achievements of mainstream physics well, or, essentially, someone who has been a physicist. It also pretty much has to be someone who’s not too deeply embedded in the current world of physics and its existing ideas, prejudices and “market forces”. Oh, and it requires tools and resources. It doesn’t hurt to have experience in doing and leading large projects. And it requires the confidence and resolve to try a big and difficult project that few people will believe in—as well, of course, as a deep interest in actually finding out the answer. The project that we’re now launching almost didn’t happen. And if a few more years had gone by, it’s pretty certain it wouldn’t have. But things lined up in just such a way that a small window was opened. And I’m thrilled at the way it’s turning out. But let me now tell a little more of the story of how it got here.
How We Got Here: The Backstory of the Wolfram Physics Project 1.3 The Beginning of the Story : As a young kid growing up in England in the 1960s I viewed the space program as a kind of beacon of the future, and I intently followed it. But when I wanted to know how spacecraft and their instruments worked I realized I had to learn about physics—and soon I left space behind, and was deeply into physics. I was probably 11 years old when I started reading my first college physics textbook, and when I was 12 I compiled a “Concise Directory of Physics” with 111 pages of carefully typed information and data about physics: All this information collection had definite shades of “Wolfram|Alpha-ism”. And the “visualizations” presaged a lifelong interest in information presentation. But my Concise Directory also had something else: pages listing the “elementary particles”. And soon these became my great obsession. The pions. The kaons. The muon. The cascade hyperons. To me they were the ultimate story in science, and I was soon learning all their quirks (“The K zero isn’t its own antiparticle!” “The omega minus has strangeness -3!”) I spent the summer when I was 13 writing the 132-page, single-spaced, typed “The Physics of Subatomic Particles”. At the time, I basically showed it to no one, and it’s strange to look at it now—47 years later. It’s basically an exposition of particle physics, told in a historical arc. Some of it is shockingly similar to what I just wrote in the previous section of this piece—except for the change of tense and the Americanization: It’s charming to read my 13-year-old self’s explanation of quantum field theory (not bad), or my authoritative description of a just-proposed theory of the muon that I’m guessing I found out about from New Scientist, and that turned out to be completely wrong. By the next summer I was writing a 230-page treatise “Introduction to the Weak Interaction”, featuring some of my most favorite elementary particles, and showing a pretty good grasp of quantum mechanics and field theory: Pretty soon I had reached the edge of what was known in particle physics, but so far what I had done was basically exposition; I hadn’t really tried to figure out new things. But by the summer of 1974 it was increasingly clear that something unexpected was going in physics. Several experiments were showing an unanticipated rise in the electron-positron annihilation cross-section—and then, rather dramatically, in November, the J/ψ particle was discovered. It was all a big surprise, and at first people had no idea what was going on. But 14-year-old me decided I was going to figure it out. Those were days long before the web, and it wasn’t easy to get the latest information. But from where I lived when I wasn’t at school, it was about a 6-mile bicycle ride to the nearest university library, and I did it often. And pretty soon I had come up with a theory: maybe—contrary to what had long been believed—the electron is not in fact a point particle, but actually has internal structure. By then, I had read many academic papers, and pretty soon I had written one of my own. It took two tries, but then, there it was, my first published paper, complete—I now notice—with some self-references to earlier work of mine, in true academic style: It was a creative and decently written paper, but it was technically a bit weak (heck, I was only 15), and, at least at the time, its main idea did not pan out. But of course there’s an irony to all this. Because—guess what—45 years later, in our current model for fundamental physics, the electron is once again not a point particle! Back in 1975, though, I thought maybe it had a radius of 10-18 meters; now I think it’s more likely 10-81 meters. So at the very least 15-year-old me was wrong by 63 orders of magnitude! Being a “teenage physicist” had its interesting features. At my boarding school (the older-than-the-discovery-of-America Eton), there was much amusement when mail came addressed to me as “Dr. S. Wolfram”. Soon I started doing day trips to go to physics seminars in Oxford—and interacting with “real physicists” from the international physics community. I think I was viewed as an exotic phenomenon, usually referred to in a rather Wild-West way as “The Kid”. (Years later, I was amused when one of my children, precocious in a completely different domain, earned the very same nickname.) I really loved physics. And I wanted to do as much physics as I could. I had started using computers back in 1973—basically to do physics simulations. And by 1976 I’d realized something important about computers. The one thing I didn’t like about physics was that it involved doing all sorts of—to me, tedious—mathematical calculations. But I realized that I could get computers to do those for me. And, needless to say, that’s how, eventually, Mathematica, Wolfram|Alpha, etc. came to be. I left high school when I was 16, worked doing physics at a government lab in England for about 6 months, and then went to Oxford. By this point, I was producing physics papers at a decent rate, and the papers were getting progressively better. (Or at least good enough that by age 17 I’d had my first run-in with academic thievery.) Mostly I worked on particle physics—at that time by far the hottest area of physics. But I was also very interested in questions like the origin of the Second Law of thermodynamics, and particularly its relation to gravity. (If things always become more disordered, how come galaxies form, etc.?) And from this (as well as questions like “where’s the antimatter in the universe?”) I got interested in cosmology, and, inevitably, in connecting it to particle physics. Nowadays everyone knows about that connection, but back then few people were interested in it. Particle physics, though, was a completely different story. There were exciting discoveries practically every week, and the best and brightest were going into the field. QCD (the theory of quarks and gluons) was taking off, and I had a great time doing some of the “obvious” calculations. And of course, I had my secret weapon: computers. I’ve never really understood why other people weren’t using them, but for me they were critical. They let me figure out all this stuff other people couldn’t. And I think the process of writing programs made me a better physicist too. Looking at my papers from back then, the notation and structure got cleaner and cleaner—as might befit a future lifelong language designer. After a bit more than a year in Oxford, and now with ten physics papers to my name, I “dropped out of college”, and went to Caltech as a graduate student. It was a very productive time for me. At the peak I was writing a physics paper every couple of weeks, on quite a range of topics. (And it’s nice to see that some of those papers still get referenced today, 40 years later.) Caltech was at the time a world center for particle physics, with almost everyone who was someone coming through at one time or another. Most of them were much older than me, but I still got to know them—not just as names in the physics literature but as real people with their various quirks. Murray Gell-Mann and Richard Feynman were the two biggest names in physics at Caltech at the time. I got on particularly well with Feynman, even if—in his rather competitive way—he would often lament that he was three times my age. (In the way these things come around, I’m now the same age as he was when I first met him…) After a bit more than a year, I put together some of the papers I’d written, officially got my PhD, and took up a nice research faculty position at Caltech. I’d had the goal of “being a physicist” since I was about 10 years old, and now, at age 20, I was actually officially a physicist. “So what now?” I wondered. There were lots of things I wanted to do in physics. But I felt limited by the computer tools I had. So—actually within a couple of weeks of getting my PhD—I resolved that I should spend the time just to build the tools I needed. And that’s how I came to start developing my first big computer system and language. I approached it a bit like a problem in natural science, trying to develop a theory, find principles, etc. But it was different from anything I’d done before: it wasn’t constrained by the universe as the universe is. I just had to invent abstract structures that would fit together and be useful. The system I built (that I called SMP, for “Symbolic Manipulation Program”) had all sorts of ideas, some good, some not so good. One of the most abstract—and, arguably, obscure—ideas had to do with controlling how recursive evaluation works. I thought it was neat, and perhaps powerful. But I don’t think anyone (including me) ever really understood how to use it, and in the end it was effectively relegated to a footnote. But here’s the irony: that footnote is now a front-and-center issue in our models of fundamental physics. And there’s more. Around the time I was building SMP I was also thinking a lot about gauge theories in physics. So there I was thinking about recursion control and about gauge invariance. Two utterly unrelated things, or so I thought. Until just recently, when I realized that in some fundamental sense they’re actually the same thing.
How We Got Here: The Backstory of the Wolfram Physics Project 1.4 “You Can’t Leave Physics” : It took a couple of years to build the first version of SMP. I continued to do particle physics, though I could already feel that the field was cooling, and my interests were beginning to run to more general, theoretical questions. SMP was my first large-scale “practical” project. And not only did it involve all sorts of software engineering, it also involved managing a team—and ultimately starting my first company. Physicists I knew could already tell I was slipping away from physics. “You can’t leave physics”, they would say. “You’re really good at this.” I still liked physics, and I particularly liked its “let’s just figure this out” attitude. But now I wasn’t just applying that methodology in quantum field theory and cosmology; I was also using it in language design, in software development, in entrepreneurism, and in other things. And it was working great. The process of starting my first company was fraught with ahead-of-my-time-in-the-interaction-between-companies-and-universities issues, that ultimately caused me to leave Caltech. And right in the middle of that, I decided I needed to take a break from my mainline “be a physicist” activities, and just spend some time doing “something fun”. I had been thinking for a long time about how it is that complex things manage to happen in nature. My two favorite examples were neural networks (yes, back in 1981, though I never figured out how to make them do anything very useful back then) and self-gravitating gases. And in my “just have fun” approach I decided to try to make the most minimal model I could, even if it didn’t really have much to do with either of these examples, or officially with “physics”. It probably helped that I’d spent all that time developing SMP, and was basically used to just inventing abstract things from scratch. But in any case, what I came up with were very simple rules for arrays of 0s and 1s. I was pretty sure that—as such—they wouldn’t do anything interesting. But it was basically trivial for me to just try running them on a computer. And so I did. And what I found was amazing, and gradually changed my whole outlook on science and really my whole worldview—and sowed the seeds that have now, I believe, brought us a path to the fundamental theory of physics. What I was looking at were basically some of the very simplest programs one can imagine. And I assumed that programs that simple wouldn’t be able to behave in anything other than simple ways. But here’s what I actually saw in my first computer experiment (here rendered a bit more crisply than in my original printouts): Yes, some of the behavior is simple. And some of it involves nice, recognizable fractal patterns. But then there are other things going on, like my all-time favorite—what I called “rule 30”. At first, I didn’t understand what I was seeing, and I was convinced that somehow the simplicity of the underlying rules must ultimately force the behavior to be simple. I tried using all sorts of methods from physics, mathematics, computer science, statistics, cryptography and so on to “crack” these systems. But I always failed. And gradually I began to realize that something fundamental was going on—that somehow in just running their rules, simple as they were, these systems were intrinsically creating some kind of irreducible complexity. I started writing papers about what I’d discovered, at first couched in very physics-oriented terms: The papers were well received—in physics, in mathematics, and in other fields too, like biology. (Where perhaps it helped that—in a nod to historical antecedents—I called my models “cellular automata”, though I meant abstract cells, not biological ones.) Meanwhile, I had moved to the Institute for Advanced Study, in Princeton (where there were still people telling stories about their interactions with Kurt Gödel and “Johnny” von Neumann and his computer, and where, yes, my office was upstairs from where Einstein had once worked). I started building up a whole effort around studying “complexity” and how it could arise from simple rules. And gradually I started to realize that what I’d seen in that little computer experiment in 1981 was actually a first sign of something very big and very important. Looking back, I see that experiment as my personal analog of turning a telescope to the sky and seeing the moons of Jupiter. But the challenge was really to understand the significance of what I’d seen—which in the end took me decades. But the first step was just to start thinking not in terms of the kinds of methods I’d used in physics, but instead fundamentally in terms of computation, treating computation not just as a methodology but a paradigm. The summer of 1984 was when I think I finally began to seriously understand computation as a paradigm. Early that summer I’d finally recognized rule 30 for what it was: a powerful computational system. Then—in writing an article for Scientific American (nominally on “Computer Software in Science and Mathematics”)—I came up with the term “computational irreducibility”, and began to understand its significance. That fall I wrote a short paper that outlined the correspondence with physics, and the fundamental implications (which now loom large in our current project) of computational irreducibility for physics: One of the nice things for me about the Institute for Advanced Study is that it was a small place, with not only physicists, but also lots of world-class mathematicians. (I had interacted a bit with Michael Atiyah and Roger Penrose about mathematics-for-physics when I was in Oxford, but at Caltech it was physics and nothing but.) Two top-of-the-line mathematicians, John Milnor and Bill Thurston, both got interested in my cellular automata. But try as they might, they could prove pretty much nothing; they basically hit a wall of computational irreducibility. Yes, there is undecidability in mathematics, as we’ve known since Gödel’s theorem. But the mathematics that mathematicians usually work on is basically set up not to run into it. But just being “plucked from the computational universe”, my cellular automata don’t get to avoid it. And ultimately our physics project will run into the same issues. But one of the wonderful things that’s become clear in the last few months is that actually there’s quite a layer of computational reducibility in our models of physics—which is critical for our ability to perceive the world coherently, but also makes math able to be useful. But back to the story. In addition to my life doing basic science, I had a “hobby” of doing consulting for tech companies. One of those companies was a certain ultimately-poorly-named Thinking Machines Corporation, that made massively parallel computers that happened to be ideally suited to running cellular automata. And in an effort to find uses for their computers, I decided to see whether one could model fluid flow with cellular automata. The idea was to start not with the standard physics equations for fluid flow, but instead just to have lots of computational particles with very simple rules, and then see whether on a large scale fluid flow could emerge. As it turned out, with my interest in the Second Law of thermodynamics, I’d actually tried something quite similar back in 1973, as one of the very first programs I ever wrote. But I hadn’t seen anything interesting then, partly because of what one might think of as a piece of technical bad luck, but probably more importantly because I didn’t yet grasp the paradigm that would allow me to understand what was going on. But in 1985 I did understand, and it was neat: from tiny computational rules that didn’t immediately have physics in them was emerging a piece of physics that was normally described with the equations of physics. And, yes, now it looks like that’s how all of physics may work—but we’ll come to that. By 1985 I was pretty clear on the notion that one could use the computational paradigm and the methods around it to explore a wide range of phenomena and questions. But for me the “killer app” was understanding the origins of complexity, and trying to build a general “theory of complexity”. It wasn’t physics, it wasn’t mathematics, it wasn’t computer science. It was something new. I called it “complex systems theory” (avoiding, at least for a while, a preexisting and completely different field of computer science called “complexity theory”). I was 25 years old but already pretty established in science, with “mainstream cred” from my early work in physics, and a lot of momentum from my work in complexity and in practical computing. I liked a lot doing complex systems research myself, but I thought that to really make progress more people needed to be involved. So I started organizing. I launched a journal (which is still thriving today). And then I talked to universities (and other places) to see where the best place to start a research center would be. Eventually I picked the University of Illinois, and so in the fall of 1986 there I went, themed as a professor of physics, mathematics and computer science, and director of the Center for Complex Systems Research. It was a good setup, but I quickly realized it wasn’t a good fit for me. Yes, I can organize things (and, yes, I’ve been a CEO now for more than half my life). But I do best when I’m organizing my own things, rather than being inside another organization. And, most important, I like actually doing things—like science—myself. So rather quickly, I went to Plan B: instead of trying to get lots of other people to help push forward the science I wanted to see done, I’d set myself up to be as efficient as possible, and then I’d try to just do what I thought should be done myself. But the first thing I needed was good computational tools. And so it was that I started to build Mathematica, and what’s now the Wolfram Language, and to start my company, Wolfram Research. We launched the first version of Mathematica in June 1988, and I think it’s fair to say that it was an instant hit. Physicists were particularly keen on it, and rather quickly it induced an interesting transition. Before Mathematica, if a typical physicist needed to compute something on a computer, they’d delegate it to someone else to actually do. But Mathematica for the first time made computing “high level” enough that physicists themselves could do their own computations. And it’s been wonderful to see over the years immense amounts of physics research done with the tools we’ve built. (It’s very nice to have been told many times that, apart from the internet, Mathematica is the largest methodological advance in the doing of physics in this generation.) For a few years, the rapid development of Mathematica and our company entirely consumed me. But by 1991 it was clear that if I concentrated full-time on it, I could generate far more ideas than our company—at the size it then was—could possibly absorb. And so I decided it was time for me to execute the next step in my plan—and start actually using the tools we’d developed, to do the science I wanted to do. And so in 1991 I became a remote CEO (as I still am) and started work on my “science project”.
How We Got Here: The Backstory of the Wolfram Physics Project 1.5 Maybe It Could Apply to Physics : Pretty quickly I had a table of contents for a book I planned to write—that would work through the consequences of the computational paradigm for complexity and other things. Part of it was going to be exploration: going out into the computational universe and studying what programs do—and part of it was going to be applications: seeing how to apply what I’d learned to different areas of science, and beyond. I didn’t know what I’d end up discovering, but I figured the process of writing the book would take a year or two. My first question was just how general the phenomena I’d discovered in cellular automata actually were. Did they depend on things updating in parallel? Did they depend on having discrete cells? And so on. I started doing computer experiments. Often I’d think “this is finally a kind of system that isn’t going to do anything interesting”. And I kept on being wrong. I developed a mantra, “The computational animals are always smarter than you are”. Even when you can give all sorts of arguments about why such-and-such a system can’t do anything interesting, it’ll find a way to surprise you, and do something you’d never predict. What was going on? I realized it was something very general, and very fundamental to basically any system. I call it the Principle of Computational Equivalence, and it’s now the guiding principle for a lot of my thinking. It explains computational irreducibility. It gives us a way to organize the computational universe. It tells us about the power of minds. It shows us how to think about the possibilities of artificial intelligence. It gives us perspectives on alien intelligence. It gives us a way to think about free will. And now it seems to give us a way to understand some ultimate questions about our perception of possible physical universes. I think it was in 1990, right before I began the book project, that I started wondering about applying my ideas to fundamental physics. There’d been a whole “digital physics” movement (particularly involving my friend Ed Fredkin) around using cellular automata to model fundamental physics. But frankly it had put me off. I’d hear “I’ve discovered an electron in my cellular automaton”, but it just sounded like nonsense to me. “For goodness’ sake, learn what’s already known in physics!”, I would say. Of course I loved cellular automata, but—particularly with their rigid built-in notions of space and time—I didn’t think they could ever be more than allegories or toy models for actual physics, and pushing them as more than that seemed damaging, and I didn’t like it. But, OK, so not cellular automata. But what underlying computational structure might actually work? I was pretty sure it had to be something that didn’t have its own built-in notion of space. And immediately I started thinking about networks. Things like cellular automata are very clean and easy to define, and program on a computer. Networks—at least in their most obvious form—aren’t. My first foray into studying network-based systems was in 1992 and wound up as part of “Chapter 5: Two Dimensions and Beyond”. And like every other kind of system I studied, I found that these network systems could generate complex behavior. By 1993 I’d studied lots of kinds of abstract systems. And I was working down the table of contents of my planned book, and starting to ask questions like: “What can all this tell us about biology?” “What about human perception?” “Mathematics?” And it was quite exciting, because every time I’d look at a new area I’d realize “Yes, the things I’ve found in the computational universe really tell us new and interesting things here!” So finally in 1994 I decided to try and tackle fundamental physics. I’ve got this whole shelf of drafts of what became my book, and I just pulled down the versions from 1994. It’s already got “Chapter 9: Fundamental Physics”, but the contents are still embryonic. It gradually grows through 1995 and 1996. And then in 1997, there it is: “Space as a Network”, “Time and Causal Networks”, etc. I’d figured out the story of how space could be made as the limit of a discrete network and how different possible updating sequences for graphs led to different threads of time. And I’d come up with the idea of causal invariance, and realized that it implied special relativity. I’d also begun to understand how curvature in space worked, but I didn’t yet “have” general relativity. I’ve got all my notebooks from those times (and they’re even now in our online archives). It’s a little weird to pull them up now, and realize how tiny screens were back then. But for the most part everything still runs, and I can see how I started to do searches for “the rule” that could build something like our universe. By then I was in year 6 of my “one-year” book project. At the beginning I’d called my book A Science of Complexity. But even by 1994 I’d realized that it was a bigger story than that—and I’d renamed the book A New Kind of Science. There was a whole intellectual edifice to discover, and I was determined to work through all the “obvious questions” so I could coherently describe it. From a personal point of view it’s certainly the hardest project I’ve ever done. I was still remote-CEOing my company, but every day from early in the evening until perhaps 6 am I’d work on science, painstakingly trying to figure out everything I could. On a good day, I’d write a whole page of the book. Sometimes I’d spend the whole day just computing one number that would end up in tiny print in the notes at the back. When I first embarked on the book project I talked to people quite a bit about it. But they’d always be saying “What about this? What about that?” But no! I had a plan and if I was ever going to get the project done, I knew I had to stick to it, and not get distracted. And so I basically decided to become a hermit, focus intensely on doing the project, and not talk to anyone about it (except that I did have a sequence of research assistants, including some very talented individuals). The years went by. I’d started the book not long after I turned 30. Now I was approaching 40. But, slowly, inexorably, I was working through the table of contents, and getting towards the end. It was 2001 when I returned to put the finishing touches on Chapter 9. By then I had a pretty good idea how general relativity could work in my model, but in 2001 I got it: a derivation of general relativity that was kind of an analog for the emergence of spacetime from networks of my derivation from 16 years earlier of the emergence of fluid flow from simple cellular automata. And finally, in 2002, after ten and a half years of daily work, my book was finished. And what I had imagined might be a short “booklet” of perhaps 150 pages had become a 1280-page tome, with nearly a third of a million words of detailed notes at the back. I intended the book to be a presentation (as its title said) of a new kind of science, based on the computational paradigm, and informed by studying the computational universe of simple programs. But I had wanted to include some “use cases”, and physics was one of those, along with biology, mathematics and more. I thought what I had done in physics was a pretty interesting beginning, and gave great evidence that the computational paradigm would provide an important new way to think about fundamental physics. As I look back now, I realize that a whole 100 pages of A New Kind of Science are devoted to physics, but at the time I think I considered them mostly just a supporting argument for the value of the new kind of science I was developing.
How We Got Here: The Backstory of the Wolfram Physics Project 1.6 “Please Don’t Do That Project” : A New Kind of Science launched on May 14, 2002, and quickly climbed onto bestseller lists. I don’t think there’s a perfect way to deliver big ideas to the world, but all the trouble I’d taken trying to “package” what I’d figured out, and trying make my book as clear and accessible as possible, seemed to be paying off. And it was great: lots of people seemed to get the core ideas of the book. Looking back, though, it’s remarkable how often media coverage of the book talked about physics, and the idea that the universe might be described by a simple program (complete with headlines like “Is the Universe a Computer?” and “The Cosmic Code”). But as someone who’d studied the history of science for a long time, I full well knew that if the new paradigm I was trying to introduce was as important as I believed, then inevitably it would run into detractors, and hostility. But what surprised me was that almost all the hostility came from just one field: physics. There were plenty of physicists who were very positive, but there were others for whom my book somehow seemed to have touched a nerve. As an almost lifelong lover of physics, I didn’t see a conflict. But maybe from the outside it was more obvious—as a cartoon in a review of my book in the New York Times (with a remarkably prescient headline) perhaps captured: If social media had existed at the time, it would undoubtedly have been different. But as it was, it was a whole unchecked parade: from Nobel prizewinners with pitchforks, to a then-graduate-student launching their career by “proving” that my physics was “wrong”. Why did they feel so strongly? I think they thought (and some of them told me as much) that if I was right, then what they’d done with their traditional mathematical methods, and all the wonderful things they’d built, would get thrown away. I never saw it that way (and, ironically, I made my living building a tool used to support those traditional mathematical methods). But at the time—without social media—I didn’t have a useful way to respond. (To be fair, it often wasn’t clear there was much to say beyond “I don’t share your convictions”, or “Read what the book actually says… and don’t forget the 300,000 words of notes at the back!”.) But there was unfortunately a casualty from all this: physics. As it now turns out (and I’m very happy about it), far from my ideas being in conflict with what’s been done in physics, they are actually beautifully aligned. Yes, the foundations are different. But all those traditional mathematical methods now get extra power and extra relevance. But it’s taken an additional 18 years for us to find that out. And it almost didn’t happen at all. It’s been interesting to watch the general progression of the ideas I discussed in A New Kind of Science. What’s been most dramatic (and I’m certainly not solely responsible) has been the quiet but rapid transition—after three centuries—of new models for things being based not on equations but instead on programs. It’s happened across almost every area. With one notable exception: fundamental physics. Perhaps it’s partly because the tower of mathematical sophistication in models is highest there. Perhaps it’s because of the particular stage of development of fundamental physics as a field, and the fact that, for the most part, it’s in a “work out the existing models” phase rather than in a “new models” phase. A few months after my book appeared, I did a big lecture tour of universities and the like. People would ask about all kinds of things. But pretty much everywhere, some people (quite often physicists) would ask about fundamental physics. But, somewhat to my disappointment, their questions tended to be more philosophical than technical. Somehow the notion of applying these ideas to fundamental physics was just a little too dangerous to discuss. But I decided that whatever other people might think, I should see what it would take to make progress. So in 2004 I set about expanding what I’d figured out so far. I made my explorations more streamlined than before, and pretty soon I was beginning to write summaries of what was out there: But there was something that bugged me. Somehow my model felt a bit fragile, a bit contrived. At least with the formalism I had, I couldn’t just “write down any rule”; it was a bit like writing down numbers, but they had to be prime. And there was another, more technical, problem as well. For my derivations of special and general relativity to work, I needed a model that was causal invariant, and my searches were having a hard time finding nontrivial examples. And right in the middle of trying to figure out what to do about this, something else happened: I started working on Wolfram|Alpha. In a sense Wolfram|Alpha was an outgrowth of A New Kind of Science. Before the book I had assumed that to build a serious computational knowledge engine (which is something I had, in one form or another, been interested in since I was a kid) one would first have to solve the general problem of AI. But one of the implications of my Principle of Computational Equivalence is that there is no bright line between “intelligence” and “mere computation”. And that meant that with all our computational capabilities we should already be able to build a computational knowledge engine. And so I decided to try it. Of course at the beginning we didn’t know if it would work. (Is there too much data in the world? Is it too hard to make it computable? Is it too hard to understand natural language? Etc.) But it did work. And in 2009 we launched Wolfram|Alpha. But I was still enthusiastic about my physics project. And in February 2010 I made it a major part of a talk I gave at TED, which the TED team initially titled “Computing a Theory of Everything” (confusingly, there also now seems to be a version of the same talk with the alternate title “Computing a Theory of All Knowledge”). And—as I was recently reminded—I told the audience that I was committed to seeing the project done, “to see if, within this decade, we can finally hold in our hands the rule for our universe”. OK, well, it’s now April 2020. So we didn’t make it “within the decade”. Though, almost exactly 10 years later, we’re now launching the Wolfram Physics Project and I think we’re finally on a path to it. So why didn’t this happen sooner? Frankly, in retrospect, it should have. And if I’d known what I know now, I absolutely would have done it. Yes, our Wolfram Language technology has gotten better in the course of the decade, and that’s made the project considerably easier. But looking back at what I had done even in 2004, I can now see that I was absolutely on the right track, and I could have done then almost everything I’m doing now. Most of the projects I’ve ever done in my life—from my “Concise Directory of Physics” onward—I’ve done first and foremost because I was interested in them, and because I thought I would find them intellectually fulfilling. But particularly as I’ve gotten older, there’s been another increasingly important factor: I find I get pleasure out of doing projects that I think other people will find useful—and will get their own fulfillment out of. And with the tools I’ve built—like Mathematica and Wolfram|Alpha and the Wolfram Language—as well as with A New Kind of Science and my other books and writings, that’s worked well, and it’s been a source of great satisfaction to me. But with the physics project, there was a problem. Because after I effectively “tested the market” in 2002, it seemed as if my core “target customers” (i.e. physicists interested in fundamental physics) didn’t want the project. And in fact a few of them came right out and said it: “Please don’t do that project”. I personally thought the project would be really interesting. But it wasn’t the only project I thought would be interesting. And basically I said “Nah, let me not put lots of effort into a project people basically don’t want”. What did I do instead? The most important theme of the past decade for me has been the emergence of the Wolfram Language as a full-scale computational language, and my increasing realization of the significance of having such a language. I view it as being a key step in the development of the computational paradigm—and the crucial link between what computation makes possible, and the way we humans think about things. It provides a way for us to express ourselves—and organize our thoughts—in computational terms. I view it in some ways as analogous to the creation of mathematical notation four centuries or so ago. And just as that launched the modern development of mathematical science and mathematical thinking, so now I believe that having a full-scale computational language will open up the development of all the “computational X” fields, and the full potential of computational thinking. And this is not something just limited to science. Through ideas like computational contracts I think it’s going to inform a lot of how our world operates in the years to come, and how we want to shape (through ethics, etc.) what AIs do, and how we define the future of the human condition. It’s not yet nearly as obvious as it will become. But I think computational language is eventually going to be seen as a pivotal intellectual idea of our times. It also has the rare and interesting feature of being something that is both fundamental and creative. It’s about “drilling down” to find the essence both of our thinking and of what computation makes possible. But it’s also about the creative design of a language. And for me personally it’s in many ways the ideal project. It involves developing deep understanding across as many areas as possible. It involves the continual exercise of creativity. And it’s also a big project, that benefits from organizational skills and resources. And I’m very happy indeed to have spent the past decade on it. Sometimes I’ve thought about how it compares as a project to fundamental physics. At a practical level, building a computational language is like building a progressively taller tower—from which one can progressively see further, and occasionally reach major new kinds of applications and implications. Fundamental physics is much more of a one-shot project: you try an approach to fundamental physics and either it works, or it doesn’t; there’s not the same kind of feeling of progressively building something. Computational language also began to feel to me like an ultimately more fundamental project—at least for us humans—than fundamental physics. Because it’s about the generality of computation and the generality of our ways of thinking, not the specifics of the physical universe in which we “happen to exist”. And as I thought about the distant future (complete with my “box of a trillion souls” image), the physical universe seemed less and less relevant to the essence of the human condition. As a kind of “disembodied digital soul”, it doesn’t matter what the underlying “machine code” of the universe is; you’re operating just at the level of abstract computation. So maybe the fundamental theory of physics is ultimately just an “implementation note”. (As I now realize from our recent discoveries, the actual situation is more nuanced, and much more philosophically fascinating.) But even though my main focus has been computational language and its implications, I’ve been doing quite a few other things. Occasionally I’ve even written about physics. And I’ve kept thinking about the fundamental physics project. Is there a “positive” way, I wondered, to do the project, so as many people as possible will be pleased to see it done? I wondered about offering a prize for finishing what I had started. I had a great experience with something like that in 2007, when Alex Smith won the prize I had set up for proving my conjecture that a particular Turing machine was universal, thereby establishing what the very simplest possible universal Turing machine is. And in fact last fall I put up some new prizes for longstanding questions about rule 30. But for fundamental physics, I didn’t think a prize could work. For the Turing machine problem or the rule 30 problems it’s realistic for someone to just “swoop in” and figure it out. For fundamental physics, there’s a big a tower of ideas to learn just to get started. From time to time I would talk to physicist friends of mine about the fundamental physics project. (I usually didn’t even try with physicists I didn’t know; they would just give me quizzical looks, and I could tell they were uncomfortably wondering if I had lost my marbles.) But even with my friends, when I started to describe the details of the project, I don’t think over the course of 18 years I managed to keep anyone’s attention for more than 15 minutes. And quite soon I would just ask “So, what’s new in physics as far as you are concerned?”, and off we would go talking about string theory or particle phenomenology or conformal field theory or whatever. (And sometimes they would say, surprised that I cared, “Wow, you still really know about this stuff!”) Finally, though, a few years ago I had an idea about the fundamental physics project: why not just do the project as an educational project? Say, more or less, “We’re going to try to climb the Mount Everest of science. We don’t know if we’ll succeed, but you might enjoy seeing what we do in trying to make the climb.” After all, when I talked to non-physicists—or kids–about the project, they were often very excited and very curious. And with all the effort put into STEM education, and into encouraging people to learn about science, I thought this would be a good opportunity. But whenever I really thought about doing the project (and I was still assuming that we’d just be “starting the climb”; I had no idea we’d be able to get as far as we have now), I came back to the “problem of the physicists” (or “phyzzies” as I nicknamed them). And I didn’t have a solution. And so it was that year after year, my project of trying to find the fundamental theory of physics languished.
How We Got Here: The Backstory of the Wolfram Physics Project 1.7 Two Young Physicists and a Little Idea : Every year for the past 17 years—starting the year after A New Kind of Science was published—we’ve held an annual summer school. It always ends up with an outstanding group of students (mostly college, grad and postdoc). And for me (and also some of our R&D staff) it’s become a once-a-year three-week opportunity to explore all sorts of new ideas. In the early years, the Summer School concentrated specifically on what was in my book (it was originally designed to solve the problem of people asking us for guidance on how to do the kind of science in the book). In more recent years, it’s basically become about all aspects of the methodology that I and our company have developed. But from the beginning until now, there’ve always been a few students each year who say they want to work on “Chapter 9”. Many interesting projects have come out of that, though few really used the full network models I’d developed, basically because those were too technically difficult to use in projects that could get done in three weeks. In 2014, though, a young student just graduating with a degree in physics from Moscow State University (and with various competitive coding achievements to his name) came to the Summer School, determined to work on network-based models of fundamental physics. As the beginning of his project description put it: “The ultimate goal is to figure out the fundamental theory of physics.” His actual project was a nice study of the longtime behavior of networks with planarity-preserving rules. The next year, having now completed a master’s degree in physics in Moscow, the same student—whose name is Max Piskunov—came to the Summer School a second time (something we rarely allow), to continue his work on network-based models of fundamental physics. After the Summer School, he was very keen to continue working on these models, and asked me if I could be a PhD advisor for him. I said that unfortunately I wasn’t in that business anymore, and that even more unfortunately I didn’t know any currently active physicists who’d be suitable. As it turned out, he succeeded in finding a university where there were physicists who were now working on “network science”—though eventually they apparently told him “It’s too risky for you to work on network models for physics; there isn’t a well-defined criterion for success”. From time to time I would ask after Max, and was a little disappointed to hear that he was off doing a PhD on “traditional” cosmology-meets-particle-physics. But then, in 2018 Max showed up again as a visitor at our Summer School—still really wanting to work on network-based models of fundamental physics. I said I’d really like to work on them too, but just didn’t see a way to do it. He said he at least wanted to try his hand at writing more streamlined code for them. Over the next couple of months I would occasionally talk to Max on the phone, and every time I felt more and more like I really should actually try to do something on the project; I’d been putting it off far too long. But then I had a little idea. I’d always been saying that I wanted models that are as minimal and structureless as possible. And then I’d say that networks were the best way I knew to get these, but that there were probably others. But even though I thought about lots of abstract structures through my work on the Wolfram Language, I never really came up with anything I was happy with. Until September 9, 2018. I was asking myself: what lies at the heart of abstract representations, in computation, in mathematics, and so on? Well, I realized, I should know! Because in a sense that’s what I’ve been trying to model all these years in the Wolfram Language, and in SMP before it. And, actually, for more than 40 years, everything I’ve done has basically been built on the same ultimate underlying answer: transformation rules for symbolic expressions. It’s what the Wolfram Language is based on (and it’s what SMP was based on too).  So why hadn’t I ever thought of using it for models of fundamental physics? The main reason was that somehow I never fully internalized that there’d be something useful left if one “took all the content out of it”. Most of the time we’re defining transformation rules for symbolic expressions that are somehow useful and meaningful to us—and that for example contain functions that we think of as “representing something”. It’s a little shocking that after all these years I could basically make the same mistake again: of implicitly assuming that the setup for a system would be “too simple for it to do anything interesting”. I think I was very lucky all those years ago with cellular automata, that it was so easy to try an experiment that I did it, just “on a whim”. But in September 2018 I think I was feeling more motivated by the abstract aesthetics than anything else. I realized there might be an elegant way to represent things—even things that were at least vaguely similar to the network-based models I had studied back in the 1990s. My personal analytics record that it took about 8 minutes to write down the basics: There it was: a model defined by basically a single line of Wolfram Language code. It was very elegant, and it also nicely generalized the network models I had long thought about. And even though my description was written (for myself) in language-designer-ese, I also had the sense that this model had a certain almost-mathematical purity to it. But would it do anything interesting? Pretty soon I was doing what I basically always seem to end up doing: going out into the computational universe of possibilities and exploring. And immediately I was finding things like: When one looks at the array of squares produced, say, by cellular automata, our human visual system is pretty good at giving us an impression of how much complexity is involved. But that works much less well for things like graphs and networks, where in particular there is inevitably much more arbitrariness in their rendering. I wanted to do more systematic studies, but I expected it was going to be somewhat complicated, and I was in the middle of working on the final stages of design for Version 12 of the Wolfram Language. Meanwhile, Max took it upon himself to create some optimized low-level code. But in the fall of 2018 he was taking a break from graduate school, working at Lyft in Silicon Valley on machine vision for autonomous driving. Still, by January 2019 he had code running, and within a few minutes of trying it out, I was finding things like: This was going to be interesting. But I was still in the middle of other things, and Max was going to come to the Summer School again—so I put it aside again for a few months. Then on May 24 Murray Gell-Mann, the inventor of quarks, and a physicist I had known at Caltech, died. And as has become something of a tradition for me, I spent some days writing an obituary piece about him. And in doing that, I began thinking about all those things I had liked so much so long ago in particle physics. But what had happened to them in the past 40 years? I started looking around on the web. Some things had definitely advanced. The mass of the lambda, that I had always known as 1115 MeV, was now measured as 1115.683 MeV. Calculations that I’d done to a first order of approximation had now been done to three orders. But in general I was shocked, and saddened.  Things that had generated so much excitement and had been the pride of particle physics were now barely making it as stubs on Wikipedia. What had happened to this beautiful field? It felt like I was seeing what had once been a bustling and glorious city, now lying almost abandoned, and in some ways in ruins. Of course, this is often the rhythm of science: some methodological advance sparks a golden age, and once everything easily accessible with that methodology has been done, one is faced with a long, hard slog that can last a century before there is some new methodological advance. But going to the Summer School in June, I was again thinking about how to do my fundamental physics project. Max was there. And so—as an instructor—was Jonathan Gorard. Jonathan had first come to the Summer School in 2017, just before his last year as an undergraduate in mathematics (+ theoretical physics, computer science and philosophy) at King’s College London. He’d been publishing papers on various topics since he was 17, most recently on a new algorithm for graph isomorphism. He said that at the Summer School he wanted to work either on cosmology in the context of “Chapter 9”, or on something related to the foundations of mathematics. I suggested that he try his hand at what I considered something of an old chestnut: finding a good symbolic way to represent and analyze automated proofs, like the one I had done back in 2000 of the simplest axiom system for logic. And though I had no idea at the time, this turned out to be a remarkably fortuitous choice. But as it was, Jonathan threw himself into the project, and produced the seeds of what would become through his later work the Wolfram Language function FindEquationalProof. Jonathan had come back to the Summer School in 2018 as an instructor, supervising projects on things like infinite lists and algebraic cryptography. And now he was back again as an instructor in 2019, having now also become a graduate student at Cambridge, with a nice fellowship, and nominally in a group doing general relativity. It had been planned that Jonathan, Max and I would “talk about physics” at the Summer School. I was hopeful, but after so many years a bit pessimistic. I thought my little idea defined a new, immediate path about what one might do. But I still wasn’t convinced there was a “good way to do the project”. But then we started discussing things. And I started feeling a stronger and stronger sense of responsibility. These ideas needed to be explored. Max and Jonathan were enthusiastic about them. What excuse did I have not to pursue the ideas, and see where they could go. Wouldn’t it be terrible if we failed to find the fundamental theory of physics just because I somehow got put off working on it? Of course, there were technical, physics issues too. One of the big ones—which had got me stuck back in 2004—was that I’d had difficulty finding examples of rules that both had nontrivial behavior, and showed the property of causal invariance needed to basically “generate a single thread of time”. Why did I care so much about causal invariance? First, because it gave me derivations of both special and general relativity. But philosophically even more important to me, because it avoided something I considered highly undesirable: a view of quantum mechanics in which there is a giant tree of possible histories, with no way to choose between them. Jonathan had said a few times early in the Summer School that he didn’t see why I was so concerned about causal invariance. I kept on pushing back. Then one day we went on a long walk, and Jonathan explained an idea he had (which, knowing him, he may have just come up with right there). What if the underlying rules didn’t need to have causal invariance, because us observers would implicitly add it just by the way we analyze things? What was this idea really? It was an application of things Jonathan knew from working on automated theorem proving, mixing in ideas from general relativity, and applying them to the foundations of quantum mechanics. (Basically, his concept was that we observers, because we’re branching just like the system we’re observing, effectively define “lemmas” to help us make sense of what we observe, and these lead to effective rules that have causal invariance.) At first I was skeptical. But the issue with not finding enough causal invariance had been a blocker 16 years earlier. And it felt like a big weight lifted if that issue could be removed. So by the end of the walk I was convinced that, yes, it was worth looking at rules even if they were not explicitly causal invariant, because they could still be “saved” by the “Jonathan Interpretation of Quantum Mechanics” as I called it (Jonathan prefers the more formal term “completion interpretation”, referring to the process of creating lemmas, which is called “completion” in automated theorem proving). As it turns out, the jury is still out on whether causal invariance is intrinsic or “in the eye of the observer”. But Jonathan’s idea was crucial as far as I was concerned in clearing the way to exploring these models without first doing a giant search for causal invariance. It took another month or so, but finally on August 10 I sent back to Jonathan and Max a picture we had taken, saying “The origin picture … and …. *I’m finally ready to get to work*!”.
How We Got Here: The Backstory of the Wolfram Physics Project 1.8 Oh My Gosh, It’s Actually Going to Work! : August 29, 2019, was a big birthday for me. Shockingly quickly I had gone from being “the youngest person in the room” to the oldest. But now I was turning 60. I did a “looking to the future” livestream that day, and a few days later I gave a speech at my birthday party. And both times I said that now, finally, I was going to make a serious effort on my project to find the fundamental theory of physics. And to myself I was saying “This is something I’ve been talking about doing for more than half my life; if I don’t do it now, it’s time to give up on the idea that I ever will.” “Maybe it’ll work, maybe it won’t”, I was thinking to myself. “But this is sort of the last chance for me to find out, so let’s give it a try.” And so we started. My original plan was in a sense fairly modest. I wanted to take the things I’d already investigated, and “spruce them up” in the context of my new models—then get everything out there for other people to help in what I expected would be a long, hard grind towards a fundamental theory of physics. The first step was to build tools. Nice, streamlined Wolfram Language tools. Max had already written some core simulation functions. But now it was a question of figuring out about visualizations, enumerations and various forms of analysis. How do you best display a hypergraph? What’s the right way to enumerate rules? And so on. But by the middle of October we had the basics, and by the end of October I’d pretty much cleared my calendar of everything but the “bare CEO essentials”, and was ready to just “do physics” for a while. It felt a little like being back where I was in the 1970s. Except for one huge difference: now I had the whole technology tower I’d spent most of the intervening 40 years building. No scratch paper. No handwritten calculations. Just notebooks and Wolfram Language. A medium for thinking directly in computational terms. And it was exhilarating. Everything went so fast. I was basically forming my thoughts directly in the language, typing as I went, then immediately having the computer show me the results. It felt as if the computer was providing about as direct an amplification of my cognitive abilities as I could imagine. And I even started to feel a bit better about the multi-decade delay in the project. Because I realized that even if my only goal from the beginning had been to just do this project, my best chance would pretty much have been to build the Wolfram Language first. There was something that made me nervous, though. Back in 1991 when I started working on A New Kind of Science, I’d also had the experience of rapid discovery. But what had happened then was that I hadn’t been able to stop—and I’d just dug in and gone on and on discovering things, for a decade. Intellectually it had been very rewarding, but personally it was extremely grueling. And I didn’t want to go through anything like that again. So I resolved that instead of going on until we’d “answered all the obvious questions”, we’d just figure out the minimum needed to coherently explain the ideas, then turn it over to the world to share the fun of taking it further. Pretty soon we started outlining the website. There’d be lots of technical information and exposition. There’d be a Registry of Notable Universes for candidate models we’d identified. To lighten the load of what I thought might be a project with glacially slow progress to report, there’d be “universe swag”. And on the front of the website I was planning to write, a little apologetically: “We’re going to try to find a fundamental theory of physics. It may be the wrong approach, or the wrong century, but we’re going to try anyway”. But meanwhile I was spending almost every waking hour doing that “trying”. I was looking at thousands of rules, slowly building up intuition. And we were talking about how what I was seeing might relate to things in physics, like space and time and quantum mechanics and general relativity. And it got more and more interesting. Things I’d thought vaguely about in the past we were now starting to see very explicitly in the rules I was running. We knew enough to know what to look for. But thinking abstractly about something is very different from seeing an actual example. And there were many surprises. So many “I never thought it might do that”s. But having seen examples one could then start to build up an abstract framework. Without the examples one wouldn’t ever have had the imagination to come up with it. But once one saw it, it often seemed maddeningly “obvious”. Our first big target was to understand the nature of space. How could the mathematical structures that have been used to characterize space emerge from our simple rules? I thought I already knew the basic answer from what I did back in the 1990s. But now I had a more streamlined model, and more streamlined tools, and I wanted to tighten my understanding. I generated thousands of screenfuls of visualizations: I think if I had lived a century earlier I would have been a zoologist. And what I was doing here was a kind of zoology: trying to catalog the strange forms and habits of these rules, and identify their families and phyla. It was a glimpse into an unseen part of the computational universe; a view of something there was no particular reason that us humans would have a way to understand. But I was pretty sure that at least some of these rules would connect with things we already knew. And so I started to hunt for examples. Most of what I do on a daily basis I can do on just one computer. But now I needed to search millions of cases. Conveniently, there’s pretty seamless support for parallel computation in the Wolfram Language. So soon I’d commandeered about 100 cores, and every computation I could immediately parallelize. (I was also set up to use external cloud services, but most of the time I was doing computations that with the 100X speedup were either taking only seconds, and were part of my “interactive thinking loop”, or were easy enough to run overnight on my own machines, with the minor thrill of seeing in the morning what they’d produced.) Back when I was studying things like cellular automata in the 1980s and 1990s I used to print out endless arrays of little thumbnails, then look through them, and type in the identifiers for ones I thought were worth another look. Now that was all a lot more streamlined, with images in notebooks, selectable with a simple click. But how could I automate actually looking through all these rules? One of the things I’ve learned from decades of studying the computational universe is to take seriously my mantra “The computational animals are always smarter than you are”. You think you’ve come up with a foolproof test for catching rules that have such-and-such a behavior. Well, some rule will turn out to have a way around it, doing something you never thought about. And what I’ve found is that in the end the best way to have a chance of catching the unexpected is to use the “broadest spectrum” tools one has, which typically means one’s own eyes. Pretty soon one begins to have a mental classification of the kinds of forms one’s seeing. And if one verbalizes it, one ends up describing them in terms of objects we’re used to (“ball of wool”, “sea urchin”, etc.) And in modern times that suggests a way to get some help: use machine learning that’s been trained, like we have, to distinguish these different kinds of things. And so instead of just making simple arrays of pictures, I often made feature space plots, where forms that “seem similar” were grouped together. And that meant that in just a glance I could typically see what unexpected outliers there might be. I looked through a particular collection of 79 million rules this way (with just a little additional filtering). First I found this—something that might seem more in line with my childhood interest in space, as in spacecraft, than with space in fundamental physics. And pretty soon things I also found things like these: These are not things I could ever guess would be there. But having found them, they can be connected to existing mathematical ideas (in this case, about manifolds). But seeing these examples embedded in so many others that don’t immediately connect to anything we know immediately makes one wonder whether perhaps our existing mathematical ideas can be generalized—and whether maybe this could be the key to understanding how space can emerge from our underlying rules. Both in its early history, and in modern times, mathematics has been inspired by the natural world. Now we’re seeing it inspired by the computational world. How does one generalize curvature to fractional-dimensional space? What does it mean to have a space with smoothly varying dimension? And so on. They’re elegant and interesting mathematical questions raised by looking at the computational world. It could have been that everything in the computational world of our models would immediately run into computational irreducibility, and that mathematical ideas would be essentially powerless—as they were when I was studying cellular automata in the 1980s. But by November of last year, it was beginning to become clear that things were different now, and that there was a chance of a bridge between the mathematical traditions of existing theoretical physics and the kinds of things we needed to know about our models. Once there’s sophisticated mathematics, we can begin to rely on that. But to explore, we still have to use things like our eyes. And that makes visualization critical. But in our models, what’s ultimately there are graphs, or hypergraphs. Nowadays we’ve got good automated tools in the Wolfram Language for coming up with “good” ways to lay out graphs. But it’s always arbitrary. And it would be much better if we could just “intrinsically” understand the graph. But unfortunately I don’t think we humans are really built for that. Or at least I’m not. (Though years ago, before computers could do automated graph layout, I once looked for a human “graph detangler” and found a young student who was spectacularly better than everyone else. Interestingly, she later became a distinguished knitwear designer.) But to try to help in “understanding” graphs I did have one plan—that actually I’d already hatched when I was first thinking about these things in the early 1990s: use VR to really “get inside” and experience graphs. So now—with VR back in vogue—I decided to give it a try. We’re still working on a fully interactive VR environment for manipulating graphs, but to start off I tried just using VR to explore static graphs. And, yes, it was somewhat useful. But there was a practical problem for me: rapid descent into motion sickness. An occupational hazard, I suppose. But not one I expected in studying fundamental physics. Given a better understanding of space in our models, we started looking more carefully at things like my old derivation of the Einstein equations for gravity. Jonathan tightened up the formalism and the mathematics. And it began to become clear that it wasn’t just a question of connecting our models to existing mathematical physics: our models were actually clarifying the existing mathematical physics. What had been pure, abstract mathematics relying on potentially arbitrary collections of “axiomatic” assumptions one could now see could arise from much more explicit structures. Oh, and one could check assumptions by just explicitly running things. Doing something like deriving the Einstein equations from our models isn’t at some level particularly easy. And inevitably it involves a chain of mathematical derivations. Pure mathematicians are often a little horrified by the way physicists tend to “hack through” subtle mathematical issues (“Do these limits really commute?” “Can one uniquely define that parameter?” Etc.). And this was in many ways an extreme example. But of course we weren’t adrift with no idea whether things were correct—because at least in many cases we could just go and run a model and measure things, and explicitly check what was going on. But I did feel a little bad. Here we were coming up with beautiful mathematical ideas and questions. But all I could do was barbarically hack through them—and I just kept thinking “These things deserve a mathematician who’ll really appreciate them”. Which hopefully in time they’ll get. As we went through November, we were starting to figure out more and more. And it seemed like every conversation we had, we were coming up with interesting things. I didn’t know where it would all go. But as a committed preserver of data I thought it was time to start recording our conversations, as well as my own experiments and other work on the project. And altogether we’ve so far accumulated 431 hours of recordings. We’re going to make these recordings available online. And with Wolfram Language speech-to-text it’s easy to process their audio, and to get word clouds that indicate some of the flow of the project:.
How We Got Here: The Backstory of the Wolfram Physics Project 1.9 What Terrible Timing! : So there we were in the middle of February. Things had gone better than I’d ever imagined they could. And we were working intensely to get everything ready to present to the world. We set a date: March 16, 2020. We were planning announcements, technical documents, an 800ish-page book, an extensive website, livestreams, outreach. All the kinds of things needed to launch this as a project, and explain it to people. But meanwhile—like so many other people—we were watching the developing coronavirus epidemic. I’d asked the data science and biology teams at our company to start curating data and making it available. I’d been looking at some epidemic modeling—some of it even done with cellular automata. I’d noted that the spreading of an epidemic in a human network was bizarrely similar to the growth of geodesic balls in hypergraphs. What should we do? We kept going, steadily checking off items on our project-management tracking list. But as March 16 approached, it was clear there was now a pandemic. The US began to shut down. I did an AMA on my experience as a 29-year work-from-home CEO. Meetings about physics were now interspersed with meetings about shutting down offices. Numerous people at our company pointed out to me that Isaac Newton had come up with the core ideas for both calculus and his theory of gravity in 1665, when Cambridge University had been closed because of the plague. I oscillated between thinking that in the midst of such a worldwide crisis it was almost disrespectful to be talking about something like a fundamental theory of physics, and thinking that perhaps people might like an intellectual distraction. But in the end we decided to wait. We’d get everything ready, but then pause. And after all, I thought, after waiting more than thirty years to do this project, what’s a few more weeks.
How We Got Here: The Backstory of the Wolfram Physics Project 1.10 What Happens Now : If you’re reading this, it means our project is finally released. And we begin the next stage in the long journey I’ve described here. I can’t help echoing Isaac Newton’s words from the 1686 preface to his Principia: “I heartily beg that what I have done here may be read with forbearance; and that my labors in a subject so difficult may be examined, not so much with a view to censure, as to remedy their defects.” But the world has changed since then, and now we can send out tweets and do livestreams. I’m thrilled about what we’ve been able to figure out, not least because I consider it so elegant and so intellectually satisfying. Sometimes back when I was doing particle physics, I’d think “That’s a bit hacky, but if that’s how our universe works, so be it”. Now I feel a certain pride that we seem to live in a universe that works in such an elegant way. Forty years ago I thought I’d spend my life as a physicist. Things didn’t work out that way, and I’m very happy with what happened instead. But now after decades “in the wilderness” I’m back. Not just “doing physics”, but trying to attack the very center of it. I’m quite certain that if I’d spent the past 40 years as a physicist nothing like this would have been possible. It’s one of those cases where it’s almost inevitable that making progress will need that strange combination of having inside knowledge yet “being an outsider”. Of course, we’re not finished. I think we finally have a path to a fundamental theory of physics. But we’re not there yet. And what I’m hoping now is that we can mount a project that will succeed in getting us there. It’s going to take physicists, mathematicians, computer scientists and others.  It’s going to take ideas and work, and perhaps quite a bit of time. I hope it will be a worldwide effort that can happen across a spectrum of academic and other environments. Most of it will be a decentralized effort. I personally look forward to continuing to be deeply involved—and I’m hoping that we’ll be able to set up a substantial centralized effort to apply the decades of experience we’ve had in doing highly challenging R&D projects to make progress as rapidly as possible on this project. It’s been a great privilege for me to be “in the right place at the right time” to discover what we’ve discovered. Physics did so much for me in my early years, and I’m thrilled to have the opportunity to “give something back” so many years later. I can’t wait to see what will develop as we home in on a fundamental theory of physics. But at this stage in my life perhaps my greatest pleasure is to see others get excitement and fulfillment from things I put into the world.  And to provide something for the next generation of 12-year-old physics wannabes. So let’s all go and try to find the fundamental theory of physics together! It’s going to be great.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.1 I Never Expected This : It’s unexpected, surprising—and for me incredibly exciting. To be fair, at some level I’ve been working towards this for nearly 50 years. But it’s just in the last few months that it’s finally come together. And it’s much more wonderful, and beautiful, than I’d ever imagined. In many ways it’s the ultimate question in natural science: How does our universe work? Is there a fundamental theory? An incredible amount has been figured out about physics over the past few hundred years. But even with everything that’s been done—and it’s very impressive—we still, after all this time, don’t have a truly fundamental theory of physics. Back when I used do theoretical physics for a living, I must admit I didn’t think much about trying to find a fundamental theory; I was more concerned about what we could figure out based on the theories we had. And somehow I think I imagined that if there was a fundamental theory, it would inevitably be very complicated. But in the early 1980s, when I started studying the computational universe of simple programs I made what was for me a very surprising and important discovery: that even when the underlying rules for a system are extremely simple, the behavior of the system as a whole can be essentially arbitrarily rich and complex. And this got me thinking: Could the universe work this way? Could it in fact be that underneath all of this richness and complexity we see in physics there are just simple rules? I soon realized that if that was going to be the case, we’d in effect have to go underneath space and time and basically everything we know. Our rules would have to operate at some lower level, and all of physics would just have to emerge. By the early 1990s I had a definite idea about how the rules might work, and by the end of the 1990s I had figured out quite a bit about their implications for space, time, gravity and other things in physics—and, basically as an example of what one might be able to do with science based on studying the computational universe, I devoted nearly 100 pages to this in my book A New Kind of Science. I always wanted to mount a big project to take my ideas further. I tried to start around 2004. But pretty soon I got swept up in building Wolfram|Alpha, and the Wolfram Language and everything around it. From time to time I would see physicist friends of mine, and I’d talk about my physics project. There’d be polite interest, but basically the feeling was that finding a fundamental theory of physics was just too hard, and only kooks would attempt it. It didn’t help that there was something that bothered me about my ideas. The particular way I’d set up my rules seemed a little too inflexible, too contrived. In my life as a computational language designer I was constantly thinking about abstract systems of rules. And every so often I’d wonder if they might be relevant for physics. But I never got anywhere. Until, suddenly, in the fall of 2018, I had a little idea. It was in some ways simple and obvious, if very abstract. But what was most important about it to me was that it was so elegant and minimal. Finally I had something that felt right to me as a serious possibility for how physics might work. But wonderful things were happening with the Wolfram Language, and I was busy thinking about all the implications of finally having a full-scale computational language. But then, at our annual Summer School in 2019, there were two young physicists (Jonathan Gorard and Max Piskunov) who were like, “You just have to pursue this!” Physics had been my great passion when I was young, and in August 2019 I had a big birthday and realized that, yes, after all these years I really should see if I can make something work. So—along with the two young physicists who’d encouraged me—I began in earnest in October 2019. It helped that—after a lifetime of developing them—we now had great computational tools. And it wasn’t long before we started finding what I might call “very interesting things”. We reproduced, more elegantly, what I had done in the 1990s. And from tiny, structureless rules out were coming space, time, relativity, gravity and hints of quantum mechanics. We were doing zillions of computer experiments, building intuition. And gradually things were becoming clearer. We started understanding how quantum mechanics works. Then we realized what energy is. We found an outline derivation of my late friend and mentor Richard Feynman’s path integral. We started seeing some deep structural connections between relativity and quantum mechanics. Everything just started falling into place. All those things I’d known about in physics for nearly 50 years—and finally we had a way to see not just what was true, but why. I hadn’t ever imagined anything like this would happen. I expected that we’d start exploring simple rules and gradually, if we were lucky, we’d get hints here or there about connections to physics. I thought maybe we’d be able to have a possible model for the first seconds of the universe, but we’d spend years trying to see whether it might actually connect to the physics we see today. In the end, if we’re going to have a complete fundamental theory of physics, we’re going to have to find the specific rule for our universe. And I don’t know how hard that’s going to be. I don’t know if it’s going to take a month, a year, a decade or a century. A few months ago I would also have said that I don’t even know if we’ve got the right framework for finding it. But I wouldn’t say that anymore. Too much has worked. Too many things have fallen into place. We don’t know if the precise details of how our rules are set up are correct, or how simple or not the final rules may be. But at this point I am certain that the basic framework we have is telling us fundamentally how physics works. It’s always a test for scientific models to compare how much you put in with how much you get out. And I’ve never seen anything that comes close. What we put in is about as tiny as it could be. But what we’re getting out are huge chunks of the most sophisticated things that are known about physics. And what’s most amazing to me is that at least so far we’ve not run across a single thing where we’ve had to say “oh, to explain that we have to add something to our model”. Sometimes it’s not easy to see how things work, but so far it’s always just been a question of understanding what the model already says, not adding something new. At the lowest level, the rules we’ve got are about as minimal as anything could be. (Amusingly, their basic structure can be expressed in a fraction of a line of symbolic Wolfram Language code.) And in their raw form, they don’t really engage with all the rich ideas and structure that exist, for example, in mathematics. But as soon as we start looking at the consequences of the rules when they’re applied zillions of times, it becomes clear that they’re very elegantly connected to a lot of wonderful recent mathematics. There’s something similar with physics, too. The basic structure of our models seems alien and bizarrely different from almost everything that’s been done in physics for at least the past century or so. But as we’ve gotten further in investigating our models something amazing has happened: we’ve found that not just one, but many of the popular theoretical frameworks that have been pursued in physics in the past few decades are actually directly relevant to our models. I was worried this was going to be one of those “you’ve got to throw out the old” advances in science. It’s not. Yes, the underlying structure of our models is different. Yes, the initial approach and methods are different.  And, yes, a bunch of new ideas are needed. But to make everything work we’re going to have to build on a lot of what my physicist friends have been working so hard on for the past few decades. And then there’ll be the physics experiments. If you’d asked me even a couple of months ago when we’d get anything experimentally testable from our models I would have said it was far away. And that it probably wouldn’t happen until we’d pretty much found the final rule. But it looks like I was wrong. And in fact we’ve already got some good hints of bizarre new things that might be out there to look for. OK, so what do we need to do now? I’m thrilled to say that I think we’ve found a path to the fundamental theory of physics. We’ve built a paradigm and a framework (and, yes, we’ve built lots of good, practical, computational tools too). But now we need to finish the job. We need to work through a lot of complicated computation, mathematics and physics. And see if we can finally deliver the answer to how our universe fundamentally works. It’s an exciting moment, and I want to share it. I’m looking forward to being deeply involved. But this isn’t just a project for me or our small team. This is a project for the world. It’s going to be a great achievement when it’s done. And I’d like to see it shared as widely as possible. Yes, a lot of what has to be done requires top-of-the-line physics and math knowledge. But I want to expose everything as broadly as possible, so everyone can be involved in—and I hope inspired by—what I think is going to be a great and historic intellectual adventure. Today we’re officially launching our Physics Project. From here on, we’ll be livestreaming what we’re doing—sharing whatever we discover in real time with the world. (We’ll also soon be releasing more than 400 hours of video that we’ve already accumulated.) I’m posting all my working materials going back to the 1990s, and we’re releasing all our software tools. We’ll be putting out bulletins about progress, and there’ll be educational programs around the project. Oh, yes, and we’re putting up a Registry of Notable Universes. It’s already populated with nearly a thousand rules. I don’t think any of the ones in there yet are our own universe—though I’m not completely sure. But sometime—I hope soon—there might just be a rule entered in the Registry that has all the right properties, and that we’ll slowly discover that, yes, this is it—our universe finally decoded.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.2 How It Works : OK, so how does it all work? I’ve written a 448-page technical exposition (yes, I’ve been busy the past few months!). Another member of our team (Jonathan Gorard) has written two 60-page technical papers. And there’s other material available at the project website. But here I’m going to give a fairly non-technical summary of some of the high points. It all begins with something very simple and very structureless. We can think of it as a collection of abstract relations between abstract elements. Or we can think of it as a hypergraph—or, in simple cases, a graph. We might have a collection of relations like: {{1, 2}, {2, 3}, {3, 4}, {2, 4}} that can be represented by a graph like: All we’re specifying here are the relations between elements (like {2,3}). The order in which we state the relations doesn’t matter (although the order within each relation does matter). And when we draw the graph, all that matters is what’s connected to what; the actual layout on the page is just a choice made for visual presentation. It also doesn’t matter what the elements are called. Here I’ve used numbers, but all that matters is that the elements are distinct. OK, so what do we do with these collections of relations, or graphs? We just apply a simple rule to them, over and over again. Here’s an example of a possible rule: {{x, y}, {x, z}} → {{x, z}, {x, w}, {y, w}, {z, w}} What this rule says is to pick up two relations—from anywhere in the collection—and see if the elements in them match the pattern {{x,y},{x,z}} (or, in the Wolfram Language, {{x_,y_},{x_,z_}}), where the two x’s can be anything, but both have to be the same, and the y and z can be anything. If there’s a match, then replace these two relations with the four relations on the right. The w that appears there is a new element that’s being created, and the only requirement is that it’s distinct from all other elements. We can represent the rule as a transformation of graphs: Now let’s apply the rule once to: {{1, 2}, {2, 3}, {3, 4}, {2, 4}} The {2,3} and {2,4} relations get matched, and the rule replaces them with four new relations, so the result is: {{1, 2}, {3, 4}, {2, 4}, {2, 5}, {3, 5}, {4, 5}} We can represent this result as a graph (which happens to be rendered flipped relative to the graph above): OK, so what happens if we just keep applying the rule over and over? Here’s the result: Let’s do it a few more times, and make a bigger picture: What happened here? We have such a simple rule. Yet applying this rule over and over again produces something that looks really complicated. It’s not what our ordinary intuition tells us should happen. But actually—as I first discovered in the early 1980s—this kind of intrinsic, spontaneous generation of complexity turns out to be completely ubiquitous among simple rules and simple programs. And for example my book A New Kind of Science is about this whole phenomenon and why it’s so important for science and beyond. But here what’s important about it is that it’s what’s going to make our universe, and everything in it. Let’s review again what we’ve seen. We started off with a simple rule that just tells us how to transform collections of relations. But what we get out is this complicated-looking object that, among other things, seems to have some definite shape. We didn’t put in anything about this shape. We just gave a simple rule. And using that simple rule a graph was made. And when we visualize that graph, it comes out looking like it has a definite shape. If we ignore all matter in the universe, our universe is basically a big chunk of space. But what is that space? We’ve had mathematical idealizations and abstractions of it for two thousand years. But what really is it?  Is it made of something, and if so, what? Well, I think it’s very much like the picture above.  A whole bunch of what are essentially abstract points, abstractly connected together. Except that in the picture there are 6704 of these points, whereas in our real universe there might be more like 10400 of them, or even many more.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.3 All Possible Rules : We don’t (yet) know an actual rule that represents our universe—and it’s almost certainly not the one we just talked about. So let’s discuss what possible rules there are, and what they typically do. One feature of the rule we used above is that it’s based on collections of “binary relations”, containing pairs of elements (like {2,3}). But the same setup lets us also consider relations with more elements. For example, here’s a collection of two ternary relations: {{1, 2, 3}, {3, 4, 5}} We can’t use an ordinary graph to represent things like this, but we can use a hypergraph—a construct where we generalize edges in graphs that connect pairs of nodes to “hyperedges” that connect any number of nodes: (Notice that we’re dealing with directed hypergraphs, where the order in which nodes appear in a hyperedge matters. In the picture, the “membranes” are just indicating which nodes are connected to the same hyperedge.) We can make rules for hypergraphs too: {{x, y, z}} → {{w, w, y}, {w, x, z}} And now here’s what happens if we run this rule starting from the simplest possible ternary hypergraph—the ternary self-loop {{0,0,0}}: Alright, so what happens if we just start picking simple rules at random? Here are some of the things they do: Somehow this looks very zoological (and, yes, these models are definitely relevant for things other than fundamental physics—though probably particularly molecular-scale construction). But basically what we see here is that there are various common forms of behavior, some simple, and some not. Here are some samples of the kinds of things we see: And the big question is: if we were to run rules like these long enough, would they end up making something that reproduces our physical universe? Or, put another way, out in this computational universe of simple rules, can we find our physical universe? A big question, though, is: How would we know? What we’re seeing here are the results of applying rules a few thousand times; in our actual universe they may have been applied 10500 times so far, or even more.  And it’s not easy to bridge that gap. And we have to work it from both sides. First, we have to use the best summary of the operation of our universe that what we’ve learned in physics over the past few centuries has given us. And second, we have to go as far as we can in figuring out what our rules actually do. And here there’s potentially a fundamental problem: the phenomenon of computational irreducibility. One of the great achievements of the mathematical sciences, starting about three centuries ago, has been delivering equations and formulas that basically tell you how a system will behave without you having to trace each step in what the system does.  But many years ago I realized that in the computational universe of possible rules, this very often isn’t possible. Instead, even if you know the exact rule that a system follows, you may still not be able to work out what the system will do except by essentially just tracing every step it takes. One might imagine that—once we know the rule for some system—then with all our computers and brainpower we’d always be able to “jump ahead” and work out what the system would do. But actually there’s something I call the Principle of Computational Equivalence, which says that almost any time the behavior of a system isn’t obviously simple, it’s computationally as sophisticated as anything.  So we won’t be able to “outcompute” it—and to work out what it does will take an irreducible amount of computational work. Well, for our models of the universe this is potentially a big problem. Because we won’t be able to get even close to running those models for as long as the universe does. And at the outset it’s not clear that we’ll be able to tell enough from what we can do to see if it matches up with physics. But the big recent surprise for me is that we seem to be lucking out.  We do know that whenever there’s computational irreducibility in a system, there are also an infinite number of pockets of computational reducibility. But it’s completely unclear whether in our case those pockets will line up with things we know from physics. And the surprise is that it seems a bunch of them do.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.4 What Is Space? : Let’s look at a particular, simple rule from our infinite collection: {{x, y, y}, {z, x, u}} → {{y, v, y}, {y, z, v}, {u, v, v}} Here’s what it does: And after a while this is what happens: It’s basically making us a very simple “piece of space”. If we keep on going longer and longer it’ll make a finer and finer mesh, to the point where what we have is almost indistinguishable from a piece of a continuous plane. Here’s a different rule: {{x, x, y}, {z, u, x}} → {{u, u, z}, {v, u, v}, {v, y, x}} It looks it’s “trying to make” something 3D. Here’s another rule: {{x, y, z}, {u, y, v}} → {{w, z, x}, {z, w, u}, {x, y, w}} Isn’t this strange? We have a rule that’s just specifying how to rewrite pieces of an abstract hypergraph, with no notion of geometry, or anything about 3D space. And yet it produces a hypergraph that’s naturally laid out as something that looks like a 3D surface. Even though the only thing that’s really here is connections between points, we can “guess” where a surface might be, then we can show the result in 3D: If we keep going, then like the example of the plane, the mesh will get finer and finer, until basically our rule has grown us—point by point, connection by connection—something that’s like a continuous 3D surface of the kind you might study in a calculus class. Of course, in some sense, it’s not “really” that surface: it’s just a hypergraph that represents a bunch of abstract relations—but somehow the pattern of those relations gives it a structure that’s a closer and closer approximation to the surface. And this is basically how I think space in the universe works. Underneath, it’s a bunch of discrete, abstract relations between abstract points. But at the scale we’re experiencing it, the pattern of relations it has makes it seem like continuous space of the kind we’re used to. It’s a bit like what happens with, say, water. Underneath, it’s a bunch of discrete molecules bouncing around. But to us it seems like a continuous fluid. Needless to say, people have thought that space might ultimately be discrete ever since antiquity. But in modern physics there was never a way to make it work—and anyway it was much more convenient for it to be continuous, so one could use calculus. But now it’s looking like the idea of space being discrete is actually crucial to getting a fundamental theory of physics.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.5 The Dimensionality of Space : A very fundamental fact about space as we experience it is that it is three-dimensional. So can our rules reproduce that? Two of the rules we just saw produce what we can easily recognize as two-dimensional surfaces—in one case flat, in the other case arranged in a certain shape. Of course, these are very bland examples of (two-dimensional) space: they are effectively just simple grids. And while this is what makes them easy to recognize, it also means that they’re not actually much like our universe, where there’s in a sense much more going on. So, OK, take a case like: If we were to go on long enough, would this make something like space, and, if so, with how many dimensions? To know the answer, we have to have some robust way to measure dimension. But remember, the pictures we’re drawing are just visualizations; the underlying structure is a bunch of discrete relations defining a hypergraph—with no information about coordinates, or geometry, or even topology. And, by the way, to emphasize that point, here is the same graph—with exactly the same connectivity structure—rendered four different ways: But getting back to the question of dimension, recall that the area of a circle is πr2; the volume of a sphere is . In general, the “volume” of the d-dimensional analog of a sphere is a constant multiplied by rd. But now think about our hypergraph. Start at some point in the hypergraph. Then follow r hyperedges in all possible ways. You’ve effectively made the analog of a “spherical ball” in the hypergraph. Here are examples for graphs corresponding to 2D and 3D lattices: And if you now count the number of points reached by going “graph distance r” (i.e. by following r connections in the graph) you’ll find in these two cases that they indeed grow like r2 and r3. So this gives us a way to measure the effective dimension of our hypergraphs. Just start at a particular point and see how many points you reach by going r steps: Now to work out effective dimension, we in principle just have to fit the results to rd. It’s a bit complicated, though, because we need to avoid small r (where every detail of the hypergraph is going to matter) and large r (where we’re hitting the edge of the hypergraph)—and we also need to think about how our “space” is refining as the underlying system evolves. But in the end we can generate a series of fits for the effective dimension—and in this case these say that the effective dimension is about 2.7: If we do the same thing for: it’s limiting to dimension 2, as it should: What does the fractional dimension mean? Well, consider fractals, which our rules can easily make: {{x, y, z}} → {{x, u, w}, {y, v, u}, {z, w, v}} If we measure the dimension here we get 1.58—the usual fractal dimension for a Sierpiński structure: Our rule above doesn’t create a structure that’s as regular as this. In fact, even though the rule itself is completely deterministic, the structure it makes looks quite random. But what our measurements suggest is that when we keep running the rule it produces something that’s like 2.7-dimensional space. Of course, 2.7 is not 3, and presumably this particular rule isn’t the one for our particular universe (though it’s not clear what effective dimension it’d have if we ran it 10100 steps). But the process of measuring dimension shows an example of how we can start making “physics-connectable” statements about the behavior of our rules. By the way, we’ve been talking about “making space” with our models. But actually, we’re not just trying to make space; we’re trying to make everything in the universe. In standard current physics, there’s space—described mathematically as a manifold—and serving as a kind of backdrop, and then there’s everything that’s in space, all the matter and particles and planets and so on. But in our models there’s in a sense nothing but space—and in a sense everything in the universe must be “made of space”. Or, put another way, it’s the exact same hypergraph that’s giving us the structure of space, and everything that exists in space. So what this means is that, for example, a particle like an electron or a photon must correspond to some local feature of the hypergraph, a bit like in this toy example: To give a sense of scale, though, I have an estimate that says that 10200 times more “activity” in the hypergraph that represents our universe is going into “maintaining the structure of space” than is going into maintaining all the matter we know exists in the universe.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.6 Curvature in Space & Einstein’s Equations : Here are a few structures that simple examples of our rules make: But while all of these look like surfaces, they’re all obviously different. And one way to characterize them is by their local curvature. Well, it turns out that in our models, curvature is a concept closely related to dimension—and this fact will actually be critical in understanding, for example, how gravity arises. But for now, let’s talk about how one would measure curvature on a hypergraph. Normally the area of a circle is πr2. But let’s imagine that we’ve drawn a circle on the surface of a sphere, and now we’re measuring the area on the sphere that’s inside the circle: This area is no longer πr2. Instead it’s π, where a is the radius of the sphere. In other words, as the radius of the circle gets bigger, the effect of being on the sphere is ever more important. (On the surface of the Earth, imagine a circle drawn around the North Pole; once it gets to the equator, it can never get any bigger.) If we generalize to d dimensions, it turns out the formula for the growth rate of the volume is , where R is a mathematical object known as the Ricci scalar curvature. So what this all means is that if we look at the growth rates of spherical balls in our hypergraphs, we can expect two contributions: a leading one of order rd that corresponds to effective dimension, and a “correction” of order r2 that represents curvature. Here’s an example. Instead of giving a flat estimate of dimension (here equal to 2), we have something that dips down, reflecting the positive (“sphere-like”) curvature of the surface: What is the significance of curvature? One thing is that it has implications for geodesics. A geodesic is the shortest distance between two points. In ordinary flat space, geodesics are just lines. But when there’s curvature, the geodesics are curved: In the case of positive curvature, bundles of geodesics converge; for negative curvature they diverge. But, OK, even though geodesics were originally defined for continuous space (actually, as the name suggests, for paths on the surface of the Earth), one can also have them in graphs (and hypergraphs). And it’s the same story: the geodesic is the shortest path between two points in the graph (or hypergraph). Here are geodesics on the “positive-curvature surface” created by one of our rules: And here they are for a more complicated structure: Why are geodesics important? One reason is that in Einstein’s general relativity they’re the paths that light (or objects in “free fall”) follows in space. And in that theory gravity is associated with curvature in space. So when something is deflected going around the Sun, that happens because space around the Sun is curved, so the geodesic the object follows is also curved. General relativity’s description of curvature in space turns out to all be based on the Ricci scalar curvature R that we encountered above (as well as the slightly more sophisticated Ricci tensor). But so if we want to find out if our models are reproducing Einstein’s equations for gravity, we basically have to find out if the Ricci curvatures that arise from our hypergraphs are the same as the theory implies. There’s quite a bit of mathematical sophistication involved (for example, we have to consider curvature in space+time, not just space), but the bottom line is that, yes, in various limits, and subject to various assumptions, our models do indeed reproduce Einstein’s equations. (At first, we’re just reproducing the vacuum Einstein equations, appropriate when there’s no matter involved; when we discuss matter, we’ll see that we actually get the full Einstein equations.) It’s a big deal to reproduce Einstein’s equations. Normally in physics, Einstein’s equations are what you start from (or sometimes they arise as a consistency condition for a theory): here they’re what comes out as an emergent feature of the model. It’s worth saying a little about how the derivation works. It’s actually somewhat analogous to the derivation of the equations of fluid flow from the limit of the underlying dynamics of lots of discrete molecules. But in this case, it’s the structure of space rather than the velocity of a fluid that we’re computing. It involves some of the same kinds of mathematical approximations and assumptions, though. One has to assume, for example, that there’s enough effective randomness generated in the system that statistical averages work. There is also a whole host of subtle mathematical limits to take. Distances have to be large compared to individual hypergraph connections, but small compared to the whole size of the hypergraph, etc. It’s pretty common for physicists to “hack through” the mathematical niceties. That’s actually happened for nearly a century in the case of deriving fluid equations from molecular dynamics. And we’re definitely guilty of the same thing here. Which in a sense is another way of saying that there’s lots of nice mathematics to do in actually making the derivation rigorous, and understanding exactly when it’ll apply, and so on. By the way, when it comes to mathematics, even the setup that we have is interesting. Calculus has been built to work in ordinary continuous spaces (manifolds that locally approximate Euclidean space). But what we have here is something different: in the limit of an infinitely large hypergraph, it’s like a continuous space, but ordinary calculus doesn’t work on it (not least because it isn’t necessarily integer-dimensional). So to really talk about it well, we have to invent something that’s kind of a generalization of calculus, that’s for example capable of dealing with curvature in fractional-dimensional space. (Probably the closest current mathematics to this is what’s been coming out of the very active field of geometric group theory.) It’s worth noting, by the way, that there’s a lot of subtlety in the precise tradeoff between changing the dimension of space, and having curvature in it. And while we think our universe is three-dimensional, it’s quite possible according to our models that there are at least local deviations—and most likely there were actually large deviations in the early universe. In our models, space is defined by the large-scale structure of the hypergraph that represents our collection of abstract relations. But what then is time? For the past century or so, it’s been pretty universally assumed in fundamental physics that time is in a sense “just like space”—and that one should for example lump space and time together and talk about the “spacetime continuum”.  And certainly the theory of relativity points in this direction. But if there’s been one “wrong turn” in the history of physics in the past century, I think it’s the assumption that space and time are the same kind of thing. And in our models they’re not—even though, as we’ll see, relativity comes out just fine. So what then is time? In effect it’s much as we experience it: the inexorable process of things happening and leading to other things. But in our models it’s something much more precise: it’s the progressive application of rules, that continually modify the abstract structure that defines the contents of the universe. The version of time in our models is in a sense very computational. As time progresses we are in effect seeing the results of more and more steps in a computation. And indeed the phenomenon of computational irreducibility implies that there is something definite and irreducible “achieved” by this process. (And, for example, this irreducibility is what I believe is responsible for the “encrypting” of initial conditions that is associated with the law of entropy increase, and the thermodynamic arrow of time.) Needless to say, of course, our modern computational paradigm did not exist a century ago when “spacetime” was introduced, and perhaps if it had, the history of physics might have been very different. But, OK, so in our models time is just the progressive application of rules. But there is a subtlety in exactly how this works that might at first seem like a detail, but that actually turns out to be huge, and in fact turns out to be the key to both relativity and quantum mechanics. At the beginning of this piece, I talked about the rule {{x, y}, {x, z}} → {{x, z}, {x, w}, {y, w}, {z, w}} and showed the “first few steps” in applying it: But how exactly did the rule get applied? What is “inside” these steps? The rule defines how to take two connections in the hypergraph (which in this case is actually just a graph) and transform them into four new connections, creating a new element in the process. So each “step” that we showed before actually consists of several individual “updating events” (where here newly added connections are highlighted, and ones that are about to be removed are dashed): But now, here is the crucial point: this is not the only sequence of updating events consistent with the rule. The rule just says to find two adjacent connections, and if there are several possible choices, it says nothing about which one. And a crucial idea in our model is in a sense just to do all of them. We can represent this with a graph that shows all possible paths: For the very first update, there are two possibilities. Then for each of the results of these, there are four additional possibilities. But at the next update, something important happens: two of the branches merge. In other words, even though we have done a different sequence of updates, the outcome is the same. Things rapidly get complicated. Here is the graph after one more update, now no longer trying to show a progression down the page: So how does this relate to time? What it says is that in the basic statement of the model there is not just one path of time; there are many paths, and many “histories”. But the model—and the rule that is used—determines all of them. And we have seen a hint of something else: that even if we might think we are following an “independent” path of history, it may actually merge with another path. It will take some more discussion to explain how this all works. But for now let me say that what will emerge is that time is about causal relationships between things, and that in fact, even when the paths of history that are followed are different, these causal relationships can end up being the same—and that in effect, to an observer embedded in the system, there is still just a single thread of time.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.7 The Graph of Causal Relationships : In the end it’s wonderfully elegant. But to get to the point where we can understand the elegant bigger picture we need to go through some detailed things. (It isn’t terribly surprising that a fundamental theory of physics—inevitably built on very abstract ideas—is somewhat complicated to explain, but so it goes.) To keep things tolerably simple, I’m not going to talk directly about rules that operate on hypergraphs. Instead I’m going to talk about rules that operate on strings of characters. (To clarify: these are not the strings of string theory—although in a bizarre twist of “pun-becomes-science” I suspect that the continuum limit of the operations I discuss on character strings is actually related to string theory in the modern physics sense.) OK, so let’s say we have the rule: {A → BBB, BB → A} This rule says that anywhere we see an A, we can replace it with BBB, and anywhere we see BB we can replace it with A. So now we can generate what we call the multiway system for this rule, and draw a “multiway graph” that shows everything that can happen: At the first step, the only possibility is to use A→BBB to replace the A with BBB. But then there are two possibilities: replace either the first BB or the second BB—and these choices give different results.  On the next step, though, all that can be done is to replace the A—in both cases giving BBBB. So in other words, even though we in a sense had two paths of history that diverged in the multiway system, it took only one step for them to converge again. And if you trace through the picture above you’ll find out that’s what always happens with this rule: every pair of branches that is produced always merges, in this case after just one more step. This kind of balance between branching and merging is a phenomenon I call “causal invariance”. And while it might seem like a detail here, it actually turns out that it’s at the core of why relativity works, why there’s a meaningful objective reality in quantum mechanics, and a host of other core features of fundamental physics. But let’s explain why I call the property causal invariance. The picture above just shows what “state” (i.e. what string) leads to what other one. But at the risk of making the picture more complicated (and note that this is incredibly simple compared to the full hypergraph case), we can annotate the multiway graph by including the updating events that lead to each transition between states: But now we can ask the question: what are the causal relationships between these events? In other words, what event needs to happen before some other event can happen? Or, said another way, what events must have happened in order to create the input that’s needed for some other event? Let us go even further, and annotate the graph above by showing all the causal dependencies between events: The orange lines in effect show which event has to happen before which other event—or what all the causal relationships in the multiway system are. And, yes, it’s complicated. But note that this picture shows the whole multiway system—with all possible paths of history—as well as the whole network of causal relationships within and between these paths. But here’s the crucial thing about causal invariance: it implies that actually the graph of causal relationships is the same regardless of which path of history is followed. And that’s why I originally called this property “causal invariance”—because it says that with a rule like this, the causal properties are invariant with respect to different choices of the sequence in which updating is done. And if one traced through the picture above (and went quite a few more steps), one would find that for every path of history, the causal graph representing causal relationships between events would always be: or, drawn differently.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.8 The Importance of Causal Invariance : To understand more about causal invariance, it’s useful to look at an even simpler example: the case of the rule BA→AB. This rule says that any time there’s a B followed by an A in a string, swap these characters around. In other words, this is a rule that tries to sort a string into alphabetical order, two characters at a time. Let’s say we start with BBBAAA. Then here’s the multiway graph that shows all the things that can happen according to the rule: There are lots of different paths that can be followed, depending on which BA in the string the rule is applied to at each step. But the important thing we see is that at the end all the paths merge, and we get a single final result: the sorted string AAABBB. And the fact that we get this single final result is a consequence of the causal invariance of the rule. In a case like this where there’s a final result (as opposed to just evolving forever), causal invariance basically says: it doesn’t matter what order you do all the updates in; the result you’ll get will always be the same. I’ve introduced causal invariance in the context of trying to find a model of fundamental physics—and I’ve said that it’s going to be critical to both relativity and quantum mechanics. But actually what amounts to causal invariance has been seen before in various different guises in mathematics, mathematical logic and computer science. (Its most common name is “confluence”, though there are some technical differences between this and what I call causal invariance.) Think about expanding out an algebraic expression, like (x + (1 + x)2)(x + 2)2. You could expand one of the powers first, then multiply things out. Or you could multiply the terms first. It doesn’t matter what order you do the steps in; you’ll always get the same canonical form (which in this case Mathematica tells me is 4 + 16x + 17x2 + 7x3 + x4). And this independence of orders is essentially causal invariance. When one thinks about parallel or asynchronous algorithms, it’s important if one has causal invariance. Because it means one can do things in any order—say, depth-first, breadth-first, or whatever—and one will always get the same answer. And that’s what’s happening in our little sorting algorithm above. OK, but now let’s come back to causal relationships. Here’s the multiway system for the sorting process annotated with all causal relationships for all paths: (By the way—as the picture suggests—the cross-connections between these copies aren’t trivial, and later on we’ll see they’re associated with deep relations between relativity and quantum mechanics, that probably manifest themselves in the physics of black holes. But we’ll get to that later…) OK, so every different way of applying the sorting rule is supposed to give the same causal graph. So here’s one example of how we might apply the rule starting with a particular initial string: But now let’s show the graph of causal connections. And we see it’s just a grid: Here are three other possible sequences of updates: But now we see causal invariance in action: even though different updates occur at different times, the graph of causal relationships between updating events is always the same. And having seen this—in the context of a very simple example—we’re ready to talk about special relativity.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.9 Deriving Special Relativity : It’s a typical first instinct in thinking about doing science: you imagine doing an experiment on a system, but you—as the “observer”—are outside the system. Of course if you’re thinking about modeling the whole universe and everything in it, this isn’t ultimately a reasonable way to think about things. Because the “observer” is inevitably part of the universe, and so has to be modeled just like everything else. In our models what this means is that the “mind of the observer”, just like everything else in the universe, has to get updated through a series of updating events. There’s no absolute way for the observer to “know what’s going on in the universe”; all they ever experience is a series of updating events, that may happen to be affected by updating events occurring elsewhere in the universe. Or, said differently, all the observer can ever observe is the network of causal relationships between events—or the causal graph that we’ve been talking about. So as toy model let’s look at our BA→AB rule for strings. We might imagine that the string is laid out in space. But to our observer the only thing they know is the causal graph that represents causal relationships between events. And for the BA→AB system here’s one way we can draw that: But now let’s think about how observers might “experience” this causal graph. Underneath, an observer is getting updated by some sequence of updating events. But even though that’s “really what’s going on”, to make sense of it, we can imagine our observers setting up internal “mental” models for what they see. And a pretty natural thing for observers like us to do is just to say “one set of things happens all across the universe, then another, and so on”. And we can translate this into saying that we imagine a series of “moments” in time, where things happen “simultaneously” across the universe—at least with some convention for defining what we mean by simultaneously. (And, yes, this part of what we’re doing is basically following what Einstein did when he originally proposed special relativity.) Here’s a possible way of doing it: One can describe this as a “foliation” of the causal graph. We’re dividing the causal graph into leaves or slices. And each slice our observers can consider to be a “successive moment in time”. It’s important to note that there are some constraints on the foliation we can pick. The causal graph defines what event has to happen before what. And if our observers are going to have a chance of making sense of the world, it had better be the case that their notion of the progress of time aligns with what the causal graph says. So for example this foliation wouldn’t work—because basically it says that the time we assign to events is going to disagree with the order in which the causal graph says they have to happen: But, so given the foliation above, what actual order of updating events does it imply? It basically just says: as many events as possible happen at the same time (i.e. in the same slice of the foliation), as in this picture: OK, now let’s connect this to physics. The foliation we had above is relevant to observers who are somehow “stationary with respect to the universe” (the “cosmological rest frame”).  One can imagine that as time progresses, the events a particular observer experiences are ones in a column going vertically down the page: But now let’s think about an observer who is uniformly moving in space. They’ll experience a different sequence of events, say: And that means that the foliation they’ll naturally construct will be different. From the “outside” we can draw it on the causal graph like this: But to the observer each slice just represents a successive moment of time. And they don’t have any way to know how the causal graph was drawn. So they’ll construct their own version, where the slices are horizontal: But now there’s a purely geometrical fact: to make this rearrangement, while preserving the basic structure (and here, angles) of the causal graph, each moment of time has to sample fewer events in the causal graph, by a factor of where β is the angle that represents the velocity of the observer. If you know about special relativity, you’ll recognize a lot of this. What we’ve been calling foliations correspond directly to relativity’s “reference frames”. And our foliations that represent motion are the standard inertial reference frames of special relativity. But here’s the special thing that’s going on here: we can interpret all this discussion of foliations and reference frames in terms of the actual rules and evolution of our underlying system. So here now is the evolution of our string-sorting system in the “boosted reference frame” corresponding to an observer going at a certain speed: And here’s the crucial point: because of causal invariance it doesn’t matter that we’re in a different reference frame—the causal graph for the system (and the way it eventually sorts the string) is exactly the same. In special relativity, the key idea is that the “laws of physics” work the same in all inertial reference frames. But why should that be true? Well, in our systems, there’s an answer: it’s a consequence of causal invariance in the underlying rules. In other words, from the property of causal invariance, we’re able to derive relativity. Normally in physics one puts in relativity by the way one sets up the mathematical structure of spacetime. But in our models we don’t start from anything like this, and in fact space and time are not even at all the same kind of thing. But what we can now see is that—because of causal invariance—relativity emerges in our models, with all the relationships between space and time that that implies. So, for example, if we look at the picture of our string-sorting system above, we can see relativistic time dilation. In effect, because of the foliation we picked, time operates slower. Or, said another way, in the effort to sample space faster, our observer experiences slower updating of the system in time. The speed of light c in our toy system is defined by the maximum rate at which information can propagate, which is determined by the rule, and in the case of this rule is one character per step. And in terms of this, we can then say that our foliation corresponds to a speed 0.3 c. But now we can look at the amount of time dilation, and it’s exactly the amount that relativity says it should be. By the way, if we imagine trying to make our observer go “faster than light”, we can see that can’t work. Because there’s no way to tip the foliation at more than 45° in our picture, and still maintain the causal relationships implied by the causal graph. OK, so in our toy model we can derive special relativity. But here’s the thing: this derivation isn’t specific to the toy model; it applies to any rule that has causal invariance. So even though we may be dealing with hypergraphs, not strings, and we may have a rule that shows all kinds of complicated behavior, if it ultimately has causal invariance, then (with various technical caveats, mostly about possible wildness in the causal graph) it will exhibit relativistic invariance, and a physics based on it will follow special relativity.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.10 What Is Energy? What Is Mass? : In our model, everything in the universe—space, matter, whatever—is supposed to be represented by features of our evolving hypergraph. So within that hypergraph, is there a way to identify things that are familiar from current physics, like mass, or energy? I have to say that although it’s a widespread concept in current physics, I’d never thought of energy as something fundamental. I’d just thought of it as an attribute that things (atoms, photons, whatever) can have. I never really thought of it as something that one could identify abstractly in the very structure of the universe. So it came as a big surprise when we recently realized that actually in our model, there is something we can point to, and say “that’s energy!”, independent of what it’s the energy of. The technical statement is: energy corresponds to the flux of causal edges through spacelike hypersurfaces. And, by the way, momentum corresponds to the flux of causal edges through timelike hypersurfaces. OK, so what does this mean? First, what’s a spacelike hypersurface? It’s actually a standard concept in general relativity, for which there’s a direct analogy in our models. Basically it’s what forms a slice in our foliation. Why is it called what it’s called? We can identify two kinds of directions: spacelike and timelike. A spacelike direction is one that involves just moving in space—and it’s a direction where one can always reverse and go back.  A timelike direction is one that involves also progressing through time—where one can’t go back. We can mark spacelike () and timelike () hypersurfaces in the causal graph for our toy model: (They might be called “surfaces”, except that “surfaces” are usually thought of as 2-dimensional, and our 3-space + 1-time dimensional universe, these foliation slices are 3-dimensional: hence the term “hypersurfaces”.) OK, now let’s look at the picture. The “causal edges” are the causal connections between events, shown in the picture as lines joining the events. So when we talk about a “flux of causal edges through spacelike hypersurfaces”, what we’re talking about is the net number of causal edges that go down through the horizontal slices in the pictures. In the toy model that’s trivial to see. But here’s a causal graph from a simple hypergraph model, where it’s already considerably more complicated: (Our toy-model causal graph starts from a line of events because we set up a long string as the initial condition; this starts from a single event because it’s starting from a minimal initial condition.) But when we put a foliation on this causal graph (thereby effectively defining our reference frame) we can start counting how many causal edges go down through successive (“spacelike”) slices: We can also ask how many causal edges go “sideways”, through timelike hypersurfaces: OK, so why do we think these fluxes of edges correspond to energy and momentum? Imagine what happens if we change our foliation, say tipping it to correspond to motion at some velocity, as we did in the previous section. It takes a little bit of math, but what we find out is that our fluxes of causal edges transform with velocity basically just like we saw distance and time transform in the previous section. In the standard derivation of relativistic mechanics, there’s a consistency argument that energy has to transform with velocity like time does, and momentum like distance. But now we actually have a structural reason for this to be the case. It’s a fundamental consequence of our whole setup, and of causal invariance. In traditional physics, one often says that position is the conjugate variable to momentum, and energy to time. And that’s something that’s burnt into the mathematical structure of the theory. But here it’s not something we’re burning in; it’s something we’re deriving from the underlying structure of our model. And that means there’s ultimately a lot more we can say about it. For example, we might wonder what the “zero of energy” is. After all, if we look at one of our causal graphs, a lot of the causal edges are really just going into “maintaining the structure of space”. So if in a sense space is uniform, there’s inevitably a uniform “background flux” of causal edges associated with that. And whatever we consider to be “energy” corresponds to the fluctuations of that flux around its background value. By the way, it’s worth mentioning what a “flux of causal edges” corresponds to. Each causal edge represents a causal connection between events, that is in a sense “carried” by some element in the underlying hypergraph (the “spatial hypergraph”). So a “flux of causal edges” is in effect the communication of activity (i.e. events), either in time (i.e. through spacelike hypersurfaces) or in space (i.e. through timelike hypersurfaces). And at least in some approximation we can then say that energy is associated with activity in the hypergraph that propagates information through time, while momentum is associated with activity that propagates information in space. There’s a fundamental feature of our causal graphs that we haven’t mentioned yet—that’s related to information propagation. Start at any point (i.e. any event) in a causal graph. Then trace the causal connections from that event. You’ll get some kind of cone (here just in 2D): The cone is more complicated in a more complicated causal graph. But you’ll always have something like it. And what it corresponds to physically is what’s normally called a light cone (or “forward light cone”). Assuming we’ve drawn our causal network so that events are somehow laid out in space across the page, then the light cone will show how information (as transmitted by light) can spread in space with time. When the causal graph gets complicated, the whole setup with light cones gets complicated, as we’ll discuss for example in connection with black holes later. But for now, we can just say there are cones in our causal graph, and in effect the angle of these cones represents the maximum rate of information propagation in the system, which we can identify with the physical speed of light. And in fact, not only can we identify light cones in our causal graph: in some sense we can think of our whole causal graph as just being a large number of “elementary light cones” all knitted together. And, as we mentioned, much of the structure that’s built necessarily goes into, in effect, “maintaining the structure of space”. But let’s look more closely at our light cones. There are causal edges on their boundaries that in effect correspond to propagation at the speed of light—and that, in terms of the underlying hypergraph, correspond to events that “reach out” in the hypergraph, and “entrain” new elements as quickly as possible. But what about causal edges that are “more vertical”? These causal edges are associated with events that in a sense reuse elements in the hypergraph, without involving new ones. And it looks like these causal edges have an important interpretation: they are associated with mass (or, more specifically, rest mass). OK, so the total flux of causal edges through spacelike hypersurfaces corresponds to energy. And now we’re saying that the flux of causal edges specifically in the timelike direction corresponds to rest mass. We can see what happens if we “tip our reference” frames just a bit, say corresponding to a velocity v ≪ c. Again, there’s a small amount of math, but it’s pretty easy to derive formulas for momentum (p) and energy (E). The speed of light c comes into the formulas because it defines the ratio of “horizontal” (i.e. spacelike) to “vertical” (i.e timelike) distances on the causal graph. And for v small compared to c we get: So from these formulas we can see that just by thinking about causal graphs (and, yes, with a backdrop of causal invariance, and a whole host of detailed mathematical limit questions that we’re not discussing here), we’ve managed to derive a basic (and famous) fact about the relation between energy and mass: Sometimes in the standard formalism of physics, this relation by now seems more like a definition than something to derive. But in our model, it’s not just a definition, and in fact we can successfully derive it.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.11 General Relativity & Gravity : Earlier on, we talked about how curvature of space can arise in our models. But at that point we were just talking about “empty space”. Now we can go back and also talk about how curvature interacts with mass and energy in space. In our earlier discussion, we talked about constructing spherical balls by starting at some point in the hypergraph, and then following all possible sequences of r connections. But now we can do something directly analogous in the causal graph: start at some point, and follow possible sequences of t connections. There’s quite a bit of mathematical trickiness, but essentially this gets us “volumes of light cones”. If space is effectively d-dimensional, then to a first approximation this volume will grow like &period But like in the spatial case, there’s a correction term, this time proportional to the so-called Ricci tensor . (The actual expression is roughly where the are timelike vectors, etc.) OK, but we also know something else about what is supposed to be inside our light cones: not only are there “background connections” that maintain the structure of space, there are also “additional” causal edges that are associated with energy, momentum and mass. And in the limit of a large causal graph, we can identify the density of these with the so-called energy-momentum tensor . So in the end we have two contributions to the “volumes” of our light cones: one from “pure curvature” and one from energy-momentum. Again, there’s some math involved. But the main thing is to think about the limit when we’re looking at a very large causal graph. What needs to be true for us to have d-dimensional space, as opposed to something much wilder? This puts a constraint on the growth rates of our light cone volumes, and when one works everything out, it implies that the following equation must hold: But this is exactly Einstein’s equation for the curvature of space when matter with a certain energy-momentum is present. We’re glossing over lots of details here. But it’s still, in my view, quite spectacular: from the basic structure of our very simple models, we’re able to derive a fundamental result in physics: the equation that for more than a hundred years has passed every test in describing the operation of gravity. There’s a footnote here. The equation we’ve just given is without a so-called cosmological term.  And how that works is bound up with the question of what the zero of energy is, which in our model relates to what features of the evolving hypergraph just have to do with the “maintenance of space”, and what have to do with “things in space” (like matter). In existing physics, there’s an expectation that even in the “vacuum” there’s actually a formally infinite density of pairs of virtual particles associated with quantum mechanics. Essentially what’s happening is that there are always pairs of particles and antiparticles being created, that annihilate quickly, but that in aggregate contribute a huge effective energy density. We’ll discuss how this relates to quantum mechanics in our models later. But for now let’s just recall that particles (like electrons) in our models basically correspond to locally stable structures in the hypergraph. But when we think about how “space is maintained” it’s basically through all sorts of seemingly random updating events in the hypergraph. But in existing physics (or, specifically, quantum field theory) we’re basically expected to analyze everything in terms of (virtual) particles. So if we try to do that with all these random updating events, it’s not surprising that we end up saying that there are these infinite collections of things going on. (Yes, this can be made much more precise; I’m just giving an outline here.) But as soon as we say this, there is an immediate problem: we’re saying that there’s a formally infinite—or at least huge—energy density that must exist everywhere in the universe. But if we then apply Einstein’s equation, we’ll conclude that this must produce enough curvature to basically curl the universe up into a tiny ball. One way to get out of this is to introduce a so-called cosmological term, that’s just an extra term in the Einstein equations, and then posit that this term is sized so as to exactly cancel (yes, to perhaps one part in 1060 or more) the energy density from virtual particles. It’s certainly not a pretty solution. But in our models, the situation is quite different. It’s not that we have virtual particles “in space”, that are having an effect on space. It’s that the same stuff that corresponds to the virtual particles is actually “making the space”, and maintaining its structure.  Of course, there are lots of details about this—which no doubt depend on the particular underlying rule. But the point is that there’s no longer a huge mystery about why “vacuum energy” doesn’t basically destroy our universe: in effect, it’s because it’s what’s making our universe.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.12 Black Holes, Singularities, etc. : One of the big predictions of general relativity is the existence of black holes. So how do things like that work in our models? Actually, it’s rather straightforward. The defining feature of a black hole is the existence of an event horizon: a boundary that light signals can’t cross, and where in effect causal connection is broken. In our models, we can explicitly see that happen in the causal graph. Here’s an example: At the beginning, everything is causally connected. But at some point the causal graph splits—and there’s an event horizon. Events happening on one side can’t influence ones on the other, and so on. And that’s how a region of the universe can “causally break off” to form something like a black hole. But actually, in our models, the “breaking off” can be even more extreme. Not only can the causal graph split; the spatial hypergraph can actually throw off disconnected pieces—each of which in effect forms a whole “separate universe”: By the way, it’s interesting to look at what happens to the foliations observers make when there’s an event horizon. Causal invariance says that paths in the causal graph that diverge should always eventually merge. But if the paths go into different disconnected pieces of the causal graph, that can’t ever happen. So how does an observer deal with that? Well, basically they have to “freeze time”. They have to have a foliation where successive time slices just pile up, and never enter the disconnected pieces. It’s just like what happens in general relativity. To an observer far from the black hole, it’ll seem to take an infinite time for anything to fall into the black hole. For now, this is just a phenomenon associated with the structure of space. But later we’ll see that it’s also the direct analog of something completely different: the process of measurement in quantum mechanics. Coming back to gravity: we can ask questions not only about event horizons, but also about actual singularities in spacetime. In our models, these are places where lots of paths in a causal graph converge to a single point. And in our models, we can immediately study questions like whether there’s always an event horizon associated with any singularity (the “cosmic censorship hypothesis”). We can ask about other strange phenomena from general relativity. For example, there are closed timelike curves, sometimes viewed as allowing time travel. In our models, closed timelike curves are inconsistent with causal invariance. But we can certainly invent rules that produce them. Here’s an example: We start from one “initial” state in this multiway system. But as we go forward we can enter a loop where we repeatedly visit the same state. And this loop also occurs in the causal graph. We think we’re “going forward in time”. But actually we’re just in a loop, repeatedly returning to the same state. And if we tried to make a foliation where we could describe time as always advancing, we just wouldn’t be able to do it.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.13 Cosmology : In our model, the universe can start as a tiny hypergraph—perhaps a single self-loop. But then—as the rule gets applied—it progressively expands. With some particularly simple rules, the total size of the hypergraph has to just uniformly increase; with others it can fluctuate. But even if the size of the hypergraph is always increasing, that doesn’t mean we’d necessarily notice. It could be that essentially everything we can see just expands too—so in effect the granularity of space is just getting finer and finer. This would be an interesting resolution to the age-old debate about whether the universe is discrete or continuous. Yes, it’s structurally discrete, but the scale of discreteness relative to our scale is always getting smaller and smaller. And if this happens fast enough, we’d never be able to “see the discreteness”—because every time we tried to measure it, the universe would effectively have subdivided before we got the result. (Somehow it’d be like the ultimate calculus epsilon-delta proof: you challenge the universe with an epsilon, and before you can get the result, the universe has made a smaller delta.) There are some other strange possibilities too. Like that the whole hypergraph for the universe is always expanding, but pieces are continually “breaking off”, effectively forming black holes of different sizes, and allowing the “main component” of the universe to vary in size. But regardless of how this kind of expansion works in our universe today, it’s clear that if the universe started with a single self-loop, it had to do a lot of expanding, at least early on. And here there’s an interesting possibility that’s relevant for understanding cosmology. Just because our current universe exhibits three-dimensional space, in our models there’s no reason to think that the early universe necessarily also did. There are very different things that can happen in our models: In the first example here, different parts of space effectively separate into non-communicating “black hole” tree branches. In the second example, we have something like ordinary—in this case 2-dimensional—space. But in the third example, space is in a sense very connected. If we work out the volume of a spherical ball, it won’t grow like rd; it’ll grow exponentially with r (e.g. like 2r). If we look at the causal graph, we’ll see that you can effectively “go everywhere in space”, or affect every event, very quickly. It’d be as if the speed of light is infinite. But really it’s because space is effectively infinite dimensional. In typical cosmology, it’s been quite mysterious how different parts of the early universe managed to “communicate” with each other, for example, to smooth out perturbations. But if the universe starts effectively infinite-dimensional, and only later “relaxes” to being finite-dimensional, that’s no longer a mystery. So, OK, what might we see in the universe today that would reflect what happened extremely early in its history?  The fact that our models deterministically generate behavior that seems for all practical purposes random means that we can expect that most features of the initial conditions or very early stages of the universe will quickly be “encrypted”, and effectively not reconstructable. But it’s just conceivable that something like a breaking of symmetry associated with the first few hypergraphs might somehow survive. And that suggests the bizarre possibility that—just maybe—something like the angular structure of the cosmic microwave background or the very large-scale distribution of galaxies might reflect the discrete structure of the very early universe. Or, in other words, it’s just conceivable that what amounts to the rule for the universe is, in effect, painted across the whole sky. I think this is extremely unlikely, but it’d certainly be an amazing thing if the universe were “self-documenting” that way.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.14 Elementary Particles—Old and New : We’ve talked several times about particles like electrons. In current physics theories, the various (truly) elementary particles—the quarks, the leptons (electron, muon, neutrinos, etc.), the gauge bosons, the Higgs—are all assumed to intrinsically be point particles, of zero size. In our models, that’s not how it works. The particles are all effectively “little lumps of space” that have various special properties. My guess is that the precise list of what particles exist will be something that’s specific to a particular underlying rule. In cellular automata, for example, we’re used to seeing complicated sets of possible localized structures arise: In our hypergraphs, the picture will inevitably be somewhat different. The “core feature” of each particle will be some kind of locally stable structure in the hypergraph (a simple analogy might be that it’s a lump of nonplanarity in an otherwise planar graph). But then there’ll be lots of causal edges associated with the particle, defining its particular energy and momentum. Still, the “core feature” of the particles will presumably define things like their charge, quantum numbers, and perhaps spin—and the fact that these things are observed to occur in discrete units may reflect the fact that it’s a small piece of hypergraph that’s involved in defining them. It’s not easy to know what the actual scale of discreteness in space might be in our models. But a possible (though potentially unreliable) estimate might be that the “elementary length” is around 10–93 meters. (Note that that’s very small compared to the Planck length ~10–35 meters that arises essentially from dimensional analysis.) And with this elementary length, the radius of the electron might be 10–81 meters. Tiny, but not zero. (Note that current experiments only tell us that the size of the electron is less than about 10–22 meters.) One feature of our models is that there should be a “quantum of mass”—a discrete amount that all masses, for example of particles, are multiples of. With our estimate for the elementary length, this quantum of mass would be small, perhaps 10–30, or 1036 times smaller than the mass of the electron. And this raises an intriguing possibility. Perhaps the particles—like electrons—that we currently know about are the “big ones”. (With our estimates, an electron would have hypergraph elements in it.) And maybe there are some much smaller, and much lighter ones. At least relative to the particles we currently know, such particles would have few hypergraph elements in them—so I’m referring to them as “oligons” (after the Greek word ὀλιγος for “few”). What properties would these oligons have? They’d probably interact very very weakly with other things in the universe. Most likely lots of oligons would have been produced in the very early universe, but with their very weak interactions, they’d soon “drop out of thermal equilibrium”, and be left in large numbers as relics—with energies that become progressively lower as the universe expands around them. So where might oligons be now?  Even though their other interactions would likely be exceptionally weak, they’d still be subject to gravity. And if their energies end up being low enough, they’d basically collect in gravity wells around the universe—which means in and around galaxies. And that’s interesting—because right now there’s quite a mystery about the amount of mass seen in galaxies. There appears to be a lot of “dark matter” that we can’t see but that has gravitational effects. Well, maybe it’s oligons. Maybe even lots of different kinds of oligons: a whole shadow physics of much lighter particles.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.15 The Inevitability of Quantum Mechanics : “But how will you ever get quantum mechanics?”, physicists would always ask me when I would describe earlier versions of my models. In many ways, quantum mechanics is the pinnacle of existing physics. It’s always had a certain “you-are-not-expected-to-understand-this” air, though, coupled with “just-trust-the-mathematical-formalism”. And, yes, the mathematical formalism has worked well—really well—in letting us calculate things. (And it almost seems more satisfying because the calculations are often so hard; indeed, hard enough that they’re what first made me start using computers to do mathematics 45 years ago.) Our usual impression of the world is that definite things happen. And before quantum mechanics, classical physics typically captured this in laws—usually equations—that would tell one what specifically a system would do. But in quantum mechanics the formalism involves any particular system doing lots of different things “in parallel”, with us just seeing samples—ultimately with certain probabilities—of these possibilities. And as soon as one hears of a model in which there are definite rules, one might assume that it could never reproduce quantum mechanics. But, actually, in our models, quantum mechanics is not just possible; it’s absolutely inevitable. And, as we’ll see, in something I consider quite beautiful, the core of what leads to it turns out to be the same as what leads to relativity. OK, so how does this work? Let’s go back to what we discussed when we first started talking about time. In our models there’s a definite rule for updates to make in our hypergraphs, say: But if we’ve got a hypergraph like this: there will usually be many places where this rule can be applied. So which update should we do first? The model doesn’t tell us. But let’s just imagine all the possibilities. The rule tells us what they all are—and we can represent them (as we discussed above) as a multiway system—here illustrated using the simpler case of strings rather than hypergraphs: Each node in this graph now represents a complete state of our system (a hypergraph in our actual models). And each node is joined by arrows to the state or states that one gets by applying a single update to it. If our model had been operating “like classical physics” we would expect it to progress in time from one state to another, say like this: But the crucial point is that the structure of our models leaves us no choice but to consider multiway systems. The form of the whole multiway system is completely determined by the rules. But—in a way that is already quite reminiscent of the standard formalism of quantum mechanics—the multiway system defines many different possible paths of history. But now there is a mystery. If there are always all these different possible paths of history, how is it that we ever think that definite things happen in the world? This has been a core mystery of quantum mechanics for a century. It turns out that if one’s just using quantum mechanics to do calculations, the answer basically doesn’t matter. But if one wants to “really understand what’s going on” in quantum mechanics, it’s something that definitely does matter. And the exciting thing is that in our models, there’s an obvious resolution. And actually it’s based on the exact same phenomenon—causal invariance—that gives us relativity. Here’s roughly how this works. The key point is to think about what an observer who is themselves part of the multiway system will conclude about the world. Yes, there are different possible paths of history. But—just as in our discussion of relativity—the only aspect of them that an observer will ever be aware of is the causal relationships between the events they involve. But the point is that—even though when looked at from “outside” the paths are different—causal invariance implies that the network of relationships between causal events (which is all that’s relevant when one’s inside the system) will always be exactly the same. In other words—much as in the case of relativity—even though from outside the system there may seem to be many possible “threads of time”, from inside the system causal invariance implies that there’s in a sense ultimately just one thread of time, or, in effect, one objective reality. How does this all relate to the detailed standard formalism of quantum mechanics? It’s a little complicated. But let me make at least a few comments here. (There’s some more detail in my technical document; Jonathan Gorard has given even more.) The states in the multiway system can be thought of as possible states of the quantum system. But how do we characterize how observers experience them? In particular, which states is the observer aware of when? Just like in the relativity case, the observer can in a sense make a choice of how they define time. One possibility might be through a foliation of the multiway system like this: In the formalism of quantum mechanics, one can then say that at each time, the observer experiences a superposition of possible states of the system. But now there’s a critical point. In direct analogy to the case of relativity, there are many different possible choices the observer can make about how to define time—and each of them corresponds to a different foliation of the multiway graph. Again by analogy to relativity, we can then think of these choices as what we can call different “quantum observation frames”. Causal invariance implies that as long they respect the causal relationships in the graph, these frames can basically be set up in any way we want. In talking about relativity, it was useful to just have “tipped parallel lines” (“inertial frames”) representing observers who are moving uniformly in space. In talking about quantum mechanics, other frames are useful. In particular, in the standard formalism of quantum mechanics, it’s common to talk about “quantum measurement”: essentially the act of taking a quantum system and determining some definite (essentially classical) outcome from it. Well, in our setup, a quantum measurement basically corresponds to a particular quantum observation frame. Here’s an example: The successive pink lines effectively mark off what the observer is considering to be successive moments in time. So when all the lines bunch up below the state ABBABB what it means is that the observer is effectively choosing to “freeze time” for that state. In other words, the observer is saying “that’s the state I consider the system to be in, and I’m sticking to it”. Or, put another way, even though in the full multiway graph there’s all sorts of other “quantum mechanical” evolution of states going on, the observer has set up their quantum observation frame so that they pick out just a particular, definite, classical-like outcome. OK, but can they consistently do that? Well, that depends on the actual underlying structure of the multiway graph, which ultimately depends on the actual underlying rule. In the example above, we’ve set up a foliation (i.e. a quantum observation frame) that does the best possible job in this rule at “freezing time” for the ABBABB state. But just how long can this “reality distortion field” be maintained? The only way to keep the foliation consistent in the multiway graph above is to have it progressively expand over time. In other words, to keep time frozen, more and more quantum states have to be pulled into the “reality distortion field”, and so there’s less and less coherence in the system. The picture above is for a very trivial rule. Here’s a corresponding picture for a slightly more realistic case: And what we see here is that—even in this still incredibly simplified case—the structure of the multiway system will force the observer to construct a more and more elaborate foliation if they are to successfully freeze time. Measurement in quantum mechanics has always involved a slightly uncomfortable mathematical idealization—and this now gives us a sense of what’s really going on. (The situation is ultimately very similar to the problem of decoding “encrypted” thermodynamic initial conditions that I mentioned above.) Quantum measurement is really about what an observer perceives. But if you are for example trying to construct a quantum computer, it’s not just a question of having a qubit be perceived as being maintained in a particular state; it actually has to be maintained in that state. And for this to be the case we actually have to freeze time for that qubit. But here’s a very simplified example of how that can happen in a multiway graph: All this discussion of “freezing time” might seem weird, and not like anything one usually talks about in physics. But actually, there’s a wonderful connection: the freezing of time we’re talking about here can be thought of as happening because we’ve got the analog in the space of quantum states of a black hole in physical space. The picture above makes it plausible that we’ve got something where things can go in, but if they do, they always get stuck. But there’s more to it.  If you’re an observer far from a black hole, then you’ll never actually see anything fall into the black hole in finite time (that’s why black holes are called “frozen stars” in Russian). And the reason for this is precisely because (according to the mathematics) time is frozen at the event horizon of the black hole. In other words, to successfully make a qubit, you effectively have to isolate it in quantum space like things get isolated in physical space by the presence of the event horizon of a black hole.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.16 General Relativity and Quantum Mechanics Are the Same Idea! : General relativity and quantum mechanics are the two great foundational theories of current physics. And in the past it’s often been a struggle to reconcile them. But one of the beautiful outcomes of our project so far has been the realization that at some deep level general relativity and quantum mechanics are actually the same idea. It’s something that (at least so far) is only clear in the context of our models. But the basic point is that both theories are consequences of causal invariance—just applied in different situations. Recall our discussion of causal graphs in the context of relativity above. We drew foliations and said that if we looked at a particular slice, it would tell us the arrangement of the system in space at what we consider to be a particular time. So now let’s look at multiway graphs. We saw in the previous section that in quantum mechanics we’re interested in foliations of these. But if we look at a particular slice in one of these foliations, what does it represent? The foliation has got a bunch of states in it. And it turns out that we can think of them as being laid out in an abstract kind of space that we’re calling “branchial space”. To make sense of this space, we have to have a way to say what’s near what. But actually the multiway graph gives us that. Take a look at this multiway graph: At each slice in the foliation, let’s draw a graph where we connect two states whenever they’re both part of the same “branch pair”, so that—like AA and ABB here—they both come from the same state on the slice before. Here are the graphs we get by doing this for successive slices: We call these branchial graphs. And we can think of them as representing the correlation—or entanglement—of quantum states. Two states that are nearby in the graph are highly entangled; those further away, less so. And we can imagine that as our system evolves, we’ll get larger and larger branchial graphs, until eventually, just like for our original hypergraphs, we can think of these graphs as limiting to something like a continuous space. But what is this space like? For our original hypergraphs, we imagined that we’d get something like ordinary physical space (say close to three-dimensional Euclidean space). But branchial space is something more abstract—and much wilder. And typically it won’t even be finite-dimensional. (It might approximate a projective Hilbert space.) But we can still think of it mathematically as some kind of space.  OK, things are getting fairly complicated here. But let me try to give at least a flavor of how things work. Here’s an example of a wonderful correspondence: curvature in physical space is like the uncertainty principle of quantum mechanics. Why do these have anything to do with each other? The uncertainty principle says that if you measure, say, the position of something, then its momentum, you’ll get a different answer than if you do it in the opposite order. But now think about what happens when you try to make a rectangle in physical space by going in direction x first, then y, and then you do these in the opposite order. In a flat space, you’ll get to the same place. But in a curved space, you won’t: And essentially what’s happening in the uncertainty principle is that you’re doing exactly this, but in branchial space, rather than physical space. And it’s because branchial space is wild—and effectively very curved—that you get the uncertainty principle. Alright, so the next question might be: what’s the analog of the Einstein equations in branchial space? And again, it’s quite wonderful: at least in some sense, the answer is that it’s the path integral—the fundamental mathematical construct of modern quantum mechanics and quantum field theory. This is again somewhat complicated. But let me try to give a flavor of it. Just as we discussed geodesics as describing paths traversed through physical space in the course of time, so also we can discuss geodesics as describing paths traversed through branchial space in the course of time. In both cases these geodesics are determined by curvature in the corresponding space. In the case of physical space, we argued (roughly) that the presence of excess causal edges—corresponding to energy—would lead to what amounts to curvature in the spatial hypergraph, as described by Einstein’s equations. OK, so what about branchial space? Just like for the spatial hypergraph, we can think about the causal connections between the updating events that define the branchial graph. And we can once again imagine identifying the flux of causal edges—now not through spacelike hypersurfaces, but through branchlike ones—as corresponding to energy. And—much like in the spatial hypergraph case—an excess of these causal edges will have the effect of producing what amounts to curvature in branchial space (or, more strictly, in branchtime—the analog of spacetime). But this curvature will then affect the geodesics that traverse branchial space. In general relativity, the presence of mass (or energy) causes curvature in space which causes the paths of geodesics to turn—which is what is normally interpreted as the action of the force of gravity. But now we have an analog in quantum mechanics, in our branchial space. The presence of energy effectively causes curvature in branchial space which causes the paths of geodesics through branchial space to turn. What does turning correspond to? Basically it’s exactly what the path integral talks about. The path integral (and the usual formalism of quantum mechanics) is set up in terms of complex numbers. But it can just as well be thought of in terms of turning through an angle. And that’s exactly what’s happening with our geodesics in branchial space. In the path integral there’s a quantity called the action—which is a kind of relativistic analog of energy—and when one works things out more carefully, our fluxes of causal edges correspond to the action, but are also exactly what determine the rate of turning of geodesics. It all fits together beautifully. In physical space we have Einstein’s equations—the core of general relativity. And in branchial space (or, more accurately, multiway space) we have Feynman’s path integral—the core of modern quantum mechanics. And in the context of our models they’re just different facets of the same idea. It’s an amazing unification that I have to say I didn’t see coming; it’s something that just emerged as an inevitable consequence of our simple models of applying rules to collections of relations, or hypergraphs.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.17 Branchial Motion and the Entanglement Horizon : We can think of motion in physical space as like the process of exploring new elements in the spatial hypergraph, and potentially becoming affected by them. But now that we’re talking about branchial space, it’s natural to ask whether there’s something like motion there too. And the answer is that there is. And it’s basically exactly the same kind of thing: but instead of exploring new elements in the spatial hypergraph, we’re exploring new elements in the branchial graph, and potentially becoming affected by them. There’s a way of talking about it in the standard language of quantum mechanics: as we move in branchial space, we’re effectively getting “entangled” with more and more quantum states. OK, so let’s take the analogy further. In physical space, there’s a maximum speed of motion—the speed of light, c. So what about in branchial space? Well, in our models we can see that there’s also got to be a maximum speed of motion in branchial space. Or, in other words, there’s a maximum rate at which we can entangle with new quantum states. In physical space we talk about light cones as being the regions that can be causally affected by some event at a particular location in space. In the same way, we can talk about entanglement cones that define regions in branchial space that can be affected by events at some position in branchial space. And just as there’s a causal graph that effectively knits together elementary light cones, there’s something similar that knits together entanglement cones. That something similar is the multiway causal graph: a graph that represents causal relationships between all events that can happen anywhere in a multiway system. Here’s an example of a multiway causal graph for just a few steps of a very simple string substitution system—and it’s already pretty complicated: But in a sense the multiway causal graph is the most complete description of everything that can affect the experience of observers. Some of the causal relationships it describes represent spacelike connections; some represent branchlike connections. But all of them are there. And so in a sense the multiway causal graph is where relativity and quantum mechanics come together. Slice one way and you’ll see relationships in physical space; slice another way and you’ll see relationships in branchial space, between quantum states. To help see how this works here’s a very toy version of a multiway causal graph: Each point is an event that happens in some hypergraph on some branch of a multiway system. And now the graph records the causal relationship of that event to other ones. In this toy example, there are purely timelike relationships—indicated by arrows pointing down—in which basically some element of the hypergraph is affecting its future self. But then there are both spacelike and branchlike relationships, where the event affects elements that are either “spatially” separated in the hypergraph, or “branchially” separated in the multiway system. But in all this complexity, there’s something wonderful that happens. As soon as the underlying rule has causal invariance, this implies all sorts of regularities in the multiway causal graph. And for example it tells us that all those causal graphs we get by taking different branchtime slices are actually the same when we project them into spacetime—and this is what leads to relativity. But causal invariance has other consequences too. One of them is that there should be an analog of special relativity that applies not in spacetime but in branchtime. The reference frames of special relativity are now our quantum observation frames. And the analog of speed in physical space is the rate of entangling new quantum states. So what about a phenomenon like relativistic time dilation? Is there an analog of that for motion in branchial space? Well, actually, yes there is. And it turns out to be what’s sometimes called the quantum Zeno effect: if you repeatedly measure a quantum system fast enough it won’t change. It’s a phenomenon that’s implied by the add-ons to the standard formalism of quantum mechanics that describe measurement. But in our models it just comes directly from the analogy between branchial and physical space. Doing new measurements is equivalent to getting entangled with new quantum states—or to moving in branchial space. And in direct analogy to what happens in special relativity, as you get closer to moving at the maximum speed you inevitably sample things more slowly in time—and so you get time dilation, which means that your “quantum evolution” slows down. OK, so there are relativistic phenomena in physical space, and quantum analogs in branchial space. But in our models these are all effectively facets of one thing: the multiway causal graph. So are there situations in which the two kinds of phenomena can mix? Normally there aren’t: relativistic phenomena involve large physical scales; quantum phenomena tend to involve small ones. But one example of an extreme situation where they can mix is black holes. I’ve mentioned several times that the formation of an event horizon around a black hole is associated with disconnection in the causal graph. But it’s more than that. It’s actually disconnection not only in the spacetime causal graph, but in the full multiway causal graph. And that means that there’s not only an ordinary causal event horizon—in physical space—but also an “entanglement horizon” in branchial space. And just as a piece of the spatial hypergraph can get disconnected when there’s a black hole, so can a piece of the branchial graph. What does this mean? There are a variety of consequences. One of them is that quantum information can be trapped inside the entanglement horizon even when it hasn’t crossed the causal event horizon—so that in effect the black hole is freezing quantum information “at its surface” (at least its surface in branchial space). It’s a weird phenomenon implied by our models, but what’s perhaps particularly interesting about it is that it’s very much aligned with conclusions about black holes that have emerged in some of the latest work in physics on the so-called holographic principle in quantum field theory and general relativity. Here’s another related, weird phenomenon. If you pass the causal event horizon of a black hole, it’s an inevitable fact that you’ll eventually get infinitely physically elongated (or “spaghettified”) by tidal forces. Well, something similar happens if you pass the entanglement horizon—except now you’ll get elongated in branchial space rather than physical space. And in our models, this eventually means you won’t be able to make a quantum measurement—so in a sense as an observer you won’t be able to “form a classical thought”, or, in other words, beyond the entanglement horizon you’ll never be able to “come to a definite conclusion” about, for example, whether something fell into the black hole or didn’t. The speed of light c is a fundamental physical constant that relates distance in physical space to time. In our models, there’s now a new fundamental physical constant: the maximum entanglement speed, that relates distance in branchial space to time. I call this maximum entanglement speed ζ (zeta) (ζ looks a bit like a “tangled c”). I’m not sure what its value is, but a possible estimate is that it corresponds to entangling about 10102 new quantum states per second. And in a sense the fact that this is so big is why we’re normally able to “form classical thoughts”. Because of the relation between (multiway) causal edges and energy, it’s possible to convert ζ to units of energy per second, and our estimate then implies that ζ is about 105 solar masses per second. It’s a big value, although conceivably not irrelevant to something like a merger of galactic black holes. (And, yes, this would mean that for an intelligence to “quantum grok” our galaxy would take maybe six months..
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.18 Finding the Ultimate Rule : I’m frankly amazed at how much we’ve been able to figure out just from the general structure of our models. But to get a final fundamental theory of physics we’ve still got to find a specific rule. A rule that gives us 3 (or so) dimensions of space, the particular expansion rate of the universe, the particular masses and properties of elementary particles, and so on. But how should we set about finding this rule? And actually even before that, we need to ask: if we had the right rule, would we even know it? As I mentioned earlier, there’s potentially a big problem here with computational irreducibility. Because whatever the underlying rule is, our actual universe has applied it perhaps times. And if there’s computational irreducibility—as there inevitably will be—then there won’t be a way to fundamentally reduce the amount of computational effort that’s needed to determine the outcome of all these rule applications. But what we have to hope is that somehow—even though the complete evolution of the universe is computationally irreducible—there are still enough “tunnels of computational reducibility” that we’ll be able to figure out at least what’s needed to be able to compare with what we know in physics, without having to do all that computational work. And I have to say that our recent success in getting conclusions just from the general structure of our models makes me much more optimistic about this possibility. But, OK, so what rules should we consider? The traditional approach in natural science (at least over the past few centuries) has tended to be: start from what you know about whatever system you’re studying, then try to “reverse engineer” what its rules are. But in our models there’s in a sense too much emergence for this to work. Look at something like this: Given the overall form of this structure, would you ever figure that it could be produced just by the rule: {{x, y, y}, {y, z, u}} → {{u, z, z}, {u, x, v}, {y, u, v}} Having myself explored the computational universe of simple programs for some forty years, I have to say that even now it’s amazing how often I’m humbled by the ability of extremely simple rules to give behavior I never expected. And this is particularly common with the very structureless models we’re using here. So in the end the only real way to find out what can happen in these models is just to enumerate possible rules, and then run them and see what they do. But now there’s a crucial question. If we just start enumerating very simple rules, how far are we going to have to go before we find our universe? Or, put another way, just how simple is the rule for our universe going to end up being? It could have been that in a sense the rule for the universe would have a special case in it for every element of the universe—every particle, every position in space, etc.  But the very fact that we’ve been able to find definite scientific laws—and that systematic physics has even been possible—suggests that the rule at least doesn’t have that level of complexity. But how simple might it be? We don’t know. And I have to say that I don’t think our recent discoveries shed any particular light on this—because they basically say that lots of things in physics are generic, and independent of the specifics of the underlying rule, however simple or complex it may be.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.19 Why This Universe? The Relativity of Rules : But, OK, let’s say we find that our universe can be described by some particular rule. Then the obvious immediate question would be: why that rule, and not another? The history of science—certainly since Copernicus—has shown us over and over again evidence that we’re “not special”. But if the rule we find to describe our universe is simple, wouldn’t that simplicity be a sign of “specialness”? I have long wondered about this. Could it for example be that the rule is only simple because of the way that we, as entities existing in our particular universe, choose to set up our ways of describing things? And that in some other universe, with some other rule, the entities that exist there would set up their ways of describing things so that the rule for their universe is simple to them, even though it might be very complex to us? Or could it be that in some fundamental sense it doesn’t matter what the rules for the universe are: that to observers embedded in a universe, operating according to the same rules as that universe, the conclusions about how the universe works will always be the same? Or could it be that this is a kind of question that’s just outside the realm of science? To my considerable surprise, the paradigm that’s emerging from our recent discoveries potentially seems to suggest a definite—though at first seemingly bizarre—scientific answer. In what we’ve discussed so far we’re imagining that there’s a particular, single rule for our universe, that gets applied over and over again, effectively in all possible ways. But what if there wasn’t just one rule that could be used? What if all conceivable rules could be used? What if every updating event could just use any possible rule? (Notice that in a finite universe, there are only ever finitely many rules that can ever apply.) At first it might not seem as if this setup would ever lead to anything definite. But imagine making a multiway graph of absolutely everything that can happen—including all events for all possible rules. This is a big, complicated object. But far from being structureless, it’s full of all kinds of structure. And there’s one very important thing about it: it’s basically guaranteed to have causal invariance (basically because if there’s a rule that does something, there’s always another rule somewhere that can undo it). So now we can make a rule-space multiway causal graph—which will show a rule-space analog of relativity. And what this means is that in the rule-space multiway graph, we can expect to make different foliations, but have them all give consistent results. It’s a remarkable conceptual unification. We’ve got physical space, branchial space, and now also what we can call rulial space (or just rule space). And the same overall ideas and principles apply to all of them. And just as we defined reference frames in physical space and branchial space, so also we can define reference frames in rulial space. But what kinds of reference frames might observers set up in rulial space? In a typical case we can think of different reference frames in rulial space as corresponding to different description languages in which an observer can describe their experience of the universe. In the abstract, it’s a familiar idea that given any particular description language, we can always explicitly program any universal computer to translate it to another description language. But what we’re saying here is that in rulial space it just takes choosing a different reference frame to have our representation of the universe use a different description language. And roughly the reason this works is that different foliations of rulial space correspond to different choices of sequences of rules in the rule-space multiway graph—which can in effect be set up to “compute” the output that would be obtained with any given description language. That this can work ultimately depends on the fact that sequences of our rules can support universal computation (which the Principle of Computational Equivalence implies they ubiquitously will)—which is in effect why it only takes “choosing a different reference frame in rule space” to “run a different program” and get a different description of the observed behavior of the universe. It’s a strange but rather appealing picture. The universe is effectively using all possible rules. But as entities embedded in the universe, we’re picking a particular foliation (or sequence of reference frames) to make sense of what’s happening. And that choice of foliation corresponds to a description language which gives us our particular way of describing the universe. But what is there to say definitely about the universe—independent of the foliation? There’s one immediate thing: that the universe, whatever foliation one uses to describe it, is just a universal computer, and nothing more. And that hypercomputation is never possible in the universe. But given the structure of our models, there’s more. Just like there’s a maximum speed in physical space (the speed of lightc), and a maximum speed in branchial space (the maximum entanglement speed ζ), so also there must be a maximum speed in rulial space, which we can call ρ—that’s effectively another fundamental constant of nature. (The constancy of ρ is in effect a reflection of the Principle of Computational Equivalence.) But what does moving in rulial space correspond to? Basically it’s a change of rule. And to say that this can only happen at a finite speed is to say that there’s computational irreducibility: that one rule cannot emulate another infinitely fast. And given this finite “speed of emulation” there are “emulation cones” that are the analog of light cones, and that define how far one can get in rulial space in a certain amount of time. What are the units of ρ? Essentially they are program length divided by time. But whereas in the theory of computation one typically imagines that program length can be scaled almost arbitrarily by different models of computation, here this is a measure of program length that’s somehow fundamentally anchored to the structure of the rule-space multiway system, and of physics. (By the way, there’ll be an analog of curvature and Einstein’s equations in rulial space too—and it probably corresponds to a geometrization of computational complexity theory and questions like P?=NP.) There’s more to say about the structure of rulial space. For example, let’s imagine we try to make a foliation in which we freeze time somewhere in rulial space. That’ll correspond to trying to describe the universe using some computationally reducible model—and over time it’ll get more and more difficult to maintain this as emulation cones effectively deliver more and more computational irreducibility. So what does all this mean for our original goal—of finding a rule to describe our universe? Basically it’s saying that any (computation universal) rule will do—if we’re prepared to craft the appropriate description language. But the point is that we’ve basically already defined at least some elements of our description language: they are the kinds of things our senses detect, our measuring devices measure, and our existing physics describes. So now our challenge is to find a rule that successfully describes our universe within this framework. For me this is a very satisfactory solution to the mystery of why some particular rule would be picked for our universe. The answer is that there isn’t ultimately ever a particular rule; basically any rule capable of universal computation will do. It’s just that—with some particular mode of description that we choose to use—there will be some definite rule that describes our universe. And in a sense whatever specialness there is to this rule is just a reflection of the specialness of our mode of description. In effect, the only thing special about the universe to us is us ourselves. And this suggests a definite answer to another longstanding question: could there be other universes? The answer in our setup is basically no. We can’t just “pick another rule and get another universe”. Because in a sense our universe already contains all possible rules, so there can only be one of it. (There could still be other universes that do various levels of hypercomputation.) But there is something perhaps more bizarre that is possible. While we view our universe—and reality—through our particular type of description language, there are endless other possible description languages which can lead to descriptions of reality that will seem coherent (and even in some appropriate definition “meaningful”) within themselves, but which will seem to us to correspond to utterly incoherent and meaningless aspects of our universe. I’ve always assumed that any entity that exists in our universe must at least “experience the same physics as us”. But now I realize that this isn’t true. There’s actually an almost infinite diversity of different ways to describe and experience our universe, or in effect an almost infinite diversity of different “planes of existence” for entities in the universe—corresponding to different possible reference frames in rulial space, all ultimately connected by universal computation and rule-space relativity.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.20 The Challenge of Language Design for the Universe : What does it mean to make a model for the universe? If we just want to know what the universe does, well, then we have the universe, and we can just watch what it does. But when we talk about making a model, what we really mean is that we want to have a representation of the universe that somehow connects it to what we humans can understand. Given computational irreducibility, it’s not that we expect a model that will in any fundamental sense “predict in advance” the precise behavior of the universe down to every detail (like that I am writing this sentence now). But we do want to be able to point to the model—whose structure we understand—and then be able to say that this model corresponds to our universe. In the previous section we said that we wanted to find a rule that we could in a sense connect with the description language that we use for the universe. But what should the description language for the rule itself be? Inevitably there is a great computational distance between the underlying rule and features of the universe that we’re used to describing. So—as I’ve said several times here in different ways—we can’t expect to use the ordinary concepts with which we describe the world (or physics) directly in the construction of the rule. I’ve spent the better part of my life as a language designer, primarily building what’s now the full-scale computational language that is the Wolfram Language.  And I now view the effort to find a fundamental theory of physics as in many ways just another challenge in language design—perhaps even the ultimate such challenge. In designing a computational language what one is really trying to do is to create a bridge between two domains: the abstract world of what is possible to do computationally, and the “mental” world of what people understand and are interested in doing. There are all sorts of computational processes that one can invent (say running randomly picked cellular automaton rules), but the challenge in language design is to figure out which ones people care about at this point in human history, and then to give people a way to describe these. Usually in computational language design one is leveraging human natural language—or the more formal languages that have been developed in mathematics and science—to find words or their analogs to refer to particular “lumps of computation”. But at least in the way I have done it, the essence of language design is to try to find the purest primitives that can be expressed this way. OK, so let’s talk about setting up a model for the universe. Perhaps the single most important idea in my effort to find a fundamental theory of physics is that the theory should be based on the general computational paradigm (and not, for example, specifically on mathematics). So when we talk about having a language in which to describe our model of the universe we can see that it has to bridge three different domains. It has to be a language that humans can understand. It has to be a language that can express computational ideas. And it has to be a language that can actually represent the underlying structure of physics. So what should this language be like? What kinds of primitives should it contain? The history that has led me to what I describe here is in many ways the history of my attempts to formulate an appropriate language. Is it trivalent graphs? Is it ordered graphs? Is it rules applied to abstract relations? In many ways, we are inevitably skating at the edge of what humans can understand. Maybe one day we will have built up familiar ways of talking about the concepts that are involved. But for now, we don’t have these. And in a sense what has made this project feasible now is that we’ve come so far in developing ways to express computational ideas—and that through the Wolfram Language in particular those forms of expression have become familiar, at the very least to me. And it’s certainly satisfying to see that the basic structure of the models we’re using can be expressed very cleanly and succinctly in the Wolfram Language. In fact, in what perhaps can be viewed as some sort of endorsement of the structure of the Wolfram Language, the models are in a sense just a quintessential example of transformation rules for symbolic expressions, which is exactly what the Wolfram Language is based on. But even though the structure is well represented in the Wolfram Language, the “use case” of “running the universe” is different from what the Wolfram Language is normally set up to do. In the effort to serve what people normally want, the Wolfram Language is primarily about taking input, evaluating it by doing computation, and then generating output. But that’s not what the universe does. The universe in a sense had input at the very beginning, but now it’s just running an evaluation—and with all our different ideas of foliations and so on, we are sampling certain aspects of that ongoing evaluation. It’s computation, but it’s computation sampled in a different way than we’ve been used to doing it. To a language designer like me, this is something interesting in its own right, with its own scientific and technological spinoffs. And perhaps it will take more ideas before we can finish the job of finding a way to represent a rule for fundamental physics. But I’m optimistic that we actually already have pretty much all the ideas we need. And we also have a crucial piece of methodology that helps us: our ability to do explorations through computer experiments. If we based everything on the traditional methodology of mathematics, we would in effect only be able to explore what we somehow already understood. But in running computer experiments we are in effect sampling the raw computational universe of possibilities, without being limited by our existing understanding. Of course, as with physical experiments, it matters how we define and think about our experiments, and in effect what description language we use. But what certainly helps me, at least, is that I’ve now been doing computer experiments for more than forty years, and over that time I’ve been able to slowly refine the art and science of how best to do them. In a way it’s very much like how we learn from our experience in the physical world. From seeing the results of many experiments, we gradually build up intuition, which in turn lets us start creating a conceptual framework, which then informs the design of our language for describing things. One always has to keep doing experiments, though. In a sense computational irreducibility implies that there will always be surprises, and that’s certainly what I constantly find in practice, not least in this project. Will we be able to bring together physics, computation and human understanding to deliver what we can reasonably consider to be a final, fundamental theory of physics? It is difficult to know how hard this will be. But I am extremely optimistic that we are finally on the right track, and may even have effectively already solved the fascinating problem of language design that this entails.
Finally We May Have a Path to the Fundamental Theory of Physics… and It’s Beautiful 2.21 Let’s Go Find the Fundamental Theory! : OK, so given all this, what’s it going to take to find the fundamental theory of physics? The most important thing—about which I’m extremely excited—is that I think we’re finally on the right track. Of course, perhaps not surprisingly, it’s still technically difficult. Part of that difficulty comes directly from computational irreducibility and from the difficulty of working out the consequences of underlying rules. But part of the difficulty also comes from the very success and sophistication of existing physics. In the end our goal must be to build a bridge that connects our models to existing knowledge about physics. And there is difficult work to do on both sides. Trying to frame the consequences of our models in terms that align with existing physics, and trying to frame the (usually mathematical) structures of existing physics in terms that align with our models. For me, one of the most satisfying aspects of our discoveries over the past couple of months has been the extent to which they end up resonating with a huge range of existing—sometimes so far seemingly “just mathematical”—directions that have been taken in physics in recent years. It almost seems like everyone has been right all along, and it just takes adding a new substrate to see how it all fits together. There are hints of string theory, holographic principles, causal set theory, loop quantum gravity, twistor theory, and much more. And not only that, there are also modern mathematical ideas—geometric group theory, higher-order category theory, non-commutative geometry, geometric complexity theory, etc.—that seem so well aligned that one might almost think they must have been built to inform the analysis of our models. I have to say I didn’t expect this. The ideas and methods on which our models are based are very different from what’s ever been seriously pursued in physics, or really even in mathematics. But somehow—and I think it’s a good sign all around—what’s emerged is something that aligns wonderfully with lots of recent work in physics and mathematics. The foundations and motivating ideas are different, but the methods (and sometimes even the results) often look to be quite immediately applicable. There’s something else I didn’t expect, but that’s very important. In studying things (like cellular automata) out in the computational universe of simple programs, I have normally found that computational irreducibility—and phenomena like undecidability—are everywhere. Try using sophisticated methods from mathematics; they will almost always fail. It is as if one hits the wall of irreducibility almost immediately, so there is almost nothing for our sophisticated methods, which ultimately rely on reducibility, to do. But perhaps because they are so minimal and so structureless our models for fundamental physics don’t seem to work this way. Yes, there is computational irreducibility, and it’s surely important, both in principle and in practice. But the surprising thing is that there’s a remarkable depth of richness before one hits irreducibility. And indeed that’s where many of our recent discoveries come from. And it’s also where existing methods from physics and mathematics have the potential to make great contributions. But what’s important is that it’s realistic that they can; there’s a lot one can understand before one hits computational irreducibility. (Which is, by the way, presumably why we are fundamentally able to form a coherent view of physical reality at all.) So how is the effort to try to find a fundamental theory of physics going to work in practice? We plan to have a centralized effort that will push forward with the project using essentially the same R&D methods that we’ve developed at Wolfram Research over the past three decades, and that have successfully brought us so much technology—not to mention what exists of this project so far. But we plan to do everything in a completely open way. We’ve already posted the full suite of software tools that we’ve developed, along with nearly a thousand archived working notebooks going back to the 1990s, and soon more than 400 hours of videos of recent working sessions. We want to make it as easy for people to get involved as possible, whether directly in our centralized effort, or in separate efforts of their own. We’ll be livestreaming what we do, and soliciting as much interaction as possible. We’ll be running a variety of educational programs. And we also plan to have (livestreamed) working sessions with other individuals and groups, as well as providing channels for the computational publishing of results and intermediate findings. I have to say that for me, working on this project both now and in past years has been tremendously exciting, satisfying, and really just fun. And I’m hoping many other people will be able to share in this as the project goes forward. I think we’ve finally got a path to finding the fundamental theory of physics. Now let’s go follow that path. Let’s have a blast. And let’s try to make this the time in human history when we finally figure out how this universe of ours works.
The First Two Weeks—Stephen Wolfram Writings 3.1 : First, Thank You! We launched the Wolfram Physics Project two weeks ago, on April 14. And, in a word, wow! People might think that interest in fundamental science has waned.  But the thousands of messages we’ve received tell a very different story. People really care!  They’re excited. They’re enjoying understanding what we’ve figured out. They’re appreciating the elegance of it. They want to support the project. They want to get involved. It’s tremendously encouraging—and motivating.  I wanted this project to be something for the world—and something lots of people could participate in. And it’s working. Our livestreams—even very technical ones—have been exceptionally popular. We’ve had lots of physicists, mathematicians, computer scientists and others asking questions, making suggestions and offering help. We’ve had lots of students and others who tell us how eager they are to get into doing research on the project. And we’ve had lots of people who just want to tell us they appreciate what we’re doing.  So, thank you!.
The First Two Weeks—Stephen Wolfram Writings 3.2 Real-Time Science : Science is usually done behind closed doors. But not this project. This project is an open project where we’re sharing—in as real time as we can—what we’re doing and the tools we’re using. In the last two weeks, we’ve done more than 25 hours of livestreams about the project. We’ve given introductions to the project—both lecture style and Q&A. We’ve done detailed technical sessions. And we’ve started livestreaming our actual working research sessions. And in a couple of those sessions we’ve made the beginnings of some real discoveries—live and in public. It’s pretty cool to see thousands of people joining us to experience real-time science. (Our peak so far was nearly 8000 simultaneous viewers, and a fairly technical 2-hour session ended up being watched for a total of more than three-quarters of a million minutes.) And we’re starting to see serious “public collaboration” happening, in real time. People are making technical suggestions, sending us links to relevant papers, even sending us pieces of Wolfram Language code to run—all in real time. One of the great—and unexpected—things about the project is how well what we’ve discovered seems to dovetail with existing initiatives (like string theory, holographic principles, spin networks, higher categories, twistor theory, etc.) We’re keen to understand more about this, so one of the things we’ll be doing is having livestreamed discussions with experts in these various areas.
The First Two Weeks—Stephen Wolfram Writings 3.3 The Summer School Approaches : It’s only been two weeks since our project was launched—and there’ve already been some interesting things written about it that have helped sharpen my philosophical understanding. There hasn’t yet been time for serious scientific work to have been completed around the project… but we know people are on this path. We also know that there are lots of people who want to get to the point where they can make serious contributions to the project. And to help with that, we’ve got an educational program coming up: we’ve added a Fundamental Physics track to our annual Wolfram Summer School. Our Summer School—which has been running since 2003—is a 3-week program, focused on every participant doing a unique, original project. For the Fundamental Physics track, we’re going to have a “week 0” (June 22–27) that will be lectures and workshops about the Physics Project, followed by a 3-week project-based program (June 28–July 17). This year’s Summer School will (for the first time) be online (though synchronous), so it’s going to be easier for students from around the world to attend.  Many of the students for the Fundamental Physics track will be graduate students or postdocs, but we also expect to have students who are more junior, as well as professors and professionals. Since announcing the program last week, we’ve already received many good applications… but we’re going to try to expand the program to accommodate everyone who makes sense. (So if you’re thinking of applying, please just apply… though do it as soon as you can!) I’m very excited about what’s going to be achieved at the Summer School. I never expected our whole project to develop as well—or as quickly—as it has. But at this point I think we’ve developed an approach and a methodology that are going to make possible rapid progress in many directions. And I’m fully expecting that there’ll be projects at the Summer School that lead, for example, to academic papers that rapidly become classics. This is one of those rare times when there’s a lot of exceptionally juicy low-hanging fruit—and I’m looking forward to helping outstanding students find and pick that scientific fruit at our Summer School.
The First Two Weeks—Stephen Wolfram Writings 3.4 New Science in the First Two Weeks : It’s not too surprising that much of our time in the first two weeks after launching the project has been spent on “interfacing with the world”—explaining what we’re doing, trying to respond to thousands of messages, and setting up internal and external systems that can make future interactions easier. But we’ve been very keen to go on working on the science, and some of that has been happening too.  We’ve so far done five livestreamed working sessions, three on spin and charge, one on the interplay with distributed computing, and one on combinators and physics.  Of course, this is just what we’re directly working on ourselves. We’ve also already helped several people get started on projects that use their expertise—in physics, mathematics or computer science—and it’s wonderful to see the beginning of this kind of “scaling up”. But let me talk a bit about things I think I’ve learned in the past two weeks. Some of this comes from the working sessions we’ve had; some is in response to questions at our Q&As and some is just the result of my slowly growing understanding—particularly helped by my efforts in explaining the project to people.
The First Two Weeks—Stephen Wolfram Writings 3.5 What Is Angular Momentum? : OK, so here’s something concrete that came out of our working session last Thursday: I think we understand what angular momentum is. Here’s part of where we figured that out: We already figured out a few months ago what linear momentum is. If you want to know the amount of linear momentum in a particular direction at a particular place in the hypergraph, you just have to see how much “activity” at that place in the hypergraph is being transferred in that “direction”. Directions are defined by geodesics that give the shortest path between one point and another. Momentum in a particular direction then corresponds to the extent to which an update at one point leads to updates at nearby points along that direction. (More formally, the statement is that momentum is given by the flux of causal edges through timelike hypersurfaces.) OK, so how about angular momentum? Well, it took us a total of nearly 6 hours, over three sessions, but here’s what we figured out. (And kudos to Jonathan Gorard for having had a crucial idea.) So, first, what’s the usual concept of angular momentum in physics? It’s all about turning. It’s all about momentum that doesn’t add up to go in any particular direction but just circulates around. Here’s the picture we used on the livestream: Imagine this is a fluid, like water. The fluid isn’t flowing in a particular direction. Instead, it’s just circulating around, creating a vortex. And this vortex has angular momentum. But what might the analog of this be in a hypergraph? To figure this out, we have to understand what rotation really is. It took us a little while to untangle this, but in the end it’s very simple. In any number of dimensions, a rotation is something that takes two vectors rooted at a particular point, and transforms one into the other. On the livestream, we used the simple example: And in the act of transforming one of these vectors into the other we’re essentially sweeping out a plane. We imagined filling in the plane by making something like a string figure that joins points on the two vectors: But now there’s an easy generalization to the hypergraph. A single geodesic defines a direction. Two geodesics—and the geodesics “strung” between them—define a plane. Here’s what we created to give an illustration of this: So now we are beginning to have a picture of angular momentum: it is “activity” that “circulates around” in this little “patch of plane” defined by two geodesics from a particular point. We can get considerably more formal than this, talking about flux of causal edges in slices of tubes defined by pairs of geodesics. On the livestream, we started relating this to the tensor Jμν which defines relativistic angular momentum (the two indices of Jμν basically correspond to our two geodesics). There are details to clean up, and further to go. (Rotating frames in general relativity? Rotating black holes? Black-hole “no hair” theorems? Etc.) But this was our first real “aha” moment in a public working session. And of course there’s an open archive both of the livestream itself, and the notebook created in it.
The First Two Weeks—Stephen Wolfram Writings 3.6 What about Quantum Angular Momentum and Spin? : One of the reasons I wanted to think about angular momentum was because of quantum mechanics. Unlike ordinary momentum, angular momentum is quantized, even in traditional physics. And, more than that, even supposedly point particles—like electrons—have nonzero quantized spin angular momentum. We don’t yet know how this works in our models. (Stay tuned for future livestreamed working sessions!) But one point is clear: it has to involve not just the spatial hypergraph and the spacetime causal graph (as in our discussion of angular momentum above), but also the multiway causal graph. And that means we’re dealing not just with a single rotation, but a whole collection of interwoven ones. I have a suspicion that the quantization is going to come from something essentially topological. If you’re looking at, say, fluid flow near a vortex, then when you go around a small circle adding up the flow at every point, you’ll get zero if the circle doesn’t include the center of the vortex, and some quantized value if it does (the value will be directly proportional to the number of times you wind around the vortex). Assuming we’ve got a causal-invariant system, one feature of the multiway causal graph is that it must consist of many copies of the same spacetime causal graph—in a sense laid out (albeit with complicated interweaving) in branchial space. And it’s also possible (as Jonathan suggested on the livestream) that somehow when one measures an angular momentum—or a spin—one is effectively picking up just a certain discrete number of “histories”, or a discrete number of identical copies of the spacetime causal graph. But we’ll see. I won’t be surprised if both ideas somehow dovetail together. But maybe we’ll need some completely different idea. Either way, I suspect there’s going to be somewhat sophisticated math involved. We have a guess that the continuum limit of the multiway causal graph is something like a twistor space. So then we might be dealing with homotopy in twistor space—or, more likely, some generalization of that. On the livestream, various people asked about spinors. We ordinarily think of a rotation through 360° as bringing everything back to where it started from.  But in quantum mechanics that’s not how things work. Instead, for something like an electron, it takes a rotation through 720°. And mathematically, that means we’re dealing with so-called spinors, rather than vectors. We don’t yet know how this could come out in our models (though we have some possible ideas)—but this is something we’re planning to explore soon. (It’s again mathematically complicated, because we’re not intrinsically dealing with integer-dimensional space, so we’ve got to generalize the notion of rotation, rotation groups, etc.) And as I write this, I have a new idea—of trying to see how relativistic wave equations (like the Klein–Gordon equation for spin-0 particles or the Dirac equation for spin-1/2 particles) might arise from thinking about bundles of geodesics in the multiway causal graph. The suspicion is that there would be a subtle relationship between effective spacetime dimension and symmetries associated with the bundle of geodesics, mirroring the way that in traditional relativistic quantum mechanics one can identify different spins with objects transforming according to different irreducible representations of the symmetry group of spacetime.
The First Two Weeks—Stephen Wolfram Writings 3.7 CPT Invariance? : Related to the whole story about spinors, there’s a fundamental result in quantum field theory called the spin-statistics theorem that says that particles with half-integer spins (like electrons) are fermions (and so obey the exclusion principle), while particles with integer spins (like photons) are bosons (and so can form condensates). And this in turn is related to what’s called CPT invariance. And one of the things that came out of a livestream last week is that there’s potentially a very beautiful interpretation of CPT invariance in our models. What is CPT invariance? C, P and T correspond to three potential transformations applied to physical systems. T is time reversal, i.e. having time run in reverse. P is parity, or space inversion: reversing the sign of all spatial coordinates. And C is charge conjugation: turning particles (like electrons) into antiparticles (like positrons). One might think that the laws of physics would be invariant under any of these transformations. But in fact, each of C, P and T invariance is violated somewhere in particle physics (and this fact was a favorite of mine back when I did particle physics for a living). However, the standard formalism of quantum field theory implies that there is still invariance under the combined CPT transformation—and, so far as one can tell, this is experimentally correct. OK, so what do C, P and T correspond to in our models? Consider the multiway causal graph. Here’s a toy version of it, that we discussed in a livestream last week: Edges in one direction (say, down) correspond to time. Edges in another direction correspond to space. And edges in the third direction correspond to branchial space (i.e. the space of quantum spaces). T and P then have simple interpretations: they correspond to reversing time edges and space edges, respectively. C is a little less clear, but we suspect that it just corresponds to reversing branchial edges (and this very correspondence probably tells us something about the nature of antiparticles). So then CPT is like a wholesale inversion of the multiway causal graph. But what can we say about this? Well, we’ve argued that (with certain assumptions) spacetime slices of the multiway causal graph must obey the Einstein equations. Similarly, we’ve argued that branchtime slices follow the Feynman path integral. But now there’s a generalization of both these things: in effect, a generalization of the Einstein equations that applies to the whole multiway causal graph. It’s mathematically complicated—because it must describe the combined geometry of physical and branchial space. But it looks as if CPT invariance must just correspond to a symmetry of this generalized equation. And to me this is something very beautiful—that I can hardly wait to investigate more.
The First Two Weeks—Stephen Wolfram Writings 3.8 What’s Going On in Quantum Computing? : One feature of our models is that they potentially make it a lot more concrete what’s going on in quantum computing. And over the past couple of weeks we’ve started to think about what this really means. There are two basic points. First, the multiway graph provides a very explicit representation of “quantum indeterminacy”. And, second, thinking about branchial space (and quantum observation frames) gives more concreteness to the notion of quantum measurement. A classical computer like an ordinary Turing machine is effectively just following one deterministic path of evolution. But the qualitative picture of a quantum computer is that instead it’s simultaneously following many paths of evolution, so that in effect it can do many Turing-machine-like computations in parallel. But at the end, there’s always the issue of finding which path or paths have the answer you want: and in effect you have to arrange your measurement to just pick out these paths. In ordinary Turing machines, there are problems (like multiplying numbers) that are in the class P, meaning that they can be done a number of steps polynomial in the size of the problem (say, the number of digits in the numbers). There are also problems (like factoring numbers) that are in the class NP, meaning that if you were to “non-deterministically” guess the answer, you could check it in polynomial time. A core question in theoretical computing science (which I have views on, but won’t discuss here) is whether P=NP, that is, whether all NP problems can actually be done in polynomial time. One way to imagine doing an NP problem in polynomial time is not to use an ordinary Turing machine, but instead to use a “non-deterministic Turing machine” in which there is a tree of possible paths where one can pick any path to follow. Well, our multiway system representing quantum mechanics essentially gives that whole tree (though causal invariance implies that ultimately the branches always merge). For the last several years, we’ve been developing a framework for quantum computing in the Wolfram Language (which we’re hoping to release soon). And in this framework we’re essentially describing two things: how quantum information is propagated with time through some series of quantum operations, and how the results of quantum processes are measured. More formally, we have time evolution operators, and we have measurement operators. Well, here’s the first neat thing we’ve realized: we can immediately reformulate our quantum computing framework directly in terms of multiway systems. The quantum computing framework can in effect just be viewed as an application of our MultiwaySystem function that we put in the Wolfram Function Repository for the Physics Project. But now that we’re thinking in terms of multiway systems—or the multiway causal graph—we realize that standard quantum operations are effectively associated with timelike causal edges, while measurement operations are associated with branchlike causal edges. And the extent to which one can get answers before decoherence takes over has to do with a competition between these kinds of edges. This is all very much in progress right now, but in the next few weeks we’re expecting to be able to look at well-known quantum algorithms in this context, and see whether we can analyze them in a way that treats time evolution and measurement on a common footing. (I will say that ever since the work I did with Richard Feynman on quantum computing back in the early 1980s, I have always wanted to really understand the “cost of measurement”, and I’m hoping that we’ll finally be able to do that now..
The First Two Weeks—Stephen Wolfram Writings 3.9 Numerical Relativity; Numerical Quantum Field Theory : Although the traditional view in physics is that space and time are continuous, when it comes to doing actual computer simulations they usually in the end have to be discretized. And in general relativity (say for simulating a black hole merger) it’s usually a very subtle business, in which the details of the discretization are hard to keep track of, and hard to keep consistent. In our models, of course, discretization is not something “imposed after the fact”, but rather something completely intrinsic to the model. So we started wondering whether somehow this could be used in practice to set up simulations. It’s actually a very analogous idea to something I did rather successfully in the mid-1980s for fluid flow. In fluids, as in general relativity, there’s a traditional continuum description, and the most obvious way of doing simulations is by discretizing this. But what I did instead was to start with an idealized model of discrete molecules—and then to simulate lots of these molecules. My interest was to understand the fundamental origins of things like randomness in fluid turbulence, but variants of the method I invented have now become a standard approach to fluid simulation. So can one do something similar with general relativity? The actual “hypergraph of the universe” would be on much too tiny a scale for it to be directly useful for simulations. But the point is that even on a much larger scale our models can still approximate general relativity—but unlike “imposed after the fact” discretization, they are guaranteed to have a certain internal consistency. In usual approaches to “numerical relativity” one of the most difficult things is dealing with progressive “time evolution”, not least because of arbitrariness in what coordinates one should use for “space” and “time”. But in our models there’s a way of avoiding this and directly getting a discrete structure that can be used for simulation: just look at the spacetime causal graph. There are lots of details, but—just like in the fluid flow case—I expect many of them won’t matter. For example, just like lots of rules for discrete molecules yield the same limiting thermodynamic behavior, I expect lots of rules for the updating events that give the causal graph will yield the same limiting spacetime structure. (Like in standard numerical analysis, though, different rules may have different efficiency and show different pathologies.) It so happens that Jonathan Gorard’s “day job” has centered around numerical relativity, so he was particularly keen to give this a try. But even though we thought we had just started talking about the idea in the last couple of weeks, Jonathan noticed that actually it was already there on page 1053 of A New Kind of Science—and had been languishing for nearly 20 years! Still, we immediately started thinking about going further. Beyond general relativity, what about quantum field theory? Things like lattice gauge theory typically involve replacing path integrals by “thermal averages”—or effectively operating in Euclidean rather than Minkowski spacetime. But in our models, we potentially get the actual path integral as a limit of the behavior of geodesics in a multiway graph. Usually it’s been difficult to get a consistent “after the fact” discretization of the path integral; but now it’s something that emerges from our models. We haven’t tried it yet (and someone should!). But independent of nailing down precisely what’s ultimately underneath quantum field theory it seems like the very structure of our models has a good chance of being very helpful just in dealing in practice with quantum field theory as we already know it.
The First Two Weeks—Stephen Wolfram Writings 3.10 Surprise: It’s Not Just about Physics : One of the big surprises of the past two weeks has been our increasing realization that the formalism and framework we’re developing really aren’t just relevant to physics; they’re potentially very important elsewhere too. In a sense this shouldn’t be too surprising. After all, our models were constructed to be as minimal and structureless as possible. They don’t have anything intrinsically about physics in them. So there’s no reason they can’t apply to other things too. But there’s a critical point here: if a model is simple enough, one can expect that it could somehow be a foundation for many different kinds of things. Long ago I found that with the 1D cellular automata I studied. The 256 “elementary” cellular automata are in a sense the very simplest models of a completely discrete system with a definite arrangement of neighbors. And over the years essentially all of these 256 cellular automata found uses as models for bizarrely different things (pigmentation, catalysis, traffic, vision, etc.). Well, our models now are in a sense the most minimal that describe systems with rules based on arbitrary relationships (as represented by collections of relations). And the first big place where it seems the models can be applied is in distributed computing. What is distributed computing? Essentially it’s about having a whole collection of computing elements that are communicating with others to collectively perform a computation. In the simplest setup, one just assumes that all the computing elements are operating in lockstep—like in a cellular automaton. But what if the computing elements are instead operating asynchronously, sending data to each other when it happens to be ready? Well, this setup immediately seems a lot more like the situation we have in our models—or in physics—where different updates can happen in any order, subject only to following the causal relationships defined by the causal graph. But now there start to be interesting analogies between the distributed computing case and physics. And indeed what’s got me excited is that I think there’s going to be a very fruitful interplay between these areas. Ideas in distributed computing are going to be useful for thinking about physics—and vice versa. I’m guessing that phenomena and results in distributed computing are going to have direct analogs in general relativity and in quantum mechanics. (“A livelock is like a closed timelike curve”, etc.) And that ideas from physics in the context of our models are going to give one new ways to think about distributed computing. (Imagine “programming in a particular reference frame”, etc.) In applying our models to physics, a central idea is causal invariance. And this has an immediate analog in distributed computing: it’s the idea of eventual consistency, or in other words that it doesn’t matter what order operations are done in; the final result is always the same. But here’s something from physics: our universe (fortunately!) doesn’t seem like it’s going to halt with a definite “final result”. Instead, it’s just continually evolving, but with causal invariance implying various kinds of local equivalence and consistency. And indeed many modern distributed computing systems are again “just running” without getting to “final results” (think: the internet, or a blockchain). Well, in our approach to physics the way we handle this is to think in terms of foliations and reference frames—which provide a way to organize and understand what’s going on. And I think it’s going to be possible to think about distributed computing in the same kind of way. We need some kind of “calculus of reference frames” in terms of which we can define good distributed computing primitives. In physics, reference frames are most familiar in relativity. The most straightforward are inertial frames. But in general relativity there’s been slow but progressive understanding of other kinds of frames. And in our models we’re also led to think about “quantum observation frames”, which are essentially reference frames in the branchial space of quantum states. Realistically, at least for me, it’s so far quite difficult to wrap one’s head around these various kinds of reference frames. But I think in many ways this is at its root a language design problem. Because if we had a good way to talk about working with reference frames we’d be able to use them in distributed computing and so we’d get familiar with them. And then we’d be able to import our understanding to physics. One of the most notable features of our models for physics when it comes to distributed computing is the notion of multiway evolution. Usually in distributed computing one’s interested in looking at a few paths, and making sure that, for example, nothing bad can happen as a result of different orders of execution. But in multiway systems we’re not just looking at a few paths; we’re looking at all paths. And in our models this isn’t just some kind of theoretical concept; it’s the whole basis for quantum mechanics. And given that we’re looking at all paths, we’re led to invent things like quantum observation frames, and branchial space. We can think of the branching of paths in the multiway system as corresponding to elementary pieces of ambiguity. And in a sense the handling of our model—and the features of physics that emerge—is about having ways to deal with “ambiguity in bulk”. Is there an analog of the Feynman path integral in distributed computing? I expect so—and I wouldn’t be surprised if it’s very useful in giving us a way to organize our thinking and our programs. In theoretical analyses of distributed computing, one usually ignores physical space—and the speed of light. But with our models, it’s going to be possible to account for such things, alongside branchial connections, which are more like “instantaneous network connections”. And, for example, there’ll be analogs of time dilation associated with motion in both physical space and branchial space. (Maybe such effects are already known in distributed computing; I’m not sure.) I think the correspondence between distributed computing and physics in the context of our models is going to be incredibly fertile. We already did one livestreamed working session about it (with Taliesin Beynon as a guest); we’ll be doing more. In the working session we had, we started off discussing vector clocks in distributed computing, and realized that they’re the analog of geodesic normal coordinates in physics. Then we went on to discuss more of the translation dictionary between distributed computing and physics. We realized that race conditions correspond to branch pairs. The branchial graph defines sibling tasks. Reads and writes are just incoming and outgoing causal edges. We invented the idea of a “causal exclusion graph”, which is a kind of complement of a causal graph, saying not what events can follow a given event, but rather what events can’t follow a given event. We started discussing applications. Like clustered databases, multiplayer games and trading in markets. We talked about things like Git, where merge conflicts are like violations of causal invariances. We talked a bit about blockchains—but it seemed like there were richer analogs in hashgraphs and things like NKN and IOTA. Consensus somehow seemed to be the analog of “classicality”, but then there’s the question of how much can be achieved in the “quantum regime”. Although for me the notion of seriously using ideas from physics to think about distributed computing is basically less than two weeks old, I’ve personally been wondering about how to do programming for distributed computing for a very long time. Back in the mid-1980s, for example, when I was helping a company (Thinking Machines Corporation) that was building a 65536-processor computer (the Connection Machine), I thought the most plausible way to do programming on such a system would be through none other than graph rewriting. But at the time I just couldn’t figure out how to organize such programming so that programmers could understand what was going on. But now—through thinking about physics—I’m pretty sure there’s going to be a way. We’re already used to the idea (at least in the Wolfram Language) that we can write a program functionally, procedurally, declaratively, etc. I think there are going to be ways to write distributed programs “in different reference frames”. It’s probably going to be more structured and more parametrized than these different traditional styles of programming. But basically it’ll be a framework for looking at a given program in different ways, and using different foliations to understand and describe what it’s supposed to do. I have to mention one more issue that’s been bugging me since 1979. It has to do with recursive evaluation. Imagine we’ve defined a Fibonacci recursion: In my Mathematica-precursor system SMP, I tried to parametrize this behavior, but realistically nobody understood it. So my question now is: given the idea of reference frames, can we invent some kind of notion of “evaluation fronts” that can be described like foliations, and that define the order of recursive evaluation? An extreme case of this arises in evaluating S, K combinators. Even though S, K combinators are 100 years old this year, they remain extremely hard to systematically wrap one’s head around. And part of the reason has to do with evaluation orders. It’s fine when one manages to get a combinator expression that can successfully be evaluated (through some path) to a fixed point. But what about one that just keeps “evolving” as you try to evaluate it? There doesn’t seem to be any good formalism for handling that. But I think our physics-based approach may finally deliver this. So, OK, the models that we invented for physics also seem highly relevant for distributed computing. But what about for other things? Already we’ve thought about two other—completely different—potential applications. It’s Not Just about Physics: So, OK, the models that we invented for physics also seem highly relevant for distributed computing. But what about for other things? Already we’ve thought about two other—completely different—potential applications. : The first, that we actually discussed a bit even the week before the Physics Project was launched, has to do with digital contact tracing in the context of the current pandemic. The basic idea—that we discussed in a livestreamed brainstorming session—is that as people move around with their cellphones, Bluetooth or other transactions can say when two phones are nearby. But the graph of what phones were close to what phones can be thought of as being like a causal graph. And now the question of whether different people might have been close enough in space and time for contagion becomes one of reconstructing spatial graphs by making plausible foliations of the causal graph. There are bigger practical problems to solve in digital contact tracing, but assuming these are solved, the issues that can be informed by our models are likely to become important. (By the way, given a network of contacts, the spreading of a contagious disease on it can be thought of as directly analogous to the growth of a geodesic ball in it.) One last thing that’s still just a vague idea is to apply our models to develop a more abstract approach to biological evolution and natural selection (both for the overall tree of life, and for microorganisms and tumors). Why might there be a connection? The details aren’t yet clear. Perhaps something like the multiway graph (or rule-space multiway graph) can be used to represent the set of all possible sequences of genetic variations. Maybe there’s some way of thinking about the genotype-phenotype correspondence in terms of the correspondence between multiway graphs and causal graphs. Maybe different sequences of “environments” correspond to different foliations, sampling different parts of the possible sequence of genetic variations. Maybe speciation has some correspondence with event horizons. Most likely there’ll need to be some other layer or variation on the models to make them work. But I have a feeling that something is going to be possible. It’s been possible for a long time to make “aggregated” models of biological evolution, where one’s looking at total numbers of organisms of some particular type (with essentially the direct analog of differential-equation-based aggregated epidemiological models). But at a more individual-organism level one’s typically been reduced to doing simulations, which tend to have messy issues like just how many “almost fittest” organisms should be kept at every “step” of natural selection. It could be that the whole problem is mired in computational irreducibility. But the robust way in which one seems to be able to reason in terms of natural selection suggests to me that—like in physics—there’s some layer of computational reducibility, and one just has to find the right concepts to be able to develop a more general theory on the basis of it. And maybe the models we’ve invented for physics give us the framework to do this.
The First Two Weeks—Stephen Wolfram Writings 3.11 Some Coming Attractions : We’re at a very exciting point—where there are an incredible amount of “obvious directions” to go. But here are a few that we’re planning on exploring in the next few days, in our livestreamed working sessions.
The First Two Weeks—Stephen Wolfram Writings 3.12 The Fine Structure of Black Holes : In traditional continuum general relativity it always seems a bit shocking when there’s some kind of discontinuity in the structure of spacetime. In our fundamentally discrete model it’s a bit less shocking, and in fact things like black holes (and other kinds of spacetime singularities) seem to arise very naturally in our models. But what exactly are black holes like in our models? Do they have the same kind of “no hair” perfection as in general relativity—where only global properties like mass and angular momentum affect how they ultimately look from outside? And how do our black holes generate things like Hawking radiation? In a livestream last week, we generated a very toy version of a black hole, with a causal graph of the form: This “black hole” has the feature that causal edges go into it, but none come out. In other words, things can affect the black hole, but the black hole can’t causally affect anything else. It’s the right basic idea, but there’s a lot missing from the toy version, which isn’t surprising, not least because it’s based on a simple string substitution system, and not even a hypergraph. What we now need to do is to find more realistic examples. Then what we’re expecting is that it’ll actually be fairly obvious that the black hole only has certain properties. The mass will presumably relate to the number of causal edges that go into the black hole. And now that we have an idea what angular momentum is, we should be able to identify how much of that is going in as well. And maybe we’ll be able to see that there’s a limit on the amount of angular momentum a black hole of a given mass can have (as there seems to be in general relativity). Some features of black holes we should be able to see by looking at ordinary spacetime causal graphs. But to understand Hawking radiation we’re undoubtedly also going to have to look at multiway causal graphs. And we’re hoping that we’ll actually be able to explicitly see the presence of both the causal event horizon and the entanglement event horizon—so that we’ll be able to trace the fate of quantum information in the “life cycle” of the black hole.
The First Two Weeks—Stephen Wolfram Writings 3.13 All the Spookiness of Quantum Mechanics : Quantum mechanics is notorious for yielding strange phenomena that can be computed within its formalism, but which seem essentially impossible to account for in any other way. Our models, however, finally provide a definite suggestion for what is “underneath” quantum mechanics—and from our models we’ve already been able to derive many of the most prominent phenomena in quantum mechanics. But there are plenty more phenomena to consider, and we’re planning to look at this in working sessions starting later this week. One notable phenomenon that we’ll be looking at is the violation of Bell’s inequality—which is often said to “prove” that no “deterministic” theory can reproduce the predictions of quantum mechanics. Of course, our theory isn’t “deterministic” in the usual sense. Yes, the whole multiway graph is entirely determined by the underlying rule. But what we observe depends on measurements that sample collections of branches determined by the quantum observation frames we choose. But we’d still like to see explicitly how Bell’s inequality is violated—and in fact we suspect that in our multiway graph formalism it’ll be much more straightforward to see how this and its various generalizations work. But we’ll see. In Q&A sessions that we’ve done, and messages that we’ve received, there’ve been many requests to reproduce a classic quantum result: interference in the double-slit experiment. A few months ago, I would have been very pessimistic about being able to do this. I would have thought that first we’d have to understand exactly what particles are, and then we’d only slowly be able to build up something we could consider a realistic “double slit”. But one of the many surprises has been that quantum phenomena seem much more robust than I expected—and it seems possible to reproduce their essential features without putting all the details in. So maybe we’ll be able, for example, just to look at a multiway system generated by a string substitution system, and already be able to see something like interference fringes in an idealized double-slit experiment. We’ll see. When we’re talking about quantum mechanics, many important practical phenomena arise from looking at bound states where for example some particle is restricted to a limited region (like an electron in a hydrogen atom), and we’re interested in various time-repeating eigenstates. My first instinct—as in the case of the double-slit experiment—was to think that studying bound states in our models would be very complicated. After all, at some level, bound states are a limiting idealization, and even in quantum field theory (or with quantum mechanics formulated in terms of path integrals) they’re already a complicated concept. But actually, it seems as if it may be possible to capture the essence of what’s going on in bound states with even very simple toy examples in our models—in which for instance there are just cycles in the multiway graph. But we need to see just how this works, and how far we can get, say in reproducing the features of the harmonic oscillator in quantum mechanics. In traditional treatments of quantum mechanics, the harmonic oscillator is the kind of thing one starts with. But in our models its properties have to be emergent, and it’ll be interesting to see just how “close to the foundations” or how generic their derivation will be able to be.
The First Two Weeks—Stephen Wolfram Writings 3.14 People Do Care about Thermodynamics : Understanding the Second Law of thermodynamics was one of the things that first got me interested in fundamental physics, nearly 50 years ago. And I was very pleased that by the 1990s I thought I finally understood how the Second Law works: basically it’s a consequence of computational irreducibility, and the fact that even if the underlying rules for a system are reversible, they can still so “encrypt” information about the initial conditions that no computationally limited observer can expect to recover it. This phenomenon is ultimately crucial to the derivation of continuum behavior in our models—both for spacetime and for quantum mechanics. (It’s also critical to my old derivation of fluid behavior from idealized discrete underlying molecules.) The Second Law was big news at the end of the 1800s and into the early 1900s. But I have to say that I thought people had (unfortunately) by now rather lost interest in it, and it had just become one of those things that everyone implicitly assumes is true, even though if pressed they’re not quite sure why. So in the last couple of weeks I’ve been surprised to see so many people asking us whether we’ve managed to understand the Second Law. Well, the answer is “Yes!”. And in a sense the understanding is at an even more fundamental level than our models: it’s generic to the whole idea of computational models that follow the Principle of Computational Equivalence and exhibit computational irreducibility. Or, put another way, once everything is considered to be computational, including both systems and observers, the Second Law is basically inevitable. But just where are its limits, and what are the precise mathematical conditions for its validity? And how, for example, does it relate in detail to gravity? (Presumably the reference frames that can be set up are limited by the computational capabilities of observers, which must be compared to the computations being done in the actual evolution of spacetime.) These are things I’ve long wanted to clarify, and I’m hoping we’ll look at these things soon.
The First Two Weeks—Stephen Wolfram Writings 3.15 What about Peer Review and All That? : There’s a lot in our Physics Project. New ideas. New methods. New conclusions. And it’s not easy to deliver such a thing to the world. We’ve worked hard the last few months to write the best expositions we can, and to make software tools that let anyone reproduce—and extend—everything we’ve done. But the fact remains that to seriously absorb what we just put into the world is going to take significant effort. It isn’t the way science usually works. Most of the time, progress is slow, with new results trickling out, and consensus about them gradually forming. And in fact—until a few months ago—that’s exactly how I expected things would go with our Physics Project. But—as I explained in my announcement—that’s not how it worked out. Because, to my great surprise, once we started seriously working on the ideas I originally hatched 30 years ago we suddenly discovered that we could make dramatic progress. And even though we were keen to open the project up, even the things we discovered—together with their background ideas and methods—are a lot to explain, and, for example, fill well over 800 pages. But how does that fit into the normal, academic way of doing science? It’s not a great fit. When we launched the project two weeks ago, I sent mail to a number of people. A historian of science I’ve known for a long time responded: Please remember as you go forward that, many protestations to the contrary, most scientists hate originality, which feels strange, uncomfortable, and baffling. They like novelty well within the boundaries of what they’re doing and the approach that they’re taking, but originality is harder for them to grasp. Therefore expect opposition based on incomprehension rather than reasoned disagreement. Hold fast. My knowledge of history, and my own past experiences, tell me that there’s a lot of truth to this. Although I’m happy to say that in the case of our project it seems like there are actually a very good number of scientists who are enthusiastically making the effort to understand what we’ve done. Of course, there are people who think “This isn’t the way science usually works; something must be wrong”.  And the biggest focus seems to be around “What about peer review?”. Well, that’s an interesting question. What’s ultimately the point of peer review? Basically it’s that people want external certification that something is correct—before they go to the effort of understanding it themselves, or start building on it. And that’s a reasonable thing to want. But how should it actually work? When I used to publish academic papers in the 1970s and early 1980s I quickly discovered something disappointing about actual peer review—that closely mirrors what my historian-of-science friend said. If a paper of mine was novel though not particularly original, it sailed right through peer review. But if it was actually original (and those are the papers that have had the most impact in the end) it essentially always ran into trouble with peer review. I think there’s also always been skullduggery with anonymous peer review—often beyond my simplified “natural selection” model: “If paper cites reviewer, accept; otherwise reject”. But particularly for people outside of science, it’s convenient to at least imagine that there’s some perfect standard of academic validity out there. I haven’t published an ordinary academic paper since 1986, but I was rather excited two weeks ago to upload my first-ever paper to arXiv. I was surprised it took about a week to get posted, and I was thinking it might have run into some filter that blocks any paper about a fundamental theory of physics—on the “Bayesian” grounds that there’s never been a meaningful paper with such a claim during the time arXiv has been operating. But my friend Paul Ginsparg (founder of arXiv) tells me there’s nothing like that in place; it’s just a question of deciding on categories and handling hundreds of megabytes of data. OK, but is there a good way to achieve the objectives of peer review for our project? I was hoping I could submit my paper to some academic journal and then leave it to the journal to just “run the peer-review process”. But on its own, it doesn’t seem like it could work. And in particular, it’s hard to imagine that in the normal course of peer reviewing there could be serious traditional peer review on a 450-page document like this that would get done in less than several years. So over the past week we’ve been thinking about additional, faster things we can do (and, yes, we’ve also been talking to people to get “peer reviews” of possible peer-review processes, and even going to another meta level). Here’s what we’ve come up with. It’s based on the increasingly popular concept of “post-publication peer review”. The idea is to have an open process, where people comment on our papers, and all relevant comments and comments-on-comments, etc. are openly available on the web. We’re trying—albeit imperfectly—to get the best aspects of peer review, and to do it as quickly as possible. Among other things, what we’re hoping is that people will say what they can “certify” and what they cannot: “I understand this, but don’t have anything to say about that”. We’re fully expecting people will sometimes say “I don’t understand this” or “I don’t think that is correct”. Then it’s up to us to answer, and hopefully before long consensus will be reached. No doubt people will point out errors and limitations (including “you should also refer to so-and-so”)—and we look forward to using this input to make everything as good as possible. (Thanks, by the way, to those who’ve already pointed out typos and other mistakes; much appreciated, and hopefully now all fixed.) One challenge about open post-publication peer review is who will review the reviewers. Here’s what we’ve set up. First, every reviewer gives information about themselves, and we validate that the person posting is who they say they are. Then we ask the reviewer to fill out certain computable facts about themselves. (Academic affiliation? PhD in physics? Something else? Professor? Published on arXiv? ISI highly cited author? Etc.) Then when people look at the reviews, they can filter by these computable facts, essentially deciding for themselves how they want to “review the reviewers”. I’m optimistic that this will work well, and will perhaps provide a model for review processes for other things. And as I write this, I can’t help noticing that it’s rather closely related to work we’ve done on validating facts for computable contracts, as well as to the ideas that came up in my testimony last summer for the US Senate about “ranking providers” for automated content selection on the internet.
The First Two Weeks—Stephen Wolfram Writings 3.16 Can We Explain the Project to Kids? : The way I know I really understand something is when I can explain it absolutely from the ground up. So one of the things I was pleased to do a week or so ago was to try to explain our fundamental theory of physics on a livestream aimed at kids, assuming essentially no prior knowledge. How does one explain discrete space? I decided to start by talking about pixels on a screen. How about networks? Who’s friends with whom. Dimension? Look at 2×2×2… grid graphs. Etc. I thought I managed to get decently far, talking about general relativity, and even quantum mechanics, all, I hope, without relying on more than extremely everyday knowledge. And particularly since my livestream seemed to get good reviews from both kids and others, I’m planning in the next week or two to put together a written version of this as a kind of “very elementary” introduction to our project.
The First Two Weeks—Stephen Wolfram Writings 3.17 Project Q&A : Thousands of people have been asking us questions about our project.  But fortunately, many of the questions have been the same. And over the last couple of weeks we’ve been progressively expanding the Q&A section of the project website to try to address the most common of the questions.
The First Two Weeks—Stephen Wolfram Writings 3.18 Visual Gallery : In addition to being (we hope) very interesting from a scientific point of view, our models also produce interesting visual forms. And we’ve started to assemble a “Visual Gallery” of these forms. They can be screen backgrounds, or Zoom backgrounds. Or they can be turned into stickers or T-shirt designs (or put on mouse pads, if people other than me still use those). We’ll be adding lots more items to the Visual Gallery. But it won’t just be pictures. We’ll also be adding 3D geometry for rendering of graphs and hypergraphs. In principle, this 3D geometry should let one immediately 3D print “universes”. But so far we’ve had difficulty doing this. It seems as if unless we thicken up the connections to the point where they merge into each other, it’s not possible to get enough structural integrity to successfully make a 3D printout with existing technologies. But there’s undoubtedly a solution to this, and we’re hoping someone will figure it out, say using our Wolfram Language computational geometry capabilities. It’s pretty difficult (at least for me) to “understand” the structure of the graphs and hypergraphs we’re generating. And ever since I started thinking about network models for physics in the 1990s, I’ve wanted to try to use VR to do this. Well, we’re just starting to have a system that lets one interactively manipulate graphs in 3D in VR. We’ll be posting the code soon, and we hope other people will help add features. But it’s getting closer.
The First Two Weeks—Stephen Wolfram Writings 3.19 It’s an Exciting Time… : This piece is already quite long, but there’s even much more I could say. It’s very exciting to be seeing all this activity around our Physics Project, after only two weeks. There’s a lot to do in the project, and with the project. This is a time of great opportunity, where all sorts of discoveries are ripe to be made. And I’m certainly enjoying trying to figure out more with our models—and trying to understand all sorts of things I’ve wondered about for nearly half a century. But for me it’s been particularly wonderful to see so many other people engaging with the project. I personally think physics is great. And I really love the elegance of what’s emerging from our models. But right now what’s most important to me is what a tremendous pleasure it is to share all this with such a broad spectrum of people. I’m looking forward to seeing what the next few weeks bring. We’re off to a really great start….
Event Horizons, Singularities and Other Exotic Spacetime Phenomena 4.1 The Structure and Pathologies of Spacetime : In our models, space emerges as the large-scale limit of our spatial hypergraph, while spacetime effectively emerges as the large-scale limit of the causal graph that represents causal relationships between updating events in the spatial hypergraph. An important result is that (subject to various assumptions) there is a continuum limit in which the emergent spacetime follows Einstein’s equations from general relativity. And given this, it is natural to ask what happens in our models with some of the notable phenomena from general relativity, such as black holes, event horizons and spacetime singularities. I already discussed this to some extent in my technical introduction to our models. My purpose here is to go further, both in more completely understanding the correspondence with general relativity, and in seeing what additional or different phenomena arise in our models. It should be said at the outset that even after more than 100 years the whole issue of what weird features of spacetime Einstein’s equations can imply remains a rather confusing subject, that is still very far from completely understood. And in some ways I think our models may help clarify what’s going on. Yes, there’s complexity in taking large-scale limits in our models. But unlike with the Einstein equations, which effectively just tell one to “find a spacetime that satisfies certain constraints defined by the equations”, our models give a direct computational way to determine the structure that is supposed to limit to spacetime. In what follows, we’ll see that our models can reproduce known implications of Einstein’s equations such as black holes, event horizons and spacetime singularities. But they also suggest the possibility of other, in some ways yet more exotic, spacetime phenomena. Some of these may in fact be things that could be found in the Einstein equations; others require descriptions of spacetime more general than the mathematical structure of general relativity can provide (notably in connection with dimension and topology change). There’s long been a view that at small enough length scales the Einstein equations will have to be supplemented by some lower-level description of spacetime, presumably connected to quantum mechanics. Our models immediately provide a lower-level representation for spacetime in terms of the discrete structure of the spatial hypergraph and the causal graph—with familiar, continuum spacetime emerging as a large-scale limit, much like continuum fluid behavior emerges as a large-scale limit of molecular dynamics. And there is already a lot that can be said about the structure and behavior of spacetime in our models just on the basis of the spatial hypergraph and the causal graph. But the models also have the feature that they inexorably involve quantum mechanics as a result of the multiway systems obtained by following different possible sequences of updating events. And this means that we can expect to see how various extreme features of spacetime play out at a quantum level, in particular through the multiway causal graph, which includes not only connections associated with ordinary spacetime, but also with branchtime and the branchial space of quantum entanglements. I won’t go far in this particular bulletin into the exploration of things like the quantum mechanics of black holes, but we’ll get at least an idea of how these kinds of things can be investigated in our models. In addition to seeing the correspondence with what are now standard questions in mathematical physics, we’ll begin to see some indications of more exotic ideas, like the possibility that in our models particles like electrons may correspond to a kind of generalization of black holes.
Event Horizons, Singularities and Other Exotic Spacetime Phenomena 4.2 What General Relativity Says : Perhaps the most famous prediction of general relativity is the existence of black holes. At a mathematical level, a black hole is characterized by the presence of an event horizon that prevents events occurring inside it from affecting ones outside (though events occurring outside can affect what’s inside). So how does this actually occur in general relativity? Let’s talk about the simplest nontrivial case: the Schwarzschild solution to Einstein’s equations. Physically, the Schwarzschild solution gives the gravitational field outside a spherically symmetric mass. But there’s a wild thing that happens if the mass is high enough and localized enough: one gets a black hole—with an event horizon. The physical story one can tell is that the escape velocity is larger than the speed of light, so nothing can escape. The mathematical story is more subtle and complicated, and took many years to untangle. But in the end there’s a clear description of the event horizon in terms of the causal structure of spacetime: of what events can causally affect what other ones. But what about the actual Schwarzschild solution? An important simplifying feature is that it’s a solution to Einstein’s equation in the vacuum, not really in any place where there’s actually mass present. Yes, there’s mass that produces the gravitational field. But the Schwarzschild solution just describes the form of the gravitational field outside of the mass. What about in the black hole case? In a physical black hole created by a collapsing star, there are presumably remnants of the star inside the event horizon. But that’s not what the Schwarzschild solution describes: it just says there’s vacuum everywhere, with one exception—that at the very center of the black hole there’s some kind of singularity. What can one say about this singularity? The equations imply that right at the singularity the curvature of spacetime is infinite. But there’s also something else. Once any geodesic (corresponding, for example, to the world line of a photon) goes inside the event horizon, it’ll only continue for a limited time, always in effect ending up “at the singularity”. There’s no necessary connection between this kind of geodesic incompleteness and infinite curvature; but in a Schwarzschild black hole both occur. What is really “at” the center of the Schwarzschild black hole? Is it a place where Einstein’s equations don’t apply? Is it even part of the manifold that corresponds to spacetime? The mathematical structure of general relativity says one starts by setting up a manifold, then one puts a metric on it which satisfies Einstein’s equations. There’s no way the dynamics of the system can change the fundamental structure of the manifold. Yes, its curvature can change, but its dimension cannot, and nor can its topology. So if one wants to say that the point at the center of a Schwarzschild black hole “isn’t part of the manifold”, that’s something one has to put in from the beginning, from outside the Einstein equations. There’s a certain amount that has been done to classify the kinds of singularities that can occur in general relativity. The singularity in a Schwarzschild black hole is a so-called spacelike one—that essentially “cuts off time” for any geodesic, anywhere in space inside the event horizon. Discovered 50 years after the Schwarzschild solution is the Kerr solution, representing a rotating black hole. Its structure is much wilder than the Schwarzschild solution. A notable feature is the presence of a timelike singularity—in which time can progress, but space is effectively cut off. (Later, we’ll see that spacelike and timelike singularities have quite simple interpretations in our models.) If the angular momentum is below a critical value, there’s an event horizon (or, actually, two) in a Kerr black hole. But above the critical value, there’s no longer an event horizon, and the singularity is “naked”, and potentially able to affect the rest of the universe. Like the Schwarzschild solution, the Kerr solution is a mathematical construct that defines a static configuration that a gravitational field can have according to Einstein’s equations when no matter is present (and when the underlying manifold has holes cut out for singularities). There has been a long-running debate about what features of such solutions would survive in more realistic situations, such as matter collapsing to form a black hole. An early conjecture was the weak cosmic censorship hypothesis which stated that in any realistic situation any singularity must be “hidden” behind an event horizon. This meant for example that Kerr-like black holes formed in practice must have angular momenta below the critical value. Numerical simulations did seem to support that supercritical Kerr-like black holes couldn’t form, but a class of constructions were nevertheless found that could in a certain limit generate at least infinitesimal naked singularities. Ordinary general relativity is firmly rooted to (3+1)-dimensional spacetime. But it can be generalized to higher dimensions. And in this case one finds new and more complex black-hole-like phenomena—and it seems to be easier for naked singularities at least theoretically to arise. There are lots of other results from general relativity. One example is Penrose’s singularity theorem, which (with various assumptions) implies that (so long as there is nothing with negative mass AKA gravitational repulsion) as soon as geodesics are trapped within a region (e.g. by an event horizon) they must eventually all converge, and thus form a singularity. (The Hawking singularity theorem is basically a time-reversed version, which implies that there must be a singularity at the beginning of the universe.) Another idea from general relativity is the “no-hair theorem” (or conjecture), which states that, at least from the outside, a limited number of parameters (such as mass and angular momentum) completely characterize any black hole—at least if it’s static. In other words, black holes are in a sense “perfectly smooth” objects; there aren’t gravitational effects outside the event horizon that reveal “inner complexity”. (Note that this is a classical conjecture; it’s quite likely not to be true when quantum effects are included.) The Einstein equations are nonlinear partial differential equations, and it’s interesting to compare them with other such equations, such as the Navier–Stokes equations for fluid flow. One feature of the Navier–Stokes equations is that for sufficiently high fluid velocities (larger than the speed of sound) shocks develop that involve discontinuities that cannot be directly described by the equations. And particularly when one gets to hypersonic flow what physically happens is that the approximation that the fluid is continuous—and can be modeled by a partial differential equation—breaks down, and the specific dynamics of the molecules in the fluid start to matter. Does something similar happen in the Einstein equations? It’s not known whether there can be shocks per se. But it seems likely that to understand things like what appear to be singularities one will have to go “underneath” the Einstein equations—as our models do. What else can one learn from the fluid dynamics analogy? A notable feature of fluid flow is the phenomenon of fluid turbulence, in which random patterns of flow are ubiquitous. It’s still not clear to what extent turbulence is a true feature of the Navier–Stokes equations, and to what extent it’s associated with sensitive dependence on molecular-scale details. I strongly suspect, though, that like in rule 30 (or in the digits of π) the primary effect is an intrinsic generation of randomness, associated with the phenomenon of computational irreducibility. I’d be amazed if something similar doesn’t happen in Einstein’s equations, but it’ll no doubt be mathematically difficult to establish. Plenty of “randomness” is seen in numerical simulations, but absent a precise underlying computational model it’s very difficult to know if the “randomness” is a true feature of the equations that one is trying to approximate, or is just a feature of the approximation scheme used. In our models, computational irreducibility and intrinsic randomness generation are ubiquitous. But exactly how such “microscopic” randomness will scale up to “gravitational turbulence” isn’t clear. One further point, applicable to both fluid flow and gravitation, has to do with computational capability. The Principle of Computational Equivalence strongly suggests that in both cases, sophisticated computation will be ubiquitous. One consequence of this will be computation universality. And in fact it seems likely that this will already be manifest even in such simple cases as the interactions of a few fluid vortices, or just three point masses. The result is that highly complex behavior will be possible. But that doesn’t mean that—for example in the spacetime case—one can’t summarize certain features of the behavior in simple terms, say by describing overall causal structure, or identifying the presence of an event horizon. However, the presence of underlying computational sophistication does mean that there may be computational limitations in coming up with such summaries. For example, to determine whether a certain system has a certain global causal structure valid for all time may in effect require determining the infinite-time behavior of an irreducible computation, which cannot in general be done by a finite computation, and so must be considered formally undecidable.
Event Horizons, Singularities and Other Exotic Spacetime Phenomena 4.3 Identifying Causal Structure : An event horizon is a feature of the global causal structure of a spacetime—a boundary where events “inside” it can’t affect events (or “observers”) “outside” it. In traditional general relativity, it can be mathematically complicated to establish where there’s an event horizon; it effectively requires proving a theorem about how all possible geodesics will be trapped. (In numerical relativity, one can try to just identify “apparent horizons” where geodesics at least seem to be going in directions where they’ll be trapped, even if one can’t be sure what will happen later.) But in our models, everything is considerably more explicit. We’re dealing with discrete updating events, occurring at discrete points. And at least in principle we can construct causal graphs that fully describe the causal relationships between all possible events that can occur in spacetime. Here’s an example of such a causal graph: Remember that the only thing that’s ultimately defined by this causal graph is the causal dependency of events—or in effect the partial ordering of events. Time emerges as an overall inexorable progression of events. Space is associated with the relationship of events in slices defined by foliations of the causal graph. Any given event can affect any event which it can reach by following directed edges in the causal graph, or, in the language of physics, any event which is in its future light cone. But what happens in the infinite limit of the future light cone of a given event? It could be that that infinite limit effectively covers the whole (“spatial”) universe. Or it could be that it only reaches part of the universe. And in that latter case we can expect that there’ll be an event horizon: a boundary to where the effects of our event can reach. But how can we map which events have how large an effect—or in a sense what the causal structure of spacetime is? Let’s define what we can call a “causal connection graph” that shows the relationships between the future light cones of events. Given two events, it could be that the limits of their future light cones are exactly the same (both covering the whole universe, or both covering the same part of it). In that case, let’s consider these events “causally equivalent”. In the causal connection graph, the nodes are collections of causally equivalent events. The edges in the graph are determined by the relationships between the future light cones of these collections of events. Consider two collections A and B. If the limit of the future light cone of A contains the limit of the future light cone of B, then we draw a directed edge in the graph from A to B. And if the limit of the future light cone of A intersects the limit of the future light cone of B (but doesn’t contain it), we draw an undirected edge. There’ll be a causal connection graph defined for events on any spacelike hypersurface that can be specified as a slice through the causal graph. To find the ultimate causal connection graph, we’d then have to look at the infinite limits of future light cones from events in this spacelike hypersurface. But in practice, we can at least try to approximate this by looking not at the infinite limit of the future light cones, but instead at their limit on some spacelike hypersurface “far enough” in the future. (By the way, this is similar to issues that arise in thinking about “absolute” vs. “apparent” horizons in general relativity.) OK, so let’s look at the causal graph we had above, and consider four events starting at the top: The future light cones for events 1 and 2 contain the whole causal graph. But the future light cones for events 3 and 4 are respectively: These future light cones are distinct. So if we construct the causal connection graph for the spacelike hypersurface containing events 3 and 4, it’ll just consist of two separated nodes. In effect the universe here has broken into two non-communicating subuniverses. In the language of general relativity, we can say that there’s a cosmological event horizon that separates these two subuniverses, and no information can go in either direction between them. The signature of this in the causal connection graph is quite straightforward. Looking at the first few levels in the causal graph, one has: Computing the causal connection graph for successive levels, one gets: At the first and second levels, no “event horizon” has yet formed, and there’s just a single set of causally equivalent events. But at level 3, there start to be two separate sets of causally equivalent events; an “event horizon” has formed. So what does the spatial hypergraph “underneath” these look like? Here’s how it evolves (using our standard updating order) for the first few steps: It’s very simple compared to the spatial hypergraph for any “real universe”. But it’s still useful as an example for understanding ideas about causal structure. And what we see is that in this tiny “toy universe”, two “lobes” develop, which evolve like two separate subuniverses, with no causal interdependence. But while there’s no causal interdependence between the “lobes”, the spatial hypergraph is still connected. In our models, however, there’s nothing to say that the spatial hypergraph can’t actually become disconnected—and here’s an example where it does: What’s the significance of this? If the spatial hypergraph stays connected, then even if two regions are at some point causally disconnected, it still remains possible for them to reconnect (in effect revealing that one did not have a true event horizon). But if the hypergraph is actually disconnected, no such reconnection is ever possible (unless one has a fundamentally nonlocal rule whose left-hand side is itself a disconnected hypergraph, so that it can effectively pick up elements “anywhere in any universe”, regardless of whether there are any existing relations between the elements). In traditional general relativity, it is certainly possible to have multiple universes, each with their own disconnected manifold representing space. But while the Einstein equations support the formation of event horizons and causal disconnection, they do not support “forming a new universe” with its own topologically distinct disconnected manifold. To get that requires something like our model, with its much greater flexibility in the fundamental description of space. (In the usual mathematical analysis of general relativity, one defines up front the manifold on which one’s operating, though it’s perhaps conceivable that at least in some limit a metric could develop that’s compatible only with a different topology.) Note that at the level of the causal connection graph, one just sees progressive formation of subuniverses, with cosmological event horizons; there’s no direct trace of disconnection in the spatial hypergraph.
Event Horizons, Singularities and Other Exotic Spacetime Phenomena 4.4 Black-Hole-Like Event Horizons : The type of causal disconnection that we’ve discussed so far is bidirectional: information can’t propagate in either direction across the event horizon. But for black holes, the story is different, basically with effects able to flow into the black hole, but not out. Here’s a very simple example of something like this in our models: In the “universe at large”, represented rather trivially here by the outermost vertical edges, information can propagate back and forth. But if it goes into the “black hole” at the center, it never comes out again. Here’s the rule that’s being used: and here’s what’s going on in the spatial hypergraph: It’s a bit easier to see what’s happening if we look at every single event in the spatial hypergraph: And basically what we see is that events propagate causal effects in both directions on the “ring” that represents the “universe at large”, but any causal effect on the “prong” only goes in one direction, making it correspond to a very simple analog of a black hole. It is important to note that the black hole “doesn’t form” immediately. The “universe” has to “expand” for a couple of steps before one can distinguish a “black hole” event horizon. Given the causal graph: one can construct a causal connection graph. Looking at this on successive steps one gets: For the first three steps, there’s only one set of causally equivalent events. But by step 4, a second set forms. And there’s now a one-way causal connection between the first set of causally equivalent events and the second: in other words, a “black-hole-like” event horizon has formed. Let’s look at a slightly more complicated case, though still with a very simple rule: The causal graph in this case is: Looking at the causal connection graph for successive steps, we see that after a little while a “black hole event horizon” forms: If we look at forward light cones of different events, we see that some “fill the whole universe”, but others stay localized to the region on the left. One can think of the first set as being the light cones of events that are in the “universe at large”; the second set are the light cones of events that are “trapped inside a black hole”: Events trapped inside the black hole cannot affect events outside. But events outside can affect events inside, with events on the boundary in effect representing the process of things falling into the black hole. Note that since energy (and mass) are measured by the flux of causal edges through spacelike hypersurfaces, the black hole shown here effectively gains mass through events that deliver things into the black hole. (Our complete universe is also expanding, so there’s no overall energy conservation to be seen here.) What does the underlying spatial hypergraph look like in this case? Once again, it’s quite simple: So where is the black hole? We can number the events in the causal graph: Then we can match these up with events in the spatial hypergraph: The basic conclusion is that the “main circle” acts like a black hole, with the smaller “handle” being like the “rest of the universe”. Here’s another, slightly less trivial example: The causal graph in this case is: Some initial events have light cones which cover the whole space; others always avoid the “spine” on the left, which behaves like a black hole: Here’s the underlying spatial hypergraph: Comparing the list of events with the annotated causal graph: we see that again the “black hole” is associated with a definite part of the spatial hypergraph.
Event Horizons, Singularities and Other Exotic Spacetime Phenomena 4.5 Spacelike and Timelike Singularities : Disconnection is one kind of “pathology” that can occur in our models. Another is termination: a configuration can be reached in which the rules no longer apply, and “time stops”. (In the language of term rewriting systems, one can say in such a case that a “normal form” has been reached.) Here is an example of the spatial hypergraph for a rule in which this happens: The corresponding causal graph is: And given this causal graph, we can see that the future light cone of any event winds up “compressing down” to just the single final event—after which “time stops”. Is there an analog of this in ordinary general relativity? Yes, it’s just a spacelike singularity—like what occurs in the Schwarzschild solution. In the Schwarzschild solution the spacelike singularity is in the future of events inside the event horizon; here it’s in the future of the “whole universe”. But it’s exactly the same idea. The future of anything that happens—or any geodesic—inevitably winds up in the “dead end” of the causal graph, at which “time stops”. Here’s a slightly more complicated example that again involves termination, but now with two distinct “final events”: Like in a cosmological event horizon, different collections of events in effect lead to different branches of future light cones—though in this case whichever branch is followed, “time eventually stops”. The causal connection graphs in this case basically just indicate the formation of a cosmological event horizon: But now each branch does not lead to a “full subuniverse”; instead they lead to two subuniverses that each end up in a spacelike singularity at which time stops. By the way, consider a “typical” causal graph like: Starting from any event in this causal graph, its future light cone will presumably eventually “cover the whole universe”. But what about its past light cone? Wherever we start, the past light cone will eventually “compress down” to the single initialization event at the top of our causal graph. In other words—just like in standard general relativity—the beginning of the universe also behaves like a spacelike singularity. What about timelike singularities? What is their analog in our models? Remember that a spacelike singularity effectively corresponds to “time stopping”. Well, a timelike singularity presumably corresponds to “space stopping”, but time continuing. Here’s an example: In this example, the future light cones of all events concentrate down onto two separated “tracks”, in each of which there is an inexorable progression from one event to the next, with “no room for any space”. Here’s a minimal version of the same kind of behavior, for the rule: {{x, x}} → {{x, x}, {x, y}} Unsurprisingly, its spatial hypergraph does not succeed in “growing space”: If one starts looking at other rules, one quickly sees all sorts of strange behavior. The almost trivial unary rule {{x}} → {{x}, {x}} effectively gives a whole tree of timelike singularities: Here are a few other ways that collections of timelike singularities can be produced: Notice that for example in the last case only some events don’t lead to timelike singularities. A bit like in a Kerr black hole, it’s both possible to avoid the singularity, and be trapped in it. But let’s go back to spacelike singularities. We’ve seen examples where “time stops” for the whole universe. But what about for just some events? Here’s a simple example: For most events, the future light cone continues forever. But that’s not true for the highlighted events. Each of these events is a “dead end”, for which “time stops”. In this case, the dead ends are associated with spatial disconnections, but this does not need to be true: Here is a slightly more complicated case: Again, though, these are “single-event” dead ends, in the sense that after those particular events, time stops. But predecessors of these events still “have a choice”: their future light cones include both the spacelike singularity, and other parts of the causal graph. It is perfectly possible to have a combination of spacelike and timelike singularities. On the left here is a series of spacelike singularities, while on the right are timelike singularities, effectively all separated by cosmological event horizons: Recognizing when there’s actually a genuine spacelike singularity can be difficult. Consider the case: At first it seems as if some of the events in the last few steps we have generated have no successors. But continuing for a few more steps, we find out that these ones do—though now there are new events that seem not to: Plotting the event numbers for “potential spacelike singularities” against the number of steps of evolution generated, we can see that most events only remain “candidates” for a few steps: In the limit of arbitrarily many steps, will any spacelike-singularity events survive? It doesn’t look likely, but it is hard to tell for sure. And this is exactly one of those cases where the general problem is likely to be undecidable: there is no finite computation which can guarantee to give the result. One feature of all these examples is that they all involve spacelike singularities that affect just single events. At the beginning we also saw examples where “all the events in the universe” eventually wind up at a spacelike singularity. But what about a case that’s more analogous to a Schwarzschild black hole, where a collection of events inside an event horizon wind up at a spacelike singularity, but ones “outside” do not? I haven’t explicitly found an example where this happens, but I’m confident that one exists, perhaps with slightly more complicated initial conditions than I’ve been using here. In ordinary general relativity, Penrose’s singularity theorem implies that as soon as there is a trapped surface from which geodesics cannot escape, the geodesics will inevitably reach a singularity. This theorem is known to apply in any number of dimensions (with codimension-2 trapped hypersurfaces). And in our models, it might make one think that as soon as there is a black-hole-like event horizon, there must be a singularity of the kind we have discussed. But what about a case like the following one we discussed above? There is a “trapped region” in the causal graph, but no sign of a singularity. There seem to be a couple of (not-mutually-exclusive) possibilities for what is going on here. The first is that there is in a sense so much “overall expansion in the universe” and lack of energy conservation (reflected in an increasing flux of causal edges) that the energy conditions in the theorem effectively do not apply. A second possibility is that what we’re seeing is “another universe” forming inside the event horizon. In the Kerr solution, the (probably unrealistic and fragile) mathematics implies that it is in principle possible to “go through” the singularity and emerge into “another universe” that is just as infinite as ours. Perhaps what we are seeing here is a similar phenomenon.
Event Horizons, Singularities and Other Exotic Spacetime Phenomena 4.6 The Distribution of Causal Structures : What kinds of causal structures are possible in our systems? To start answering this, we can explicitly look at all 4702 possible 22 → 32 rules. We have to pick how many steps after the “beginning of the universe” we want to consider; we’ll start with 5. We also have to pick how many steps we want to use to check causal connectivity; we’ll start with 10. Then here are the possible causal connection graphs that occur, together with how many times they occur: Nearly 40% of these rules give causal graphs that terminate before 10 steps, corresponding to “universe-scale” spacelike singularities. Of the remainder, 76% produce no event horizons (at least at the steps we’re measuring). Then the next most common behavior is to generate “subuniverses” separated by cosmological event horizons. And then there are black holes. In aggregate these are produced in about 3% of cases. The most common is a single black hole, followed by a black hole combined with a subuniverse, two black holes, etc. There are also more exotic cases, like the last case shown. The rule here is: And here is the causal graph: This is how the causal connection graph develops at successive steps: It takes a few steps for “black holes to form”. But in the end, we get what we can interpret as a nested set of black holes. The causally equivalent events corresponding to the node at the top have future light cones that eventually cover the universe. But events associated with “lower” nodes yield progressively more localized future light cones: Here’s what’s going on in the spatial hypergraph: Once again, it’s at first pretty obscure where the “black holes” are. It’s mildly helpful to see individual events. What seems to be happening is that the “exterior of the universe” is the main loop. But somehow information can get propagated into the “long hair”, but then can’t get out again. What is the analog of all this in ordinary continuum general relativity? Presumably it’s some kind of nested black hole phenomenon. Inside the event horizon of a black hole other black holes form. Perhaps all these black holes in some sense eventually merge (as the singularity theorem might seem to suggest), or perhaps there’s some structure with multiple levels of event horizons, as roughly happens in the Kerr solution.
Event Horizons, Singularities and Other Exotic Spacetime Phenomena 4.7 Properties of Black Holes : When there’s a black hole in our models, what can one tell about what’s inside it? Causal edges go into the event horizon, but none come out. However, that does not mean that the structure of the spatial hypergraph doesn’t reflect what’s inside the event horizon. After all, every causal edge that crosses the event horizon originated in an event outside the event horizon. And that event represented some update in the spatial hypergraph. Or, in other words, while causal edges are “lost” into the event horizon, the “memory” of what they did is progressively imprinted in the spatial hypergraph outside the event horizon. In ordinary general relativity, there are “no-hair” theorems that say that the gravitational effects of a black hole depend only on a few parameters, such as its overall mass and overall angular momentum. In our models, the overall mass is essentially just determined by the number of causal edges that end up crossing the event horizon. (Angular momentum is related to a kind of vorticity in the causal graph.) So the no-hair theorem for mass says that when there is an event horizon, none of the details of these causal edges matter; only their total number. It’s not clear why this would be true, but it seems conceivable that it could be an essentially purely graph theoretic result. There’s another potential effect, however. Consider the following causal graph: The edges highlighted in red correspond in effect to the interior of an event horizon. The edges highlighted in purple form a pair. The one on the left will cross the event horizon, and “fall into the black hole”. The one on the right will escape to affect the rest of the universe. Later on, we will see how similar phenomena occur in the multiway causal graph, and we will discuss how this relates to quantum phenomena such as Hawking radiation. But for now, here, everything we’re discussing is purely classical. But recall that while the flux of causal edges through spacelike surfaces corresponds to energy, the flux of causal edges through timelike surfaces corresponds to momentum. On average, the net momentum associated with a pair of causal edges should always cancel out. But if one of the edges crosses inside an event horizon, then whatever momentum it carries will just be aggregated into the total momentum of what’s inside the event horizon, effectively leaving the momentum associated with the other edge uncanceled. What does this mean? It needs a more detailed analysis. But there seems to be some reason to think that these uncanceled causal edges should lead to a net momentum flux away from the black hole—a kind of “black hole wind”. The effect will be small, because it’ll essentially be proportional to the elementary length divided by the black hole radius (or, alternatively, the black hole surface area times the elementary length divided by the black hole volume). But conceivably in some circumstances—notably with small black holes—it could have measurable consequences. By the way, it’s worth understanding the “physical origin” of this “wind”. Essentially it’s the result of a classical analog of vacuum polarization. Whereas in ordinary general relativity spacetime is a continuum, in our models it consists of discrete components, and this discreteness causes there to be “fluctuations” that are sensitive to the presence of the event horizon.
Event Horizons, Singularities and Other Exotic Spacetime Phenomena 4.8 More Complicated Cases : If one starts looking at causal graphs for our models, there are plenty of complex things one sees, even with very simple underlying rules. Here are a few examples with various forms of overall causal structure: Other Exotic Phenomena in Spacetime, etc.? Event horizons and singularities can be surprising and confusing in the Einstein equations, but the mathematical structure of the equations at least in principle allows them to be described there. But might there be other kinds of exotic phenomena in spacetime that just can’t be described using this mathematical structure? Our models definitely suggest so. One example we already encountered is disconnection in the spatial hypergraph. Another major class of phenomena involve what amounts to dynamical change of dimension. In the timelike singularities we saw above, parts of the spatial hypergraph effectively degenerate to being zero-dimensional. But in general, the whole or part of the spatial hypergraph can change its effective dimension. I’ve discussed elsewhere the possibility that the early universe might have had a larger effective dimension than the current universe. But what would happen if there was just a region of higher dimension in our current universe? It’s not clear how to make a traditional continuum mathematical description of this, though presumably if one tried to describe it in terms of a manifold, it would show up as a region with infinite curvature. But given a finite—but perhaps very large—hypergraph, it’s much more straightforward to see how regions of different (approximate) dimension might exist together. Inevitably, though, there will be lots of intricate issues to unravel in seeing exactly how all the appropriate mathematical limits fit together. Nevertheless, one can at least get some sense of what “dimension anomalies” might be like. A region of higher effective dimension will—by definition—have a denser pattern of connections than the surrounding parts of the spatial hypergraph. It’s not clear what the boundary of the structure will be like. But conceivably, to be stable, it will have to correspond to an event horizon of some sort. One thing that’s fairly clear is that a region of higher dimension will tend to involve more causal edges than the surrounding hypergraph, and will therefore behave like a region of higher energy, so that it’ll show up as a tiny region of space in which energy (or mass) is concentrated. But of course we’re already very familiar with one example of tiny regions of space in which energy (or mass) are concentrated: elementary particles, like electrons. I’ve always assumed that particles in our models are some kind of stable localized structures in the spatial hypergraph. Perhaps their stability has an essentially topological or graph theoretical origin. But perhaps instead it’s more connected to some kind of event-horizon-like phenomenon—that would be visible in the causal graph. But what’s “inside” such a particle? It could be a collection of elements of the spatial hypergraph that are connected to form some higher (or effectively infinite) dimensional space. It could also simply be that at scales involving fairly few elements in the hypergraph, there are only a discrete set of possible forms for the local region of the hypergraph that support something like an event horizon. As an example of the kinds of localization one can see in a causal graph, here are the forms of forward light cones one gets starting from different events in a particular causal graph: It’s not clear what the best signature to use in searching for particles in our models will be. But it seems like an intriguing—and appealing—possibility that in some sense particles like electrons are “generalized black holes”, or in effect that the apparent “perfection” of black holes is also manifest in the “perfection” of the smallest distinct constituents of matter. (This reminds me of a personal anecdote. Back around 1975, when I was about 15 years old, I attended an evening physics talk in Oxford about black holes and their quantum features. After the talk, I went up to the speaker and asked if perhaps electrons could be black holes. “Absolutely not”, he said, rather dismissively. Well, we’ll see…) What about other kinds of “anomalies in spacetime”? In our models, the structure of space is maintained by continual activity in the spatial hypergraph. Presumably in most situations such activity will on a large scale reach a certain statistically uniform “equilibrium” state. But it is conceivable that different regions of space could show different overall levels of activity—which would lead to different “vacuum energy densities”, effectively corresponding to different values of the cosmological constant. It is also conceivable that there could be different domains in space, all with the same overall activity level (and thus the same energy density) but with different configurations of some large-scale (perhaps effectively topological) feature. And in such a case—as in many cosmological models—one can expect things like domain walls. In ordinary general relativity, the basic topology of space remains fixed over the course of time. In our models, it can dynamically change, with the structure of space potentially being “re-knitted” in different topological configurations. I haven’t explicitly found examples of rules that generate something like a wormhole, but I’m sure they exist. I mentioned above the possibility of small regions of space having different effective dimensions. It’s also possible that there could be extended structures, such as tubes, with different effective dimensions. And this raises the intriguing possibility of what one might call “space tunnels” in which there is a tube of “higher-dimensional space” connecting two points in the spatial hypergraph. No doubt there are many possibilities, and many kinds of exotic phenomena that can occur. Some may be visible directly in the large-scale structure of the spatial hypergraph, while others may be more obvious in the causal graph, or the causal connection graph. It is interesting to imagine classifying different possibilities, although it seems almost inevitable that there will ultimately be undecidable aspects to essentially any classification. Some exotic phenomena may rely on the discrete structure of our models, and not have meaningful continuum limits. But my guess is that there will be plenty of exotic phenomena that can occur even in continuum models of spacetime, but which just haven’t been looked for before.
Event Horizons, Singularities and Other Exotic Spacetime Phenomena 4.9 Cauchy Surfaces, Closed Timelike Curves, etc. : The questions of causal structure that we’re considering here are already quite complicated. But there are even more issues to consider. One of them is the question of whether it is possible to define a definite notion of “progression of time” in spacetime. The causal graph in effect specifies the partial ordering of events in a system. But now we can ask whether there is a valid foliation that respects this partial ordering, in the sense that all events in a given slice are strictly “before” those in subsequent slices. In the continuum limit, this is effectively asking whether the dynamics of our system exhibit strong hyperbolicity. One case where this will definitely not be seen is when there are closed timelike curves, represented by loops in the causal graph. But there are also plenty of other cases where there is no strict way to make a “thickness-1” foliation in which no event occurring within a given slice has a successor in the same slice, as in: When it comes to taking a continuum limit, one should presumably “thicken” the foliations first. But there will inevitably still be cases where no limited amount of thickening will suffice to allow foliations to be created. And such cases one will presumably correspond to failures of Cauchy development in the continuum limit. And it seems likely that this phenomenon can be fairly directly connected to the singularities we have discussed above, but exactly how is not clear.
Event Horizons, Singularities and Other Exotic Spacetime Phenomena 4.10 The Quantum Case : Everything I’ve said so far has involved “purely classical” evolution of the spatial hypergraph, and the causal graph that represents causal relationships within this evolution. But we can also consider multiway evolution and the multiway causal graph which in our models represent quantum behavior. So what happens to various forms of causal disconnection in this case? The basic answer is that things rapidly become very complicated. Consider the very simple “cosmological horizon” causal graph: Here is the corresponding multiway system: And here is the multiway causal graph: And here is its transitive reduction: This multiway causal graph includes both spacelike causal edges, of the kind shown in the ordinary causal graph, and branchlike causal edges, which represent causal relationships between different branches in the multiway graph. If one looked only at spacelike edges, one would see the spacetime event horizon. But branchlike edges can in effect connect across the event horizon. Another way to say this is that quantum entanglements can span the event horizon. And if we look at the branchial graph associated with the multiway system above, we see that all states are entangled, even across the event horizon: There’s lots to understand here. And it’s all quite complicated. But let’s look at a simpler case: The ordinary spacetime causal graph in this case is: The underlying spatial hypergraphs are: and the multiway graph is just: but the multiway causal graph is: although its transitive reduction is just: As another example, consider: whose ordinary causal graph is: The multiway causal graph is: But once again the transitive reduction of this graph is just: As a marginally more realistic example, consider the minimal “black-hole-like” case from above: The ordinary causal graph in this case is: The multiway causal graph in this case is: The transitive reduction of this is: Meanwhile, the branchial graph has the structure: And once again, even though in the spacetime causal graph there’s a (rather minimal) black-hole-like event horizon, things are considerably more complicated in the multiway (i.e. quantum) case, notably with quantum entanglements in the branchial graph apparently spanning the event horizon. There is much more to study here. But it’s already clear that there are some complicated relationships between ordinary causal connectivity, and multiway (i.e. quantum) causal connectivity. Just as in ordinary causal graphs there can be event horizons that limit the spatial effects of events, so in multiway causal graphs there can be event horizons that limit the branchial effects of events. Or, said another way, there should be “entanglement event horizons” that limit the causal relationships between quantum states. One way to view causal effects in spacetime is to say that a given event can affect events in its future light cone. But in the space of quantum states the causal effect of an event is also limited, but now to an entanglement cone rather than a light cone. And just as we looked at causal connection graphs on the basis of light cones, we can do the same thing for entanglement cones. The result will hopefully be an elucidation of the quantum character of event horizons. But that will have to await another bulletin.
Exploring Rulial Space: The Case of Turing Machines 5.1 Generalized Physics and the Theory of Computation : Let’s say we find a rule that reproduces physics. A big question would then be: “Why this rule, and not another?” I think there’s a very elegant potential answer to this question, that uses what we’re calling rule space relativity—and that essentially says that there isn’t just one rule: actually all possible rules are being used, but we’re basically picking a reference frame that makes us attribute what we see to some particular rule. In other words, our description of the universe is a sense of our making, and there can be many other—potentially utterly incoherent—descriptions, etc. But so how does this work at a more formal level?  This bulletin is going to explore one very simple case. And in doing so we’ll discover that what we’re exploring is potentially relevant not only for questions of “generalized physics”, but also for fundamental questions in the theory of computation. In essence, what we’ll be doing is to study the structure of spaces created by applying all possible rules, potentially, for example, allowing us to “geometrize” spaces of possible algorithms and their applications. In our models of physics, we begin by considering spatial hypergraphs that describe relations between “atoms of space”. Then, looking at all possible ways a given rule can update these hypergraphs, we form what we call a multiway graph. The transversals of this graph define what we call branchial space, in which we can see the pattern of entanglements between quantum states. But there’s also a third level we can consider. Instead of just forming a multiway graph in which we do all possible updates with a given rule, we form a rulial multiway (or “ultramultiway”) graph in which we follow not only all possible updates, but also all possible rules. The transversals to this rulial multiway graph define what we call rulial space. Causal invariance in the rulial multiway graph then implies “rule space relativity” which is what allows us to use different possible reference frames to describe the universe. In the end, the Principle of Computational Equivalence implies a certain invariance to the limiting structure of the rulial multiway graph, independent of the particular parametrization of the set of possible rules. But for the sake of understanding rulial space and the rulial multiway graph—and getting intuition about these—this bulletin is going to look at a specific set of possible rules, defined by simple Turing machines. The rules we’ll use aren’t a good fit for describing our universe. But the comparative simplicity of their structure will help us in trying to elucidate some of the complexities of rulial space. In addition, using Turing machines will make it easier for us to make contact with the theory of computation, where Turing machines are standard models.
Exploring Rulial Space: The Case of Turing Machines 5.2 Turing Machines : Here’s a representation of the rule for a particular 2-state (s = 2), 2-color (k = 2) Turing machine: The pointer represents the “head” of the Turing machine and its orientation represents the state of the head. Here’s what this particular Turing machine does over the course of a few steps starting from a “blank tape” (i.e. all squares white): There are (2 s k)sk possible s-state k-color Turing machines, or 4096 s = 2, k = 2 machines. Here are all the distinct behaviors that occur (up to left-right reflection) in these 4096 machines, starting from a blank tape: But note that all these are deterministic Turing machines, in the sense that, for a given machine, it is always the same rule that is applied at every step. But what about non-deterministic Turing machines? The idea here is to allow several different rules at any given step. As a simple example, consider the pair of rules: To show the possible evolution histories, we can construct a multiway system: Each arrow represents an application of one of the two possible rules. A non-deterministic Turing machine is usually considered to follow a particular path, corresponding to a particular evolution history. Then a typical question is whether there exists some path that leads to a particular final state (which might be viewed as the solution to some particular problem). (In a quantum Turing machine, one considers collections of multiple states, viewed as being in a quantum superposition.) In what we’re going to be doing here, we want to study the “extreme case” of non-determinism—where at every step, every possible Turing machine rule (at least with a given s, k) is applied. Then we’ll be interested in the full rulial multiway graph that’s created.
Exploring Rulial Space: The Case of Turing Machines 5.3 The Rulial Multiway Graph : Start with a blank tape. Then apply all possible 2,2 Turing machine rules. This is the rulial multiway graph that’s formed after 1 step: Each individual arrow here represents many possible rules from all the 4096 discussed above. Here’s what happens after 2 steps: In layered form, this becomes: In creating these rulial multiway graphs, there’s an important simplification we’re able to make. We don’t need to separately apply all 4096 2,2 Turing machine rules to each state at each step—because all that ever matters is the one case of the rule that’s relevant to the particular state one has. There are 4 possible individual cases—and for each of these there are 8 possible outcomes, leading to 32 “micro-rules”: After 3 steps, the rulial multiway graph has the form: In 3D this becomes: In layered form it is: After 5 steps, the rulial multiway graph is: What about other numbers of states and colors? Here are the rulial multiway graphs after 3 steps for Turing machines with various numbers of states and colors : Notice the presence of s = 1 examples. Even with a single possible state, it is already possible to form a nontrivial rulial multiway system. Here are the results for k = 2 after 1 step: 2 steps: 6 steps: In principle one can also consider the even simpler case of s = k = 1. After 1 step one gets: After 2 steps this becomes: And after 3 steps: As a layered graph rendering, this becomes: With s = 2, k = 1 one gets after 1 step: After 2 steps this becomes: And after 3 steps: After 6 steps this becomes: At least in the central part of this graph, there are edges going in both directions. Combining these, and treating the graph as undirected, one gets: With 3 states (s = 3) there are 3 tracks: One can also consider Turing machines in which the head can not only move left or right, but can also stay still. In this case, with s = 2, k = 1 one gets after 1 step: After 2 steps this becomes: After 4 steps, removing repeated edges, etc. this gives: As another generalization, one can consider Turing machines that can move not just one, but also two squares: After 2 steps one gets: After 3 steps, removing repeated edges, etc. this gives.
Exploring Rulial Space: The Case of Turing Machines 5.4 The Limit of the Rulial Multiway Graph : What is the limiting structure of the rulial multiway graph after an infinite number of steps? The first crucial observation is that it’s in a sense homogeneous: the structure of the graph around any given node is always the same (i.e. it’s a vertex-transitive graph). To see why this is true, recall that each node in the graph corresponds to a particular distinct configuration of the Turing machine. This node will lead to all nodes that can be obtained from it by one step of Turing machine evolution. But (assuming the head always moves ±1 square) there are always exactly 2 s k of these. And because we are following all possible “micro-rules”, we can think of ourselves as just “blindly overwriting” whatever configuration we had, so that the structure of the graph is the same independent of what configuration we’re at. As a simple example, here’s what happens in the s = 2, k = 1 case, after 3 steps: There’s some trickiness at the ends, but in the central region we see as expected that each node has exactly 4 successors. If we pick out the subgraph around the blank-tape starting node it has the form: In general, the limiting neighborhood of every node is just: In the full graph, these neighborhoods are knitted together, giving pieces like: Let’s now consider the case s = 1, k = 2. After 4 steps the full graph is: Here the neighborhood of the start node is: And in the limiting graph, every node will have a local neighborhood with this same structure.  If we look at successively larger neighborhoods, the limiting form of these for every node will be: Here’s a table of the number of distinct nodes in these successively larger neighborhoods for Turing machines with various values of s and k: In the limit t → ∞ the number of nodes reached in all cases goes like: For k = 1, one finds (for ): For t = 1 one has: If one ignores directedness in the graph, and just counts the total number of neighbors out to distance t, the results for k = 1 are the same as before, but in other cases they are different: (These results asymptotically seem larger by a factor .) So for k = 1, the limiting rulial multiway graph behaves like a 1-dimensional space, but for all k ≥ 2, it behaves like an infinite-dimensional space, in which the volumes of geodesic balls grow exponentially with volume.
Exploring Rulial Space: The Case of Turing Machines 5.5 Finite Tapes : So far we’ve always assumed that our Turing machine tape is unbounded. But in some ways it’s easier to see what’s going on if we instead limit the tape, so we have a finite total number of states (n s kn). One thing we can do is to consider a cyclic tape. For s = k = 1 with successively longer tapes we then get: For s = 2, k = 1 we get: For tape size 10, the results for successive values of s are: If instead of having a cyclic tape, we just have a finite tape, and insist that the head never goes off either end, then for s = k = 1 we get: For s = 2, k = 1 we get: and the overall behavior for successive s is exactly as we saw above for unbounded tapes: For s = 1, k = 2, this is what happens in the cyclic case for length 3: The results for lengths 1 through 6 are: Rendering these in 3D makes the connection with progressively high-dimensional hypercubes slightly clearer: The non-cyclic case for lengths 2 through 6 (length 1 is trivial): Rendered in 3D these become: In the case s = 2, k = 2 for a length-3 cyclic tape we get: Rendering the results for lengths 1 through 6 in 3D gives: In the non-cyclic case the results for lengths 2 through 6 in 3D are (the length-1 case is trivial).
Exploring Rulial Space: The Case of Turing Machines 5.6 The Turing Machine Group : It turns out that there’s a nice mathematical characterization of rulial multiway graphs for Turing machines: they’re just Cayley graphs of groups that we can call “Turing machine groups”. Why is this? Basically it’s because the possible configurations of a Turing machine have a direct correspondence with transformations that can act on these configurations. And in particular, one can pick out certain transformations that correspond to individual transitions in a non-deterministic Turing machine, and use these as generators in a presentation of the group. Let’s start by considering the case of Turing machines with finite cyclic tapes. If the tape has length n, the total number of possible configurations of the machine is n s kn. (Note that if the tape is finite but not cyclic then one doesn’t get a group.) Assume for now s = 1, k = 2. Then for n = 3, there are 24 possible configurations, and the rulial multiway graph is: But this graph turns out to be a Cayley graph for the finite group A4 × 2: To construct this Cayley graph let’s represent the configurations of the Turing machine by pairs of integers , where 0 ≤ i ≤ n – 1 gives the position of the head, and the bits of u (with 0 ≤ u ≤ 2n – 1) give the values on the tape. (Since s = 1, we don’t have to worry about the state of the head.) With this representation of the configurations, consider the “multiplication” operation: When this operation acts on the configurations it defines a group. Here’s the multiplication table for n = 3: Or equivalently: But now consider the four elements , (position –1 wraps around on the cyclic tape): We can consider these as the result of applying the 4 possible Turing machine transitions to the configuration (corresponding to ): And now by treating these elements as generators (and effectively applying them not just to the initial configuration, but to any configuration), we get as the Cayley graph of the group exactly the rulial multiway graph above. It’s worth noting that the 4 elements we’ve used don’t correspond to the minimal set of generators for the group. Two elements suffice. And for example, we can use and , which can be thought of respectively as moving the head right, and flipping the color of one cell (again relative to the “identity” configuration ): With these generators, we get the Cayley graph: How else can we characterize the group? For any tape size n we can write it in terms of explicit permutations: (For , the group can be generated by the permutations .) We can also represent it symbolically in terms of generators and relations. Calling our “move-right” generator R, and “bit-flip” generator F, the group then satisfies at least the relations: OK, so does the group have a name? For n = 2, it’s the 8-element dihedral group D4 and for n = 3, it’s the 24-element group A4 × 2. For larger n, there doesn’t seem to be a standard name. But given our derivation we can just call it . And we can express it as a semidirect product (or wreath product): The normal subgroup (2)n here represents states of the tape, and corresponds to a Boolean n-cube. The cyclic group n represents the position of the head, and acts on the Boolean n-cube by rotating its coordinates. For any n, we can use just two generators, producing the sequence of Cayley graphs: Undirected versions of these are exactly the cube-connected cycle graphs that have arisen in studying communications networks: So now what about the limit n → ∞?  Now, the group is no longer finite, but we’ve still got the relations: and in the end, we can see that the group can be described as a semidirect product: In a sense, the group—and its Cayley graph—is dominated by the infinite-dimensional Boolean hypercube. But there’s more going on. And perhaps there’s a useful characterization of the limit that can be derived by the methods of modern geometric theory. For arbitrary s and k, we can potentially generalize to get.
Exploring Rulial Space: The Case of Turing Machines 5.7 Causal Graphs for Deterministic Turing Machines : In a deterministic Turing machine, every step involves one updating event—and the causal graph can be drawn by just joining successive locations of the head, and successive points where the head returns to a square where it has been before: Continuing for a few more steps, the causal graph for this particular Turing machine becomes: Continuing for more steps, and redrawing the graph, we see that we get a simple grid: Of s = 2, k = 2 Turing machines, the one with the most exotic causal graph is the “binary counter” machine 1953: The causal graph gives a good “overall map” of the behavior of a Turing machine. Here are a few Turing machines (with s = 3, k = 2 and s = 4, k = 2) from A New Kind of Science (compare also the s = 2, k = 3 universal Turing machine): And here are their respective causal graphs.
Exploring Rulial Space: The Case of Turing Machines 5.8 Rulial Multiway Causal Graphs : What can we say about the causal graph associated with a rulial multiway system? The first important observation is that rulial multiway graphs always exhibit causal invariance, since by including transitions associated with all possible rules one is inevitably including both rules and their inverses, with the result that every branching of edges in the rulial multiway graph is always associated with a corresponding merging. It’s fairly easy to see this for two steps of s = 2, k = 2 Turing machines. The rulial multiway states graph here is: This can also be rendered: Explicitly showing events we get: Including causal connections we get: or after 3 steps: The pure rulial multiway causal graph in this case is then: In layered form this becomes: After 5 steps the growth in the number of nodes reached going from the root grows like: Causal invariance implies that this multiway causal graph is ultimately composed of a large number of interwoven copies of a single causal graph. After any given number of steps of evolution, the various copies of this causal graph will have “reached different stages”. Here are the results after 3 steps: And after 4 steps: One can also look at rulial multiway causal graphs for other sets of Turing machines. For s = 1, k = 1 one has (here after 5 steps) which is equivalent to: The individual causal graphs in this case are immediately all the same, and are just: For s = 2, k = 1 one already has after 3 steps: or in layered form: After 5 steps this becomes: or in layered form: In this case, the individual causal graphs after 5 steps are: For s = 1, k = 2 one has very similar results to the case s = 2, k = 2. After 3 steps the causal graph is: Or after 5 steps: Removing multiple edges this is: The individual causal graphs in this case are.
Exploring Rulial Space: The Case of Turing Machines 5.9 Rulial Graphs : Just as for ordinary multiway graphs one can study branchial graphs which represent their transversals, so similarly for rulial multiway graphs one can study rulial graphs which represent their transversals. The layered way we have drawn multiway graphs corresponds to a particular choice of foliation—and with this choice, we can immediately generate rulial graphs. For s = 2, k = 1 one has after 2 steps: while after 3 steps one gets: For s = 1, k = 2 one gets after 2 steps: while after 3 steps one has: The sequence of results for steps 1 through 4 is: For s = 2, k = 2 the corresponding results are: What do these pictures mean? Just as branchial graphs in ordinary multiway systems can be thought of as “entanglement maps” for states (interpreted in our models as quantum states) in ordinary “multiway space”, so here these rulial graphs can be thought of as “entanglement maps” for states in rulial space. In other words, they are a kind of map of which Turing machine configurations are “evolutionarily close” to which other ones, in the sense that they can be reached with only a few different choices of rules.
Exploring Rulial Space: The Case of Turing Machines 5.10 Deterministic Turing Machine Paths in Rulial Space : The rulial multiway graph defines all paths that can be followed by all non-deterministic Turing machines. At each node in this graph there is therefore an outgoing edge corresponding to any possible Turing machine transition from the configuration corresponding to that node. But what if one considers just a single deterministic Turing machine? Its evolution is then a single path within the rulial multiway graph. Consider for example the s = 2, k = 2 Turing machine: This machine evolves from a blank tape according to: This corresponds to a path in the rulial multiway graph: With a different initial condition the path can be very different: This pictures might make it seem that the paths corresponding to the evolution of deterministic Turing machines are geodesics in the rulial multiway graph. But in general they are definitely not. For example, starting from a blank tape, the rule follows this path on the rulial multiway graph: If one allows any possible Turing machine transition at every step, then one can follow a geodesic path from a node corresponding to an initial condition to a node corresponding to any other configuration. But if one restricts oneself to the transitions in a particular (deterministic) Turing machine, then there will in general be many configurations one will never reach, and even those that one can reach, one may reach by a circuitous route in the rulial multiway graph. Given a particular configuration (corresponding to a node in the rulial multiway graph), there may be many deterministic Turing machines that can reach it from a given initial state. One can consider each of these Turing machines to be implementing a certain algorithm. So then the “optimal algorithm” will be the one which is shortest among deterministic Turing machine paths. As I just mentioned, this won’t typically be the shortest possible path: that will usually be achieved by a non-deterministic Turing machine with a particular sequence of transitions. But there is still an optimal case (i.e. an “optimal algorithm”) among deterministic Turing machines. But now we can ask across all possible deterministic Turing machines where they can reach in the rulial multiway graph. Here is the result for 4 steps for the s = 2, k = 2 Turing machines we have been considering: These are the results for 1, 2 and 3 steps: What is the significance of this? Essentially what we’re seeing is a comparison of what can be achieved with deterministic computation versus non-deterministic. Taking the 4-step case as an example, the “background” gray rulial multiway graph shows what can be achieved with arbitrary non-deterministic computation in 4 steps. The red region is what deterministic computation can achieve in the same number of steps. In a sense this is a very simple empirical analog of the P vs. NP problem. Unlike the real P vs. NP case, we’re not allowing arbitrary polynomial-time algorithms here; we’re just looking at possible s = 2, k = 2 Turing machine algorithms running specifically for 4 steps. But if we were to generalize this appropriately, P = NP would imply that the “red region” must in some limit in effect “reach anywhere in the graph”. A little more precisely, the official definition of P and NP is for decision problems: you start from some initial condition which defines an instance of a problem (“Is this Boolean formula satisfiable?” or whatever), and the system must eventually evolve to a state representing either “yes” or “no”. We can imagine setting things up so that the outcomes correspond to particular configurations of the Turing machine.  The inputs are then also encoded as initial configurations of the Turing machine, and we want to know what happens as we consider inputs of progressively larger sizes. We can imagine drawing the rulial multiway graph so that progressively larger inputs are shown “progressively further from the center”. If we consider non-deterministic Turing machines (associated with the class of NP computations), then the shortest computation will be a geodesic path in the rulial multiway graph from the input configuration to the outcome configuration. But for deterministic Turing machines (associated with the class P) it will in general be some much more circuitous path. The standard P vs. NP problem asks about limiting behavior as one increases the size of the input computation—and one might imagine that the question could be “geometrized” in terms of some continuum limit of the rulial multiway graph. Of course, there is no guarantee that any reasonable limit exists. And it could perfectly well be that the question of whether P ⊂ NP is actually undecidable, or in other words, that no finite proof of it can be given within a standard axiom system (such as Peano arithmetic or ZFC set theory). One could imagine empirically testing more and more Turing machines, and seeing how they perform on an NP-complete problem. One might think of plotting their running times as a function of n for increasingly large n. For a while a particular Turing machine might be the winner. But then another one might take over. And there might be no end to how many “surprises” would occur as one increases n. (Somehow this is reminiscent of the story of the Skewes number and whether LogIntegral[n] > Prime[n].) In computational complexity theory one usually thinks about explicitly constructing optimal algorithms by standard “engineering-like” methods. But I think there’s a lot to be learned from a more empirical approach—in which one doesn’t try to construct optimal algorithms, but just finds them “in the wild” by searching all possible programs. In the past, it might not have seemed that “just searching for programs” would ever produce anything terribly interesting. But one of the big consequences of what I discussed in A New Kind of Science is that even among tiny programs—small enough that one can, for example, enumerate all of them—there’s often very complex and potentially “useful” behavior. And that makes it seem much more reasonable to try to do “empirical computational complexity theory”—enumerating possible programs to find optimal ones, and so on. Small programs can be thought of as ones with low algorithmic complexity. So searching for “fast programs” among these can be thought of as searching for programs with low time complexity, and low algorithmic complexity. We don’t know exactly how strong the constraint of low algorithmic complexity is, but from what I’ve seen in the computational universe (and what’s embodied in things like the Principle of Computational Equivalence), it seems as if it’s not such a big constraint. I studied “empirical computational complexity theory” a bit in A New Kind of Science, notably for Turing machines. And one of the interesting observations was that the optimal algorithm for things was often not something that could readily be constructed in a step-by-step engineering way. Instead, it was something that one basically could only find pretty much by doing a search of possible programs—and where there usually didn’t seem to be a “general form” that would “apply for all n”, and let one readily deduce the properties of the n → ∞ limit. In other words, it didn’t seem like what we’d now think of as the sequence of paths in rulial space would show any sign of converging to a “continuum limit”. A decent example of all this occurs in sorting networks. Imagine that you are given a collection of n numbers to sort. You can straightforwardly do this by making about n2 pairwise comparisons, and it’s pretty easy to optimize this a bit. But explicit searches have revealed that the actual optimal networks for successive n are: What’s notable is how complicated and “random” they look; there doesn’t seem to be any obvious pattern to them (and my guess is that there fundamentally isn’t). Here’s a plot of the sizes of these networks (divided by n2): It’s worth noting that these are deterministic networks. One could also imagine non-deterministic networks (and indeed one could construct a rulial multiway graph by considering all possible successive placements of pairwise comparisons)—and in a non-deterministic network it’s always possible to sort n numbers in at most n – 1 steps.
Exploring Rulial Space: The Case of Turing Machines 5.11 The Space of Deterministic Turing Machine Computations : We’ve just seen how the results of deterministic Turing machine computations lay out in the rulial multiway space of all possible non-deterministic Turing machine computations. But what happens if we just look at the graph of deterministic Turing machine computations on their own? Here are the full rulial multiway graphs for 2 and 3 steps with the graphs of deterministic Turing machine computations superimposed, as before: But now let’s “pull out just the red subgraphs”—in other words, include as nodes only those configurations that at least one of the 4096 s = 2, k = 2 Turing machines can reach after 2 or 3 steps: Notice that after 2 steps, deterministic Turing machines can still reach all 36 configurations that non-deterministic ones can reach (though not through quite as many paths). But after 3 steps, among all the deterministic Turing machines, they can only reach 68 possible configurations, while non-deterministic ones can reach 100 configurations. For all possible non-deterministic Turing machines with s = 2, k = 2 the total number of configurations that can be reached eventually roughly doubles on successive steps: For deterministic Turing machines, however, the number of possible configurations that can be reached soon increases much more slowly: And in fact there’s an obvious bound here: at any given step, the most that can happen is that each of the 4096 s = 2, k = 2 Turing machines leads to a new configuration—or, in other words, the maximum number of configurations reached increases by 4096. Looking at the differences on successive steps, we find: In other words, among all 4096 Turing machines, about 100 “novel configurations” are reached at each successive step. (The actual sequence here looks surprisingly random; it’s not clear whether there’s any particular regularity.) Now let’s look at the actual graphs formed. With 4 through 7 steps we get: After 10 and 20 steps the results are: Here is the result after 50 steps: There’s a surprising amount of structure in these graphs. There’s a “central region” near the initial blank-tape configuration (shown highlighted below) in which many different Turing machines end up visiting the same configurations: Here’s a 3D rendering of this region: But away from this region there end up being “spokes” (about 100 of them) corresponding to Turing machines that “independently explore new territory” in the space of configurations. What are those “configurations on the edge” like? Here are sorted collections of them for the first few steps: For comparison, here is the result for all configurations that can be reached by non-deterministic Turing machines after just 2 steps: Here are the results for deterministic Turing machines after more steps: We can also ask which machines are the ones that typically “explore new territory”. Here’s the result for 30 steps: As we go to more steps, the graph of configurations that can be reached by deterministic Turing machines grows. But does at least the core of it reach some kind of limit after sufficiently many steps? We can get a sense of this by looking—as we have done so many times before—at the growth rate of the geodesic ball in the graph starting from the initial blank-tape configuration. The total number of new configurations that can be reached on each new layer of the geodesic ball is at most 4096—and in reality it’s much smaller. Here are the numbers of new configurations added on successive layers at steps 100, 200, …, 500 in the overall evolution: The cause of the “steps down” becomes clearer if one examines more closely the “spokes” in the graph above.  Here’s one example: And basically what’s happening is that multiple Turing machines are tracing out roughly the same sequences of configurations, but some do it “efficiently”, while others “waste time”, for example having the head flip around on alternate steps. The “inner parts” of the spokes—that are closer to the initial node—involve both “inefficient” and more efficient Turing machines. But the “inefficient” Turing machines will simply not get as far, so they do not contribute to the outer layers of the geodesic ball. The final “step down” in the plot above—at basically half the total number of steps used for the Turing machines—involves the “petering out” of the roughly half the Turing machines that effectively “waste half their steps”. For the “spoke” shown, here are the actual Turing machine histories involved (there are 8 machines that made identical copies of the first history).
Exploring Rulial Space: The Case of Turing Machines 5.12 The Cellular Automaton Analog : The kind of graphs we’ve just made for deterministic Turing machines can be made for any family of deterministic computational systems. And in particular they can be made for the (at least for me, much more familiar) case of the 256 k = 2, r = 1 cellular automata. (And, yes, it’s somewhat amazing that in all these years I’ve never made such pictures before—though there’s a note on page 956 of A New Kind of Science that gets close.) Here are the results for 5 and 10 steps, starting from an initial condition containing a single black cell: And here is the result for 50 steps: Here is the result for 10 steps, annotated with the actual cellular automaton evolution for the rules that “reach furthest”: It’s slightly easier to see what’s going on if we include only even-numbered rules (which leave a blank state blank): It’s an interesting map of “cellular automaton space”. (Note the presence of rule 30 on the lower left, and rule 110 on the lower right.) The total number of new configurations explored by all rules on successive steps has a more regular form than for Turing machines: The result mostly alternates with period 4 between 72 and 84, though with dips at steps of the form 2m. If we go for a certain number of steps (say 200), and then look at the geodesic ball centered on the initial condition, the number of configurations in successive layers is just: These results are all for ordinary, deterministic cellular automata. So are there non-deterministic cellular automata? Typically, cellular automata are defined to consistently update every cell at every step. But one can also consider sequential cellular automata, where specific cells are updated at each step—and in this case it is straightforward to define a non-deterministic version, for which things like multiway systems can be constructed. (Note that for a full rulial multiway system, such non-deterministic cellular automata are basically the same as non-deterministic mobile automata, which are close to Turing machines..
Exploring Rulial Space: The Case of Turing Machines 5.13 Computation Capabilities and the Structure of Rulial Space : What do the computational capabilities of Turing machines mean for their rulial space? Let’s start with computation universality. An important fact about deterministic Turing machines is that there are universal ones. Among the 4096 s = 2, k = 2 rules there aren’t any. But as soon one goes to s = 2, k = 3 rules (of which there are 2,985,984) it’s known (thanks to my results in A New Kind of Science, and Alex Smith’s 2007 proof) that there is a universal machine. The rule is: Starting from a blank tape, this machine gives: and the corresponding causal graph is: But what does the existence of a universal machine mean in the rulial multiway system? From any given initial condition, any deterministic Turing machine will trace out some trajectory in the rulial multiway graph. And if a machine is universal it means that by appropriately picking its initial conditions it can be “programmed” to “emulate” any other Turing machine, in the sense that its trajectory will track the trajectory of whatever Turing machine it’s emulating. What does “track” mean? Basically, that there’s some fixed scheme that allows one to go from the states of the universal Turing machine to the states of the machine it’s emulating. The “scheme” will correspond to some translation in the rulial multiway graph, and the requirement is that this translation is somehow always limited. In other words, the trajectory of a universal machine will “flail around” as its starting point (i.e. initial condition) moves in the rulial multiway graph, and this “flailing” will be diverse enough that the trajectory can get close to the trajectory of any given other machine. If, on the other hand, the machine one’s dealing with isn’t universal, then its trajectory won’t “flail around” enough for this work; the trajectory will in a sense be too constrained to successfully “track” all possible other deterministic Turing machine trajectories. What about non-deterministic Turing machines? What universality can be interpreted to mean in this case is that given a particular initial condition, the output one wants occurs somewhere on the different paths followed by the non-deterministic Turing machine. (If one’s trying to do a decision problem—as in the computational complexity class NP—then one can arrange to “signal that one’s got an answer” through some feature of the Turing machine state.) In the case of “extreme non-determinism”—as used to construct the rulial multiway graph—computation universality in a sense becomes a statement purely about the structure of the rulial multiway graph. And basically it just requires that the rulial multiway graph is sufficiently connected—which is guaranteed if there’s causal invariance (so there’s nothing like an “event horizon” anywhere). But does one have to allow “extreme non-determinism” to get universality? With s = 2, k = 3 we know that there’s a purely deterministic Turing machine that achieves it. And my guess is that among s = 2, k = 2 there are “slightly multiway” rules that also do. In a standard deterministic s = 2, k = 2 Turing machine, there are 4 cases of the rule that are each specified to have a unique outcome. But what if even just one of those cases in the rule has two outcomes? The rule is non-deterministic, but it can be thought of as just being the result of specifying 5 defining cases for the rule. And it’s my guess that even this will be sufficient to get universality in the system. A non-deterministic rule like this will not trace out a single path in the rulial multiway graph. Instead, it’ll give a bundle of paths, with different paths corresponding to different non-deterministic choices. But the story of universality is very similar to the deterministic case: one simply has to ask whether anything in the bundle successfully manages to track the trajectory of the machine one’s emulating. It’s worth remembering that any given rule won’t typically be following geodesics in rulial space. It’ll be following some more circuitous path (or bundle of paths). But let’s say one has some rule that traces out some trajectory—corresponding to performing some computation. The Principle of Computational Equivalence implies that across different possible rules, there’s a standard “maximum computational sophistication” for these computations, and many rules achieve it. But then the principle also implies that there’s equivalence between these maximally sophisticated computations, in the sense that there’s always a limited computation that translates between them. Let’s think about this in rulial space. Let’s say we have two rules, starting from the same initial condition. Well, then, at the beginning it’s trivial to translate between the rules. But after t steps, the states reached by these rules can have diverged—making it potentially progressively more difficult to translate between them. A key idea of the rulial multiway graph—and rulial space—is that it lets one talk about both computations and translations between them in uniform ways. Let’s say that the trajectories of two rules have gone a certain distance in rulial space. Then one can look at their divergence, and see how long a “translation computation” one has to do to get from one to the other. In ordinary spacetime, let’s say a certain time t has elapsed. Then we know that the maximum spatial distance that can have been traversed is c t, where c is the speed of light. In rulial space, there’s something directly analogous: in time t, there’s a maximum rulial distance that can be traversed, which we can call ρ t. But here the Principle of Computational Equivalence makes it a crucial contribution: it implies that throughout rulial space, and in all situations, ρ is fixed. It can take an irreducible amount of computational work to successfully translate from the outcome of one rule to another. But this always scales the same way. There’s in effect one scale of computational irreducibility, and it’s characterized by ρ. Like the constancy of the speed of light uniformly limits physical motion, the constancy of ρ uniformly limits rulial motion. But let’s say you’re trying to get from one point in rulial space to another. If from your starting point you can follow the path of an irreducible—and effectively universal—computation, then you’ll successfully be able to reach the other point. But if from your starting point you can only follow a reducible computation this won’t generally be true. And what this means is that “pockets of computational reducibility” in rulial space act a bit like black holes in physical space. You can get into them from regions of irreducibility, but you can’t get out of them. There are probably signs of phenomena like this even in the rulial space for simple Turing machines that we’ve explored here. But there’s considerably more that needs to be worked out to be able to make all the necessary connections.
Exploring Rulial Space: The Case of Turing Machines 5.14 The Emerging Picture of Rulial Space : There are lots of analogies between physical space, the branchial space, and rulial space. For example, in physical space, there are light cones that govern the maximum rate at which effects can propagate between different parts of physical space. In branchial space, there are entanglement cones that govern the maximum rate of quantum entanglement. And in rulial space, one can think of “emulation cones”, which govern the maximum rate at which one description of behavior can be translated into another. And when it comes to applying these things to modeling the physical universe, a crucial point is that the observer is necessarily part of the system, governed by the same rules as everything else. And that means that the observer can only be sensitive to certain “appropriately modded-out” aspects of the system. But in actually imagining how an observer “perceives” a system it’s almost always convenient to think about coordinatizing the system—by defining some appropriate foliation. In physical space, this involves foliating the causal graph using reference frames like in relativity. In branchial space, it involves our concept of quantum observation frames. And in rulial space, we can invent another such concept: a rulial description frame, or just a rulial frame. Different rulial frames in effect correspond to describing the evolution of the universe as operating according to different rules. Causal invariance implies that in the end different rulial frames must give equivalent results. But the specific way one describes the time evolution of the universe will depend on what rulial frame one’s using. In one frame one would be describing the universe in one way; in another frame, another way. And the “story one tells” about how the universe evolves will be different in the different frames. Much like with superposition in quantum mechanics, there’s probably some notion of regions in rulial space, in which one’s somehow viewing the universe as operating according to “rulially entangled” collections of rules. But while our original motivation was understanding physics, a lot of what we’re studying about rulial space also applies to purely computational systems. For example, we can think of rulial space even without having any notion of physical space. And we can in effect imagine that rulial space is some kind of map of a space of possible rules for computational systems.  (Notice that because of computation universality and the Principle of Computational Equivalence it ultimately doesn’t matter what particular type of rule—Turing machine, cellular automaton, combinator, whatever—is used to “parametrize” rulial space.) Different places in rulial space in some sense correspond to different rules. Paths at different places in rulial space correspond to evolution according to different rules. So what is the analog of motion in rulial space? In effect it’s having a frame which progressively changes the rule one’s using. If one’s trying to find out what happens in one’s system, it’s fundamentally most efficient to “stick with one rule”. If one progressively changes the rule, one’s going to have to keep “translating back” to the original rule, by somehow emulating this rule with whatever rule one’s reached. And the result of this is there’ll be exactly the analog of relativistic time dilation. Faster motion in rulial space leads to slower progression in time. Of course, to discuss this properly, we really have to talk about the rulial multiway causal graph, etc. But one thing is clear: motion faster than some maximum speed ρ is impossible. From within the system, you simply can’t keep the correct rulial causal connections if you’re changing your rulial location faster than ρ. In an abstract study of Turing machines, ρ is just an arbitrary parameter. But in the actual physical universe, it must have a definite value, like the speed of light c, or our maximum entanglement speed ζ. It’s difficult even to estimate what ζ might be. And presumably estimating ρ will be even harder. But it’s interesting to discuss at least how we might start to think about estimating ρ. The first key observation about rulial space is that in our models, it’s discrete. In other words, there’s a discrete space of possible rules. Or, put another way, theories are quantized, and there’s somehow an elementary distance between theories—or a minimum distance in “theory space” between neighboring theories. But what units is this distance in? Basically it’s in units of rule—or program—size. Given any program—or rule—we can imagine writing that program out in some language (say in Wolfram Language, or as a program for a particular universal Turing machine, or whatever) And now we can characterize the size of the program by just looking at how many tokens it takes to write the program out. Of course, with different languages, that number will be different—at the simplest level just like the number of decimal digits necessary to represent a number is different from the number of binary digits, or the length of its representation in terms of primes. But it’s just like measuring a length in feet or meters: even though the numerical value is different, we’re still describing the same length. It’s important to point out that it’s not enough to just measure things in terms of “raw information content”, or ordinary bits, as discussed in information theory. Rather, we want some kind of measure of “semantic information content”: information content that directly tells us what computation to do. It’s also important to point out that what we need is different from what’s discussed in algorithmic information theory. Once one has a computation universal system, one can always use it to translate from any one language to any other. And in algorithmic information theory the concept is that one can measure the length of a program up to an additive constant by just expecting to include an “emulation program” that adapts to whatever language one’s measuring the length in. But in the usual formalism of algorithmic information theory one doesn’t worry about how long it’s going to take for the emulation to be done; it’s just a question of whether there’s ultimately enough information to do it. In our setup, however, it does matter how long the emulation takes, because that process of emulation is actually part of our system. And basically we need the number of steps needed for the emulation to be in some sense bounded by a constant. So, OK, what does this mean for the value of ρ? Its units are presumably program size per unit time. And so to define its value, we’ll have to say how we’re measuring program size. Perhaps we could imagine we write our rules in the Wolfram Language. Then there should be a definite value of ρ for our universe, measured in Wolfram-Language-tokens per second. If we chose to use (2,3)-Turing-machine-tape-values per year then we’d get a different numerical value. But assuming we used the correct conversions, the value would be the same. (And, yes, there’s all sorts of subtlety about constant-time or not emulation, etc.) In some sense, we may be able to think of ρ as the ultimate “processor speed for the universe”: how fast tokens in whatever language we’re using are being “interpreted” and actually “executed” to determine the behavior of the universe. Can we estimate the value of ρ? If our units are Wolfram-Language-tokens per second we could start by imagining just computing in the Wolfram Language some piece of the rulial multiway graph for our models and seeing how many operations it takes. To allow “all possible rules” we’d have to increase the possible left- (and right-) hand sides of our rules to reflect the size of the hypergraph representing the universe at each step. But now we’d need to divide by the “number of parallel threads” in the rulial multiway graph. So we can argue that all we’d be left with is something like (size of spatial hypergraph represented in Wolfram Language) / (elementary time). So, based on our previous estimates (which I don’t consider anything more than vaguely indicative yet) we might conclude that perhaps: The number of “parallel threads” in the rulial multiway graph (the rulial analog of Ξ) might then be related to the number of possible hypergraphs that contain about the number of nodes in the universe, or very roughly (10350)^(10350) ≈ . If we ask the total number of Wolfram Language tokens processed by the universe, there’ll be another factor ~10467, but this “parallelism” will completely dominate, and the result will be about. OK, so given a value of ρ, how might we conceivably observe it? Presumably there’s an analog of quantum uncertainty in rulial space, that’s somehow proportional to the value of ρ. It’s not completely clear how this would show up, but one possibility is that it would lead to intrinsic limits on inductive inference. For example, given only a limited observation time, it might be fundamentally impossible to determine beyond a certain (“rulial”) accuracy what rule the universe is following in your description language. There’d be a minimum rate of divergence of behaviors from different rules, associated with the minimum distance between theories—and it would take a certain time to distinguish theories at this rate of divergence. In our models, just like every causal edge in physical and branchial space is associated with energy, so should every causal edge in rulial space be. In other words, the more processing that happens in a particular part of rulial space, the more physical energy one should consider exists there. And just as with the Einstein equations in physical space, or the Feynman path integral in branchial space, we should expect that the presence of energy in a particular region of rulial space should reflect in a deflection of geodesics there. Geodesics in rulial space are the shortest paths from one configuration of the universe to another, using whatever sequences of rules are needed. But although that’s somewhat like what’s considered at a theoretical level in non-deterministic computation, it’s not something we’re usually familiar with: we’re used to picking a particular description language and sticking with it. So exactly what the interpretation of deflections of geodesics in rulial space should be isn’t clear. But there are a few other things we can consider. For example, presumably the universe is expanding in rulial space, perhaps implying that in some sense more descriptions of it are becoming possible over time. What about rulial black holes? As mentioned above, parts of rulial space that correspond to computational reducibility should behave like black holes, where in effect “time stops”. Or, in other words, while in most of the universe time is progressing, and irreducible computation is going on, computational reducibility will cause that process to stop in a rulial black hole. Presumably geodesics near the rulial black hole will be pulled towards it. Somehow when there’s a description language that leads to computational reducibility, languages near it will tend to also stop being able to successfully describe computationally irreducible processes. Can we estimate the density of rulial black holes? Let’s consider the Turing machine case. In effect we’re going to want to know what the density of non-universality is among all possible non-deterministic Turing machines. Imagine we emulate all Turing machines using a single universal machine. Then this is effectively equivalent to asking what fraction of initial conditions for that machine lead to reducible behaviors—or, in essence, in the traditional characterization of Turing machines, halt. But the probability across all possible inputs that a universal Turing machine will halt is exactly Greg Chaitin’s Ω. In other words, the density of rulial black holes in rulial space is governed by Ω. But Ω isn’t just a number like π that we can compute as accurately as we want; it’s noncomputable, in the sense that it can’t be computed to arbitrary accuracy in any finite time by a Turing machine. Now, in a sense it’s not too surprising that the density of rulial black holes is noncomputable—because, given computational irreducibility, to determine for certain whether something is truly a rulial black hole from which nothing can escape one might have to watch it for an unbounded amount of time. But for me there’s something personally interesting about Greg Chaitin’s Ω showing up in any way in a potential description of anything to do with the universe. You see, I’ve had a nearly 40-year-long debate with Greg about whether the universe is “like π” or “like Ω”. In other words, is it possible to have a rule that will let us compute what the universe does just like we can compute (say, in principle, with a Turing machine) the digits of π? Or will we have to go beyond a Turing machine to describe our universe? I’ve always thought that our universe is “like π”; Greg has thought that it might be “like Ω”. But now it looks as if we might both be right! In our models, we’re saying that we can compute what the universe does, in principle with a Turing machine. But what we’re now finding out is that in the full rulial space, general limiting statements pull in Ω. In a particular rulial observation frame, we’re able to analyze the universe “like π”. But if we want to know about all possible rulial observation frames—or in a sense the space of all possible descriptions of the universe—we’ll be confronted with Ω. In our models, the actual operation of the universe, traced in a particular rulial observation frame, is assumed never to correspond to anything computationally more powerful than a Turing machine. But let’s say there was a hypercomputer in our universe. What would it look like? It’d be a place where the effective ρ is infinite—a place where rulial geodesics infinitely diverge—a kind of white hole in rulial space (or perhaps a cosmic event horizon). (We can also think about the hypercomputer as introducing infinite-shortcut paths in the rulial multiway graph which ordinary Turing-machine paths can never “catch up to” and therefore causally affect.) But given a universe that does hypercomputation, we can then imagine defining a rulial multiway graph for it. And then our universe will show up as a black hole in this higher-level rulial space. But OK, so if there’s one level of hypercomputer, why not consider all possible levels? In other words, why not define a hyperrulial multiway graph in which the possible rules that are used include both ordinary computational ones, but also hypercomputational ones? And once we’re dealing with hypercomputational systems, we can just keep going, adding in effect more and more levels of oracles—and progressively ascending the arithmetic hierarchy. (The concept of “intermediate degrees” might be thought to lead to something that isn’t a perfect hierarchy—but I suspect that it’s not robust enough to apply to complete rulial spaces.) Within the hyperrulial multiway graph at a particular level, levels below it will be presumably appear as rulial black holes, while ones above will appear as rulial white holes. And there’s nothing to keep us only considering finite levels of the arithmetic hierarchy; we can also imagine ascending to transfinite levels, and then just keeping going to higher and higher levels of infinity. Of course, according to our models, none of this is relevant to our particular physical universe. But at a theoretical level, we can still at least to some extent “symbolically describe it”, even in our universe.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.1 And We’re Off and Running… : We recently wrapped up the four weeks of our first-ever “Physics track” Wolfram Summer School—and the results were spectacular!  More than 30 projects potentially destined to turn into academic papers—reporting all kinds of progress on the Wolfram Physics Project. When we launched the Wolfram Physics Project just three months ago one of the things I was looking forward to was seeing other people begin to seriously contribute to the project. Well, it turns out I didn’t have to wait long! Because—despite the pandemic and everything—things are already very much off and running! Six weeks ago we made a list of questions we thought we were ready to explore in the Wolfram Physics Project. And in the past five weeks I’m excited to say that through projects at the Summer School lots of these are already well on their way to being answered. If we ever wondered whether there was a way for physicists (and physics students) to get involved in the project, we can now give a resounding answer, “yes”. So what was figured out at the Summer School? I’m not going to get even close to covering everything here; that’ll have to await the finishing of papers (that I’ll be most interested to read!). But I’ll talk here about a few things that I think are good examples of what was done, and on which I can perhaps provide useful commentary. I should explain that we’ve been doing our Wolfram Summer School for 18 years now (i.e. since just after the 2002 publication of A New Kind of Science), always focusing on having each student do a unique original project. This year—for the first time—we did the Summer School virtually, with 79 college/graduate/postdoc/… students from 21 countries around the world (and, yes, 13 time zones). We had 30 students officially on the “Physics track”, but at least 35 projects ended up being about the Wolfram Physics Project. (Simultaneous with the last two weeks of the Summer School we also had our High School Summer Camp, with another 44 students—and several physics projects.) My most important role in the Summer School (and Summer Camp) is in defining projects. For the Physics track Jonathan Gorard was the academic director, assisted by some very able mentors and TAs. Given how new the Wolfram Physics Project is, there aren’t many people who yet know it well, but one of the things we wanted to achieve at the Summer School was to fix that.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.2 Nailing Down Quantum Mechanics : One of the remarkable features of our models is that they basically imply the inevitability of quantum mechanics. But what is the precise correspondence between our models and all the traditional formalism of quantum mechanics? Some projects at the Summer School helped the ongoing process of nailing that down. The starting point for any discussion of quantum mechanics in our models is the notion of multiway systems, and the concept that there can be many possible paths of evolution, represented by a multiway graph. The nodes in the multiway graph represent quantum (eigen)states. Common ancestry among these states defines entanglements between them. The branchial graph then in effect gives a map of the entanglements of quantum states—and in the large-scale limit one can think of this as corresponding to a “branchial space”: The full picture of multiway systems for transformations between hypergraphs is quite complicated. But a key point that has become increasingly clear is that many of the core phenomena of quantum mechanics are actually quite generic to multiway systems, independent of the details of the underlying rules for transitions between states. And as a result, it’s possible to study quantum formalism just by looking at string substitution systems, without the full complexity of hypergraph transformations. A quantum state corresponds to a collection of nodes in the multiway graph. Transitions between states through time can be studied by looking at the paths of bundles of geodesics through the multiway graph from the nodes of one state to another. In traditional quantum formalism different states are assigned quantum amplitudes that are specified by complex numbers. One of our realizations has been that this “packaging” of amplitudes into complex numbers is quite misleading. In our models it’s much better to think about the magnitude and phase of the amplitude separately. The magnitude is obtained by looking at path weights associated with multiplicity of possible paths that reach a given state. The phase is associated with location in branchial space. One of the most elegant results of our models so far is that geodesic paths in branchial space are deflected by the presence of relativistic energy density represented by the multiway causal graph—and therefore that the path integral of quantum mechanics is just the analog in branchial space of the Einstein equations in physical space. To connect with the traditional formalism of quantum mechanics we must discuss how measurement works. The basic point is that to obtain a definite “measured result” we must somehow get something that no longer shows “quantum branches”. Assuming that our underlying system is causal invariant, this will eventually always “happen naturally”.  But it’s also something that can be achieved by the way an observer (who is inevitably themselves embedded in the multiway system) samples the multiway graph. And as emphasized by Jonathan Gorard this is conveniently parametrized by thinking of the observer as effectively adding certain “completions” to the transition rules used to construct the multiway system. It looks as if it’s then straightforward to understand things like the Born rule for quantum probabilities. (To project one state onto another involves a “rectangle” of transformations that have path weights corresponding to the product of those for the sides.) It also seems possible to understand things like destructive interference—essentially as the result of geodesics for different cases landing up at sufficiently distant points in branchial space that any “spanning completion” must pull in a large number of “randomly canceling” path weights.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.3 Local versus Global Multiway Systems : A standard “global” multiway system works by merging branches that lead to globally isomorphic hypergraphs. In Jonathan Gorard’s “completion interpretation of quantum mechanics”, some of these merges represent the results of applying rules that effectively get “added by the observer” as part of their interpretation of the universe. Max Piskunov has criticized the need to consider global hypergraph isomorphism (“Is one really going to compare complete universes?”)—and has suggested instead the idea of local multiway systems. He got the first implementation of local multiway systems done just in time for the Summer School. Consider the rule: {{x, y}, {x, z}} → {{x, z}, {x, w}, {y, w}, {z, w}} Start from the initial state {{{1,1},{1,1}}}. Here’s its global multiway graph, showing both states and events: But now imagine that we trace the fate of every single relation in each hypergraph, and show it as a separate node in our graph. What we get then is a local multiway system. In this case, here are the first few steps: Continue for a few more steps: If we look only at events, we get exactly the same causal graph as for the global multiway system: But in the full local multiway graph every causal edge is “annotated” with the relation (or “expression”) that “carries” causal information between events. In general, two events can be timelike, spacelike or branchlike separated. A local multiway system provides a definite criterion for distinguishing these. When two events are timelike separated, one can go from one to another by following a causal edge. When two events are spacelike separated, their most common ancestor in the local multiway system graph will be an event. But if they are branchlike separated, it will instead be an expression. To reconstruct “complete states” (i.e. spatial hypergraphs) of the kind used in the global multiway system, one needs to assemble maximal collections of expressions that are spacelike separated (“maximal antichains” in poset jargon). But so is the “underlying physics” of local multiway systems the same as global ones? In a global multiway system one talks about applying rules to the collections of expressions that exist in spatial hypergraphs. But in a local multiway system one just applies rules to arbitrary collections of expressions (or relations). And a big difference is that the expressions in these collections can lie not just at different places in a spatial hypergraph, but on different multiway branches. Or, in other words, the evolution of the universe can pick pieces from different potential “branches of history”. This might sound like it’d lead to completely different results. But the remarkable thing is that it doesn’t—and instead global and multiway systems just seem to be different descriptions of what is ultimately the same thing. Let’s assume first that the underlying rules are causal invariant. Then in a global multiway system, branches must always reconverge. But this reconvergence means that even when there are states (and expressions) “on different branches” they can still be brought together into the same event, just like in a local multiway system. And when there isn’t immediately causal invariance, Jonathan’s “completion interpretation” posits that observers in effect add completions which lead to effective causal invariance, with the same reconvergence, and effective involvement of different branches in single events. As Jonathan and Max debated global vs. local multiway systems I joked that it was a bit like Erwin Schrödinger debating Werner Heisenberg in the early days of quantum mechanics. And then we realized: actually it was just like that! Recall that in the Schrödinger picture of quantum mechanics, time evolution operators are fixed, but states evolve, whereas in the Heisenberg picture, states are fixed, but evolution operators evolve. Well, in a global multiway system one’s looking at complete states and seeing how they change as a result of the fixed set of events defined by the rules. But in a local multiway system one has a fixed basis of expressions, and then one’s looking at how the structure of the events that involve these expressions changes. So it’s just like the Schrödinger vs. Heisenberg pictures.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.4 The Concept of Multispace : Right before the Summer School, I’d been doing quite a lot of work on what I was calling “multispace”. In a spatial hypergraph one’s representing the spatial relationships between elements. In a global multiway system one’s representing the branchial relationships between complete states. In a local multiway system spatial and branchial relationships are effectively mixed together. So what is the analog of physical space when branchial relationships are included? I’m calling it multispace. In a case where there isn’t any branching—say an ordinary, deterministic Turing machine—multispace is just the same as ordinary space. But if there’s branching, it’s different. Here’s an experiment I did just before the Summer School in the very simple case of a non-deterministic Turing machine: But I wasn’t really happy with this visualization; the most obvious structure is still the multiway system, and there are lots of “copies of space”, appearing in different states. What I wanted to figure out was how to visualize things so that ordinary space is somehow primary, and the branching is secondary. One could imagine that the elements of the system are basically laid out according to the relationships in ordinary space, merely “bulging out” in a different direction to represent branchial structure. The practical problem is that branchial space may usually be much “bigger” than ordinary space, so the “bulging” may in effect “overpower” the ordinary spatial relationships. But one idea for visualizing multispace—explored by Nikolay Murzin at the Summer School—is to use machine-learning-like methods to create a 3D layout that shows spatial structure when viewed from one direction, and branchial structure when viewed from an orthogonal direction:.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.5 Generational States, the Ontological Basis and Bohmian Mechanics : In our models, multiway graphs represent all possible “quantum paths of evolution” for a system. But is there a way to pick out at least an approximation to a “classical-like path”? Yes–it’s a path consisting of a sequence of what we call “generational states”. And in going from one generational state to another, the idea is to carry out not just one event, as in the multiway graph, but a maximal set of spacelike separated events. In other words, instead of allowing different “quantum branches” containing different orderings of events, we’re insisting that a maximal set of consistent events are all done together. Here’s an example. Consider the rule: {A → AB, B → BBA} Here’s a “classical-like path” made from generational states: These states must appear in the multiway graph, though it typically takes several events (i.e. several edges) to go from one to another (and in general there may be multiple “generational paths”, corresponding to multiple possible “classical-like paths” in a system): But what is the interpretation of generational states in previous discussions of quantum mechanics? Joseph Blazer’s project at the Summer School suggested that they are like an ontological basis. In the standard formalism used for quantum mechanics one imagines that there are lots of quantum states that can form superpositions, etc.—and that classical results emerge only when measurements are done. But even from the earliest days of quantum mechanics (and rediscovered in the 1950s) there is an alternative formalism: so-called Bohmian mechanics, in which everything one considers is a “valid classical state”, but in which there are more elaborate rules of evolution than in the standard formalism. Well, it seems as if generational states are just what Bohmian mechanics is talking about. The set of possible generational states can be thought of as forming an “ontological basis”, of states that “really can exist”, without any “quantum funny business”. But what is the rule for evolution between generational states? One of the perhaps troubling features of Bohmian mechanics is that it implies correlations between spacelike separated events, or in other words, it implies that effects can propagate at arbitrarily high speeds. But here’s the interesting thing: that’s just what happens in our generational states too! In our generational states, though, it isn’t some strange effect that seems to be arbitrarily added to the system: it’s simply a consequence of the consistency conditions we choose to impose in defining generational states.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.6 Classic Quantum Systems and Effects : An obvious check on our models is to see them reproduce classic quantum systems and effects—and several projects at the Summer School were concerned with this. A crucial point (that I mentioned above) is that it’s becoming increasingly clear that at least most of these “classic quantum systems and effects” are quite generic features of our models—and of the multiway systems that appear in them. And this meant that many of the “quantum” projects at the Summer School could be done just in terms of string substitution systems, without having to deal with all the complexities of hypergraph rewriting. Quantum Interference Hatem Elshatlawy, for example, explored quantum interference in our models. He got some nice results—which Jonathan Gorard managed to simplify to an almost outrageous extent. Let’s imagine just having a string in which o represents “empty space”, and X represents the position of some quantum thing, like a photon. Then let’s have a simple sorting rule that represents the photon going either left or right (a kind of minimal Huygens’ principle): {Xo → oX, oX → Xo} Now let’s construct a multiway system starting from a state “oooXooXooo” that we can think of as corresponding to photons going through two “slits” a certain distance apart: The merging of states that we see here is ultimately going to correspond to “quantum interference”. The path weights correspond to the magnitudes of the amplitudes of different states. But the question is: “What final state corresponds to what final photon position?” Different final photon positions effectively correspond to different quantum phases for the photon. But in our models these quantum phases are associated with positions in branchial space. And to get an idea of what’s going on, we can just use the sorting order of strings to give a sense of relative positions in branchial space. (Because of the details of the setup, we need to just use the right-hand half of the strings, then symmetrically repeat them.) If we now do this, and plot the values of the weights (here after 6 steps) this is what we get: Amazingly, this is starting to look a bit like a diffraction pattern. Let’s try “increasing the slit spacing”—by using the initial string “ooooooooXoooXoooooooo”. Now the multiway graph has the form and plotting the weights we get: which is stunningly similar to the standard quantum mechanics result: complete with the expected destructive interference away from the central peak. Computing the corresponding branchial graph we get: which in effect shows the “concentrations of amplitude” into different parts of branchial space (AKA peaks in different regions of quantum phase). (In a sense the fact that this all works is “unsurprising”, since in effect we’re just implementing a discrete version of Huygens’ principle. But it’s very satisfying to see everything come together..
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.7 The Quantum Harmonic Oscillator : The quantum harmonic oscillator is one of the first kinds of quantum systems a typical quantum mechanics textbook will discuss. But how does the quantum harmonic oscillator work in our models? Patrick Geraghty’s project at the Summer School began the process of figuring it out. A classical harmonic oscillator basically has something going back and forth in a certain region at a sequence of possible frequencies. The quantum harmonic oscillator picks up the same “modes”, but now represents them just as quantum eigenstates of a certain energy. In our models it’s actually possible to go back to something very close to the classical picture. We can set up a string substitution system in which something (here B or C) goes back and forth in a string of fixed length: We can make it a bit more obvious what’s going on by changing the characters in the strings: And it’s clear that this system will always go in a periodic cycle. If we were thinking about spacetime and relativity, it might trouble us that we’ve created a closed timelike curve, in which the future merges with the past. But that’s basically what we’re forced into by the idealization of a quantum harmonic oscillator. Recall that in our models energy is associated with the flux of causal edges. Well, in this model of the harmonic oscillator, we can immediately figure out the causal edges: And we can see that as we change the length of the string, the number of causal edges (i.e. the energy) will linearly increase, as we’d expect for a quantum harmonic oscillator: Oh, and there’s even zero-point energy: There’s a lot more to figure out even about the quantum harmonic oscillator, but this is a start.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.8 Quantum Teleportation : One of the strange, but characteristic, phenomena that’s known to occur in quantum mechanics is what’s called quantum teleportation. In a physical quantum teleportation experiment, one creates a quantum-entangled pair of particles, then lets them travel apart. But now as soon as one measures the state of one of these particles, one immediately knows something about the state of the other particle—even though there hasn’t been time to get a light signal from that other particle. At the Summer School, Taufiq Murtadho figured out a rather elegant way to understand this phenomenon in our models. I’ll not go through the details here, but here’s a representation of a key part of the construction: A feature of quantum teleportation is that even though the protocol seems to be transmitting information faster than light, that isn’t really what’s happening when one traces everything through. And what Taufiq found is that in our models the multiway causal graph reveals how this works. In essence, the “teleportation” happens through causal edges that connect branchlike separated states—but these edges cannot transmit an actual measurable message.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.9 Quantum Computing : How do we tell if our models correctly reproduce something like quantum computing? One approach is what I call “proof by compilation”. Just take a standard description of something—here quantum computing—and then systematically “compile” it to a representation in terms of our models. For example, here’s a Pauli-Z gate compiled to the rules for a multiway system: Here now is the result of starting with a superposition of states and running two steps of root-NOT gates: And, yes, we can understand entanglements through branchial graphs, etc.: Here’s a version of the quantum Fourier transform involved in factoring the integer n = 6, converted to one of our multiway systems: And, yes, there’s a lot going on here, but at least it’s happening “in parallel” in different branches of the quantum evolution—in just a few time steps. But the result here is just a superposition of quantum states; to actually find “the answer” we have to do measurements to find which quantum states have the highest amplitude, or largest path weight. In the usual formalism of quantum mechanics, we’d just talk about “doing the measurement”; we wouldn’t discuss what goes on “inside the measurement”. But in our model we can analyze the actual process of measurement. And at least in Jonathan’s “completion interpretation” we can say that the measurement is achieved by a multiway system in which the observer effectively defines completions that merge branches: We’ve included path weights here, and “the answer” can effectively be read off by asking where in branchial space the maximum path weight occurs. But notice that lots of multiway edges (or events) had to be added to get the measurement done; that’s effectively the “cost of measurement” as revealed in our model. So now the obvious question is: “How does this scale as one increases the number n? Including measurement, does the quantum computation ultimately succeed in factoring n in a polynomial number of steps?” We don’t yet know the answer to this—but we’ve now more or less got the wherewithal to figure it out. Here’s the basic picture. When we “do a quantum computation” we get to use the parallelism of having many different “threads” spread across branchial space. But when we want to measure what comes out, we have to “corral” all these threads back together to get a definite “observable” result. And the question is whether in the end we come out ahead compared to doing the purely classical computation. I have to say that I’ve actually wondered about this for a very long time. And in fact, back in the early 1980s, when Richard Feynman and I worked on quantum computing, one of the main things we talked about was the “cost of measurement”. As an example, we looked at the “null” quantum computation of generating “random numbers” (e.g. from a process like radioactive decay)—and we ended up suspecting that there would be inevitable bounds on the “minimum cost of measurement”. So it wouldn’t surprise me at all if in the end the “cost of measurement” wiped out any gains from “quantum parallelism”. But we don’t yet know for sure, and it will be interesting to continue the analysis and see what our models say. I should emphasize that even if it turns out that there can’t be a “formal speed up” (e.g. polynomial vs. super-polynomial) from quantum mechanics, it still makes perfect sense to study “quantum computing”, because it’s basically inevitable that broadening the kinds of physics that are used to do computing will open up some large practical speed ups, even if they’re only by “constant factors”. I might as well mention one slightly strange thought I had—just before the Summer School—about the power of quantum computing: it might be true that in an “isolated system” quantum parallelism would be offset by measurement cost, but that in the actual universe it might not be. Here’s an analogy. Normally in physics one thinks that energy is conserved. But when one considers general relativity on cosmological scales, that’s no longer true. Imagine connecting a very long spring between the central black holes of two distant galaxies (and, yes, it’s very much a “thought experiment”). The overall expansion of the universe will make the galaxies get further apart, and so will continually impart energy to the spring. At some level we can think of this as “mining the energy of the Big Bang”, but on a local scale the result will be an apparent increase in available energy. Well, in our models, the universe doesn’t just expand in physical space; it expands in branchial space too. So the speculation is that quantum computing might only “win” if it can “harvest” the expansion of branchial space. It seems completely unrealistic to get energy by harnessing the expansion of physical space. But it’s conceivable that there is so much more expansion in branchial space that it can be harnessed even locally—to deliver “true quantum power” to a quantum computer.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.10 Corrections to the Einstein Equations : One of the important features of our models is that they provide a derivation of Einstein’s equations from something lower level—namely the dynamics of hypergraphs containing very large numbers of “atoms of space”. But if we can derive Einstein’s equations, what about corrections to Einstein’s equations? At the Summer School, Cameron Beetar and Jonathan Gorard began to explore this. It’s immediately useful to think about an analogy. In standard physics, we know that on a microscopic scale fluids consist of large numbers of discrete molecules. But on a macroscopic scale the overall behavior of all these molecules gives us continuum fluid equations like the Navier–Stokes equations. Well, the same kind of thing happens in our models. Except that now we’re dealing with “atoms of space”, and the large-scale equations are the Einstein equations. OK, so in our analogy of fluid mechanics, what are the higher-order corrections? As it happens, I looked at this back in 1986, when I was studying how fluid behavior could arise from simple cellular automata. The algebra was messy, and I worked it out using the system I had built that was the predecessor to Mathematica. But the end result was that there was a definite form for the corrections to the Navier–Stokes equations of fluid mechanics: OK, so what’s the analog in our models? A key part of our derivation of the Einstein equations involves looking at volumes of small geodesic balls. On a d-dimensional manifold, the leading term is proportional to rd. Then there’s a correction that’s proportional to the Ricci scalar curvature, from which, in essence, we derive the Einstein equations. But what comes after that? It turns out that longtime Mathematica user Alfred Gray had done this computation (even before Mathematica): And basically using this result it’s possible to compute the form that the next-order corrections to the Einstein equations should take—as Jonathan already did in his paper a few months ago: But what determines the parameters α, β, γ that appear here? Einstein’s original equations have the nice feature that they don’t involve any free parameters (apart from the cosmological constant): so long as there’s no “external source” (like “matter”) of energy-momentum the equations in effect just express the “conservation of cross-sectional area” of bundles of geodesics in spacetime. And this is similar to what happens with the Euler equations for incompressible fluids without viscosity—that essentially just express conservation of volume and momentum for “bundles of moving fluid”. But to go further one actually has to know at least something about the structure and interactions of the underlying molecules. The analogy isn’t perfect, but working out the full Einstein equations including matter is roughly like working out the full Navier–Stokes equations for a fluid. But there’s even further one can imagine going. In fluid mechanics, it’s about dealing with higher spatial derivatives of the velocity. In the case of our models, one has to deal with higher derivatives of the spacetime metric. In fluid mechanics the basic expansion parameter is the Knudsen number (molecular mean free path vs. length). In our models, the corresponding parameter is the ratio of the elementary length to a length associated with changes in the metric. Or, in other words, the higher-order corrections are about situations where one ends up seeing signs of deviations from pure continuum behavior. In fluid mechanics, dealing with rarefied gases with higher Knudsen number and working out the so-called Burnett equations (and the various quantities that appear in them) is difficult. But it’s the analog of this that has to be done to fill in the parameters for corrections to the Einstein equations. It’s not clear to what extent the results will depend on the precise details of underlying hypergraph rules, and to what extent they’ll be at least somewhat generic—but it’s somewhat encouraging that at least to first order there are only a limited number of possible parameters. In general, though, one can say that higher-order corrections can get large when the “radius of curvature” approaches the elementary length—or in effect sufficiently close to a curvature singularity.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.11 Gauge Groups Meet Hypergraphs : Local gauge invariance is an important feature of what we know about physics. So how does it work in our models? At the Summer School, Graham Van Goffrier came up with a nice analysis that made considerably more explicit what we’d imagined before. In the standard formalism of mathematical physics, based on continuous mathematics, one imagines having a fiber bundle in which at each point in a base space one has a fiber containing a copy of the gauge group, which is normally assumed to be a Lie group. But as Graham pointed out, one can set up a direct discrete analog of this. Imagine having a base space that’s a graph like: Now consider a discrete approximation to a Lie group, say the cyclic group like C6 approximating U(1): Now imagine inserting the vertices of this at every point of the “base lattice”. Here’s an example of what one can get: The red hexagons here are just visual guides; the true object simply has connections that knit together the “group elements” on each fiber. And the remarkable thing is that this can be thought of as a very direct discrete approximation to a fiber bundle—where the connections correspond quite literally to the so-called connections in the fiber bundle, that “align” the copies of the gauge group at each point, and in effect implement the covariant derivative. In our models the structure of the discrete analog of the fiber bundle has to emerge from the actual operation of underlying hypergraph rules. And most likely this happens because there are multiple ways in which a given rule can locally be applied to a hypergraph, effectively leading to the kind of local symmetry we see appearing at every point of the base space. But even without knowing any of the details of this, we can already work some things out just from the structure of our “fiber bundle graph”. For example, consider tracing out “Wilson loops” which visit fibers around a closed loop—and ask what the “total group action” associated with this process is. But by direct analogy with electromagnetism we can now interpret this as the “magnetic flux through the Wilson loop”. But what happens if we look at the total flux emanating from a closed volume? For topological reasons, it’s inevitable that this is quantized. And even in the simple setup shown here we can start to interpret nonzero values as corresponding to the presence of “magnetic monopoles”.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.12 Not Even Just Fundamental Physics : I developed what are now being called “Wolfram models” to be as a minimal and general as possible. And—perhaps not too surprisingly therefore—the models are looking as if they’re also very relevant for all sorts of things beyond fundamental physics. Several of these things got studied at the Summer School, notably in mathematics, in biology and in other areas of physics. The applications in mathematics look to be particularly deep, and we’ve actually been working quite intensively on them over the past couple of weeks—leading to some rather exciting conclusions that I’m hoping to write about soon. When it comes to biology, it seems possible that our models may be able to provide a new approach to thinking about biological evolution, and at the Summer School Antonia Kaestner and Tali Beynon started trying to understand how graphs—and multiway systems—might be used to represent evolutionary processes: Another project at the Summer School, by Matthew Kafker (working with Christopher Wolfram), concerned hard sphere gases. I have a long personal history with hard sphere gases: looking at them was what first got me interested—back in 1972—in questions about the emergence of complexity. So I was quite surprised that after all these years, there was something new to consider with them. But a feature of our models is that they suggest a different way to look at systems. So what if we think of the collisions in a hard sphere gas as events? Then—just like in our models—we can make a causal graph that shows the causal relationships between these events: And—just like in our models—we can define light cones and so on. But what does this tell us about hard sphere gases? Standard statistical mechanics approaches look at local statistical properties—in a sense making a “molecular chaos” assumption that everything else is random. But the causal graph has the potential to give us much more global (and long-range) information—which is likely to be increasingly important as the density of the hard sphere gas increases. Hard sphere gases are based on classical physics. But given that our models naturally include quantum mechanics, does that give us a way to study quantum gases, or quantum fluids? At the Summer School Ivan Martinez studied a quantum generalization of my 1986 cellular automaton fluids model. In that model discrete idealized molecules undergo 2- and 3-body collisions. And when I originally set this up, I just picked possible outcomes from these collisions consistent with momentum conservation. But there are several choices to make—and with the understanding we now have, the obvious thing to do is just to follow all choices, and make a multiway system. Here are the collisions and possible outcomes: A single branch of the multiway system produces a specific pattern of fluid flow. But the whole multiway system represents a whole collection of quantum states—or in effect a quantum fluid (and in the most obvious version of the model, a Fermi fluid). So now we can start to ask questions about the quantum fluid, studying branchial graphs, event horizons, etc.
A Burst of Physics Progress at the 2020 Wolfram Summer School 6.13 And Lots of Other Projects Too… : I’ve talked about 11 projects so far here—but that’s less than a third of all the Wolfram Physics–related projects at the Summer School. There were projects about the large-scale structure of hypergraphs, and phenomena like the spatial distribution of dimension, time variation of dimension and possible overall growth rates of hypergraphs. There was a project about characterizing overall structures of hypergraphs by finding PDE modes on them (“Weyl’s law for graphs”). What happens if you look at the space of all possible hypergraphs, and for example form state transition graphs by applying rules? One project explored subgraphs excluded by evolution (“the approach to attractor states”). Another project explored the structure of the space of possible hypergraphs, and the mathematical analysis of ergodicity in it. One of the upcoming challenges in our models is about identifying “particles” and their properties. One project started directly hunting for particles by looking at the effects of perturbations in hypergraphs.  Another studied the dynamics of specific kinds of “topologically stable defects” in hypergraphs. There was a project looking for global conservation laws in hypergraph rewriting, and another studying local graph invariants. There was also a project that started to make a rather direct detector of gravitational waves in our models. There were projects that analysed the global behavior of our models. One continued the enumeration of cases in which black holes arise. Another looked at termination and completion in multiway systems. Still another compared growth in physical vs. branchial space. I mentioned above the concept of “proof by compilation” and its use in validating the quantum features of our models. One project at the Summer School began the process of using our models as a foundation for practical “numerical general relativity” (in much the same way as my cellular automata fluids have become the basis for practical fluid computations). There are lots of interesting questions about how our models relate to known features of physics. And there were projects at the Summer School about understanding the emergence of rotational invariance and CPT invariance as well as the AdS/CFT correspondence (and things like the Bekenstein bound). There were projects about the Wolfram Physics Project not only at our Summer School, but also at our High School Summer Camp. One explored the emergent differential geometry of a particular one of our models that makes something like a manifold with curvature. Others explored fundamental aspects of models like ours. One searched for multiway systems with intermediate growth. Another explored multiway systems based on cyclic string substitutions. There were still other projects at both the Summer School and Summer Camp that explored systems from the computational universe—now informed by ideas from the Wolfram Physics Project. One looked at non-deterministic Turing machines; another looked at combinators. I suggested most of the projects I’ve discussed here, and that makes it particularly satisfying for me to see how well they’ve progressed. Few are yet “finished”, but they’re all off and running, beginning to build up a serious corpus of work around the Wolfram Physics Project. And I’m looking forward to seeing how they develop, what they discover, how they turn into papers—and how they seed other work which will help explore the amazing basic science opportunity that’s opened up with the Wolfram Physics Project.
The Empirical Metamathematics of Euclid and Beyond 7.1 Towards a Science of Metamathematics : One of the many surprising things about our Wolfram Physics Project is that it seems to have implications even beyond physics. In our effort to develop a fundamental theory of physics it seems as if the tower of ideas and formalism that we’ve ended up inventing are actually quite general, and potentially applicable to all sorts of areas. One area about which I’ve been particularly excited of late is metamathematics—where it’s looking as if it may be possible to use our formalism to make what might be thought of as a “bulk theory of metamathematics”. Mathematics itself is about what we establish about mathematical systems. Metamathematics is about the infrastructure of how we get there—the structure of proofs, the network of theorems, and so on.  And what I’m hoping is that we’re going to be able to make an overall theory of how that has to work: a formal theory of the large-scale structure of metamathematics—that, among other things, can make statements about the general properties of “metamathematical space”. Like with physical space, however, there’s not just pure underlying “geometry” to study. There’s also actual “geography”: in our human efforts to do mathematics over the last few millennia, where in metamathematical space have we gone, and “colonized”? There’ve been a few million mathematical theorems explicitly published in the history of human mathematics. What does the “empirical metamathematics” of them reveal? Some of it presumably reflects historical accidents, but some may instead reflect general features of metamathematics and metamathematical space. I’ve wondered about empirical metamathematics for a long time, and tucked away on page 1176 at the end of the Notes for the section about “Implications for Mathematics and Its Foundations” in A New Kind of Science is something I wrote more than 20 years ago about it: This note is mostly about what a descriptive theory of empirical metamathematics might be like—for example characterizing what one might mean by a powerful theorem, a deep theorem, a surprising theorem and so on. But at the end of the note is a graph: an actual piece of quantitative empirical metamathematics, based on the best-known structured piece of mathematics in history—Euclid’s Elements. The graph shows relationships between theorems in the Elements: a kind of causal graph of how different theorems make use of each other. As presented in A New Kind of Science, it’s a small “footnote item” that doesn’t look like much. But for more than 20 years, I’ve kept wondering what more there might be to learn from it. And now that I’m trying to make a general theory of metamathematics, it seemed like it was a good time to try to find out.
The Empirical Metamathematics of Euclid and Beyond 7.2 The Most Famous Math Book in History : Euclid’s Elements is an impressive achievement. Written in Greek around 300 BC (though presumably including many earlier results), the Elements in effect defined the way formal mathematics is done for more than two thousand years. The basic idea is to start from certain axioms that are assumed to be true, then—without any further “input from outside”—use purely deductive methods to establish a collection of theorems. Euclid effectively had 10 axioms (5 “postulates” and 5 “common notions”), like “one can draw a straight line from any point to any other point”, or “things which equal the same thing are also equal to one another”. (One of his axioms was his fifth postulate—that parallel lines never meet—which might seem obvious, but which actually turns out not to be true for physical curved space in our universe.) On the basis of his axioms, Euclid then gave 465 theorems. Many were about 2D and 3D geometry; some were about arithmetic and numbers. Among them were many famous results, like the Pythagorean theorem, the triangle inequality, the fact that there are five Platonic solids, the irrationality of and the fact that there are an infinite number of primes. But certainly not all of them are famous—and some seem to us now pretty obscure. And in what has remained a (sometimes frustrating) tradition of pure mathematics for more than two thousand years, Euclid never gives any narrative about why he’s choosing the theorems he does, out of all the infinitely many possibilities. We don’t have any original Euclids, but versions from a few centuries later exist. They’re written in Greek, with each theorem explained in words, usually by referring to a diagram. Mathematical notation didn’t really start getting invented until the 1400s or so (i.e. a millennium and a half later)—and even the notation for numbers in Euclid’s time was pretty unwieldy. But Euclid had basically modern-looking diagrams, and he even labeled points and angles with (Greek) letters—despite the fact that the idea of variables standing for numbers wouldn’t be invented until the end of the 1500s. There’s a stylized—almost “legalistic”—way that Euclid states his theorems. And so far as we can tell, in the original version, all that was done was to state theorems; there was no explanation for why a theorem might be true—no proof offered. But it didn’t take long before people started filling in proofs, and there was soon a standard set of proofs, in which each particular theorem was built up from others—and ultimately from the axioms. There’ve been more than a thousand editions of Euclid printed (probably more than any other book except the Bible), and reading Euclid was until quite recently part of any serious education. (At Eton—where I went to high school—it was only in the 1960s that learning “mathematics” began to mean much other than reading Euclid, in the original Greek of course.) Here’s an edition of Euclid from the 1800s that I happen to own, with the proof of every theorem giving little references to other theorems that are used: But so what about the metamathematics of Euclid? Given all those theorems—and proofs—can we map out the structure of what Euclid did? That’s what the graph in A New Kind of Science was about. A few years ago, we put the data for that graph into our Wolfram Data Repository—and I looked at it again, but nothing immediately seemed to jump out about it; it still just seemed like a complicated mess: What else happened? One thing is that we added automated theorem proving to Mathematica and the Wolfram Language. Enter a potential theorem, and axioms from which to derive it, and FindEquationalProof will try to generate a proof. This works well for “structurally simple” mathematical systems (like basic logic), and indeed one can generate proofs with complex networks of lemmas that go significantly beyond what humans can do (or readily understand): It’s in principle possible to use these methods to prove theorems in Euclidean geometry too. But it’s a different problem to make the proofs readily understandable to humans (like the step-by-step solutions of Wolfram|Alpha). So at least for now—even after 2000 years—the most effective source of information about the empirical metamathematics of proofs of Euclid’s theorems is still basically going to be Euclid’s Elements. But when it comes to representing Euclid’s theorems there’s something new. The whole third-of-a-century story of the Wolfram Language has been about finding ways to represent more and more things in the world computationally. I had long wondered what it would take to represent Euclid-style geometry computationally. And in April I was excited to announce that we’d managed to do it:.
The Empirical Metamathematics of Euclid and Beyond 7.3 Basic Statistics of Euclid : Euclid’s Elements is divided into 13 “books”, containing a total of 465 theorems (and 131 definitions): Stating the theorems takes 9589 words (about 60k characters) of Greek (about 13,000 words in a standard English translation). (The 10 axioms take another 115 words in Greek or about 140 in English, and the definitions another 2369 words in Greek or about 3300 in English.) A typical theorem (or “proposition”)—in this case Book 1, Theorem 20—is stated as: (This is what we now call the triangle inequality. And of course, to make this statement we have to have defined what a triangle is, and Euclid does that earlier in Book 1.) If we look at the statements of Euclid’s theorems in Greek (or in English), there’s a distribution of lengths (colored here by subjects, and reasonably fit by a Pascal distribution): The “outlier” longest-to-state theorem (in both Greek and English) is the rather unremarkable 103-Greek-word 3.8 which can be illustrated as: (The runner-up, at about two-thirds the length, is the also rather unremarkable 11.35.) The nominally shortest-to-state theorems are in Book 10, Theorems 85 through 90, and all have just 4 Greek words: The shortness of these theorems is a bit of a cheat, since the successive “apotomes” (pronounced /əˈpɒtəmi/ like “hippopotamus”) actually have quite long definitions that are given elsewhere. And, yes, some emphasis in math has changed in the past 2000+ years; you don’t hear about apotomes these days. (An apotome is a number x – y where isn’t rational, but is—as for , y = 1. It’s difficult enough to describe even this without math notation. But then for a “first apotome” Euclid added the conditions that both and x must be rational—all described in words.) At five words, we’ve got one more familiar theorem (3.30) and another somewhat obscure one (10.26): In our modern Wolfram Language representation, we’ve got a precise, symbolic way to state Euclid’s theorems. But Euclid had to rely on natural language (in his case, Greek). Some words he just assumed people would know the meanings of. But others he defined.  Famously, he started at the beginning of Book 1 with his Definition 1—and in a sense changing how we think about this is what launched our whole Physics Project: There is at least an implicit network of dependencies among Euclid’s definitions. Having started by defining points and lines, he moves on to defining things like triangles, and equilaterality, until eventually, for example, by Book 11 Definition 27 he’s saying things like “An icosahedron is a solid figure contained by twenty equal and equilateral triangles”. Of course, Euclid didn’t ultimately have to set up definitions; he could just have repeated the content of each definition every time he wanted to refer to that concept. But like words in natural language—or functions in our computational language—definitions are an important form of compression for making statements. And, yes, you have to pick the right definitions to make the things you want to say easy to say. And, yes, your definitions will likely play at least some role in determining what kinds of things you choose to talk about. (Apotomes, anyone?.
The Empirical Metamathematics of Euclid and Beyond 7.4 The Interdependence of Theorems : All the theorems Euclid states represent less than 10,000 words of Greek. But the standard proofs of them are perhaps 150,000 words of Greek.  (They’re undoubtedly not minimal proofs—but the fact that the same ones are being quoted after more than 2000 years presumably tells us at least something.) Euclid is very systematic. Every theorem throughout the course of his Elements is proved in terms of earlier theorems (and ultimately in terms of his 10 axioms). Thus, for example, the proof of 1.14 (i.e. Book 1, Theorem 14) uses 1.13 as well as the axioms P2 (i.e. Postulate 2), P4, CN1 (i.e. Common Notion 1) and CN3. By the time one’s got to 12.18 the proof is written only in terms of other theorems (in this case 12.17, 12.2, 5.14 and 5.16) and not directly in terms of axioms. The total number of theorems (or axioms) directly referenced in a given proof varies from 0 (for axioms) to 21 (for 12.17, which is about inscribing polyhedra in spheres); the average is 4.3: If we put Euclid’s axioms and theorems in order, we can represent which axioms or theorems occur in a given proof by an arrangement of dots across the page. For example, for 1.12 through 1.17 we have: Doing this for all the theorems we get: We can see there’s lots of structure here. For example, there are clearly “popular” theorems near the beginning of Book 6 and Book 10, to which lots of at least “nearby” theorems refer. There are also “gaps”: ranges of theorems that no theorems in a given book refer to. At a coarse level, something we can do is to look at cross-referencing within and between books: The size of each node represents the number of theorems in each book. The thickness of each arrow represents the fraction of references in the proofs of those theorems going to different books. The self-loops are from theorems in a given book that refer to theorems in the same book.  Needless to say, the self-loop is large for Book 1, since it doesn’t have any previous book to refer to. Book 7 again has a large self-loop, because it’s the first book about numbers, and doesn’t refer much to the earlier books (which are about 2D geometry). It’s interesting to see that Books 7, 8 and 9—which are about numbers rather than geometry—“keep to themselves”, even though Book 10, which is also about numbers, is more central. It’s also interesting to see the interplay between the books on 2D and 3D geometry over on the right-hand side of the graph. But, OK, what about individual theorems? What is their network of dependencies? Here’s 1.5, whose proof is given in terms of 1.3 and 1.4, as well as the axioms P1, P2 and CN3: But now we can continue this, and show what 1.3 and 1.4 depend on—all the way down to the axioms: Later theorems depend on much more. Here are the direct dependencies for 12.18: Here’s what happens if one goes another step: Here’s 3 steps: And here’s what happens if one goes all the way down to the axioms (which in this case takes 5 steps): Things look a little simpler if we consider the transitive reduction of this graph. We’re no longer faithfully representing what’s in the text of Euclid, but we’re still capturing the core dependency information. If theorem A in Euclid refers to B, and B refers to C, then even if in Euclid A refers to C we won’t mention that. And, yes, graph theoretically A→C is just the transitive closure of A→B and B→C. But it could still be that the pedagogical structure of the proof of theorem A makes it desirable to refer to theorem B, even if in principle one could rederive theorem B from theorem C. Here’s the original 1-step graph for 12.18, along with its transitive reduction: And here, by the way, is also the “fully pedantic” transitive closure, including all indirect connections, whether they’re mentioned by Euclid or not: And now here’s the transitive reduction of the full 12.8 dependency graph, all the way down to the axioms: And what all these graphs show is that even to prove one theorem, one’s making use of lots of other theorems.  To make this quantitative, we can plot the total number of theorems that appear anywhere in the “full proof” of a given theorem, ultimately working all the way down to the axioms: At the beginnings of many of the books, there tend to be theorems that are proved more directly from the axioms, so they don’t depend on as much. But as one progresses through the books, one’s relying on more and more theorems—sometimes, as we saw above, in the same book, and sometimes in earlier books. From the picture above, we can see that Euclid in a sense builds up to a “climax” at the end—with his very last theorem (13.18) depending on more theorems than anything else. We’ll be discussing “Euclid’s last theorem” some more below...
The Empirical Metamathematics of Euclid and Beyond 7.5 The Graph of All Theorems : OK, so what is the full interdependence graph for all the theorems in Euclid? It’s convenient to go the opposite way than in our previous graphs—and put the axioms at the top, and show how theorems below are derived from them. Here’s the graph one gets by doing that: One can considerably simplify this by looking just at the transitive reduction graph (the full graph has 2054 connections; this reduction has 974, while if one went “fully pedantic” with transitive closure, one would have 25,377 connections): What can we see from this? Probably the most obvious thing is that the graphs start fairly sparse, then become much denser. And what this effectively means is that one starts off by proving certain “preliminaries”, and then after one’s done that, it unlocks a mass of other theorems. Or, put another way, if we were exploring this metamathematical space starting from the axioms, progress might seem slow at first. But after proving a bunch of preliminary theorems, we’d be able to dramatically speed up. Here’s another view of this, plotting how many subsequent theorems depend on each different theorem: In a sense, this is complementary to the plot we made above, that showed how many theorems a given theorem depends on. (From a graph-theoretical point of view they’re very directly complementary: this plot involves VertexInComponent; the previous one involved VertexOutComponent.) And what the plot shows is that there are a bunch of early theorems (particularly in Book 1) that have lots of subsequent theorems depending on them—so that they’re effectively foundational to much of what follows. The plot also shows that in most of the books the early theorems are the most “foundational”, in the sense that the most subsequent theorems depend on them. By the way, we can also look at the overall form of the basic dependency graph, not layering it starting from the axioms: The transitive reduction is slightly easier to interpret: And the main notable feature is the presence of “prongs” associated, for example, with Book 9 theorems about the properties of even and odd numbers.
The Empirical Metamathematics of Euclid and Beyond 7.6 The Causal Graph Analogy : Knowing about the Wolfram Physics Project, there’s an obvious analog of theorem dependency graphs: they’re like causal graphs. You start from a certain set of “initial events” (the “big bang”), corresponding to the axioms. Then each subsequent theorem is like an event, and the theorem dependency graph is tracing out the causal connections between these events. Just like the causal graph, the theorem dependency graph defines a partial ordering: you can’t write down the proof of a given theorem until the theorems that will appear in it have been proved. Like in the causal graph, one can define light cones: there’s a certain set of “future” theorems that can be affected by any given theorem. Here is the “future light cone” of Book 1, Theorem 5: And here is the corresponding transitive reduction graph: But now let’s think about the notion of time in the theorem dependency graph. Imagine you were rederiving the theorems in Euclid in a series of “time steps”.  What would you have to do at each time step? The theorem dependency graph tells you what you will have to have done in order to derive a particular theorem. But just like for spacetime causal graphs, there are many different foliations one can use to define consistent time steps. Here’s an obvious one, effectively corresponding to a “cosmological rest frame” in which at each step one “does as much as one consistently can at that step”: And here are the number of theorems that appear on each slice (in effect each theorem appears on the slice determined by its longest path to any axiom): But there are many other foliations that are possible, in which one for example concentrates first on a particular group of theorems, only doing others when one “needs to”. Each choice of foliation can be thought of as corresponding to a different reference frame—and a different choice of how one explores the analog of spacetime in Euclid. But, OK, if the foliations define successive moments in time—or successive “simultaneity surfaces”—what is the analog of space?  In effect, the “structure of space” is defined by the way that theorems are laid out on the slices defined by the foliations. And a convenient way to probe this is to look at branchial graphs, in which pairs of theorems on a given slice are connected by an edge if they have an immediate common ancestor on the slice before. So here are the branchial graphs for all successive slices of Euclid in the “cosmological rest frame”: And here are the branchial graphs specifically from slices 23 and 26: How should we interpret these graphs? Just like in quantum mechanics, they effectively define a map of “entanglements”, but now these are “entanglements” not between quantum states but between theorems. But potentially we can also interpret these graphs as showing how theorems are laid out in a kind of “instantaneous metamathematical space”—or, in effect, we can use the graphs to define “distances between theorems”. We can generalize our ordinary branchial graphs by connecting theorems that have common ancestors not just one slice back, but also up to δt slices back. Here are the results for slice 26 (in the cosmological rest frame): If we went all the way back to the axioms (the analog of the “big bang”) then we’d just get a complete graph, connecting all the theorems on slice 26. But here we’re seeing in effect “fuzzier and fuzzier” versions of how the theorems that exist at slice 26 can be thought of as being “metamathematically laid out”. The disconnected components in these branchial graphs represent theorems that have no recent shared history—so that in some sense they’re “causally disconnected”. In thinking about “theorem search”, it’s interesting to try to imagine measures of “distance between theorems”—and in effect branchial distance captures some of this. And even for Euclid there are presumably things to learn about the “layout” of theorems, and what should count as “close to” what. There are only 465 theorems in Euclid’s Elements. But what if there were many more? What might the “metamathematical space” they define be like? Just as for the hypergraphs—or, for that matter, the multiway graphs—in our models of physics we can ask questions about the limiting emergent geometry of this space. And—ironically enough—one thing we can immediately say is that it seems to be far from Euclidean! But does it for example have some definite effective dimension? There isn’t enough data to say much about the branchial slices we just saw. But we can say a bit more about the complete theorem dependency graph—which is the analog of the multiway graph in our physics models. For example, starting with the axioms (the analog of the “big bang”) we can ask how many theorems are reached in successive steps. The result (counting the axioms) is: If we were dealing with something that approximated a d-dimensional manifold, we’d expect these numbers to be of order rd. Computing their logarithmic differences to fit for d gives if one starts from the axioms, and if one starts from all possible theorems in the network. One gets somewhat different results if one deals not with the actual theorem dependency graph in Euclid, but instead with its transitive reduction—removing all “unnecessary” direct connections. Now the number of theorems reached on successive steps is: The “dimension estimate” based on theorems reached starting from the axioms is: while the corresponding result starting from all theorems is: Euclid’s Elements represents far too little data to make a definite statement, but perhaps there’s a hint of 2-dimensional structure, with positive curvature.
The Empirical Metamathematics of Euclid and Beyond 7.7 The Most Difficult Theorem in Euclid : One way to assess the “difficulty” of a theorem is to look at what results have to have already been built up in order to prove the theorem. And by this measure, the most difficult theorem in Euclid’s Elements is the very last theorem in the last book—what one might call “Euclid’s last theorem”, the climax of the Elements—Book 13, Theorem 18, which amounts to the statement that there are five Platonic solids, or more specifically: This theorem uses all 10 axioms, and 219 of the 464 previous theorems. Here’s its graph of dependencies: And here is the transitive reduction of this—notably with different subject areas being more obviously separated: This shows how 13.18 and its prerequisites (its “past light cone”) sit inside the whole theorem dependency graph: If we started from the axioms, the longest chains of theorems we’d have to prove to get to 13.18 are: Or in other words, from CN1 and from P1 and P3 we’d have to go 33 steps to reach 13.18. If we actually look at the paths, however, we see that after different segments at the beginning, they all merge at Book 6, Theorem 1, and then are the same for the last 14 steps: (Theorem 6.1 is the statement that both triangles and parallelograms that have the same base and same height have the same area, i.e. one can skew a triangle or parallelogram without changing its area.) How much more difficult than other theorems is 13.18? Here’s a histogram of maximum path lengths for all theorems (ignoring cases to be discussed later where a particular theorem does not use a given axiom at all): And here’s how the maximum path length varies through the sequence of all 465 theorems: In the causal graph interpretation, and using the “flat foliation” (i.e. the “cosmological rest frame”) what this basically shows is at what “time slice” a given theorem first emerges from Euclid’s proofs. Or, in other words, if one imagines exploring the “metamathematical space of Euclid” by going “one level of theorems at a time”, the order in which one will encounter theorems is: A question one might ask is whether “short-to-state” theorems are somehow “easier to prove” than longer-to-state ones. This shows the maximum path length to prove theorems as a function of the length of their statements in Euclid’s Greek. Remarkably little correlation is seen. This plot shows instead the number of “prerequisite theorems” as a function of statement length: Once again there is poor correlation.
The Empirical Metamathematics of Euclid and Beyond 7.8 The Most Popular Theorems in Euclid : How often do particular theorems get used in the proofs of other theorems? The “most popular” theorems in terms of being directly quoted in the proofs of other theorems are: Notably, all but one of 10.11’s direct mentions are in other theorems in Book 10. Theorem 6.1 (which we already encountered above) is used in 4 books. By the way, there is some subtlety here, because 26 theorems reference a particular theorem more than once in their proofs: for example, 10.4 references 10.3 three times, while 13.18 references both 13.17 and 13.16 twice: But looking simply at the distribution of the number of direct uses (here on a log scale), we see that the vast majority of theorems are very rarely used—with just a few being quite widely used: Indicating the number of direct uses by size, here are the “directly popular” theorems: If we ask also about indirect uses, the results are as follows: Not too surprisingly, the axioms and early theorems are the most popular. But overall, the distribution of total number of uses is somewhat broader than the distribution of direct uses: This shows all theorems, with their sizes in the graph essentially determined by the sizes of their “future light cone” in the theorem dependency graph: In addition to asking about direct and indirect uses, one can also assess the “centrality” of a given theorem by various graph-theoretical measures. One example is betweenness centrality (the fraction of shortest paths that pass through a given node): The theorems with top betweenness centralities are 1.31 (construction of parallel lines), 10.12 (transitivity of commensurability), 10.9 (commensurabilty in squares), 8.4 (continued ratios in lowest terms), etc. For closeness centrality (average inverse distance to all other nodes) one gets.
The Empirical Metamathematics of Euclid and Beyond 7.9 What Really Depends on What? : Euclid’s Elements starts with 10 axioms, from which all the theorems it contains are derived. But what theorems really depend on what axioms? This shows how many of the 465 theorems depend on each of the Common Notions and Postulates according to the proofs given in Euclid: The famous fifth postulate (that parallel lines do not cross) has the fewest theorems depending on it. (And actually, for many centuries there was a suspicion that no theorems really depended on it—so people tried to find proofs that didn’t use it, although ultimately it became clear it actually was needed.) Interestingly, at least according to Euclid, more than half (255 out of 465) of the theorems actually depend on all 10 axioms, though one sees definite variation through the course of the Elements: The number of theorems depending on different numbers of axioms is: Scattered through the Elements there are 86 theorems that depend only on one axiom, most often CN1 (which is transitivity of equality): In most cases, the dependence is quite direct, but there are cases in which it is actually quite elaborate, such as: These get slightly simpler after transitive reduction: We can now also ask the opposite question of how many theorems don’t depend on any given axiom (and, yes, this immediately follows from what we listed above): And in general we can ask what subsets of the axioms different theorems depend on. Interestingly, of the 1024 possible such subsets, only 19 actually occur, suggesting some considerable correlation between the axioms. Here is a representation of the partial ordering of the subsets that occur, indicating in each case for how many theorems that subset of dependencies occurs.
The Empirical Metamathematics of Euclid and Beyond 7.10 The Machine Code of Euclid: All the Way Down to Axioms : Any theorem in Euclid can ultimately be proved just by using Euclid’s axioms enough times. In other words, the proofs Euclid gave were stated in terms of “intermediate theorems”—but we can always in principle just “compile things down” so we just get a sequence of axioms. And here for example is how that works for Book 1, Theorem 5: Of course it’s much more efficient to “share the work” by using intermediate theorems: This doesn’t change the “depth”—i.e. the length of any given path to get to the axioms. But it reduces the number of independent paths that have to be followed, because every time one reaches the same theorem (or axiom) one just “uses what one already knows about it”. But to get a sense of the “axiomatic machine code” of Euclid we can just “compile” the proof of every theorem down to its underlying sequence of axioms. And for example if we do this for 3.18 the final sequence of axioms we get has length 835,416. These are broken down among the various axioms according to: Here is a plot of the lengths of axiom sequences for all the theorems, shown on a log scale: Interestingly, 3.18 isn’t the theorem with the longest axiom sequence; it’s in 4th place, and the top 10 are (in gray are the results with intermediate theorems allowed): (10.72 is about addition of incommensurable medial areas, and is never referenced anywhere; 12.14 says the volumes of cones and cylinders with equal bases are proportional to their heights; 12.15 says the heights and bases of cones and cylinders with equal volumes are inversely proportional; etc.) Here’s the distribution of the lengths of axiom sequences across all theorems: We can get some sense of the dramatic value of “remembering intermediate theorems” by comparing the total number of “intermediate steps” obtained with and without merging different instances of the same theorem: For example, for 8.13, 229 steps are needed when intermediate theorems are remembered, while 14,412,576 steps are needed otherwise. (For 10.72, it’s 184 vs. 23,921,481 steps..
The Empirical Metamathematics of Euclid and Beyond 7.11 Superaxioms, or What Are the Most Powerful Theorems? : Euclid’s 10 axioms are ultimately all we need in order to prove all the 465 theorems in the Elements. But what if we supplement these axioms with some of the theorems? Are there small sets of theorems we can add that will make the proofs of many theorems much shorter? To get a full understanding of this, we’d have to redo all the proofs. But we can get some sense of it just from the theorem dependency graph. Consider the graph representing the proof of 1.12, with 1.7 highlighted: Now imagine adding 1.7 as a “superaxiom”. Doing this, we can get a smaller proof graph for 1.12—with 4 nodes (and 14 connections) fewer: What does adding 1.7 as a superaxiom do for the proofs of other theorems? Here’s how much it shortens each of them: (The largest shortening is for 1.8, followed by 4.1.) So what are the “best” superaxioms to add? Here’s a plot of the average amount of shortening achieved by adding each possible individual theorem as a superaxiom: The rather unimpressive best result—an average shortening of 7.2—is achieved with 10.33 (which says that it’s possible to come up with numbers x and y such that and are irrational, while x y and x + y are rational). The maximum shortenings are more impressive—with 10.41 and 10.78 achieving the maximum shortening of 165 although this shortening is very concentrated around “nearby theorems”: By the way, adding a superaxiom can not only decrease the number of intermediate theorems used in a proof, it can also decrease the “depth” of the proof, i.e. the longest path needed to reach an axiom (or superaxiom). Here is the average depth reduction achieved by adding each possible theorem as a superaxiom: (The peak in Book 9 is 9.15, which reduces the depth of many subsequent theorems by 10 steps, though—in a possible goof—is not actually used by Euclid in the proofs of any of them.) Here is the maximum depth reduction achieved by adding each possible theorem.
The Empirical Metamathematics of Euclid and Beyond 7.12 Formalizing Euclid : Everything we’ve discussed so far is basically derived from the original text of Euclid’s Elements. But what if we look instead at the pure “mathematical content” of Euclid? We’ve now got a way to represent this in the Wolfram Language. Consider Euclid’s 3.16. It asserts that: Well, we can now give a “computational translation” of this: And this is all we need to say to define that theorem in Euclid. Given the definition of the Wolfram Language, this is completely self-contained, and ready to be understood by both computers and humans. And from this form, we can now for example compute a random instance of the theorem: As another example, here’s Euclid’s 4.2: This is now asking for a construction—or, effectively, stating the theorem that it’s possible to do such a construction with ruler and compass. And again we can give a computable version of this in the Wolfram Language, including the construction: It’s interesting to see, though, how the computable versions of theorems compare to their textual ones. Here are length comparisons for 2D geometry theorems: And we see that there is indeed at least some correlation between the lengths of textual and symbolic representations of theorems (the accumulation of points on the left is associated with constructions, where the text just says what’s wanted, and the symbolic form also says how to do it): In the Wolfram Language representation we’ve just been discussing, there’s a built-in Wolfram Language meaning to things like CircleThrough and PlanarAngle—and we can in a sense do general computations with these. For example, we can ask for an axiom system for Boolean algebra, or group theory: What does the ⊗ mean? We’re not saying. We’re just formally defining certain properties it’s supposed to have. In the case of Boolean algebra, we can interpret it as And. In the case of group theory, it’s group multiplication—though we’re not saying what particular group it’s for. And, yes, we could as well write the group theory axioms for example as: OK, so can we do something similar for Euclid’s geometry? It’s more complicated, but thanks particularly to work by David Hilbert and Alfred Tarski in the first half of the 1900s, we can—and here’s a version of the result: Once again, this is all just a collection of formal statements. The fact that we’re calling an operator between is just for our convenience and understanding. All we can really say for sure is that this is some ternary operator; any properties it has have to be defined by the axioms. To get to this formalization of Euclid, quite a bit of tightening up had to be done. Euclid’s theorems often had implicit assumptions, and it sometimes wasn’t even clear exactly what their logical structure was supposed to be. But the mathematical content is presumably the same, and indeed some of Euclid’s axioms (like CN1) say basically exactly the same as these. (An important addition to what Euclid explicitly said is the last axiom above, which states Euclid’s implicit assumption—that I now believe to be incorrect for the physical universe—that space is continuous. Unlike other axioms, which just make statements “true for all values of ...”, this axiom makes a statement “true for all functions ...”.) So what can we do with these axioms? Well, in principle we can prove any theorem in Euclidean geometry. Appending to the axioms (that we refer to—ignoring the last axiom—as geometry) an assertion that we can interpret as saying that if a point y is between x and z and between x and w, then either z is between y and w or w is between y and z: Here’s a graph representing this proof: The axioms (including the “setup assertion”) are at the top—and the proof, with all its various intermediate lemmas, establishes that our “hypothesis” (represented by a little purple diamond on the left) eventually leads to “true” at the bottom. As a more complicated example, we can look at Euclid’s very first theorem, 1.1, which asserts that there’s a ruler-and-compass way to construct an equilateral triangle on any line segment. In the Wolfram Language, the construction is: And now we can write this directly in terms of our low-level constructs. First we need a definition of what circles are (Euclid has this as Definition 1.15)—basically saying that two circles centered at a that go through b and c are equal if the lines from a to b and a to c are congruent: We’ll call this definition circles. We’re going to do a construction that involves having circles that overlap, as specified by the assertions: {equal[circle[a, b], circle[a, c]], equal[circle[b, a], circle[b, c]]} And then our goal is to show that we get an equilateral triangle, for which the following is true: Putting this all together we can prove Euclid’s 1.1: And, yes, it took 272 steps—and here’s a graphical representation of the proof that got generated, with all its intermediate lemmas: We can go on and prove Euclid’s 1.2 as well, all the way from the lowest-level axioms. This time it takes us 330 steps, with proof graph: These graphs are conceptually similar to, but concretely rather different from, our “empirical metamathematics” graphs above. There are differences at the level of how interdependence of theorems is defined. But, more important, this graph is generated by automated theorem proving methods; the intermediate theorems (or lemmas) it involves are produced “on the fly” for the convenience of the computer, not because they help in any way to explain the proof to a human. In our empirical metamathematics on Euclid’s Elements, however, we’re dealing with the theorems that Euclid chose to define, and that have served as a basis for explaining his proofs to humans for more than two thousand years. By the way, if our goal is simply to find out what’s true in geometry—rather than to write out step-by-step proofs—then we now know how to do that. Essentially it involves turning geometric assertions into algebraic ones—and then systematically solving the polynomial equations and inequalities that result. It can be computationally expensive, but in the Wolfram Language we now have one master function, CylindricalDecomposition, that ultimately does the job. And, yes, given Gödel's theorem, one might wonder whether this kind of finite procedure for solving any Euclid-style geometry problem was even possible. But it turns out that—unlike arithmetic, for which Gödel’s theorem was originally proved—Euclid-style geometry, like basic logic, is decidable, in the sense that there is ultimately a finite procedure for deciding whether any given statement is true or not. In principle, this procedure could be based on theorem proving from the axioms, but CylindricalDecomposition effectively leverages a tower of more sophisticated mathematics to provide a much more efficient approach.
The Empirical Metamathematics of Euclid and Beyond 7.13 All Possible Theorems : From the axioms of geometry one can in principle derive an infinite number of true theorems—of which Euclid picked just 465 to include in his Elements. But why these theorems, and not others? Given a precise symbolic representation of geometry—as in the axioms above—one can just start enumerating true theorems. One way to do this is to use a multiway system, with the axioms defining transformation rules that one can apply in all possible ways. In effect this is like constructing every possible proof, and seeing what gets proved. Needless to say, the network that gets produced quickly becomes extremely large—even if its structure is interesting for our attempt to find a “bulk theory of metamathematics”. Here’s an example of doing it, not for the full geometry axioms above, but for basic logic (which is actually part of the axiom system we’ve used for geometry). We can either start with expressions, or with statements. Here we start with the expression x∧y, and then progressively find all expressions equal to it. Here’s the first, rather pedantic step: And here’s the second step: Every path in this graph is a proof that its endpoint expressions are equal. And while eventually this approach will give us every possible theorem (in this case about equalities involving x∧y), it’ll obviously take a while, generating huge numbers of long and uninteresting results on its way to anything interesting. As a different approach, we can consider just enumerating short possible statements, then picking out ones that we determine are true. In principle we could determine truth by explicitly proving theorems using the axioms (and, yes, if there was undecidability we wouldn’t always be able to do this). But in practice for the case of basic logic that we’re using as an example here, we can basically just explicitly construct truth tables to find out what’s true and what’s not. Here are some statements in logic, sorted in increasing order of complexity (as measured by depth and number of symbols): Many (like a=b) are very obviously not true, at least not for all possible values of each variable. But—essentially by using truth tables—we can readily pick out ones that are always true. OK, so now we can get a list of true theorems: Some are “interesting”. Others seem repetitive, overly complicated, or otherwise not terribly interesting. But if we want to “channel Euclid” we somehow have to decide which are the interesting theorems that we’re going to write down. And although Euclid himself didn’t explicitly discuss logic, we can look at textbooks of logic from the last couple of centuries—and we find that there’s a very consistent set of theorems that they end up picking out from the list, and giving names to: One might assume that these named theorems were just the result of historical convention. But when I was writing A New Kind of Science I discovered something quite surprising. With all the theorems written out in “order of complexity”, I tried seeing which theorems I could prove just from theorems earlier in the list. Many were easy to prove. But some simply couldn’t be proved. And it turned out that these were essentially precisely the “named theorems”: In other words, the “named theorems” are basically the simplest statements of new facts about logic, that can’t be established from “simpler facts”. Eventually as one’s going through the list of theorems, one will have accumulated enough to fill out what can serve as full axioms for logic—so that then all subsequent theorems can be proved from “existing facts”. Now of course the setup we’ve just used relies on the idea that one’s separately got a list of true theorems. To do something more like Euclid, we’d have to pick certain theorems to serve as axioms, then derive all others from these. Back in 2000 I figured out the very simplest possible axiom system for logic, written in terms of Nand, just the single axiom: So now writing And, Or and Not in terms of Nand according to: we can, for example, derive the notable theorems of logic from my axiom. FindEquationalProof gives automated proofs of these theorems, though most of them involve quite a few steps (the — indicates a theorem that is trivially true after substituting the forms for And, Or and Not): The longer cases here involve first proving the lemma a·b = b·a which takes 102 steps. Including this lemma as an axiom, the minimal axiom system (as I also found in 2000) is: And with this axiom system FindEquationalProof succeeds in finding shorter proofs for the notable theorems of logic, even though now the definitions for And, Or and Not are just treated as theorems: Actually looking at these proofs is not terribly illuminating; they certainly don’t have the same kind of “explanatory feel” as Euclid. But combining the graphs for all these proofs is more interesting, because it shows us the common lemmas that were used in these proofs, and effectively defines a network of interdependencies between theorems: There are 361 lemmas (i.e. automatically generated intermediate theorems) here. It’s a fair number, given that we’re only proving 20 theorems—but it’s definitely much less than the total of 1978 that would be involved in proving each of the theorems separately. In our graph here—like in our Euclid theorem-dependency graphs above—the axioms are shown (in yellow) at the top. The “notable theorems” that we’re proving are shown in pink. But the structure of the graph is a little different from our earlier Euclid theorem-dependency graphs, and this alternative layout makes it clearer: In Euclid, a given theorem is proved on the basis of other theorems, and ultimately on the basis of axioms. But here the automated theorem-proving process creates lemmas that ultimately allow one to show that the theorems one’s trying to prove are equivalent to “true” (i.e. to a tautology)—shown as a red node. We can ask other questions, such as how long the lemmas are. Here are the distributions of lengths of the final notable theorems, and of the intermediate lemmas used to prove them: We get something slightly more in the spirit of Euclid if we elide the lemmas, and just find the implied effective dependency graph between notable theorems. Transitive reduction then gives: By omitting intermediate lemmas, we’re in a sense just getting a shadow of the dependencies of the notable theorems, in the “environment” defined by our particular choice of axioms. But with this setup, it’s interesting to see the distributive law be the “hardest theorem”—kind of the metamathematical analog of Euclid’s 13.18 about the Platonic solids. OK, but what we’re doing so far with logic is still fundamentally a bit different from how most of Euclid works. Because what Euclid typically does is to say something like “imagine such-and-such a geometrical setup; then the following theorem will be true about it”. And the analog of that for logic would be to take axioms of logic, then append some logical assertion, and ask if with the axioms and this assertion some particular statement is true. In other words, there are some statements—like the axioms—that will be true in “pure logic”, but there are more statements that will be true with particular setups (or, in the case of logic, particular possible values for variables). For example, in “pure logic” a∨bb∨b is not necessarily true (i.e. it is not a tautology). But if we assert that a(a∧b) is true, then this implies the following possible choices for a and b and in all these cases a∨bb∨b is true. So, in a Euclid tradition, we could say “imagine a setup where a(a∧b); then we can prove from the axioms of logic the theorem that a(a∧b)”. Above we looked at which statements in logic are true for all values of variables: Now let’s look at the ones that aren’t always true. If we assume that some particular one of these statements is true, we can see which other statements it implies are true: Or on a larger scale, with a black dot when one statement implies another: For each of these theorems we can in principle construct a proof, using the axioms: And now we could go through and find out which theorems are useful in proving other theorems—and in principle this would allow us to build up a theorem dependency network. But there are undoubtedly many ways to do this, and so we’d need additional criteria to find ones that have whatever attributes would make us say “that might have been how someone like Euclid would have done it”. And then we can ask which are “tautologically true”—though it’s in practice considerably harder to do this than for logic (the best known methods involve all sorts of elaborate algebraic computations, which Mathematica can certainly do, but which quickly become quite unwieldy). And after that, we can proceed like Euclid, and start saying “assert this, then you can prove this”. And, yes, it’s nice that after 2000+ years, we can finally imagine automating the process of producing generalizations of Euclid’s Elements. Though this just makes it more obvious that part of what Euclid did was in a sense a matter of art—picking in some kind of aesthetic way which possible sequence of theorems would best “tell his story” of geometry.
The Empirical Metamathematics of Euclid and Beyond 7.14 Math beyond Euclid : We’ve looked here at some of the empirical metamathematics of what Euclid did on geometry more than 2000 years ago. But what about more recent mathematics, and all those other areas of mathematics that have now been studied?  In the history of mathematics, there have been perhaps 5 million research papers published, as well as probably hundreds of thousands of textbooks (though few quite as systematic as Euclid). And, yes, in modern times almost all mathematics that’s published is on the web in some form. A few years ago we scraped arXiv and identified about 2 million things described as theorems there (the most popular being the central limit theorem, the implicit function theorem and Fubini’s theorem); we also scraped as much as we could of the visible web and found about 30 million theorems there. No doubt many were duplicates (though it’s hard—and in principle undecidable!—which they are). But it’s a reasonable estimate that there are a few million distinct theorems for which proofs have been published in the history of human mathematics. It’s a remarkable piece of encapsulated intellectual achievement—perhaps the largest coherent such one produced by our species. And I’ve long been interested in seeing just what it would take to make it computable, and to bring it into the whole computational knowledge framework we have in the Wolfram Language. A few years ago I hoped that we could mobilize the mathematics community to help make this happen. But formalization is hard work, and it’s not at the center of what most mathematicians aspire to. Still, we’ve at least been slowly working—much as we have for Euclid-style geometry—to define the elements of computational language needed to represent theorems in various areas of mathematics. For example, in the area of point-set topology, we have under development things like: which in traditional mathematical notation becomes: So far we have encoded in computable form 742 “topology concepts”, and 1687 theorems about them.  Here are the connections recorded between concepts (dropping the concept of “topological spaces” that a third of all concepts are connected to, and labeling concepts with high betweenness centrality): And here is the graph of what theorem references what in its description: We haven’t encoded proofs for these theorems, so we can’t yet make the kind of theorem dependency graph that we did for Euclid. But we do have the dependency graph for 76 properties of topological spaces: The longest path here (along with a similar one starting with ) is 14 steps: (And, yes, this isn’t particularly profound; it’s just an indication of what it looks like to make specific definitions in topology computable.) So far, what we’ve discussed is being able to represent pure mathematical ideas and results in a high-level computable way, understandable to both humans and computers. But what if we want to just formalize everything, from the ground up, explicitly deriving and validating every theorem from the lowest-level foundations? Over the past few decades there have been a number of large-scale projects—like Mizar, Coq, Isabelle, HOL, Metamath, Lean—that have tried to do this (nowadays often in connection with creating “proof assistants”). Ultimately each project defines a certain “machine code” for mathematics. And yes, even though people might think that “mathematics is a universal language”, if one’s really going to give full, precise, formal specifications there are all sorts of choices to be made. Should things be based on set theory, type theory, higher-order logic, calculus of constructions, etc.? Should the law of excluded middle be assumed? The axiom of choice? What if one’s axiomatic structure seems great, but implies a few silly results, like 1/0 = 0? There’s no perfect solution, but each of these projects has made a certain set of choices. And the good news here is that for our purposes in doing large-scale empirical metamathematics—as in doing mathematics in the way mathematicians usually do it—it doesn’t seem like the choices will matter much. But what’s important for us is that these projects have accumulated tens of thousands of theorems (well, OK, some are “throwaway lemmas” or simple rearrangements), and that starting from axioms (or what amount to axioms), they’ve reached decently far into quite a few areas of mathematics. Looking at them is a bit of a different experience from looking at Euclid. While the Elements has the feel of a “narrative textbook” (albeit from a different age), formalized mathematics projects tend to seem more like software codebases, with their theorem dependency graphs being more like function call graphs. But they still provide fascinating metamathematical corpuses, and there's undoubtedly lots about empirical metamathematics that one can learn from them. Here I’m going to look at two examples: the Lean mathlib collection, which includes about 36,000 theorems (and 16,000 definitions) and the Metamath set.mm (“set theory”) collection, which has about 44,000 theorems (and 1500 definitions). To get a sense of what’s in these collections, we can start by drawing interdependence graphs for the theorems they contain in different areas of mathematics. Just like for Euclid, we make the size of each node represent the number of theorems in a particular area, and the thickness of each edge represent the fraction of theorems from one area that directly reference another in their proof. Leaving out theorems that effectively just do structural manipulation, rather than representing mathematical content (as well as “self-loop” connections within a single domain) here’s the interdependence graph for Lean. And here’s the corresponding one for Metamath: It’s somewhat interesting to see how central algebra ends up being in both cases, and how comparatively “off on the side” category theory is. But it’s clear that much of what one’s seeing in these graphs is a reflection of the particular user communities of these systems, with some important pieces of modern mathematics (like the applications of algebraic geometry to number theory) notably missing. But, OK, how do individual theorems work in these systems? As an example, let’s consider the Pythagorean theorem. In Euclid, this is 1.47, and here’s the first level of its dependency graph: Here’s the full graph involving a total of 39 elements (including, by the way, all 10 of the axioms), and having “depth” 20: In Lean’s mathlib, the theorem is called euclidean_geometry.dist_square_eq _dist _square _add _dist _square _iff _angle _eq _pi _div _two—and its stated proof directly involves 7 other theorems: Going 3 steps, the theorem dependency graph looks like (where “init” and “tactic” basically refer to structure rather than mathematical content): The full graph involves a total of 2850 elements (and has “depth” 84), and after transitive reduction has the form: And, yes, this is considerably more complicated than Euclid’s version—but presumably that’s what happens if you insist on full formalization. Of the 2850 theorems used, 1503 are basically structural. The remainder bring mathematical content from different areas, and it’s notable in the picture above that different parts of the proof seem to “concentrate” on different areas. Curiously, theorems from geometry (which is basically all Euclid used) occupy only a tiny sliver of the pie chart of all theorems used: The Metamath set.mm version of the Pythagorean theorem is called pythag, and its proof directly depends on 26 other theorems: After 1 step, the theorem dependency graph is: The full graph involves 7099 elements—and has depth 270. In other words, to get from the Pythagorean theorem all the way to the axioms can take as many as 270 steps. Given the complete Lean or Metamath corpuses, we can start doing the same kind of empirical metamathematics we did for Euclid’s Elements—except now the higher level of formalization that’s being used potentially allows us to go much further. As a very simple example, here’s the distribution of numbers of theorems directly referenced in the proof of each theorem in Lean, Metamath and Euclid: The differences presumably reflect different “hierarchical modularity conventions” in Lean and Metamath (and Euclid). But it’s interesting to note, for example, that in all three cases, the Pythagorean theorem is “above average” in terms of number of theorems referenced in its proof: What are the most popular theorems used in proofs? In terms of direct references, here are the top-5 lists: Not surprisingly, for Lean and Metamath these are quite “structural”. For Lean, congr_arg is the “congruency” statement that if a=b then f(a)=f(b); congr is a variant that says if a=b and f=g then f(a)=g(b); eq.trans is the transitivity statement if a=b and b=c then a=c (Euclid’s CN1); eq.symm is the statement if a=b then b=a; etc. For Metamath, syl is “transitive syllogism”: if x⇒y and y⇒z then x⇒z; eqid is about reflexity of equality; etc. In Euclid, these kinds of low-level results—if they are even stated at all—tend to be “many levels down” in the hierarchy of theorems, leaving the single most popular theorem, 10.11, to be one about proportion and rationality. If one looks at all theorems directly and indirectly referenced by a given theorem, the distribution of total numbers of theorems is as follows (with Lean showing the most obviously exponential decay): What about the overall structure of the Lean and Metamath dependency graphs? We can ask about effective dimension, about causal invariance, about “event horizons”, and much more. But right now I’ll leave that for another time...
The Empirical Metamathematics of Euclid and Beyond 7.15 The Future of Empirical Metamathematics : I don’t think empirical metamathematics has been much of a thing in the past. In fact, looking on the web as I write this, I’m surprised to see that essentially all references to the actual term “empirical metamathematics” seem to point directly or indirectly to that one note of mine on the subject in A New Kind of Science. But as I hope this piece has made clear, there’s a lot that can be done in empirical metamathematics. In everything I’ve written here, I haven’t started analyzing questions like how one can recognize a powerful or a surprising theorem. And I’ve barely scratched the surface even of the empirical metamathematics that can be done on Euclid’s Elements from 2000 years ago. But what kind of a thing is empirical metamathematics? Assuming one’s looking at theorems and proofs constructed by humans rather than by automated systems, it’s about analyzing large-scale human output—a bit like doing data science on literary texts, or on things like websites or legal corpuses. But it’s different. Because ultimately the theorems and proofs that are the subject of empirical metamathematics are derived not from features of the world, but from a formal system that defines some area of mathematics. With computational language the goal is to be able to describe anything in formalized, computational terms. But in empirical metamathematics, things are in a sense “born formalized”. Whatever the actual presentation of theorems and proofs may be there, their “true form” is ultimately something grounded in the formal structure of the mathematics being used. Of course there is also a strong human element to the raw material of empirical metamathematics. It is (at least for now) humans who have chosen which of the infinite number of possible theorems should be considered interesting, and worthy of presentation. And at least traditionally, when humans write proofs, they usually do it less as a way to certify correctness, and more as a form of exposition: to explain to other humans why a particular theorem is true, and what structure it fits into. In a sense, empirical metamathematics is a quite desiccated way to look at mathematics, in which all the elegant conceptual structure of its content has been removed.  But if we’re to make a “science of metamathematics”, it’s almost inevitable that we have to think this way. Part of what we need to do is to understand some of the human aesthetics of mathematics, and in effect to see to deduce laws by which it may operate. In this piece I’ve mostly concentrated on doing fairly straightforward graph-oriented data science, primarily on Euclid’s Elements. But in moving forward with empirical metamathematics a key question is what kind of model one should be trying to fit one’s observations into. And this comes back to my current motivation for studying empirical metamathematics: as a window onto a general “bulk” theory of metamathematics—and as the foundation for a science not just of how we humans have explored metamathematical space, but of what fundamentally is out there in metamathematical space, and what its overall structure may be. No doubt there are already clues in what I’ve done here, but probably only after we have the general theory will we have the paradigm that’s needed to identify them. But even without this, there’s much to do in studying empirical metamathematics for its own sake—and of better characterizing the remarkable human achievement that is mathematics. And for now, it’s interesting to be able to look at something as old as Euclid’s Elements and to realize what new perspectives modern computational thinking can give us about it. Euclid was a pioneer in the notion of building everything up from formal rules—and the seeds he sowed played an important role in leading us to the modern computational paradigm. So it’s something of a thrill to be able to come back two thousand years later and see that paradigm—now all grown up—applied not only to something like the fundamental theory of physics, but also to what Euclid did all those years ago.
The Empirical Metamathematics of Euclid and Beyond 7.16 Note Added : As I was working on this piece, I couldn’t help wondering whether—in 2300 years—anyone else had worked on the empirical metamathematics of Euclid before. Turns out (as Don Knuth pointed out to me) at least one other person did—more than 400 years ago. The person in question was Thomas Harriot (1560–1621). The only thing Thomas Harriot published in his lifetime was the book A Briefe and True Report of the New Found Land of Virginia, based on a trip that he made to America in 1585. But his papers show that he did all sorts of math and science (including inventing the · notation for multiplication, < and >, as well as drawing pictures of the Moon through a telescope before Galileo, etc.). He seems to have had a well-ahead-of-his-time interest in discrete mathematics, apparently making Venn diagrams a couple of centuries before Venn doing various enumerations of structures as well as various repeated computations (but no cellular automata, so far as I can tell!): And he seems to have made a detailed study of Euclid’s Elements, listing in detail (as I did) what theorems are used in each proof (this is for Book 1): But then, in his “moment of empirical metamathematics” he lists out the full dependency table for theorems in Book 1, having computed what we’d now call the transitive closure: It’s easy for us to reproduce this now, and, yes, he did make a few mistakes: Studying the empirical metamathematics of Euclid seems (to me) like an obvious thing to do, and it’s good to know I’m not the first one doing it. And actually I’m now wondering if someone actually already did it not “just” 400 years ago, but perhaps 2000 (or more) years ago...
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.1 Can You Build a Warp Drive? : When the NASA Innovative Advanced Concepts Program asked me to keynote their annual conference I thought it would be a good excuse to spend some time on a question I’ve always wanted to explore… Can You Build a Warp Drive? “So you think you have a fundamental theory of physics. Well, then tell us if warp drive is possible!” Despite the hopes and assumptions of science fiction, real physics has for at least a century almost universally assumed that no genuine effect can ever propagate through physical space any faster than light. But is this actually true? We’re now in a position to analyze this in the context of our model for fundamental physics. And I’ll say at the outset that it’s a subtle and complicated question, and I don’t know the full answer yet. But I increasingly suspect that going faster than light is not a physical impossibility; instead, in a sense, doing it is “just” an engineering problem. But it may well be an irreducibly hard engineering problem. And one that can’t be solved with the computational resources available to us in our universe. But it’s also conceivable that there may be some clever “engineering solution”, as there have been to so many seemingly insuperable engineering problems in the past. And that in fact there is a way to “move through space” faster than light. It’s a little tricky even to define what it means to “go faster than light”. Do we allow an existing “space tunnel” (like the wormholes of general relativity)? Perhaps a space tunnel that has been there since the beginning of the universe. Or even if no space tunnel already exists, do we allow the possibility of building one—that we can then travel through? I’ll discuss these possibilities later. But the most dramatic possibility is that even if one’s going where “no one has gone before”, it might still be possible to traverse space faster than light to get there. To give a preview of why doing this might devolve into an “engineering problem”, let’s consider a loose (but, in the end, not quite so loose) analogy. Imagine you’ve got molecules of gas in a room, all bouncing around and colliding with each other. Now imagine there’s a special molecule—or even a tiny speck of dust or a virus particle—somewhere in the room. Normally the special molecule will be buffeted by the molecules in the air, and will move in some kind of random walk, gradually diffusing across the room. But imagine that the special molecule somehow knows enough about the motion of the air molecules that it can compute exactly where to go to avoid being buffeted. Then that special molecule can travel much faster than diffusion—and effectively make a beeline from one side of the room to the other. Of course this requires more knowledge and more computation than we currently imagine something like a molecule can muster (though it’s not clear this is true when we start thinking about explicitly constructing molecule-scale computers). But the point is that the limit on the speed of the molecule is less a question of what’s physically possible, and more a question of what’s “engineerable”. And so, I suspect, it is with space, and motion through space. Like our room full of air molecules, space in our theory of physics has a complex structure with many component parts that act in seemingly (but not actually) random ways. And in our theory the question of whether we can “move through space” faster than light can then be thought of as becoming a question of whether there can exist a “space demon” that can find ways to do computations fast enough to be able to successfully “hack space”. But before we can discuss this further, we have to talk about just what space—and time—are in our models.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.2 The Structure of Space and the Nature of Time : In standard physics, space (and the “spacetime continuum”) is just a background on which everything exists. Mathematically, it’s thought of as a manifold, in which every possible position can ultimately be labeled by 3 coordinate values. In our model, space is different. It’s not just a background; it’s got definite, intrinsic structure. And in fact everything in the universe is ultimately defined by that structure; in fact, at some level, everything is just “made of space”. We might think of something like water as being a continuous fluid. But we know that at a small scale it’s actually made of discrete molecules. And so it is, I suspect, with space. At a small enough scale, there are actually discrete “atoms of space”—and only on a large scale does space appear to be continuous. In our model, the “atoms of space” correspond to abstract elements whose only property is their relation to other abstract elements. Mathematically the structure can be thought of as a hypergraph, where the atoms of space are nodes, which are related by hyperedges to other nodes. On a very small scale we might have for example: On a slightly larger scale we might have: And in our actual universe we might have a hypergraph with perhaps 10400 nodes. How does a giant hypergraph behave like continuous space? In a case like this we can see that the nodes can be thought of as forming a 2D grid on a (curved) surface: There’s nothing intrinsic about our model of space that determines the effective dimensionality it will have. These are all perfectly good possible (hyper)graphs, but on a large scale they behave like space in different numbers of dimensions: It’s convenient to introduce the notion of a “geodesic ball”: the region in a (hyper)graph that one reaches by following at most r connections in the (hyper)graph. A key fact is that in a (hyper)graph that limits to d-dimensional space, the number of nodes in the geodesic ball grows like rd. In a curved space (say, on the surface of a sphere) there’s a correction to rd, proportional to the curvature of the space. The full story is quite long, but ultimately what happens is that—much as we can derive the properties of a fluid from the large-scale aggregate dynamics of lots of discrete molecules—so we can derive the properties of space from the large-scale aggregate dynamics of lots of nodes in our hypergraphs. And—excitingly enough—it seems that we get exactly Einstein’s equations from general relativity. OK, so if space is a collection of elements laid out in a “spatial hypergraph”, what is time? Unlike in standard physics, it’s something initially very different. It’s a reflection of the process of computation by which the spatial hypergraph is progressively updated. Let’s say our underlying rule for updating the hypergraph is: Here’s a representation of the results of a sequence of updates according to this: Going further we’ll get for example: But there’s a crucial point here. The underlying rule just defines how a local piece of hypergraph that has a particular form should be updated. If there are several pieces of hypergraph that have that form, it doesn’t say anything about which of them should be updated first. But once we’ve done a particular update, that can affect subsequent updates—and in general there’s a whole “causal graph” of causal relationships between updates. We can see what’s going on a little more easily if instead of using spatial hypergraphs we just use strings of characters. Here we’re updating a string by repeatedly applying the (“sorting”) rule BA → AB: The yellow boxes indicate “updating events”, and we can join them by a causal graph that represents which event affects which other ones: If we’re an observer inside this system, all we can directly tell is what events are occurring, and how they’re causally connected. But to set up a description of what’s going on, it’s convenient to be able to talk about certain events happening “at a certain time”, and others happening later. Or, in other words, we want to define some kind of “simultaneity surfaces”—or a “reference frame”. Here are two choices for how to do this where the second one can be reinterpreted as: And, yes, this can be thought of as corresponding to a reference frame with a different speed, just like in standard special relativity. But now there’s a crucial point. The particular rule we’ve used here is an example of one with the property of causal invariance—which means that it doesn’t matter “at what time” we do a particular update; we’ll always get the same causal graph. And this is why—even though space and time start out so differently in our models—we end up being able to derive the fact that they follow special relativity. Given a reference frame, we can always “reconstruct” a view of the behavior of the system from the causal graph. In the cases shown here we’d get: And the fact that the system seems to “take longer to do its thing” in the second reference frame is precisely a reflection of relativistic time dilation in that frame. Just as with strings, we can also draw causal graphs to represent the causal relationships between updating events in spatial hypergraphs. Here’s an example of what we get for the rule shown above: And once again we can set up reference frames to define what events we want to consider “simultaneous”. The only fundamental constraint on our reference frames is that in each slice of the “foliation” that defines the reference frame there can never be two events in which one follows from the other. Or, in the language of relativity, no events in a given slice can be timelike separated; instead, all of them must be spacelike separated, so that the slice defines a purely spacelike hypersurface. In drawing a causal graph like the one above, we’re picking a particular collection of relative orderings of different possible updating events in the spatial hypergraph. But why one choice and not another? A key feature of our models is that actually we can think of all possible orderings as being done, or said, differently, we can construct a whole multiway graph of possibilities. Here’s what the multiway graph looks like for the string system above: Each node in this multiway graph represents a complete state of our system (in this case, a string), and a path through the multiway system corresponds to a possible history of the system, with a particular corresponding causal graph. But now there’s an important connection with physics: the fact that we get a multiway graph makes quantum mechanics inevitable in our models. And it turns out that just like we can use reference frames to make sense of the evolution of our systems in space and time, so also we can use “quantum observation frames” to make sense of the time evolution of multiway graphs. But now the analog of space is what we call “branchial space”: in effect a space of possible quantum states, with the connections between states defined by their relationship on branches in the multiway system. And much as we can define a spatial hypergraph representing relationships between “points in space”, so we can define a branchial graph that represents relationships (or “entanglements”) between quantum states, in branchial space: I won’t go into the details here, but one of the beautiful things in our models is that just as we can derive the Einstein equations as a large-scale limiting description of the behavior of our spatial hypergraphs, so also we can figure out the large-scale limiting behavior for multiway systems—and it seems that we get the Feynman path integral for quantum mechanics! By the way, since we’re talking about faster than light and motion in space, it’s worth mentioning that there’s also a notion of motion in branchial space. And just like we have the speed of light c that defines some kind of limit on how fast we can explore physical space, so also we have a maximal entanglement rate ζ that defines a limit on how fast we can explore (and thus “entangle”) different quantum states in branchial space. And just as we can ask about “faster than c”, we can also talk about “faster than ζ”. But before we get to that, we’ve got a lot of other things to discuss.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.3 Can We Make Tunnels in Space? : Traditional general relativity describes space as a continuous manifold that evolves according to certain partial differential equations. But our models talk about what’s underneath that, and what space actually seems to be made of. And while in appropriate limits they reproduce what general relativity says, they also imply all sorts of new and different phenomena. Imagine that the hypergraph that represents space has the form of a simple 2D grid: In the limit this will be like 2D Euclidean space. But now suppose we add some extra “long-range threads” to the graph: Here’s a different rendering of the same graph: Now let’s ask about distances on this graph. Some nodes on the graph will have distances that are just like what one would expect in ordinary 2D space. But some will be “anomalously close”, because one will be able to get from one to another not by going “all the way through 2D space” but by taking a shortcut along one of the long-range threads. Let’s say that we’re able to move around so that at every elementary interval of time we traverse a single connection in the graph. Then if our view of “what space is like” is based on the general structure of the graph (ignoring the long-range threads) we’ll come to some conclusion about how far we can go in a certain time—and what the maximum speed is at which we can “go through space”. But then what happens if we encounter one of the long-range threads? If we go through it we’ll be able to get from one “place in space” to another much faster than would be implied by the maximum speed we deduced from looking at “ordinary space”. In a graph, there are many ways to end up having “long-range threads”—and we can think of these as defining various kinds of “space tunnels” that provide ways to get around in space evading usual speed-of-light constraints. We can imagine both persistent space tunnels that could be repeatedly used, and spontaneous or “just-in-time” ones that exist only transiently. But—needless to say—there is all sorts of subtlety around the notion of space tunnels. If a tunnel is a pattern in a graph, what actually happens when something “goes through it”? And if a tunnel didn’t always exist, how does it get formed? Space tunnels are a fairly general concept that can be defined on graphs or hypergraphs. But there’s at least a special case of them that can be defined even in standard general relativity: wormholes. General relativity describes space as a continuum—a manifold—in which there’s no way to have “just a few long-range threads”. The best one can do is to imagine that there’s a kind of “handle in space”, that provides an alternative path from one part of space to another: How would such a non-simply-connected manifold form? Perhaps it’s a bit like the gastrulation that happens in embryonic development. But mathematically one can’t continuously change the topology of something continuous; there has to at least be some kind of singularity. In general relativity it’s been tricky to see how this could work. But of course in our models there’s not the same kind of constraint, because one doesn’t have to “rearrange a whole continuum”; one can do something more like “growing a handle one thread at a time”. Here’s an example where one can see something a bit like this happening. We’re using the rule: And what it does is effectively to “knit handles” that provide “shortcuts” between “separated” points in patches of what limits to 2D Euclidean space: In our models—free from the constraints of continuity—space can have all sorts of exotic forms. First of all, there’s no constraint that space has to have an integer number of dimension (say 3). Dimension is just defined by the asymptotic growth rates of balls, and can have any value. Like here’s a case that approximates 2.3-dimensional space: It’s worth noting that although it’s perfectly possibly to define distance—and, in the limit, lots of other geometric concepts—on a graph like this, one doesn’t get to say that nodes are at positions defined by particular sets of coordinates, as one would in integer-dimensional space. With a manifold, one basically has to pick a certain (integer) dimension, then stick to it. In our models, dimension can effectively become a dynamical variable, that can change with position (and time). So in our models one possible form of “space tunnel” is a region of space with higher or lower dimension. (Our derivation of general relativity is based on assuming that space has a limiting finite dimension, then asking what curvature and other properties it must have; the derivation is in a sense blind to different-dimensional space tunnels.) It’s worth noting that both lower- and higher-dimensional space tunnels can be interesting in terms of “getting places quickly”. Lower-dimensional space tunnels (such as bigger versions of the 1D long-range threads in the 2D grid above) potentially connect some specific sparse set of “distant” points. Higher-dimensional space tunnels (which in the infinite-dimensional limit can be trees) are more like “switching stations” that make many points on their boundaries closer.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.4 Negative Mass, Wormholes, etc. : Let’s say we’ve somehow managed to get a space tunnel. What will happen to it? Traditional general relativity suggests that it’s pretty hard to maintain a wormhole under the evolution of space implied by Einstein’s equations. A wormhole is in effect defined by geodesic paths coming together when they enter the wormhole and diverging again when they exit. In general relativity the presence of mass makes geodesics converge; that’s the “attraction due to gravity”. But what could make the geodesics diverge again? Basically one needs some kind of gravitational repulsion. And the only obvious way to get this in general relativity is to introduce negative mass. Normally mass is assumed to be a positive quantity. But, for example, dark energy effectively has to have negative mass. And actually there are several mechanisms in traditional physics that effectively lead to negative mass. All of them revolve around the question of where one sets the zero to be. Normally one sets things up so that one can say that “the vacuum” has zero energy (and mass). But actually—even in traditional physics—there’s lots that’s supposed to be going on in “the vacuum”. For example, there’s supposed to be a constant intensity of the Higgs field, that interacts with all massive particles and has the effect of giving them mass. And there are supposed to be vacuum fluctuations associated with all quantum fields, each leading (at least in standard quantum field theory) to an infinite energy density. But if these things exist everywhere in the universe, then (at least for most purposes) we can just set our zero of energy to include them. So then if there’s anything that can reduce their effects, we’ll effectively see negative mass. And one example of where this can in some sense happen is the Casimir effect. Imagine that instead of having an infinite vacuum, we just have vacuum inside a box. Having the box cuts out some of the possible vacuum fluctuations of quantum fields (basically modes with wavelengths larger than the size of the box)—and so in some sense leads to negative energy density inside the box (at least relative to outside). And, yes, the effect is observable with metal boxes, etc. But what becomes of the Casimir effects in a purely spacetime or gravitational setting isn’t clear. (This leads to a personal anecdote. Back in 1981 I wrote two papers about the Casimir effect with Jan Ambjørn, titled Properties of the Vacuum: 1. Mechanical and …: 2. Electrodynamic. We had planned a “…: 3. Gravitational” but never wrote it, and now I’m really curious what the results would have been. By the way, our paper #1 computed Casimir effects for boxes of different shapes, and had the surprising implication that by changing shapes in a cycle it would in principle be possible to continuously “mine” energy from the vacuum. This was later suggested as a method for interstellar propulsion, but to make it work requires an infinitely impermeable box, which doesn’t seem physically constructible, except maybe using gravitational effects and event horizons… but we never wrote paper #3 to figure that out….) In traditional physics there’s been a conflict between what the vacuum is like according to quantum field theory (with infinite energy density from vacuum fluctuations, etc.) and what the vacuum is assumed to be like in general relativity (effectively zero energy density). In our models there isn’t the same kind of conflict, but “the vacuum” is something with even more structure. In particular, in our models, space isn’t some separate thing that exists; it is just a consequence of the large-scale structure of the spatial hypergraph. And any matter, particles, quantum fields, etc. that exist “in space” must also be features of this same hypergraph. Things like vacuum fluctuations aren’t something that happens in space; they are an integral part of the formation of space itself. By the way, it’s important to note that in our models the hypergraph isn’t something static—and it’s in the end knitted together only through actual update events that occur. And the energy of some region of the hypergraph is directly related to the amount of updating activity in that region (or, more accurately, to the flux of causal edges through that portion of spacelike hypersurfaces). So what does this mean for negative mass in our models? Well, if there was a region of the hypergraph where there was somehow less activity, it would have negative energy relative to the zero defined by the “normal vacuum”. It’s tempting to call whatever might reduce activity in the hypergraph a “vacuum cleaner”. And, no, we don’t know if vacuum cleaners can exist. But if they do, then there’s a fairly direct path to seeing how wormholes can be maintained (basically because geodesics almost by definition diverge wherever a vacuum cleaner has operated). By the way, while a large-scale wormhole-like structure presumably requires negative mass, vacuum cleaners, etc., and other space tunnel structures may not have the same requirements. By their very construction, they tend to operate outside the regime described by general relativity and Einstein’s equations. So things like the standard singularity theorems of general relativity can’t be expected to apply. And instead there doesn’t seem to be any choice but to analyze them directly in the context of our models. One might think: given a particular space tunnel configuration, why not just run a simulation of it, and see what happens? The problem is computational irreducibility. Yes, the simulation might show that the configuration is stable for a million or a billion steps. But that might still be far, far away from human-level timescales. And there may be no way to determine what the outcome for a given number of steps will be except in effect by doing that irreducible amount of computational work—so that if, for example, we want to find out the limiting result after an infinite time, that’ll in general require an infinite amount of computational work, and thus effectively be undecidable. Or, put another way, even if we can successfully “engineer” a space tunnel, there may be no systematic way to guarantee that it’ll “stay up”; it may require an infinite sequence of “engineering tweaks” to keep it going, and eventually it may not be possible to keep it going. But before that, of course, we have to figure out how to construct a space tunnel in the first place.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.5 It Doesn’t Mean Time Travel : In ordinary general relativity one tends to think of everything in terms of spacetime. So if a wormhole connects two different places, one assumes they are places in spacetime. Or, in other words, a wormhole can allow shortcuts between both different parts of space, and different parts of time. But with a shortcut between different parts of time one can potentially have time travel. More specifically, one can have a situation where the future of something affects its past: in other words there is a causal connection from the future to the past. At some level this isn’t particularly strange. In any system that behaves in a perfectly periodic way one can think of the future as leading to a repetition of the past. But of course it’s not a future that one can freely determine; it’s just a future that’s completely determined by the periodic behavior. How all this works is rather complicated to see in the standard mathematical treatment of general relativity, although in the end what presumably happens is that in the presence of wormholes the only consistent solutions to the equations are ones for which past and future are locked together with something like purely periodic behavior. Still, in traditional physics there’s a certain sense that “time is just a coordinate”, so there’s the potential for “motion in time” just like we have motion in space. In our models, however, things work quite differently. Because now space and time are not the same kind of thing at all. Space is defined by the structure of the spatial hypergraph. But time is defined by the computational process of applying updates. And that computational process undoubtedly shows computational irreducibility. So while we may go backwards and forwards in space, exploring different parts of the spatial hypergraph, the progress of time is associated with the progressive performance of irreducible computation by the universe. One can compute what will happen (or, with certain restrictions, what has happened), but one can only do so effectively by following the actual steps of it happening; one can’t somehow separately “move through it” to see what happens or has happened. But in our models the whole causality of events is completely tracked, and is represented by the causal graph. And in fact each connection in the causal graph can be thought of as a representation of the very smallest unit of progression in time. So now let’s look at a causal graph again: There’s a very important feature of this graph: it contains no cycles. In other words, there’s a definite “flow of causality”. There’s a partial ordering of what events can affect what other events, and there’s never any looping back, and having an event affect itself. There are different ways we can define “simultaneity surfaces”, corresponding to different foliations of this graph: But there’s always a way to do it so that all events in a given slice are “causally before” events in subsequent slices. And indeed whenever the underlying rule has the property of causal invariance, it’s inevitable that things have to work this way. But if we break causal invariance, other things can happen. Here’s an example of the multiway system for a (string) rule that doesn’t have causal invariance, and in which the same state can repeatedly be visited: If we look at the corresponding (multiway) causal graph, it contains a loop: In the language of general relativity, this loop represents a closed timelike curve, where the future can affect the past. And if we try to construct a foliation in which “time systematically moves forward” we won’t be able to do it. But the presence of these kinds of loops is a different phenomenon from the existence of space tunnels. In a space tunnel there’s connectivity in the spatial hypergraph that makes the (graph) distance between two points be shorter than you’d expect from the overall structure of the hypergraph. But it’s just connecting different places in space. An event that happens at one end of the space tunnel can affect events associated with distant places in space, but (assuming causal invariance, etc.) those events have to be “subsequent events” with respect to the partial ordering defined by the causal graph. Needless to say, there’s all sorts of subtlety about the events involved in maintaining the space tunnel, the definition of distance being “shorter than you’d expect”, etc. But the main point here is that “jumping” between distant places in space doesn’t in any way require or imply “traveling backwards in time”. Yes, if you think about flat, continuum space and you imagine a tachyon going faster than light, then the standard equations of special relativity imply that it must be going backwards in time. But as soon as space itself can have features like space tunnels, nothing like this needs to be going on. Time—and the computational process that corresponds to it—can still progress even as effects propagate, say through space tunnels, faster than light to places that seem distant in space.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.6 Causal Cones and Light Cones : OK, now we’re ready to get to the meat of the question of faster-than-light effects in our models. Let’s say some event occurs. This event can affect a cone of subsequent events in the causal graph. When the causal graph is a simple grid, it’s all quite straightforward: But in a more realistic causal graph the story is more complicated: The “causal cone” of affected events is very well defined. But now the question is: how does this relate to what happens in space and time? When one thinks about the propagation of effects in space and time one typically thinks of light cones. Given a source of light somewhere in space and time, where in space and time can this affect? And one might assume that the causal cone is exactly the light cone. But things are more subtle than that. The light cone is normally defined by the positions in space and time that it reaches. And that makes perfect sense if we’re dealing with a manifold representing continuous spacetime, on which we can, for example, set up numerical coordinates. But in our models there’s not intrinsically anything like that. Yes, we can say what element in a hypergraph is affected after some sequence of events. But there’s no a priori way to say where that element is in space. That’s only defined in some limit, relative to everything else in the whole hypergraph. And this is the nub of the issue of faster-than-light effects in our models: causal (and, in a sense, temporal) relationships are immediately well defined. But spatial ones are not. One event can affect another through a single connection in the causal graph, but those events might be occurring at different ends of a space tunnel that traverses what we consider to be a large distance in space. There are several related issues to consider, but they center around the question of what space really is in our models. We started off by talking about space corresponding to a collection of elements and relations, represented by a hypergraph. But the hypergraph is continually being updated. So the first question is: can we define an instantaneous snapshot of space? Well, that’s what our reference frames, and foliations, and simultaneity surfaces, and so on, are about. They specify which particular collection of events we should consider to have happened at the moment when we “sample the structure of space”. There is arbitrariness to this choice, which corresponds directly to the arbitrariness that we’re used to in the selection of reference frames in relativity. But can we choose any collection of events consistent with the partial ordering defined by the causal graph (i.e. where no events associated with a “single time slice” follow each other in the causal graph, and thus affect each other)? This is where things begin to get complicated. Let’s imagine we pick a foliation like this, or something even wilder: We may know what the spatial hypergraph “typically” looks like. But perhaps with a weird enough foliation, it could be very different. But for now, let’s ignore this (though it will be important later). And let’s just imagine we pick some “reasonable” foliation. Then we want to ask what the “projection” of the causal cone onto the instantaneous structure of space is. Or, in other words, what elements in space are affected by a particular event? Let’s look at a specific example. Let’s consider the same rule and same causal cone as above, with the “flat” (“cosmological rest frame”) foliation: Here are spatial hypergraphs associated with successive slices in this foliation, with the parts contained in the causal cone highlighted: For the first 3 slices the event that begins the causal cone hasn’t happened yet. But after that we start seeing the effect of the event, gradually spreading across successive spatial hypergraphs. Yes, there are more subtleties ahead. But basically what we’re seeing here is the expansion of the light cone with time. So now we’ve got to ask the critical question: how fast does the edge of this light cone actually expand? How much space does it traverse at each unit in time? In other words, what is the effective speed of light here? It is already clear from the pictures above that this is a somewhat subtle question. But let’s begin with an even more basic issue. The speed of light is something we measure in units like meters per second. But what we can potentially get from our model is instead a speed in spatial hypergraph edges per causal edge. We can say that each causal edge corresponds to a certain elementary time elapsing. And as soon as we quote the elementary time in seconds—say 100–100 s—we’re basically defining the second. And similarly, we can say that each spatial hypergraph edge corresponds to a distance of a certain elementary length. But now imagine that in t elementary times the light cone in the hypergraph has advanced by α t spatial hypergraph edges, or α t elementary lengths. What is α t in meters? It has to be α c t, where c is the speed of light, because in effect this defines the speed of light. In other words, it’s at some level a tautology to say that the light cone in the spatial hypergraph advances at the speed of light—because this is the definition of the speed of light. But it’s more complicated than that. In continuum space there’s nothing inconsistent about saying that the speed of light is the same in every direction, everywhere. But when we’re projecting our causal cone onto the spatial hypergraph we can’t really say that anymore. But to know what happens we have to figure out more about how to characterize space. In our models it’s clear what causal effects there are, and even how they spread. But what’s far from clear is where in detail these effects show up in what we call space. We know what the causal cones are like; but we still have to figure out how they map into positions in space. And from that we can try to work out whether—relative to the way we set up space—there can be effects that go faster than light.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.7 How to Measure Distance : In a sense speeds are complicated to characterize in our models because positions and times are hard to define. But it’s useful to consider for a moment the much simpler case of cellular automata, where from the outset we just set up a grid in space and time. Given some cellular automaton, say with a random initial condition, we can ask how fast an effect can propagate. For example, if we change one cell in the initial condition, by how many cells per step can the effect of this expand? Here are a couple of typical results: The actual speed of expansion can vary, but in both cases the absolute maximum speed is 1 cell/step. And this is very straightforward to understand from the underlying rules for the cellular automata: In both cases, the rule for each step “reaches” one cell away, so 1 cell/step is the maximum rate at which effects can propagate. There’s something somewhat analogous that happens in our models. Consider a rule like: A bit like in the cellular automaton, the rule only “reaches” a limited number of connections away. And what this means is that in each updating event only elements within a certain range of connections can “have an effect” on each other. But inevitably this is only a very local statement. Because while the structure of the rule implies that effects can only spread a certain distance in a single update there is nothing that says what the “relative geometry” of successive updates will be, and what connection might be connected to what. Unlike in a cellular automaton where the global spatial structure is predefined, in our models there is no immediate global consequence to the fact that the rules are fundamentally local with respect to the hypergraph. It should be mentioned that the rules don’t strictly even have to be local. If the left-hand side is disconnected, as in then in a sense any individual update can pick up elements from anywhere in the spatial hypergraph—even disconnected parts. And as a result, something anywhere in the universe can immediately affect something anywhere else. But with a rule like this, there doesn’t seem to be a way to build up anything with the kind of locality properties that characterize what we think of as space. OK, but given a spatial hypergraph, how do we figure out “how far” it is from one node to another? That’s a subtle question. It’s easy to figure out the graph distance: just find the geodesic path from one node to another and see how many connections it involves. But this is just an abstract distance on the hypergraph: now the question is how it relates to a distance we might measure “physically”, say with something like a ruler. It’s a tricky thing: we have a hypergraph that is supposed to represent everything in the universe. And now we want something—presumably itself part of the hypergraph—to measure a distance in the hypergraph. In traditional treatments of relativity it’s common to think of measuring distances by looking at arrival times of light signals or photons. But this implicitly assumes that there’s an underlying structure of space, and photons are simply being added in to probe it. In our models, however, the photons have to themselves be part of the spatial hypergraph: they’re in a sense just “pieces of space”, albeit presumably with appropriate generalized topological properties. Or, put another way: when we directly study the spatial hypergraph, we’re operating far below the level of things like photons. But if we’re going to compare what we see in spatial hypergraphs with actual distance measurements in physics we’re going to have to find some way to bridge the gap. Or, in other words, we need to find some adequate proxy for physical distance that we can compute directly on the spatial hypergraph. A simple possibility that we’ve used a lot in practice in exploring our models is just graph distance, though with one wrinkle. The wrinkle is as follows: our hypergraphs represent collections of relations between elements, and we assume that these relations are ordered—so that the hyperedges in our hypergraphs are directed hyperedges. But in computing “physical-like distances” we ignore the directedness, and treat what we have as an undirected hypergraph. In the limit of sufficiently large hypergraphs, this shouldn’t make much difference, although it seems as if including directedness information may let us look at the analog of spinors, while the undirected case corresponds to ordinary vectors, which are what we’re more familiar with in terms of measuring distances. So is there any other proxy for distance that we could use? Actually, there are several. But one that may be particularly good is directly derived from the causal graph. It’s in some ways the analog of what we might do in traditional discussions of relativity where we imagine a grid of beacons signaling to each other over a limited period of time. In terms of our models we can say that it’s the analog of a branchial distance for the causal graph. Here’s how it works. Construct a causal graph, say: Now look at the events in the last slice shown here. For each pair of events look at their ancestry, i.e. at what previous event(s) led to them. If a particular pair of events have a common ancestor on the step before, connect them. The result in this case is the graph: One can think of this as a “reconstruction of space”, based on the causal graph. In an appropriate limit, it should be essentially the same as the structure of space associated with the original hypergraph—though with this small a graph the spatial hypergraph still looks quite different: It’s slightly complicated, but it’s important to understand the differences between these various graphs. In the underlying spatial hypergraph, the nodes are the fundamental elements in our model—that we’ve dubbed above “atoms of space”. The hyperedges connecting these nodes correspond to the relations between the elements. In the causal graph, however, the nodes represent updating events, joined by edges that represent the causal relationships between these events. The “spatial reconstruction graph” has events as its nodes, but it has a new kind of edge connecting these nodes—an edge that represents immediate common ancestry of the events. Whenever an event “causes” other events one can think of the first event as “starting an elementary light cone” that contains the other events. The causal graph represents the way that the elementary light cones are “knitted together” by the evolution of the system, and, more specifically, by the overlap of effects of different events on relations in the spatial hypergraph. The spatial reconstruction graph now uses the fact that two events lie in the same elementary light cone as a way to infer that the events are “close together”, as recorded by an edge in the spatial reconstruction graph. There is an analogy here to our discussions of quantum mechanics. In talking about quantum mechanics we start from multiway graphs whose nodes are quantum states, and then we look at (“time”) slices through these graphs, and construct branchial graphs from them—with two states being joined in this branchial graph when they have an immediate common ancestor in the multiway graph. Or, said another way: in the branchial graph we join states that are in the same elementary “entanglement cone”. And the resulting branchial graph can be viewed as a map of a space of quantum states and their entanglements: The spatial reconstruction graph is the same idea: it’s like a branchial graph, but computed from the causal graph, rather than from a multiway graph. (Aficionados of our project may notice that the spatial reconstruction graph is a new kind of graph that we haven’t drawn before—and in which we’re coloring the edges with a new, purple color that happens to be a blend of our “branchial pink” with the blue-gray used for spatial hypergraphs.) In the spatial reconstruction graph shown above, we’re joining events when they have a common ancestor one step before. But we can generalize the notion of a spatial reconstruction graph (or, for that matter, a branchial graph) by allowing common ancestors more than one step back. In the case we showed above, going even two steps back causes almost all events to have common ancestors: And indeed if we go enough steps back, every event will inevitably share a common ancestor: the “big bang” event that started the evolution of the system. Let’s say we have a rule that leads to a sequence of spatial hypergraphs: We can compare these with the spatial reconstruction graphs that we get from the causal graph for this system. Here are the results on successive steps, allowing a “lookback” of 2 steps: And as the number of steps increases, there is increasingly commonality between the spatial hypergraph and the spatial reconstruction graph—though they are not identical. It’s worth pointing out that the spatial reconstruction graphs we’ve drawn certainly aren’t the only ways to get a proxy for physical distances. One simple change is that we can look at common successors, rather than common ancestors. Another thing is to look not at a spatial hypergraph in which the nodes are elements and the hyperedges are relations, but instead at a “dual spatial hypergraph” in which the nodes are relations and the hyperedges are associated with elements, with each (unordered) hyperedge recording which relations share a given element. For example, for the spatial hypergraph the corresponding dual spatial hypergraph is and the sequence of dual spatial hypergraphs corresponding to the evolution above is: There are still other possibilities, particularly if one goes “below” the causal graph, and starts looking not just at causal relations between whole events, but also at causal relations between specific relations in the underlying spatial hypergraph. But the main takeaway is that there are various proxies we can use for physical distance. In the limit of a sufficiently large system, all of them should give compatible results. But when we’re dealing with small graphs, they won’t quite agree, and so we may not be sure what we should say the distance between two things is.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.8 Causal Balls vs. Geodesic Balls : To measure speed, we basically have to divide distance by elapsed time. But, as I just discussed at some length, when we’re constructing space and time from something lower level, it’s not straightforward to say exactly what we mean by distance and by elapsed time, and how different possibilities will correspond to what we’d actually measure, say at a human scale. But as a first approximation, let’s just ask about the effect of a single event. The effect of this event is captured by a causal cone: We can say that the elapsed time associated with a particular slice through this causal cone is the graph distance from the event at the top of the cone to events in this slice. (How the slice is chosen is determined by the reference frame we’re using.) So now we want to see how far the effect of the event spreads in space. The first step is to “project” the causal cone onto some representation of “instantaneous space”. We can do this with the ordinary spatial hypergraph: But to align with the most obvious notion of “elapsed time” in the causal cone it’s better to use the spatial reconstruction graph, whose nodes, just like those of the causal graph, are events: Let’s “watch the intersection grow” from successive slices of the causal cone, projected onto spatial reconstruction graphs: Now the question we have to ask is: how “wide” is that area of intersection? The pictures make it clear that it’s not trivial to answer—or even precisely define—that question. Yes, in the continuum limit of sufficiently large graphs we’d better get something that looks like a light cone in continuum space, but it’s far from trivial how that limiting process might work. We can think of the intersection of the causal cone with a spatial slice as defining a “causal ball” at a particular “time”. But now within that spatial slice we can ask about graph distances. So, for example, given a particular point in the slice we can ask what points lie within a certain graph distance of it—or, in other words, what the geodesic ball of some radius around that point is. And fundamentally the computation of “speed” is all about the comparison of the “widths” of causal balls and of geodesic balls. Another way to look at this is to say that given two points in the causal ball (that by definition are produced from a common ancestor some “time” back) we want to know the “spatial distance” between them. There are several ways we can assess “width”. We could compute the boundaries of causal balls, and for each point see what the “geodesically most distant” point is. Or we can just compute geodesic (i.e. spatial reconstruction graph) distances between all pairs of points in the causal ball. Here are distributions of these distances for each step shown above: How do we assess the “speed of light” from this? We might imagine we should look at the “outer edge” of this histogram, and see how it advances with “time”. If we do that, we get the result: But the full story is more complicated. Because, yes, the large-scale limit should be like a light cone, where we can measure the speed of light from its slope. But that doesn’t tell us about the “fine structure”. It doesn’t tell us whether at the edge of the causal ball, there are, for example, effectively space tunnels that “reach out” in the geodesic ball. There are lots of subtle issues here. And there’s another issue in the example we’ve been using: not only does this involve a causal cone that’s expanding, but the “whole universe” (i.e. the whole spatial hypergraph) is also expanding. So why not look at a simpler, “more static” case? Well, it isn’t so easy. Because in our models space is being “made dynamically”: it can’t really ever be “static”. At best we might imagine just having a rule that “trivially tills space”, touching elements but not “doing much” to them. But doing this introduces its own collection of artifacts.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.9 To Travel? To Communicate? : We’ve so far been talking mainly about the very low-level structure of spacetime, and how fast “threads of causality” can effectively “traverse space”. But if we’re actually going to be able to make use of faster-than-light phenomena, we’ve somehow got to “send something through them”. It’s not good enough to just have the structure of spacetime show some kind of faster-than-light phenomenon. We’ve got to be able to take something that we’ve chosen, and “send it through”. When we talk about “traveling faster than light”, what we normally mean is that we can take ourselves, made of ordinary matter, atoms, etc. and transport that whole structure faster than light across space. A lower bar is to consider faster-than-light communication. To do this we have to be able to take some message that we have chosen, and convert it to a form that can be transferred across space faster than light. To achieve true faster-than-light travel we presumably have to be able to construct some form of space tunnel in which the interior of the tunnel (and its entrance and exit) are sufficiently close to ordinary, flat space that they wouldn’t destroy us if we passed through them. It doesn’t seem difficult to imagine a spatial hypergraph that at least statically contains such a space tunnel. But it’s much more challenging to think about how this would be created dynamically. But, OK, so let’s say we just want to send individual particles, like photons, through. Well, in our models it’s not clear that’s that much easier. Because it seems likely that even a single photon of ordinary energy will correspond to a quite large region in the spatial hypergraph. Presumably the “core” of the photon is some kind of persistent topological-like structure in the hypergraph. And to understand the propagation of a photon, what one should do is to trace this structure in the causal graph. What about “communication without travel”? To propagate a “signal” in space requires that the signal has persistence of some kind, and the most obvious mechanism for such persistence would be a topological-like structure of the kind we assume exists in particles like photons. But—at least with some of the processes we’ll discuss below—there will be a premium on having our “signal carrier” involve as few underlying elements in the spatial hypergraph as possible. And one might imagine that this would be best achieved by something like the oligon particles that our models suggest could exist, and that involve many fewer elements in the spatial hypergraph than the particles we currently know about. Of course, using “oligon radio” requires that we have some kind of transducer between ordinary familiar particles and oligons, and it’s not clear how that can be achieved. There is probably a close connection in our models between what we might think of as black holes and what we might think of as particles. Quite what the details of this connection or correspondence are we don’t know yet, but both correspond to persistent structures “created purely from the structure of space”. And it’s quite possible that there is a whole spectrum of persistent structures that don’t quite have characteristics like particles (indeed, our space tunnels would presumably be examples). The question of whether any of these can be used for communication is in a sense quite easy to define. To communicate, we need some structure in the causal graph that maintains information through time, and that has parts that can be arbitrarily changed. In other words, there needs to be some way to encode something like arbitrary patterns of bits in the causal graph, and have them persist.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.10 The Second Law of Thermodynamics : I’ve been interested in the Second Law of thermodynamics and its origins for nearly 50 years, and it’s remarkable that it now seems to be intimately connected to questions about going faster than light in our models. Fundamentally, what the Second Law says is that initially orderly configurations of things like molecules have a seemingly inexorable tendency to become more disorderly over time. And as we’ll discuss, this is something very general, ultimately rooted in the general phenomenon of computational irreducibility. And it doesn’t just apply to familiar things like molecules: it also applies—in our models—to the very structure of space. So what’s the underlying story of the Second Law? I thought about this for many years, and finally in the 1990s got to the point where I felt I understood it. At first, the Second Law seems like a paradox: if the laws of physics are reversible then one would think that one could run any process as well backwards as forwards. Yet what the Second Law—and our experience—says is that things that start orderly tend to become more disorderly. But here’s a simple model that illustrates what’s going on. Consider a cellular automaton that’s reversible (like the standard laws of physics), in the sense that for every configuration (or, actually, in this case, every pair of configurations) there’s both a unique successor in time, and a unique predecessor. Now start the cellular automaton from a simple initial condition: We see a fundamental computational fact: just like my favorite rule 30 cellular automaton, even though the initial condition is simple, the system behaves in a complex—and in many ways seemingly random—way. But here’s the thing: this happens both if one runs it forward in time, and backward: The randomization is just a feature of the execution of the rule—forward or backward. At some moment we have a configuration that looks simple. But when we run it forward in time, it “randomizes”. And the same happens if we go backward in time. But why is there this apparent randomization? The evolution of the cellular automaton is effectively performing a computation. And to recognize a pattern in its output we have to do a computation too. But the point is that as soon as the evolution of the cellular automaton is computationally irreducible, recognizing a pattern inevitably takes an irreducible amount of computational work. It’s as if the cellular automaton is “encrypting” its initial condition—and so we have to do lots of computational work (perhaps even exponentially more than the cellular automaton itself) to be able to “decrypt” it. It’s not that it’s impossible to invert the final state of the cellular automaton and find that it evolved from a simple state. It’s just that to do so takes an irreducible amount of computational work. And if we as observers are bounded in our computational capabilities we eventually won’t be able to do it—so we won’t be able to recognize that the system evolved from a simple state. The picture above shows that once we have a simple state it’ll tend to evolve to a randomized state—just like we typically see. But the picture also shows that we can in principle set up a complicated initial state that will evolve to produce the simple state. So why don’t we typically see this happening in everyday life? It’s basically again a story of limited computational capabilities. Assume we have some computational system for setting up initial states. Then we can readily imagine that it would take only a limited number of computational operations to set up a simple state. But to set up the complicated and seemingly random state we’d need to be able to evolve to the simple state will take a lot more computational operations—and if we’re bounded in our computational capabilities we won’t be able to do it. What we’ve seen here in a simple cellular automaton also happens with gas molecules—or idealized hard spheres. Say you start the molecules off in some special “simple” configuration, perhaps with all the molecules in the corner of a box. Then you let the system run, with molecules repeatedly colliding and so on. Looked at in a computational way, we can say that the process of evolution of the system is a computation—and we can expect that it will be a computationally irreducible one. And just like with the cellular automaton, any computationally bounded observer will inevitably see “Second-Law behavior”. The traditional treatment of the Second Law talks a lot about entropy—which measures the number of possible configurations consistent with a measurement one makes on the system. (Needless to say, counting configurations is a lot easier in a fundamentally discrete system like a cellular automaton, than in standard real-number classical mechanics.) Well, if we measure the value of every single cell in a cellular automaton, there’s only one configuration consistent with our measurement—and given this measurement the whole past and future of the cellular automaton is determined, and we’ll always measure the same entropy for it. But imagine instead that we can’t do such complete and precise measurements. Then there may be many configurations of the system consistent with the results we get. But the point is that if the actual configuration of the system is actually simple, computationally bounded measurements will readily be able to recognize this, and determine that there’s only one (or a few) configurations consistent with their results. But if the actual configuration is complicated, computationally bounded measurements won’t be able to determine which of many configurations one’s looking at. The result is that in terms of such measurements, the entropy of the system will be considered larger. In the typical treatment of statistical mechanics over the past century one usually talks about “coarse-grained” measurements, but it’s always been a bit unclear what constitutes a “valid” coarse graining. I think what we now understand about computational irreducibility finally clarifies this, and lets us say what’s really going on in the Second Law: entropy seems to increase because the irreducible computation done by a system can’t successfully be “decrypted” by a computationally bounded observer. Even back in the 1860s James Maxwell realized that if you could have a “demon” who basically tweaked individual molecules to unrandomize a gas, then you wouldn’t see Second-Law behavior. And, yes, if the demon had sufficient computational capabilities you could make this work; the Second Law relies on the idea that no such computational capabilities are available. And as soon as the Second Law is in effect, one can start “assuming that things are random”, or, more specifically, that at least in some aggregate sense, the behavior of a system will follow statistical averages. This assumption is critical in deriving standard continuum fluid behavior from underlying molecular dynamics. And it’s also critical in deriving the continuum form of space from our underlying discrete model—and for deriving things like special and general relativity. In other words, the fact that a fluid—or space—seems like a continuum to us is a reflection of the boundedness of our computational capabilities. If we could apply as much computation as the underlying molecules in the gas—or the discrete elements in space—then we could recognize many details that would go beyond the continuum description. But with bounded computation, we just end up describing fluids—or space—in terms of aggregate continuum parameters. We talk about mechanical work—that involves patterns of motion in molecules that we can readily recognize as organized—being useful. And we talk about “heat”—that involves patterns of motion in molecules that seem random to us—as being fundamentally less useful. But this is really just a reflection of our computational boundedness. There is all sorts of detailed information in the motions associated with heat; it’s just that we can’t decode them to make use of them. Today when we describe a gas we’ll typically say that it’s characterized by temperature and pressure. But that misses all the detail associated with the motion of molecules. And I suspect that in time the coarseness of our current descriptions of things like gases will come to seem quite naive. There’ll be all sorts of other features and parameters that effectively correspond to different kinds of computations performed on the configuration of molecules. People sometimes talk disparagingly about the possible “heat death of the universe”, in which all of the orderly “mechanical work” motion has degraded into “heat”. But I don’t think that’s the right characterization. Yes, our current ways of looking at microscopic motions might only be to say it’s “generic heat”. But actually there’ll be all this rich structure in there, if only we were making the right measurements, and doing the right computations.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.11 Space Demons : If our models are going to reproduce what we currently know about physics, it’s got to be the case that in some large-scale limit, casual balls behave essentially like geodesic balls expanding at the speed of light. But this will only be an aggregate statement—that doesn’t, for example, talk about each individual relation in the spatial graph. Computational irreducibility implies that—just like with molecules in a gas—the configurations of the evolving spatial hypergraph will tend to appear seemingly random with respect to sufficiently bounded computations. And it’s important for us to use this in doing statistical averaging for our mathematical derivations. But the question is: Can we “compute around” that seeming randomness? Perhaps at the edge of the causal cone there are lots of little space tunnels that transiently arise from the detailed underlying dynamics of the system. But will these just seem to arise “randomly”, or can we compute where they will be, so we can potentially make use of them? In other words, can we have a kind of analog of Maxwell’s demon not for molecules in a gas, but for atoms of space: what we might call a “space demon”? And if we had such an entity, could it let us go faster than light? Let’s look again at the case of gas molecules. Consider an idealized hard-sphere gas in a box and track the motion of one of the “molecules”: The molecule bounces around having a sequence of collisions, and moves according to what seems to be a random walk. But now let’s imagine we have a “gas demon” who’s “riding on a molecule”. And every time its molecule collides with another one, let’s imagine that the demon can make a decision about whether to stay with the molecule it’s already on, or to jump to the other molecule in the collision. And now let’s say the demon is trying to “compute its way” across the box, deciding by looking at the history of the system which molecule it should hitch a ride on at each collision. Yes, the demon will have to do lots of computation. But the result will be that it can get itself transported across the system much faster than if it just stuck with one molecule. In other words, by using computation, it can “beat randomness” (and diffusion). If we think of the collisions between hard spheres as events, we can construct a causal graph of their causal relationships: collisions involves the same particle.*) At each event there are two incoming causal edges and two outgoing ones, corresponding to the spheres involved in a particular collision. And we can think of what the demon is doing as having to choose at each node in the causal graph which outgoing edge to follow. Or, in other words, the demon is determining its path in the causal graph. Just like for our models, we can construct a causal cone for the hard-sphere gas (here continuing for more steps)—and the path taken by the demon is restricted to not go outside this cone: But also like for our models, the relationship between positions in the causal ball obtained from this causal cone, and actual spatial positions, is in general complicated. At least if we were operating in an infinite region (as opposed to a finite box), the border of the causal ball in the hard-sphere gas would just be a circle. But the point is that there are always “tendrils” that stick out, and if there’s a finite box, it’s even more complicated: But the point is that if the demon can make a judicious choice of which “tendrils” to follow, it can move faster than the speed defined by the “average border” of the causal cone. If our “hard-sphere gas” were made, for example, of idealized electronic turtles, each with a computer and sensors on board, it wouldn’t seem too difficult to have a “demon turtle”. Even if our “hard spheres” were the size of microorganisms, it wouldn’t seem surprising to have a “demon”. It’s harder to imagine for actual molecules or particles; there just doesn’t seem to be anywhere to “put the computation apparatus”. Though if we started thinking about cooperation among many different hard spheres then it begins to seem more plausible again. After all, perhaps we could set up a configuration of a group of hard spheres, whose evolution will do the computation we need. OK, so what about the case of actual space in our models? In some ways it’s a more demanding situation: after all, every aspect of the internal structure of a space demon must—like everything else—be encoded in the structure of the spatial hypergraph. There is much we don’t know yet. For example, if there are “transient space tunnels” formed, what regularities might they show? In a hard-sphere gas, especially in 2D, there are surprisingly long time correlations between spheres, associated with what amounts to collective “hydrodynamic” behavior. And we don’t know what similar phenomena might exist in the spatial hypergraphs in our models. But then, of course, there is the question of how to actually construct “space demons” to take advantage of transient space tunnels. The Principle of Computational Equivalence has both good and bad news here. The bad news is that it implies that the evolution of the spatial hypergraph will show computational irreducibility—so it’ll take irreducible amounts of computational work to predict what it does. But the good news is that the dynamics of the hypergraph will be capable of universal computation, and can therefore in principle be “programmed” to do computations that could do whatever can be done to “figure out what will happen”. The key question is then whether there are sufficient “pockets of computational reducibility” associated with space tunnels that we’ll be able to successfully exploit. We know that in the continuum limit there’s plenty of computational reducibility: that’s why our models can reproduce mathematical theories like general relativity and quantum mechanics. But space tunnels aren’t a phenomenon of the usual continuum limit; they’re something different. We don’t know what a “mathematical theory of space tunnels” would be like. Conceivably, insofar as ordinary continuum behavior can be thought of as related to the central limit theorem and Gaussian distributions, a “theory of space tunnels” could have something to do with extreme value distributions. But most likely the mathematics—if it exists, and if we can even call it that—will be much more alien. When we say that a gas can be characterized as having a certain temperature, we’re saying that we’re not going to describe anything about the specific motions of the molecules; we’re just going to say that they’re “random”, with some average speed. But as I mentioned above, in reality there are all sorts of detailed patterns and correlations in these motions. And while as a whole they will show computational irreducibility, it is inevitable that there will be pockets of computational reducibility too. We don’t know what they are—and perhaps if we did, we could even use some of them for technological purposes. (Right now, we pretty much only use the very organized motions of molecules that we call “mechanical work”.) But now the challenge in creating a space demon is to find such pockets of reducibility in the underlying behavior of space. In a sense, much of the historical task of engineering has been to identify pockets of reducibility in our familiar physical world: circular motion, ferromagnetic alignment of spins, wave configurations of fields, etc. In any given case, we’ll never know how hard it’s going to be: the process of finding pockets of reducibility is itself a computationally irreducible process. But let’s say we could construct a space demon. We don’t know what characteristics it would have. Would it let us create borders around a space tunnel that would allow some “standard material object” to pass through the tunnel? Or would it instead allow a space tunnel to be constructed that could only pass through some special kind of hypergraph structure—that we might even characterize (in a nod to science fiction) as a means of “subspace communication” (i.e. communication that’s making use of structures that lie “below” space as we usually experience it).
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.12 Quantum Effects : Most of what I’ve said about causal graphs, etc. so far has basically been classical. I’ve assumed that there’s in a sense just one thread of history for the universe. But the full story in our models—and in physics—is more complicated. Instead of there being a single theory of history, there’s a whole multiway graph that includes all the possible choices for how updating events can happen. And in general instead of just having an ordinary causal cone, one really has a multiway causal cone—that in effect has extent not only in physical space but also in branchial space. And just as we have talked about selecting reference frames in spacetime, we also need to talk about selecting quantum observation frames in branchtime. And just as reference frames in spacetime give us a way to make sense of how events are organized in spacetime, and how we would observe or measure them there, so similarly quantum observation frames give us a way to make sense of how events are organized in branchtime, and what we would infer about them from quantum measurements. In what we’ve said so far about space tunnels, we’re basically always assuming there’s a single thread of history involved. But really we should be talking about multiway causal cones, and tunnels that have extent both in physical space and branchial space, or, in other words, multispace tunnels. We might imagine space tunnels are always “just fluctuations”, and that they’d be different on every “branch of history”. But a key point about multiway systems—and about multispace—is that they imply that we can expect coherence not only in physical space but also in branchial space, just as a “wave packet” is bounded both in physical and branchial space. In our models, “vacuum fluctuations” in quantum mechanics and in the structure of space are intimately connected; in the end they are both just facets of the multiway causal graph. In ordinary quantum field theory one is used to virtual particles which individually have propagators (typically like ) that imply they can show “virtual” faster-than-light effects. But we also know—as technically implemented in the commutation relations for field operators—that in the structure of standard quantum field theory there can be no real correlations “outside the light cone”. In our models, there can also be no correlations outside the (multiway) causal cone. But the whole issue is how projections of that multiway causal cone map onto geodesic balls representing distance in space. So what does all this mean for space demons? That they actually need to be not just space demons, but multispace demons, operating not just in physical space, but also in branchial space, or in the space of quantum states. And, yes, this is yet more complicated, but it doesn’t in any obvious way change whether things are possible. When we imagine a space demon identifying features of space that can form a space tunnel, we can expect that it’ll do this at a particular place in physical space. In other words, if we end up going faster than light, there’ll be a particular origination point in our physical space for our journey (or, in some science fiction terms, our “jump”). And it’s really no different for branchial space and multispace demons. A multispace tunnel will presumably have some location both in physical space and branchial space. In the way we currently think about things, “going there” in branchial space basically means doing a certain quantum measurement—though causal invariance implies that in the end all quantum observers will agree about what happened (and e.g. that one successfully “went faster than light”). It’s all quite complicated, and certainly far from completely worked out. And there’s another issue as well. The speed of light constrains maximum speeds in physical space. But in our models, there’s also the maximum entanglement speed, which constrains maximum speeds in branchial space. And just as we can imagine space tunnels providing ways to go “faster than c”, so also we can imagine multispace tunnels providing ways to go “faster than ζ”.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.13 Is It Possible? Can We Make It Work? : OK, so what’s the bottom line? Is it in principle possible to go faster than light? And if so, how can we actually do it? I’m pretty sure that, yes, in principle it’s possible. In fact, as soon as one views space as having an underlying structure, and not just being a mathematical manifold “all the way down”, it’s pretty much inevitable. But it still requires essentially “hacking” space, and “reverse engineering” its structure to find features like “space tunnels” that one can use. How is all this consistent with relativity, and its assumption of the absoluteness of the speed of light? Well, it isn’t. The phenomena and possibilities I’m describing here are ones that occur in the “substrate” below where relativity operates. It’s as if our standard physics—with relativity, etc.—are part of the “high-level operating system” of the universe. But what we’re talking here about doing is creating hacks down at the “machine code” level. Put another way: relativity is something that arises in our models as a large-scale limit, when one’s averaged out all the underlying details. But the whole point here is to somehow leverage and “line up” those underlying details, so they produce the effects we’re interested in. But when we look at the whole “bulk” universe, and the full large-scale limit, anything we might be able to do at the level of the details will seem infinitesimal—and won’t affect our overall conclusion that relativity is a feature of the general physics of the universe. Now of course, even though something may in principle be possible, that doesn’t mean it can be done in practice. Maybe it’s fairly easy to go a tiny distance faster than light, but to scale up to anything substantial requires resources beyond what we—or even the universe—could ever muster. And, yes, as I discussed, that is a possibility. Because in a sense what we have to do is to “beat computational irreducibility” in the evolution of space. And in the abstract there is no way to tell how hard this might be. Let’s say we have the general objective of “going faster than light”. There will be an immense (and probably infinite) number of detailed ways we could imagine achieving this. And in general there will be no upper bound on the amount of computation needed for any one of them. So if we ask “Will any of them work?”, that’ll be formally undecidable. If we find one that we can show works, great. But we could in principle have to go on testing things forever, never being sure that nothing can work. And, yes, this means that even though we might know the final underlying rule for physics, we still might fundamentally never be sure whether it’s possible to go faster than light. We might have successfully “reduced physics to mathematics”, but then we still have all the issues of mathematics—like Gödel’s theorem—to contend with. And just as Gödel’s theorem tells us there’s no upper bound on the lengths of proofs we might need in arithmetic, so now we’re in a situation where there’s no upper bound on the “complexity of the process” that we might need in physics to establish whether it’s possible to go faster than light. Still, just because something is in general undecidable, it doesn’t mean we won’t be able to figure it out. Maybe we’ll have to give up on transporting ordinary material faster than light, and we’ll only be dealing with some specially crafted form of information. But there’s no reason to think that with an objective as broad as “somehow go faster than light” that we won’t be able to, in effect, find some pocket of computational reducibility that makes it possible for us to do it. And the fact is that the history of engineering is full of cases where an initial glimmer of possibility was eventually turned into large-scale technological success. “Can one achieve heavier-than-air flight?” There were detailed hydrodynamic effects, and there were pieces of what later became control theory. And eventually there was an engineering construction that made it work. It’s hard to predict the process of engineering innovation. We’ve known the basic physics around controlled nuclear fusion for more than half a century. But when will we actually make it work as an engineering reality? Right now the idea of hacking space to go faster than light seems far away from anything we could in practice do. But we have no idea how high—or low—the barrier actually is. Might it require having our own little black hole? Or might it be something that just requires putting together things we already have in just the right way? Not long ago it was completely unclear that we could “beat the uncertainty principle” enough to measure gravitational waves. Or that we could build an atomic force microscope that could move individual atoms around. Or that we could form a physical Bose–Einstein condensate. But in cases like these it turned out that we already had the “raw materials” we needed; we just had to figure out what to do with them. A few years ago, when I was trying to make up fictional science for the movie Arrival, I thought a little about how a present-day physicist might think about the mechanism for an interstellar spacecraft that showed up one day. It was before our current models, but I had already thought a lot about the potential discrete structure of spacetime. And the best fictional idea I came up with then about how to “access it” was through some kind of “gravitational laser”. Gravitons, like photons, are bosons that can in principle form quantum condensates. And at least at the level of a made-for-a-movie whiteboard I figured out a little of how this might work. But from what we know now, there are other ideas. Perhaps the best analogy—at least for “communication” if not “travel”—is that one’s trying to get a signal “through a complex medium” as efficiently as possible. And that’s of course been the basic problem forever in communications systems based on electrical, electromagnetic or optical processes. Often it’s been claimed that there’s some fundamental limit, say to transmission rates. But then an engineering solution will be found that overcomes it. And actually the typical mechanism used is a little like our demons. If one’s signal is going to be degraded by “noise”, figure out how to predict the noise, then “sculpt” the process of transmission around it. In 5G technology for example, there’s even an explicit concept of “pilot signals” that continually probe the local radio environment so that actual communication signals can be formed in just the right ways. But, OK, let’s say there is a practical way to go faster than light, or at least to send signals faster than light. Then why aren’t we seeing lots of more-advanced-than-us extraterrestrial intelligences doing this all over the universe? Maybe we just have to figure out the right engineering trick and then we’ll immediately be able to tap into a universe-scale conversation. And while it’s fun to imagine just how wild the social network of the universe might get, I think there’s a fundamental problem here (even beyond the “what’s really the use case?”). Let’s say we can see processes that correspond to faster-than-light communication. Are they part of a “conversation”, “saying” something meaningful? Or are they just “physical processes” that are going on? Well, of course, anything that happens in the universe is, essentially by definition, a “physical process”. So then we might start talking about whether what we’re seeing is an “intentionally created” physical process, or one that’s just “happening naturally”. But—as I’ve written extensively about elsewhere—it’s a slippery slope. And the Principle of Computational Equivalence basically tells us that in the end we’ll never be able to distinguish the “intelligent” from the “merely computational”, or, given our model of physics, the “merely physical”—at least unless what we’re seeing is aligned in detail with our particular human ways of thinking. At the outset we might have imagined that going faster than light was an open-and-shut case, and that physics had basically proved that—despite a few seemingly pathological examples in general relativity—it isn’t possible. I hope what’s become clear here is that actually the opposite is true. In our models of physics, going faster than light is almost inevitably possible in principle. But to actually do it requires engineering that may be irreducibly difficult. But maybe it’s like in 1687 when a then-new model of physics implied that artificial satellites might be possible. After 270 years of steady engineering progress, there they were. And so it may be with going faster than light. Our models now suggest it’s possible. But whether the engineering required can be done in ten, a hundred, a thousand, a million or a billion years we don’t know. But maybe at least there’s now a path to turn yet another “pure-science-fiction impossibility” into reality.
Faster than Light in Our Model of Physics: Some Preliminary Thoughts 8.14 A Few Questions : My talk at NASA generated many questions. Here are a few answers. What about warp bubbles and the Alcubierre metric? Warp bubbles are a clever way to get something a bit like faster-than-light travel in ordinary general relativity. The basic idea is to set up a solution to Einstein’s equations in which space is “rapidly contracting” in front of a “bubble region”, and expanding behind it: To maintain this configuration, one needs negative mass on each side of the bubble: In a sense it’s like an asymmetric local analog of the expansion of the universe. Inside the bubble space is flat. But other parts of the universe are approaching or receding as a result of the contraction and expansion of space. And in fact this is happening so rapidly that (1) the bubble is effectively moving faster than light relative to the rest of the universe, and (2) there’s an event horizon around the bubble, so nothing can go in or out. It’s rather easy to make a toy version of this within our models; here’s the corresponding causal graph: “Reconstructions of space” will then show that “parts of space” can “slip past others”, “as fast as they want”—but without causal interaction. Our space demon / space tunnel setup is rather different: there are no horizons involved; the whole point is to trace causal connections, but then to see how these map onto space. What about quantum teleportation? In quantum teleportation, there’s some sense in which different quantum measurements seem to “communicate faster than light”. But there’s always a slower-than-light back channel that sets up the measurements. In our models, the whole phenomenon is decently easy to see. It involves measurement inducing “communication” through causal connections in the multiway causal graph, but the point is that these are branchlike edges, not spacelike ones—so there’s no “travel through physical space”. (A whole different issue is limitations on quantum teleportation associated with the maximum entanglement space ζ..
Combinators: A Centennial View 9.1 Ultimate Symbolic Abstraction : Before Turing machines, before lambda calculus—even before Gödel’s theorem—there were combinators. They were the very first abstract examples ever to be constructed of what we now know as universal computation—and they were first presented on December 7, 1920. In an alternative version of history our whole computing infrastructure might have been built on them. But as it is, for a century, they have remained for the most part a kind of curiosity—and a pinnacle of abstraction, and obscurity. It’s not hard to see why. In their original form from 1920, there were two basic combinators, s and k, which followed the simple replacement rules (now represented very cleanly in terms of patterns in the Wolfram Language): The idea was that any symbolic structure could be generated from some combination of s’s and k’s. As an example, consider a[b[a][c]]. We’re not saying what a, b and c are; they’re just symbolic objects. But given a, b and c how do we construct a[b[a][c]]? Well, we can do it with the s, k combinators. Consider the (admittedly obscure) object s[s[k[s]][s[k[k]][s[k[s]][k]]]][s[k[s[s[k][k]]]][k]] (sometimes instead written S(S(KS)(S(KK)(S(KS)K)))(S(K(S(SKK)))K)). Now treat this like a function and apply it to a,b,c s[s[k[s]][s[k[k]][s[k[s]][k]]]][s[k[s[s[k][k]]]][k]][a][b][c]. Then watch what happens when we repeatedly use the s, k combinator replacement rules: Or, a tiny bit less obscurely: After a number of steps, we get a[b[a][c]]! And the point is that whatever symbolic construction we want, we can always set up some combination of s’s and k’s that will eventually do it for us—and ultimately be computation universal. They’re equivalent to Turing machines, lambda calculus and all those other systems we know are universal. But they were discovered before any of these systems. By the way, here’s the Wolfram Language way to get the result above (&sol&sol&period repeatedly applies the rules until nothing changes anymore): s[s[k[s]][s[k[k]][s[k[s]][k]]]][s[k[s[s[k][k]]]][k]][a][b][ And, yes, it’s no accident that it’s extremely easy and natural to work with combinators in the Wolfram Language—because in fact combinators were part of the deep ancestry of the core design of the Wolfram Language. For me, though, combinators also have another profound personal resonance. They’re examples of very simple computational systems that turn out (as we’ll see at length here) to show the same remarkable complexity of behavior that I’ve spent so many years studying across the computational universe. A century ago—particularly without actual computers on which to do experiments—the conceptual framework that I’ve developed for thinking about the computational universe didn’t exist. But I’ve always thought that of all systems, combinators were perhaps the earliest great “near miss” to what I’ve ended up discovering in the computational universe.
Combinators: A Centennial View 9.2 Computing with Combinators : Let’s say we want to use combinators to do a computation on something. The first question is: how should we represent the “something”? Well, the obvious answer is: just use structures built out of combinators! For example, let’s say we want to represent integers. Here’s an (at first bizarre-seeming) way to do that. Take s[k] and repeatedly apply s[s[k[s]][k]]. Then we’ll get a sequence of combinator expressions: On their own, these expressions are inert under the s and k rules. But take each one (say e) and form e[s][k]. Here’s what happens for example to the third case above when you then apply the s and k rules: To get this in the Wolfram Language, we can use Nest, which nestedly applies functions: Then the final result above is obtained as: Here’s an example involving nesting 7 times: So this gives us a (perhaps seemingly obscure) way to represent an integer n. Just form: This is a combinator representation of n, that we can “decode” by applying to [s][k]. OK, so given two integers represented this way, how would we add them together? Well, there’s a combinator for that! And here it is: s[k[s]][s[k[s[k[s]][k]]]] If we call this plus, then let’s compute plus[1][2][s][k], where 1 and 2 are represented by combinators: plus = s[k[s]][s[k[s[k[s]][k]]]]; It takes a while, but there’s the result: 1 + 2 = 3. Here’s 4 + 3, giving the result s[s[s[s[s[s[s[k]]]]]]] (i.e. 7), albeit after 49 steps: What about doing multiplication? There’s a combinator for that too, and it’s actually rather simple: s[k[s]][k] Here’s the computation for 3 × 2—giving 6 after 58 steps: Here’s a combinator for power: s[k[s[s[k][k]]]][k] And here’s the computation of 32 using it (which takes 116 steps): One might think this is a crazy way to compute things. But what’s important is that it works, and, by the way, the basic idea for it was invented in 1920. And while it might seem complicated, it’s very elegant. All you need are s and k. Then you can construct everything from them: functions, data, whatever. So far we’re using what’s essentially a unary representation of numbers. But we can set up combinators to handle binary numbers instead. Or, for example, we can set up combinators to do logic operations. s[s][k] and we can check this works by computing a truth table (TT, TF, FT, FF): A search gives the minimal combinator expressions for the 16 possible 2-input Boolean functions: And by combining these (or even just copies of the one for Nand) one can make combinators that compute any possible Boolean function. And in fact in general one can—at least in principle—represent any computation by “compiling” it into combinators. Here’s a more elaborate example, from my book A New Kind of Science. This is a combinator that represents one step in the evolution of the rule 110 cellular automaton: And, here from the book, are representations of repeatedly applying this combinator to compute—with great effort—three steps in the evolution of rule 110: There’s a little further to go, involving fixed-point combinators, etc. But basically, since we know that rule 110 is computation universal, this shows that combinators also are.
Combinators: A Centennial View 9.3 A Hundred Years Later… : Now that a century has passed, what should we think of combinators? In some sense, they still might be the purest way to represent computation that we know. But they’re also very hard for us humans to understand. Still, as computation and the computational paradigm advance, and become more familiar, it seems like on many fronts we’re moving ever closer to core ideas of combinators. And indeed the foundational symbolic structure of the Wolfram Language—and much of what I’ve personally built over the past 40 years—can ultimately be seen as deeply informed by ideas that first arose in combinators. Computation may be the single most powerful unifying intellectual concept ever. But the actual engineering development of computers and computing has tended to keep different aspects of it apart. There’s data. There are data types. There’s code. There are functions. There are variables. There’s flow of control. And, yes, it may be convenient to keep these things apart in the traditional approach to the engineering of computer systems. But it doesn’t need to be that way. And combinators show us that actually there’s no need to have any of these distinctions: everything can be together, and can made of the same, dynamic “computational stuff”. It’s a very powerful idea. But in its raw form, it’s also very disorienting for us humans. Because to understand things, we tend to rely on having “fixed anchors” to which we can attach meaning. And in pure, ever-changing seas of s, k combinators—like the ones we saw above—we just don’t have these. Still, there’s a compromise—and in a sense that’s exactly what’s made it possible for me to build the full-scale computational language that the Wolfram Language now is. The point is that if we’re going to be able to represent everything in the world computationally we need the kind of unity and flexibility that combinator-like constructs provide. But we don’t just want raw, simple combinators. We need to in effect pre-define lots of combinator-like constructs that have particular meanings related to what we’re representing in the world. At a practical level, the crucial idea is to represent everything as a symbolic expression, and then to say that evaluating these expressions consists in repeatedly applying transformations to them. And, yes, symbolic expressions in the Wolfram Language are just like the expressions we’ve made out of combinators—except that instead of involving only s’s and k’s, they involve thousands of different symbolic constructs that we define to represent molecules, or cities or polynomials. But the key point is that—like with combinators—the things we’re dealing with are always structurally just nested applications of pure symbolic objects. Something we immediately learn from combinators is that “data” is really no different from “code”; they can both be represented as symbolic expressions. And both can be the raw material for computation. We also learn that “data” doesn’t have to maintain any particular type or structure; not only its content, but also the way it is built up as a symbolic expression can be the dynamic output of a computation. One might imagine that things like this would just be esoteric matters of principle. But what I’ve learned in building the Wolfram Language is that actually they’re natural and crucially important in having convenient ways to capture computationally how we humans think about things, and the way the world is. From the early days of practical computing, there was an immediate instinct to imagine that programs should be set up as sequences of instructions saying for example “take a thing, then do this to it, then do that” and so on. The result would be a “procedural” program like: But as the combinator approach suggests, there’s a conceptually much simpler way to write this in which one’s just successively applying functions, to make a “functional” program: (In the Wolfram Language, this can also be written h@g@f@x or x//f//g//h.) Given the notion that everything is a symbolic expression, one’s immediately led to have functions to operate on other functions, like This idea of such “higher-order functions” is quintessentially combinator informed—and very elegant and powerful. And as the years go by we’re gradually managing to see how to make more and more aspects of it understandable and accessible in the Wolfram Language (think: Fold, MapThread, SubsetMap, FoldPair, …). OK, but there’s one more thing combinators do—and it’s their most famous: they allow one to set things up so that one never needs to define variables or name things. In typical programming one might write things like: x |-> 1 + x^2 But in none of these cases does it matter what the actual name x is. The x is just a placeholder that’s standing for something one’s “passing around” in one’s code. The Wolfram Language gives us an easy way to get rid of the x here too: (1 + #^2) &[4] In a sense the # (“slot”) here acts a like a pronoun in a natural language: we’re saying that whatever we’re dealing with (which we’re not going to name), we want to find “one plus the square of it”. OK, but so what about the general case? Well, that’s what combinators provide a way to do. Consider an expression like: There’s no mention of x and y here; the combinator structure is just defining—without naming anything—how to “flow in” whatever one provides as “arguments”. Let’s watch it happen: Yes, it seems utterly obscure. And try as I might over the years to find a usefully human-understandable “packaging” of this that we could build into the Wolfram Language, I have so far failed. But it’s very interesting—and inspirational—that there’s even in principle a way to avoid all named variables. Yes, it’s often not a problem to use named variables in writing programs, and the names may even communicate useful information. But there are all sorts of tangles they can get one into. It’s particularly bad when a name is somehow global, and assigning a value to it affects (potentially insidiously) everything one’s doing. But even if one keeps the scope of a name localized, there are still plenty of problems that can occur. Consider for example: It’s two nested anonymous functions (AKA lambdas)—and here the x “gets” a, and y “gets” b: But what about this: The Wolfram Language conveniently colors things red to indicate that something bad is going on. We’ve got a clash of names, and we don’t know “which x” is supposed to refer to what. It’s a pretty general problem; it happens even in natural language. If we write “Jane chased Bob. Jane ran fast.” it’s pretty clear what we’re saying. But “Jane chased Jane. Jane ran fast.” is already confused. In natural language, we avoid names with pronouns (which are basically the analog of # in the Wolfram Language). And because of the (traditional) gender setup in English “Jane chased Bob. She ran fast.” happens to work. But “The cat chased the mouse. It ran fast.” again doesn’t. But combinators solve all this, by in effect giving a symbolic procedure to describe what reference goes where. And, yes, by now computers can easily follow this (at least if they deal with symbolic expressions, like in the Wolfram Language). But the passage of a century—and even our experience with computation—doesn’t seem to have made it much easier for us humans to follow it. By the way, it’s worth mentioning one more “famous” feature of combinators—that actually had been independently invented before combinators—and that these days, rather ahistorically, usually goes by the name “currying”. It’s pretty common—say in the Wolfram Language—to have functions that naturally take multiple arguments. GeoDistance[a, b] or Plus[a, b, c] (or a+b+c) are examples. But in trying to uniformize as much as possible, combinators just make all “functions” nominally have only one argument. But if one’s thinking “sufficiently symbolically” it’s fine. And in the Wolfram Language—with its fundamentally symbolic character (and distant ancestry in combinator concepts)—one can just as well make a definition like.
Combinators: A Centennial View 9.4 Combinators in the Wild: Some Zoology : So far we’ve been talking about combinators that are set up to compute specific things that we want to compute. But what if we just pick possible combinators “from the wild”, say at random? What will they do? In the past, that might not have seemed like a question that was worth asking. But I’ve now spent decades studying the abstract computational universe of simple programs—and building a whole “new kind of science” around the discoveries I’ve made about how they behave. And with that conceptual framework it now becomes very interesting to look at combinators “in the wild” and see how they behave. So let’s begin at the beginning. The simplest s, k combinator expressions that won’t just remain unchanged under the combinator rules have to have size 3. There are a total of 16 such expressions: And none of them do anything interesting: they either don’t change at all, or, as in for example k[s][s], they immediately give a single symbol (here s). But what about larger combinator expressions? The total number of possible combinator expressions of size n grows like or in general: or asymptotically: At size 4, again nothing too interesting happens. With all the 80 possible expressions, the longest it takes to reach a fixed point is 3 steps, and that happens in 4 cases: At size 5, the longest it takes to reach a fixed point is 4 steps, and that happens in 10 cases out of 448: At size 6, there is a slightly broader distribution of “halting times”: The longest halting time is 7, achieved by: Meanwhile, the largest expressions created are of size 10 (in the sense that they contain a total of 10 s’s or k’s): The distribution of final sizes is a little odd: For size n ≤ 5, there’s actually a gap with no final states of size n – 1 generated. But at size 6, out of 2688 expressions, there are just 12 that give size 5 (about 0.4%). OK, so what’s going to happen if we go to size 7? Now there are 16,896 possible expressions. And there’s something new: two never stabilize (S(SS)SSSS), (SSS(SS)SS): {s[s[s]][s][s][s][s], s[s][s][s[s]][s][s]} After one step, the first one of these evolves to the second, but then this is what happens over the next few steps (we’ll see other visualizations of this later): The total size (i.e. LeafCount, or “number of s’s”) grows like: A log plot shows that after an initial transient the size grows roughly exponentially: And looking at successive ratios one sees some elaborate fine structure: What is this ultimately doing? With a little effort, one finds that the sizes have a length-83 transient, followed by sequences of values of length 23 + 2n, in which the second differences of successive sizes are given by: The final sequence of sizes is obtained by concatenating these blocks and computing Accumulate[Accumulate[list]]—giving an asymptotic size that appears to be of the form . So, yes, we can ultimately “figure out what’s going on” with this little size-7 combinator (and we’ll see some more details later). But it’s remarkable how complicated it is. OK, but let’s go back and look at the other size-7 expressions. The halting time distribution (ignoring the 2 cases that don’t halt) basically falls off exponentially, but shows a couple of outliers: The maximum finite halting time is 16 steps, achieved by s[s[s[s]]][s][s][s] (S(S(SS))SSS): And the distribution of final sizes is (with the maximum of 41 being achieved by the maximum-halting-time expression we’ve just seen): OK, so what happens at size 8? There are 109,824 possible combinator expressions. And it’s fairly easy to find out that all but 76 of these go to fixed points within at most 50 steps (the longest survivor is s[s][s][s[s[s]]][k][k] (SSS(S(SS))KK), which halts after 44 steps): The final fixed points in these cases are mostly quite small; this is the distribution of their sizes: And here is a comparison between halting times and final sizes: The outlier for size is s[s][k][s[s[s]][s]][s] (SSK(S(SS)S)S), which evolves in 27 steps to a fixed expression of size 80 (along the way reaching an intermediate expression of size 86): Among combinator expressions that halt in less than 50 steps, the maximum intermediate expression size of 275 is achieved for s[s][s][s[s[s][k]]][k] (SSS(S(SSK))K) (which ultimately evolves to s[s[s[s][k]]][k] (S(S(SSK))K) after 26 steps): So what about size-8 expressions that don’t halt after 50 steps? There are altogether 76—with 46 of these being inequivalent (in the sense that they don’t quickly evolve to others in the set). Here’s how these 46 expressions grow (at least until they reach size 10,000): Some of these actually end up halting. In fact, s[s][s][s[s]][s][k[k]] (SSS(SS)S(KK)) halts after just 52 steps, with final result k[s[k][k[s[k][k]]]] (K(SK(K(SKK)))), having achieved a maximum expression size of 433: The next shortest halting time occurs for s[s][s][s[s[s]]][k][s] (SSS(S(SS))KS), which takes 89 steps to produce an expression of size 65: Then we have s[s][s][s[s[s]]][s][k] (SSS(S(SS))SK), which halts (giving the size-10 s[k][s[s[s[s[s[s]]][s]]][k]] (SK(S(S(S(S(SS))S))K)), but only after 325 steps: There’s also a still-larger case to be seen: s[s[s][s]][s][s[s]][k] (S(SSS)S(SS)K), which exhibits an interesting “IntegerExponent-like” nested pattern of growth, but finally halts after 1958 steps, having achieved a maximum intermediate expression size of 21,720 along the way: What about the other expressions? s[s][s][s[s]][s][s[k]] shows very regular growth in size: In the other cases, there’s no such obvious regularity. But one can start to get a sense of what happens by plotting differences between sizes on successive steps: There are some obvious cases of regularity here. Several show a regular pattern of linearly increasing differences, implying overall t2 growth in size: Others show regular growth in differences, leading to t3/2 growth in size: Others have pure exponential growth: There are quite a few that have regular but below-exponential growth, much like the size-7 case s[s][s][s[s]][s][s] (SSS(SS)SS) with ~ growth: All the cases we’ve just looked at only involve s. When we allow k as well, there’s for example s[s][s][s[s[s][s]]][k] (SSS(S(SSS))K)—which shows regular, essentially “stair-like” growth: There’s also a case like s[s[s]][s][s[s]][s][k] (S(SS)S(SS)SK): On a small scale, this appears somewhat regular, but the larger-scale structure, as revealed by taking differences, it doesn’t seem so regular (though it does have a certain “IntegerExponent-like” look): It’s not clear what will happen in this case. The overall form of the behavior looks a bit similar to examples above that eventually terminate. Continuing for 50,000 steps, though, here’s what’s happened: And in fact it turns out that the size-difference peaks continue to get higher—having values of the form 6 (17 × 2n + 1) and occurring at positions of the form 2 (9 × 2n+2 + n – 18). Here’s another example: s[s][s][s[s]][s][k[s]] (SSS(SS)S(KS))). The overall growth in this case—at least for 200 steps—looks somewhat irregular: And taking differences reveals a fairly complex pattern of behavior: But after 1000 steps there appears to be some regularity to be seen: And even after 2000 steps the regularity is more obvious: There’s a long transient, but after that there are systematic peaks in the size difference, with the nth peak having height 16487 + 3320 n and occurring at step 14n2 + 59n + 284. (And, yes, it’s pretty weird to see all these strange numbers cropping up.) What happens if we look at size-10 combinator expressions? There’s a lot of repeating of behavior that we’ve seen with smaller expressions. But some new things do seem to happen. After 1000 steps s[s][k][s[s][k][s[s]][s]][k] (SSK(SSK(SS)S)K) seems to be doing something quite complicated when one looks at its size differences: But it turns out that this is just a transient, and after 1000 steps or so, the system settles into a pattern of continual growth similar to ones we’ve seen before: Another example is s[s][k][s[s][k][s[s]][s]][s] (SSK(SSK(SS)S)S). After 2000 steps there seems to be some regularity, and some irregularity: And basically this continues: s[s][s][s[s[s[k]]]][s][s[k]] (SSS(S(S(SK)))S(SK)) is a fairly rare example of “nested-like” growth that continues forever (after a million steps, the size obtained is 597,871,806): As a final example, consider s[s[s]][s][s][s][s[s][k[k]]] (S(SS)SSS(SS(KK))). Here’s what this does for the first 1000 steps: It looks somewhat complicated, but seems to be growing slowly. But then around step 4750 it suddenly jumps up, quickly reaching size 51,462: Keep going further, and there are more jumps: After 100,000 steps there’s a definite pattern of jumps—but it’s not quite regular: So what’s going to happen? Mostly it seems to be maintaining a size of a few thousand or more. But then, after 218,703 steps, it dips down, to size 319. So, one might think, perhaps it’s going to “die out”. Keep going longer, and at step 34,339,093 it gets down to size 27, even though by step 36,536,622 it’s at size 105,723. Keep going even longer, and one sees it dipping down in size again (here shown in a downsampled log plot): But, then, suddenly, boom. At step 137,356,329 it stops, reaching a fixed point of size 39. And, yes, it’s totally remarkable that a tiny combinator expression like s[s[s]][s][s][s][s[s][k[k]]] (S(SS)SSS(SS(KK))) can do all this. If one hasn’t seen it before, this kind of complexity would be quite shocking. But after spending so long exploring the computational universe, I’ve become used to it. And now I just view each new case I see as yet more evidence for my Principle of Computational Equivalence. A central fact about s, k combinators is that they’re computation universal. And this tells us that whatever computation we want to do, it’ll always be possible to “write a combinator program”—i.e. to create a combinator expression—that’ll do it. And from this it follows that—just like with the halting problem for Turing machines—the problem of whether a combinator will halt is in general undecidable. But the new thing we’re seeing here is that it’s difficult to figure out what will happen not just “in general” for complicated expressions set up to do particular computations but also for simple combinator expressions that one might “find in the wild”. But the Principle of Computational Equivalence tells us why this happens. Because it says that even simple programs—and simple combinator expressions—can lead to computations that are as sophisticated as anything. And this means that their behavior can be computationally irreducible, so that the only way to find out what will happen is essentially just to run each step and see what happens. So then if one wants to know what will happen in an infinite time, one may have to do an effectively infinite amount of computation to find out. Might there be another way to formulate our questions about the behavior of combinators? Ultimately we could use any computation universal system to represent what combinators do. But some formulations may connect more immediately with existing ideas—say mathematical ones. And for example I think it’s conceivable that the sequences of combinator sizes we’ve seen above could be obtained in a more “direct numerical way”, perhaps from something like nestedly recursive functions (I discovered this particular example in 2003).
Combinators: A Centennial View 9.5 Visualizing Combinators : One of the issues in studying combinators is that it’s so hard to visualize what they’re doing. It’s not like with cellular automata where one can make arrays of black and white cells and readily use our visual system to get an impression of what’s going on. Consider for example the combinator evolution: In a cellular automaton the rule would be operating on neighboring elements, and so there’d be locality to everything that’s happening. But here the combinator rules are effectively moving whole chunks around at a time, so it’s really hard to visually trace what’s happening. But even before we get to this issue, can we make the mass of brackets and letters in something like easier to read? For example, do we really need all those brackets? In the Wolfram Language, for example, instead of writing a[b[c[d[e]]]] we can equivalently write a@b@c@d@e thereby avoiding brackets. But using @ doesn’t avoid all grouping indications. For example, to represent a[b][c][d][e] with @ we’d have to write: (((a@b)@c)@d)@e In our combinator expression above, we had 24 pairs of brackets. By using @, we can reduce this to 10: And we don’t really need to show the @, so we can make this smaller: When combinators were first introduced a century ago, the focus was on “multi-argument-function-like” expressions such as a[b][c] (as appear in the rules for s and k), rather than on “nested-function-like” expressions such as a[b[c]]. So instead of thinking of function application as “right associative”—so that a[b[c]] can be written without parentheses as a@b@c—people instead thought of function application as left associative—so that a[b][c] could be written without parentheses. (Confusingly, people often used @ as the symbol for this left-associative function application.) So this means that a[b][c][d][e] can now be written without parentheses. Of course, now a[b[c[d[e]]]] needs parentheses: In this notation the rules for s and k can be written without brackets as: Our combinator expression above becomes: or without the function application character: which now involves 13 pairs of parentheses. Needless to say, if you consider all possible combinator expressions, left and right associativity on average do just the same in terms of parenthesis counts: for size-n combinator expressions, both on average need pairs; the number of cases needing k pairs is (the “Catalan triangle”). (Without associativity, we’re dealing with our standard representation of combinator expressions, which always requires n – 1 pairs of brackets.) By the way, the number of “right-associative” parenthesis pairs is just the number of subparts of the combinator expression that match _[_][_], while for left-associative parenthesis pairs it’s the number that match _[_[_]]. (The number of brackets in the no-associativity case is the number of matches of _[_].) If we look at the parenthesis/bracket count in the evolution of the smallest nonterminating combinator expression from above s[s][s][s[s]][s][s] (otherwise known as s•s•s•(s•s)•s•s) we find: Or in other words, in this case, left associativity leads on average to about 62% of the number of parentheses of right associativity. We’ll look at this in more detail later, but for growing combinator expressions, it’ll almost always turn out to be the case that left associativity is the “parenthesis-avoidance winner”. But even with our “best parenthesis avoidance” it’s still very hard to see what’s going on from the textual form: The total number of • symbols is always equal to the number of pairs of brackets in our standard “non-associative” functional form: What if we look at this on a larger scale, “cellular automaton style”, with s being and • being ? Here’s the not-very-enlightening result: Running for 50 steps, and fixing the aspect ratio, we get (for the Polish case): We can make the same kinds of pictures from our bracket representation too. We just take a string like s[s][s][s[s]][s][s] and render each successive character as a cell of some color. (It’s particularly easy if we’ve only got one basic combinator—say s—because then we only need colors for the opening and closing brackets.) We can also make “cellular automaton–style” pictures from parenthesis representations like SSS(SS)SS. Again, all we do is render each successive character as a cell of some color. The results essentially always tend to look much like the reverse Polish case above. Occasionally, though, they reveal at least something about the “innards of the computation”. Like here’s the terminating combinator expression s[s][s][s[s[s]]][k][s]] from above, rendered in right-associative form: Pictures like this in a sense convert all combinator expressions to sequences. But combinator expressions are in fact hierarchical structures, formed by nested invocations of symbolic “functions”. One way to represent the hierarchical structure of s[s][s][s[s]][s][ is through a hierarchy of nested boxes: We can color each box by its depth in the expression: But now to represent the expression all we really need to do is show the basic combinators in a color representing its depth. And doing this, we can visualize the terminating combinator evolution above as: We can also render this in 3D (with the height being the “depth” in the expression): To test out visualizations like these, let’s look (as above) at all the size-8 combinator expressions with distinct evolutions that don’t terminate within 50 steps. Here’s the “depth map” for each case: In these pictures we’re drawing a cell for each element in the “stringified version” of the combinator expression at each step, then coloring it by depth. But given a particular combinator expression, one can consider other ways to indicate the depth of each element. Here are a few possibilities, shown for step 8 in the evolution of s[s][s][s[s]][s][s] (SSS(SS)SS) (note that the first of these is essentially the “indentation level” that might be used if each s, k were “pretty printed” on a separate line): And this is what one gets on a series of steps: But in a sense a more direct visualization of combinator expressions is as trees, as for example in: Note that these trees can be somewhat simplified by treating them as left or right “associative”, and essentially pulling left or right leaves into the “branch nodes”. But using the original trees, we can ask for example what the trees for the expressions produced by the evolution of s[s][s][s[s]][s][s] (SS(SS)SS) are. Here are the results for the first 15 steps: In a different rendering, these become: OK, so these are representations of the combinator expressions on successive steps. But where are the rules being applied at each step? As we’ll discuss in much more detail in the next section, in the way we’ve done things so far we’re always doing just one update at each step. Here’s an example of where the updates are happening in a particular case: Continuing longer we get (note that some lines have wrapped in this display): A feature of the way we’re writing out combinator expressions is that the “input” to any combinator rule always corresponds to a contiguous span within the expression as we display it. So when we show the total size of combinator expressions on each step in an evolution, we can display which part is getting rewritten: Notice that, as expected, application of the S rule tends to increase size, while the K rule decreases it. Here is the distribution of rule applications for all the examples we showed above: We can combine multiple forms of visualization by including depths: We can also do the same in 3D: So what about the underlying trees? Here are the S, K combinator rules in terms of trees: And here are the updates for the first few steps of the evolution of s[s][s][s[s[s]]][k][s] (SSS(S(SS))KS): In these pictures we are effectively at each step highlighting the “first” subtree matching s[_][_][_] or k[_][_]. To get a sense of the whole evolution, we can also simply count the number of subtrees with a given general structure (like _[_][_] or _[_[_]]) that occur at a given step (see also below): One more indication of the behavior of combinators comes from looking at tree depths. In addition to the total depth (i.e. Wolfram Language Depth) of the combinator tree, one can also look at the depth at which update events happen (here with the total size shown underneath): Here are the depth profiles for the rules shown above: Not surprisingly, total depth tends to increase when growth continues. But it is notable that—except when termination is near at hand—it seems like (at least with our current updating scheme) updates tend to be made to “high-level” (i.e. low-depth) parts of the expression tree. When we write out a combinator expression like the size-33 or show it as a tree we’re in a sense being very wasteful, because we’re repeating the same subexpressions many times. In fact, in this particular expression, there are 65 subexpressions altogether—but only 11 distinct ones. So how can we represent a combinator expression making as much use as possible of the commonality of these subexpressions? Well, instead of using a tree for the combinator expression, we can use a directed acyclic graph (DAG) in which we start from a node representing the whole expression, and then show how it breaks down into shared subexpressions, with each shared subexpression represented by a node. For s[s][s][s[s]][s][s]] things get a bit more complicated: For the size-33 expression above, the DAG representation is where the nodes correspond to the 11 distinct subexpression of the whole expression that appears at the root. So what does combinator evolution look like in terms of DAGs? Here are the first 15 steps in the evolution of s[s][s][s[s]][s][s]: And here are some later steps: Sharing all common subexpressions is in a sense a maximally reduced way to specify a combinator expression. And even when the total size of the expressions is growing roughly exponentially, the number of distinct subexpressions may grow only linearly—here roughly like 1.24 t: Looking at successive differences suggests a fairly simple pattern. Here are the DAG representations of the result of 50 steps in the evolution of the 46 “growing size-7” combinator expressions above: It’s notable that some of these show considerable complexity, while others have a rather simple structure.
Combinators: A Centennial View 9.6 Updating Schemes and Multiway Systems : The world of combinators as we’ve discussed it so far may seem complicated. But we’ve actually so far been consistently making a big simplification. And it has to do with how the combinator rules are applied. Consider the combinator expression: s[s[s][s][s[s][k[k][s]][s]]][s][s][s[k[s][k]][k][s]] There are 6 places (some overlapping) at which s[_][_][_] or k[_][_] matches some subpart of this expression: One can see the same thing in the tree form of the expression (the matches are indicated at the roots of their subtrees): But now the question is: if one’s applying combinator rules, which of these matches should one use? What we’ve done so far is to follow a particular strategy—usually called leftmost outermost—which can be thought of as looking at the combinator expression as we normally write it out with brackets etc. and applying the first match we encounter in a left-to-right scan, or in this case: In the Wolfram Language we can find the positions of the matches just using: expr = s[s[s][s][s[s][k[k][s]][s]]][s][s][s[k[s][k]][k][s]] This shows—as above—where these matches are in the expression: expr = s[s[s][s][s[s][k[k][s]][s]]][s][s][s[k[s][k]][k][s]]; Here are the matches, in the order provided by Position: The leftmost-outermost match here is the one with position {0}. We’ll talk in the next section about how leftmost outermost—and other schemes—are defined in terms of indices. But here the thing to notice is that in our example here Position doesn’t give us part {0} first; instead it gives us {0,0,0,1,1,0,1}: And what’s happening is that Position is doing a depth-first traversal of the expression tree to look for matches, so it first descends all the way down the left-hand tree branches—and since it finds a match there, that’s what it returns. In the taxonomy we’ll discuss in the next section, this corresponds to a leftmost-innermost scheme, though here we’ll refer to it as “depth first”. Now consider the example of s[s][s][k[s][s]]. Here is what it does first with the leftmost-outermost strategy we’ve been using so far, and second with the new strategy: There are two important things to notice. First, that in both cases the final result is the same. And second, that the steps taken—and the total number required to get to the final result—is different in the two cases. Let’s consider a larger example: s[s][s][s[s[s]]][k][s]] (SSS(S(SS))KS). With our standard strategy we saw above that the evolution of this expression terminates after 89 steps, giving an expression of size 65. With the depth-first strategy the evolution still terminates with the same expression of size 65, but now it takes only 29 steps: It’s an important feature of combinator expression evolution that when it terminates—whatever strategy one’s used—the result must always be the same. (This “confluence” property—that we’ll discuss more later—is closely related to the concept of causal invariance in our models of physics.) What happens when the evolution doesn’t terminate? Let’s consider the simplest non-terminating case we found above: s[s][s][s[s]][s][s] (SSS(SS)SS). Here’s how the sizes increase with the two strategies we’ve discussed: The difference is more obvious if we plot the ratios of sizes on successive steps: In both these pairs of pictures, we can see that the two strategies start off producing the same results, but soon diverge. OK, so we’ve looked at two particular strategies for picking which updates to do. But is there a general way to explore all possibilities? It turns out that there is—and it’s to use multiway systems, of exactly the kind that are also important in our Physics Project. The idea is to make a multiway graph in which there’s an edge to represent each possible update that can be performed from each possible “state” (i.e. combinator expression). Here’s what this looks like for the example of s[s][s][k[s][s]] (SSS(KSS)) above: Here’s what we get if we include all the “updating events”: Now each possible sequence of updating events corresponds to a path in the multiway graph. The two particular strategies we used above correspond to these paths: We see that even at the first step here, there are two possible ways to go. But in addition to branching, there is also merging, and indeed whichever branch one takes, it’s inevitable that one will end up at the same final state—in effect the unique “result” of applying the combinator rules. Here’s a slightly more complicated case, where there starts out being a unique path, but then after 4 steps, there’s a branch, but after a few more steps, everything converges again to a unique final result: For combinator expressions of size 4, there’s never any branching in the multiway graph. At size 5 the multiway graphs that occur are: At size 6 the 2688 possible combinator expressions yield the following multiway graphs, with the one shown above being basically as complicated as it gets: At size 7, much more starts being able to happen. There are rather regular structures like: As well as cases like: This can be summarized by giving just the size of each intermediate expression, here showing the path defined by our standard leftmost-outermost updating strategy: By comparison, here is the path defined by the depth-first strategy above: s[s][s][s[s[k]]][k] (SSS(S(SK))K) is a case where leftmost outermost-evaluation avoids longer paths and larger intermediate expressions while depth-first evaluation takes more steps: s[s[s]][s][s[s]][s] (S(SS)S(SS)S) gives a larger but more uniform multiway graph (s[s[s[s]]][s][s][s] evolves directly to s[s[s]][s][s[s]][s]): Depth-first evaluation gives a slightly shorter path: Among size-7 expressions, the largest finite multiway graph (with 94 nodes) is for s[s[s[s]]][s][s][k] (S(S(SS))SSK): Depending on the path, this can take between 10 and 18 steps to reach its final state: Our standard leftmost-outermost strategy takes 12 steps; the depth first takes 13 steps: But among size-7 combinator expressions there are basically two that do not lead to finite multiway systems: s[s[s]][s][s][s][k] (S(SS)SSSK) (which evolves immediately to s[s][s][s[s]][s][k]) and s[s[s]][s][s][s][s] (S(SS)SSSS) (which evolves immediately to s[s][s][s[s]][s][s]). Let’s consider s[s[s]][s][s][s][k]. For 8 steps there’s a unique path of evolution. But at step 9, the evolution branches as a result of there being two distinct possible updating events: Continuing for 14 steps we get a fairly complex multiway system: But this isn’t “finished”; the nodes circled in red correspond to expressions that are not fixed points, and will evolve further. So what happens with particular evaluation orders? Here are the results for our two updating schemes: Something important is visible here: the leftmost-outermost path leads (in 12 steps) to a fixed-point node, while the depth-first path goes to a node that will evolve further. In other words, at least as far as we can see in this multiway graph, leftmost-outermost evaluation terminates while depth first does not. There is just a single fixed point visible (s[k]), but there are many “unfinished paths”. What will happen with these? Let’s look at depth-first evaluation. Even though it hasn’t terminated after 14 steps, it does so after 29 steps—yielding the same final result s[k]: And indeed it turns out to be a general result (known since the 1940s) that if a combinator evolution path is going to terminate, it must terminate in a unique fixed point, but it’s also possible that the path won’t terminate at all. Here’s what happens after 17 steps. We see more and more paths leading to the fixed point, but we also see an increasing number of “unfinished paths” being generated: Let’s now come back to the other case we mentioned above: s[s[s]][s][s][s][s] (S(SS)SSSS). For 12 steps the evolution is unique: But at that step there are two possible updating events: And from there on out, there’s rapid growth in the multiway graph: And what’s important here is that there are no fixed points: there is no possible evaluation strategy that leads to a fixed point. And what we’re seeing here is an example of a general result: if there is a fixed point in a combinator evolution, then leftmost-outermost evaluation will always find it. In a sense, leftmost-outermost evaluation is the “most conservative” evaluation strategy, with the least propensity for ending up with “runaway evolution”. Its “conservatism” is on display if one compares growth from it and from depth-first evaluation in this case: Looking at the multiway graph—as well as others—a notable feature is the presence of “long necks”: for many steps every evaluation strategy leads to the same sequence of expressions, and there is just one possible match at each step. But how long can this go on? For size 8 and below it’s always limited (the longest “neck” at size 7 is for s[s[s]][s][s][s][s] and is of length 13; for size 8 it is no longer, but is of length 13 for s[s[s[s]][s][s][s][s]] and k[s[s[s]][s][s][s][s]]). But at size 9 there are four cases (3 distinct) for which growth continues forever, but is always unique: {s[s[s[s]]][s[s[s][s]]][s], s[s[s[s]]][s[s[s]]][s[s]], And as one might expect, all these show rather regular patterns of growth: The second differences are given in the first and third cases by repeats of (for successive n): In the second they are given by repeats of and in the final case by repeats o.
Combinators: A Centennial View 9.7 The Question of Evaluation Order : As a computational language designer, it’s an issue I’ve been chasing for 40 years: what’s the best way to define the order in which one evaluates (i.e. computes) things? The good news is that in a well-designed language (like the Wolfram Language!) it fundamentally doesn’t matter, at least much of the time. But in thinking about combinators—and the way they evolve—evaluation order suddenly becomes a central issue. And in fact it’s also a central issue in our new model of physics—where it corresponds to the choice of reference frame, for relativity, quantum mechanics and beyond. Let’s talk first about evaluation order as it shows up in the symbolic structure of the Wolfram Language. Imagine you’re doing this computation: And usually this is exactly what one wants, and what people implicitly expect. But there are cases where it isn’t. For example, let’s say you’ve defined x = 1 (i.e. Set[x,1]). Now you want to say x = 2 (Set[x,2]). If the x evaluated first, you’d get Set[1,2], which doesn’t make any sense. Instead, you want Set to “hold its first argument”, and “consume it” without first evaluating it. And in the Wolfram Language this happens automatically because Set has attribute HoldFirst. How is this relevant to combinators? Well, basically, the standard evaluation order used by the Wolfram Language is like the depth-first (leftmost-innermost) scheme we described above, while what happens when functions have Hold attributes is like the leftmost-outermost scheme. What’s the analog of this for combinators? Basically it’s whether when you do an update based on a particular match in a combinator expression, you then just keep on “updating the update”, or whether instead you go on and find the next match in the expression before doing anything with the result of the update. The “updating the update” scheme is basically what we’ve called our depth-first scheme, and it’s essentially what the Wolfram Language does in its automatic evaluation process. Imagine we give the combinator rules as Wolfram Language assignments: s[x_][y_][z_] := x[z][y[z]] k[x_][y_] := x Then—by virtue of the standard evaluation process in the Wolfram Language—every time we enter a combinator expression these rules will automatically be repeatedly applied, until a fixed point is reached: s[s][s][s[s[s]]][k][s] What exactly is happening “inside” here? If we trace it in a simpler case, we can see that there is repeated evaluation, with a depth-first (AKA leftmost-innermost) scheme for deciding what to evaluate: Of course, given the assignment above for s, if one enters a combinator expression—like s[s][s][s[s]][s][s]]—whose evaluation doesn’t terminate, there’ll be trouble, much as if we define x = x + 1 (or x = {x}) and ask for x. Back when I was first doing language design people often told me that issues like this meant that a language that used automatic infinite evaluation “just couldn’t work”. But 40+ years later I think I can say with confidence that “programming with infinite evaluation, assuming fixed points” works just great in practice—and in rare cases where there isn’t going to be a fixed point one has to do something more careful anyway. In the Wolfram Language, that’s all about specifically applying rules, rather than just having it happen automatically. Let’s say we clear our assignments for s and k: Now no transformations associated with s and k will automatically be made: s[s][s][s[s[s]]][k][s] But by using /. (ReplaceAll) we can ask that the s, k transformation rules be applied once: s[s][s][s[s[s]]][k][s] /. {s[x_][y_][z_] -> x[z][y[z]], With FixedPointList we can go on applying the rule until we reach a fixed point: It takes 26 steps—which is different from the 89 steps for our leftmost-outermost evaluation, or the 29 steps for leftmost-innermost (depth-first) evaluation. And, yes, the difference is the result of /. in effect applying rules on the basis of a different scheme than the ones we’ve considered so far. But, OK, so how can we parametrize possible schemes? Let’s go back to the combinator expression from the beginning of the previous section: s[s[s][s][s[s][k[k][s]][s]]][s][s][s[k[s][k]][k][s]] Here are the positions of possible matches in this expression: An evaluation scheme must define a way to say which of these matches to actually do at each step. In general we can apply pretty much any algorithm to determine this. But a convenient approach is to think about sorting the list of positions by particular criteria, and then for example using the first k positions in the result. Given a list of positions, there are two obvious potential types of sorting criteria to use: ones based on the lengths of the position specifications, and ones based on their contents. For example, we might choose (as Sort by default does) to sort shorter position specifications first: But what do the shorter position specifications correspond to? They’re the more “outer” parts of the combinator expression, higher on the tree. And when we say we’re using an “outermost” evaluation scheme, what we mean is that we’re considering matches higher on the tree first. We have to decide whether to sort first by length and then by content, or the other way around. But if we enumerate all choices, here’s what we get: And here’s where the first match with each scheme occurs in the expression tree: So what happens if we use these schemes in our combinator evolution? Here’s the result for the terminating example s[s][s][s[s[s]]][k][s] above, always keeping only the first match with a given sorting criterion, and at each step showing where the matches were applied: Here now are the results if we allow the first up to 2 matches from each sorted list to be applied: Here are the results for leftmost outermost, allowing up to between 1 and 8 updates at each step: And here’s a table of the “time to reach the fixed point” with different evaluation schemes, allowing different numbers of updates at each step: Not too surprisingly, the time to reach the fixed point always decreases when the number of updates that can be done at each step increases. For the somewhat simpler terminating example s[s[s[s]]][s][s][s] (S(S(SS))SSS) we can explicitly look at the updates on the trees at each step for each of the different schemes: OK, so what about a combinator expression that does not terminate? What will these different evaluation schemes do? Here are the results for s[s[s]][s][s][s][s] (S(SS)SSSS) over the course of 50 steps, in each case using only one match at each step: And here is what happens if we allow successively more matches (selected in leftmost-outermost order) to be used at each step: Not surprisingly, the more matches allowed, the faster the growth in size (and, yes, looking at pictures like this suggests studying a kind of “continuum limit” or “mean field theory” for combinator evolution): It’s interesting to look at the ratios of sizes on successive steps for different updating schemes (still for s[s[s]][s][s][s][s]). Some schemes lead to much more “obviously simple” long-term behavior than others: In fact, just changing the number of allowed matches (here for leftmost outermost) can have similar effects: What about for other combinator expressions? Different updating schemes can lead to quite different behavior. Here’s s[s[s]][s][s[s[s]]][k] (S(SS)S(S(SS))K): And here’s s[s[s]][s][s][s][s[k]] (S(SS)SSS(SK))—which for some updating schemes gives purely periodic behavior (something which can’t happen without a k in the original combinator expression): It’s worth noting that—at least when there are k’s involved—different updating schemes can even change whether the evaluation of a particular combinator expression ever terminates. This doesn’t happen below size 8. But at size 8, here’s what happens for example with s[s][s][s[s]][s][s][k] (SSS(SS)SSK): For some updating schemes it reaches a fixed point (always just s[k]) but for others it gives unbounded growth. The innermost schemes are the worst in terms of “missing fixed points”; they do it for 16 size-8 combinator expressions. But (as we mentioned earlier) leftmost outermost has the important feature that it’ll never miss a fixed point if one exists—though sometimes at the risk of taking an overly ponderous route to the fixed point. But so if one’s applying combinator-like transformation rules in practice, what’s the best scheme to use? The Wolfram Language /. (ReplaceAll) operation in effect uses a leftmost-outermost scheme—but with an important wrinkle: instead of just using one match, it uses as many non-overlapping matches as possible. Consider again the combinator expression: s[s[s][s][s[s][k[k][s]][s]]][s][s][s[k[s][k]][k][s]] In leftmost-outermost order the possible matches here are: But the point is that the match at position {0} overlaps the match at position {0,0,0,1} (i.e. it is a tree ancestor of it). And in general the possible match positions form a partially ordered set, here: One possibility is always to use matches at the “bottom” of the partial order—or in other words, the very innermost matches. Inevitably these matches can’t overlap, so they can always be done in parallel, yielding a “parallel innermost” evaluation scheme that is potentially faster (though runs the risk of not finding a fixed point at all). What /. does is effectively to use (in leftmost order) all the matches that appear at the “top” of the partial order. And the result is again typically faster overall updating. In the s[s][s][s[s]][s][s][k] example above, repeatedly applying /. (which is what //. does) finds the fixed point in 23 steps, while it takes ordinary one-replacement-at-a-time leftmost-outermost updating 30 steps—and parallel innermost doesn’t terminate in this case: For s[s][s][s[s[s]]][k][s] (SSS(S(SS))KS) parallel innermost does terminate, getting a result in 27 steps compared to 26 for /.—but with somewhat smaller intermediate expressions: For a case in which there isn’t a fixed point, however, /. will often lead to more rapid growth. For example, with s[s[s]][s][s][s][s] (S(SS)SSSS) it basically gives pure exponential 2t/2 growth (and eventually so does parallel innermost): In A New Kind of Science I gave a bunch of results for combinators with /. updating, finding much of the same kind of behavior for “combinators in the wild” as we’ve seen here. But, OK, so we’ve got the updating scheme of /. (and its repeated version //.), and we’ve got the updating scheme for automatic evaluation (with and without functions with “hold” attributes). But are there other updating schemes that might also be useful, and if so, how might we parametrize them? I’ve wondered about this since I was first designing SMP—the forerunner to Mathematica and the Wolfram Language—more than 40 years ago. One place where the issue comes up is in automatic evaluation of recursively defined functions. Say one has a factorial function defined by: Let’s consider instead the recursive definition of Fibonacci numbers (to make this more obviously “combinator like” we could for example use Construct instead of Plus): But the question is: how do you do it? The most obvious approach amounts to doing a depth-first scan of the tree—and doing about ϕn computations. But if you were to repeatedly use /. instead, you’d be doing more of a breadth-first scan, and it’d take more like O(n2) computations: But how can one parametrize these different kinds of behavior? From our modern perspective in the Wolfram Physics Project, it’s like picking different foliations—or different reference frames—in what amount to causal graphs that describe the dependence of one result on others. In relativity, there are some standard reference frames—like inertial frames parametrized by velocity. But in general it’s not easy to “describe reasonable reference frames”, and we’re typically reduced to just talking about named metrics (Schwarzschild, Kerr, …), much like here we’re talking about “named updated orders” (“leftmost innermost”, “outermost rightmost”, …). But back in 1980 I did have an idea for at least a partial parametrization of evaluation orders. Here it is from section 3.1 of the SMP documentation: What I called a “projection” then is what we’d call a function now; a “filter” is what we’d now call an argument. But basically what this is saying is that usually the arguments of a function are evaluated (or “simplified” in SMP parlance) before the function itself is evaluated. (Though note the ahead-of-its-time escape clause about “future parallel-processing implementations” which might evaluate arguments asynchronously.) But here’s the funky part: functions in SMP also had Smp and Rec properties (roughly, modern “attributes”) that determined how recursive evaluation would be done. And in a first approximation, the concept was that Smp would choose between innermost and outermost, but then in the innermost case, Rec would say how many levels to go before “going outermost” again. And, yes, nobody (including me) seems to have really understood how to use these things. Perhaps there’s a natural and easy-to-understand way to parametrize evaluation order (beyond the /. vs. automatic evaluation vs. hold attributes mechanism in Wolfram Language), but I’ve never found it. And it’s not encouraging here to see all the complexity associated with different updating schemes for combinators. By the way, it’s worth mentioning that there is always a way to completely specify evaluation order: just do something like procedural programming, where every “statement” is effectively numbered, and there can be explicit Goto’s that say what statement to execute next. But in practice this quickly gets extremely fiddly and fragile—and one of the great values of functional programming is that it streamlines things by having “execution order” just implicitly determined by the order in which functions get evaluated (yes, with things like Throw and Catch also available). And as soon as one’s determining “execution order” by function evaluation order, things are immediately much more extensible: without having to specify anything else, there’s automatically a definition of what to do, for example, when one gets a piece of input with more complex structure. If one thinks about it, there are lots of complex issues about when to recurse through different parts of an expression versus when to recurse through reevaluation. But the good news is that at least the way the Wolfram Language is designed, things in practice normally “just work” and one doesn’t have to think about them. Combinator evaluation is one exception, where, as we have seen, the details of evaluation order can have important effects. And presumably this dependence is in fact connected to why it’s so hard to understand how combinators work. But studying combinator evaluation once again inspires one (or at least me) to try to find convenient parametrizations for evaluation order—perhaps now using ideas and intuition from physics.
Combinators: A Centennial View 9.8 The World of the S Combinator : In the definitions of the combinators s and k {s[x_][y_][z_] -> x[z][y[z]], k[x_][y_] -> x} S is basically the one that “builds things up”, while K is the one that “cuts things down”. And historically, in creating and proving things with combinators, it was important to have the balance of both S and K. But what we’ve seen above makes it pretty clear that S alone can already do some pretty complicated things. So it’s interesting to consider the minimal case of combinators formed solely from S. For size n (i.e. LeafCount[n]), there are (~ for large n) possible such combinators, each of which can be characterized simply in terms of the sequence of bracket openings and closings it involves. Some of these combinators terminate in a limited time, but above size 7 there are ones that do not: And already there’s something weird: the fraction of nonterminating combinator expressions steadily increases with size, then precipitously drops, then starts climbing again: But let’s look first at the combinator expressions whose evaluation does terminate. And, by the way, when we’re dealing with S alone, there’s no possibility of some evaluation schemes terminating and others not: they either all terminate, or none do. (This result was established in the 1930s from the fact that the S combinator—unlike K—in effect “conserves variables”, making it an example of the so-called λI calculus.) With leftmost-outermost evaluation, here are the halting time distributions, showing roughly exponential falloff with gradual broadening: And here are the (leftmost-outermost) “champions”—the combinator expressions that survive longest (with leftmost-outermost evaluation) before terminating: The survival (AKA halting) times grow roughly exponentially with size—and notably much slower than what we saw in the SK case above: How do the champions actually behave? Here’s what happens for a sequence of sizes: There’s progressive increase in size, and then splat: the evolution terminates. Looking at the detailed behavior (here for size 9 with a “right-associative rendering”) shows that what’s going on is quite systematic: The differences again reflect the systematic character of the behavior: And it seems that what’s basically happening is that the combinator is acting as a kind of digital counter that’s going through an exponential number of steps—and ultimately building a very regular tree structure: By the way, even though the final state is the same, the evolution is quite different with different evaluation schemes. And for example our “leftmost-outermost champions” actually terminate much faster with depth-first evaluation: Needless to say, there can be different depth-first (AKA leftmost-innermost) champions, although—somewhat surprisingly—some turn out to be the same (but not sizes 8, 12, 13): We can get a sense of what happens with all possible evaluation schemes if we look at the multiway graph. Here is the result for the size-8-leftmost-outermost champion s[s[s[s]]][s][s][s]: The number of expressions at successive levels in the multiway graph starts off growing quite exponentially, but after 12 steps it rapidly drops—eventually yielding a finite graph with 74 nodes (leftmost outermost is the “slowest” evaluation scheme—taking the maximum 15 steps possible): Even for the size-9 champion the full multiway graph is too large to construct explicitly. After 15 steps the number of nodes has reached 6598, and seems to be increasingly roughly like —even though after at most 86 steps all “dangling ends” must have resolved, and the system must reach its fixed point: What happens with s combinator expressions that do not terminate? We already saw above some examples of the kind of growth in size one observes (say with leftmost-outermost evaluation). Here are examples with roughly exponential behavior, with differences between successive steps shown on a log scale: And here are examples of differences shown on a linear scale: Sometimes there are fairly long transients, but what’s notable is that among all the 8629 infinite-growth combinator expressions up to size 11 there are none whose evolution seems to show long-term irregularity in overall size. Of course, something like rule 30 also doesn’t show irregularity in overall size; one has to look “inside” to see complex behavior—and difficulties of visualization make that hard to systematically do in the case of combinators. But looking at the pictures above there seem to be a “limited number of ways” that combinator expressions grow without bound. Sometimes it’s rather straightforward to see how the infinite growth happens. Here’s a particularly “pure play” example: the size-9 case s[s[s[s]]][s[s[s]]][s[s]] (S(S(SS))(S(SS))(SS)) which evolves the same way with all evaluation schemes (in the pictures, the root of the match at each step is highlighted): Looking at the subtree “below” each match we see and it is clear that there is a definite progression which will keep going forever, leading to infinite growth. But if one looks at the corresponding sequence of subtrees for a case like the smallest infinite-growth combinator expression s[s][s][s[s]][s][s] (SSS(SS)SS), it’s less immediately obvious what’s going on: But there’s a rather remarkable result from the end of the 1990s that gives one a way to “evaluate” combinator expressions, and tell whether they’ll lead to infinite growth—and in particular to be able to say directly from an initial combinator expression whether it’ll continue evolving forever, or will reach a fixed point. One starts by writing a combinator expression like s[s[s[s]]][s[s[s]]][s[s]] (S(S(SS))(S(SS))(SS)) in an explicitly “functional” form: and in this case the value at the root just counts the total size (i.e. LeafCount). Bright red (value 38) represents the presence of an infinite growth seed—and once one exists, f makes it propagate up to the root of the tree. And with this setup, if we replace s by the value 0, the combinator expression above can be “evaluated” as: At successive steps in the evolution we get: Or after 8 steps: The “lowest 38” is always at the top of the subtree where the match occurs, serving as a “witness” of the fact that this subtree is an infinite growth seed. Here are some sample size-7 combinator expressions, showing how the two that lead to infinite growth are identified: If we were dealing with combinator expressions involving both S and K we know that it’s in general undecidable whether a particular expression will halt. So what does it mean that there’s a decidable way to determine whether an expression involving only S halts? One might assume it’s a sign that S alone is somehow computationally trivial. But there’s more to this issue. In the past, it has often been thought that a “computation” must involve starting with some initial (“input”) state, then ending up at a fixed point corresponding to a final result. But that’s certainly not how modern computing in practice works. The computer and its operating system do not completely stop when a particular computation is finished. Instead, the computer keeps running, but the user is given a signal to come and look at something that provides the output for the computation. There’s nothing fundamentally different about how computation universality works in a setup like this; it’s just a “deployment” issue. And indeed the simplest possible examples of universality in cellular automata and Turing machines have been proved this way. So how might this work for S combinator expressions? Basically any sophisticated computation has to live on top of an infinite combinator growth process. Or, put another way, the computation has to exist as some kind of “transient” of potentially unbounded length, that in effect “modulates” the infinite growth “carrier”. One would set up a program by picking an appropriate combinator expression from the infinite collection that lead to infinite growth. Then the evolution of the combinator expression would “run” the program. And one would use some computationally bounded process (perhaps a bounded version of a tree automaton) to identify when the result of the computation is ready—and one would “read it out” by using some computationally bounded “decoder”. My experience in the computational universe—as captured in the Principle of Computational Equivalence—is that once the behavior of a system is not “obviously simple”, the system will be capable of sophisticated computation, and in particular will be computation universal. The S combinator is a strange and marginal case. At least in the ways we have looked at it here, its behavior is not “obviously simple”. But we have not quite managed to identify things like the kind of seemingly random behavior that occurs in a system like rule 30, that are a hallmark of sophisticated computation, and probably computation universality. There are really two basic possibilities. Either the S combinator alone is capable of sophisticated computation, and there is, for example, computational irreducibility in determining the outcome of a long S combinator evolution. Or the S combinator is fundamentally computationally reducible—and there is some approach (and maybe some new direction in mathematics) that “cracks it open”, and allows one to readily predict everything that an S combinator expression will do. I’m not sure which way it’s going to go—although my almost-uniform experience over the last four decades has been that when I think some system is “too simple” to “do anything interesting” or show sophisticated computation, it eventually proves me wrong, often in bizarre and unexpected ways. (In the case of the S combinator, a possibility—like I found for example in register machines—is that sophisticated computation might first reveal itself in very subtle effects, like seemingly random off-by-one patterns.) But whatever happens, it’s amazing that 100 years after the invention of the S combinator there are still such big mysteries about it. In his original paper, Moses Schönfinkel expressed his surprise that something as simple as S and K were sufficient to achieve what we would now call universal computation. And it will be truly remarkable if in fact one can go even further, and S alone is sufficient: a minimal example of universal computation hiding in plain sight for a hundred years. (By the way, in addition to ordinary “deterministic” combinator evolution with a particular evaluation scheme, one can also consider the “nondeterministic” case corresponding to all possible paths in the multiway graph. And in that case there’s a question of categorizing infinite graphs obtained by nonterminating S combinator expressions—perhaps in terms of transfinite numbers..
Combinators: A Centennial View 9.9 Causal Graphs and the Physicalization of Combinators : Not long ago one wouldn’t have had any reason to think that ideas from physics would relate to combinators. But our Wolfram Physics Project has changed that. And in fact it looks as if methods and intuition from our Physics Project—and the connections they make to things like relativity—may give some interesting new insights into combinators, and may in fact make their operation a little less mysterious. In our Physics Project we imagine that the universe consists of a very large number of abstract elements (“atoms of space”) connected by relations—as represented by a hypergraph. The behavior of the universe—and the progression of time—is then associated with repeated rewriting of this hypergraph according to a certain set of (presumably local) rules. It’s certainly not the same as the way combinators work, but there are definite similarities. In combinators, the basic “data structure” is not a hypergraph, but a binary tree. But combinator expressions evolve by repeated rewriting of this tree according to rules that are local on the tree. There’s a kind of intermediate case that we’ve often used as a toy model for aspects of physics (particularly quantum mechanics): string substitution systems. A combinator expression can be written out “linearly” (say as s[s][s][s[s[s]]][k][s]), but really it’s tree-structured and hierarchical. In a string substitution system, however, one just has plain strings, consisting of sequences of characters, without any hierarchy. The system then evolves by repeatedly rewriting the string by applying some local string substitution rule. For example, one could have a rule like {A → BBB,BB → A}. And just like with combinators, given a particular string—like "BBA"—there are different possible choices about where to apply the rule. And—again like with combinators—we can construct a multiway graph to represent all possible sequences of rewritings: And again as with combinators we can define a particular “evaluation order” that determines which of the possible updates to the string to apply at each step—and that defines a path through the multiway graph. For strings there aren’t really the same notions of “innermost” and “outermost”, but there are “leftmost” and “rightmost”. Leftmost updating in this case would give the evolution history which corresponds to the path: Here’s the underlying evolution corresponding to that path, with the updating events indicated in yellow: But now we can start tracing the “causal dependence” of one event on another. What characters need to have been produced as “output” from a preceding event in order to provide “input” to a new event? Let’s look at a case where we have a few more events going on: But now we can draw a causal graph that shows causal relationships between events, i.e. which events have to have happened in order to enable subsequent events: And at a physics level, if we’re an observer embedded in the system, operating according to the rules of the system, all we can ultimately “observe” is the “disembodied” causal graph, where the nodes are events, and the edges represent the causal relationships between these events: So how does this relate to combinators? Well, we can also create causal graphs for those—to get a different view of “what’s going on” during combinator evolution. There is significant subtlety in exactly how “causal dependence” should be defined for combinator systems (when is a copied subtree “different”?, etc.). Here I’ll use a straightforward definition that’ll give us an indication of how causal relationships in combinators work, but that’s going to require further refinement to fit in with other definitions we want. Imagine we just write out combinator expressions in a linear way. Then here’s a combinator evolution: To understand causal relationships we need to trace “what gets rewritten to what”—and which previous rewriting events a given rewriting event “takes its input from”. It’s helpful to look at the rewriting process above in terms of trees: Going back to a textual representation, we can show the evolution in terms of “states”, and the “events” that connect them. Then we can trace (in orange) what the causal relationships between the events are: Continuing this for a few more steps we get: Now keeping only the causal graph, and continuing until the combinator evolution terminates, we get: It’s interesting to compare this with a plot that summarizes the succession of rewriting events: So what are we actually seeing in the causal graph? Basically it’s showing us what “threads of evaluation” occur in the system. When there are different parts of the combinator expression that are in effect getting updated independently, we see multiple causal edges running in parallel. But when there’s a synchronized evaluation that affects the whole system, we just see a single thread—a single causal edge. The causal graph is in a sense giving us a summary of the structure of the combinator evolution, with many details stripped out. And even when the size of the combinator expression grows rapidly, the causal graph can still stay quite simple. So, for example, the growing combinator s[s][s][s[s]][s][s] has a causal graph that forms a linear chain with simple “side loops” that get systematically further apart: Sometimes it seems that the growth dies out because different parts of the combinator system become causally disconnected from each other: Here are a few other examples: But do such causal graphs depend on the evaluation scheme used? This turns out to be a subtle question that depends sensitively on definitions of identity for abstract expressions and their subexpressions. The first thing to say is that combinators are confluent, in the sense that different evaluation schemes—even if they take different paths—must always give the same final result whenever the evolution of a combinator expression terminates. And closely related to this is the fact that in the multiway graph for a combinator system, any branching must be accompanied by subsequent merging. For both string and hypergraph rewriting rules, the presence of these properties is associated with another important property that we call causal invariance. And causal invariance is precisely the property that causal graphs produced by different updating orders must always be isomorphic. (And in our model of physics, this is what leads to relativistic invariance, general covariance, objective measurement in quantum mechanics, etc.) So is the same thing true for combinators? It’s complicated. Both string and hypergraph rewriting systems have an important simplifying feature: when you update something in them, it’s reasonable to think of the thing you update as being “fully consumed” by the updating event, with a “completely new thing” being created as a result of the event. But with combinators that’s not such a reasonable picture. Because when there’s an updating event, say for s[x][y][z], x can be a giant subtree that you end up “just copying”, without, in a sense, “consuming” and “reconstituting”. In the case of strings and hypergraphs, there’s a clear distinction between elements of the system that are “involved in an update”, and ones that aren’t. But in a combinator system, it’s not so obvious whether nodes buried deep in a subtree that’s “just copied” should be considered “involved” or not. There’s a complicated interplay with definitions used in constructing multiway graphs. Consider a string rewriting system. Start from a particular state and then apply rewriting rules in all possible ways: Absent anything else, this will just generate a tree of results. But the crucial idea behind multiway graphs is that when states are identical, they should be merged, in this case giving: For strings it’s very obvious what “being identical” means. For hypergraphs, the natural definition is hypergraph isomorphism. What about for combinators? Is it pure tree isomorphism, or should one take into account the “provenance” of subtrees? (There are also questions like whether one should define the nodes in the multiway graph in terms of “instantaneous states” at all, or whether instead they should be based on “causal graphs so far”, as obtained with particular event histories.) These are subtle issues, but it seems pretty clear that with appropriate definitions combinators will show causal invariance, so that (appropriately defined) causal graphs will be independent of evaluation scheme. By the way, in addition to constructing causal graphs for particular evolution histories, one can also construct multiway causal graphs representing all possible causal relationships both within and between different branches of history. This shows the multiway graph for the (terminating) evolution of s[s][s][s[s[k]]][k], annotated with casual edges: And here’s the multiway causal graph alone in this case: (And, yes, the definitions don’t all quite line up here, so the individual instances of causal graphs that can be extracted here aren’t all the same, as causal invariance would imply.) The multiway causal graph for s[s[s]][s][s][s][s] shows a veritable explosion of causal edges: In our model of physics, the causal graph can be thought of as a representation of the structure of spacetime. Events that follow from each other are “timelike separated”. Events that can be arranged so that none are timelike separated can be considered to form a “spacelike slice” (or a “surface of simultaneity”), and to be spacelike separated. (Different foliations of the causal graph correspond to different “reference frames” and identify different sets of events as being in the same spacelike slice.) When we’re dealing with multiway systems it’s also possible for events to be associated with different “threads of history”—and so to be branchlike separated. But in combinator systems, there’s yet another form of separation between events that’s possible—that we can call “treelike separation”. Consider these two pairs of updating events: In the first case, the events are effectively “spacelike separated”. They are connected by being in the same combinator expression, but they somehow appear at “distinct places”. But what about the second case? Again the two events are connected by being in the same combinator expression. But now they’re not really “at distinct places”; they’re just “at distinct scales” in the tree. One feature of hypergraph rewriting systems is that in large-scale limits the hypergraphs they produce can behave like continuous manifolds that potentially represent physical space, with hypergraph distances approximating geometric distances. In combinator systems there is almost inevitably a kind of nested structure that may perhaps be reminiscent of scale-invariant critical phenomena and ideas like scale relativity. But I haven’t yet seen combinator systems whose limiting behavior produces something like finite-dimensional “manifold-like” space. It’s common to see “event horizons” in combinator causal graphs, in which different parts of the combinator system effectively become causally disconnected. When combinators reach fixed points, it’s as if “time is ending”—much as it does in spacelike singularities in spacetime. But there are no doubt new “treelike” limiting phenomena in combinator systems, that may perhaps be reflected in properties of hyperbolic spaces. One important feature of both string and hypergraph rewriting systems is that their rules are generally assumed to be somehow local, so that the future effect of any given element must lie within a certain “cone of influence”. Or, in other words, there’s a light cone which defines the maximum spacelike separation of events that can be causally connected when they have a certain timelike separation. In our model of physics, there’s also an “entanglement cone” that defines maximum branchlike separation between events. But what about in combinator systems? The rules aren’t really “spatially local”, but they are “tree local”. And so they have a limited “tree cone” of influence, associated with a “maximum treelike speed”—or, in a sense, a maximum speed of scale change. Rewriting systems based on strings, hypergraphs and combinator expressions all have different simplifying and complexifying features. The relation between underlying elements (“characters arranged in sequence”) is simplest for strings. The notion of what counts as the same element is simplest for hypergraphs. But the relation between the “identities of elements” is probably simplest for combinator expressions. Recall that we can always represent a combinator expression by a DAG in which we “build up from atoms”, sharing common subexpressions all the way up: But what does combinator evolution look like in this representation? Let’s start from the extremely simple case of k[x][y], which in one step becomes just x. Here’s how we can represent this evolution process in DAGs: The dotted line in the second DAG indicates an update event, which in this case transforms k[x][y] to the “atom” x. Now let’s consider s[x][y][z]. Once again there’s a dotted line that signifies the evolution: Now let’s add an extra wrinkle: consider not k[x][y] but s[k[x][y]]. The outer s doesn’t really do anything here. But it still has to be accounted for, in the sense that it has to be “wrapped back around” the x that comes from k[x][y]→x. We can represent that “rewrapping” process, by a “tree pullback pseudoevent” indicated by the dotted line: If a given event happens deep inside a tree, there’ll be a whole sequence of “pullback pseudoevents” that “reconstitute the tree”. Things get quite complicated pretty quickly. Here’s the (leftmost-outermost) evolution of s[s[s]][s][k][s] to its fixed point in terms of DAGs: Or with labels: One notable feature is that this final DAG in a sense encodes the complete history of the evolution—in a “maximally shared” way. And from this DAG we can construct a causal graph—whose nodes are derived from the edges in the DAG representing update events and pseudoevents. It’s not clear how to do this in the most consistent way—particularly when it comes to handling pseudoevents. But here’s one possible version of a causal graph for the evolution of s[s[s]][s][k][s] to its fixed point—with the yellow nodes representing events, and the gray ones pseudoevents.
Combinators: A Centennial View 9.10 Combinator Expressions as Dynamical Systems : Start with all possible combinator expressions of a certain size, say involving only s. Some are immediately fixed points. But some only evolve to fixed points. So how are the possible fixed points distributed in the set of all possible combinator expressions? For size 6 there are 42 possible combinator expressions, and all evolve to fixed points—but only 27 distinct ones. Here are results for several combinator sizes: As the size of the combinator expression goes up, the fraction of distinct fixed points seems to systematically go down: And what this shows is that combinator evolution is in a sense a “contractive” process: starting from all possible expressions, there’s only a certain “attractor” of expressions that survives. Here’s a “state transition graph” for initial expressions of size 9 computed with leftmost-outermost evaluation (we’ll see a more general version in the next section): This shows the prevalence of different fixed-point sizes as a function of the size of the initial expression: What about the cases that don’t reach fixed points? Can we somehow identify different equivalent classes of infinite combinator evolutions (perhaps analogously to the way we can identify different transfinite numbers)? In general we can look at similarities between the multiway systems that are generated, since these are always independent of updating scheme (see the next section). But something else we can do for both finite and infinite evolutions is to consider the set of subexpressions common to different steps in the evolution—or across different evolutions. Here’s a plot of the number of copies of the ultimately most frequent subexpressions at successive steps in the (leftmost-outermost) evolution of s[s][s][s[s]][s][s] (SSS(SS)SS): The largest subexpression shown here has size 29. And as the picture makes clear, most subexpressions do not appear with substantial frequency; it’s only a thin set that does. Looking at the evolution of all possible combinator expressions up to size 8, one sees gradual “freezing out” of certain subexpressions (basically as a result of their involvement in halting), and continued growth of others: In an attempt to make contact with traditional dynamical systems theory it’s interesting to try to map combinator expressions to numbers. A straightforward way to do this (particularly when one’s only dealing with expressions involving s) is to use Polish notation, which represents s[s[s]][s[s[s[s]][s]][s]][s[s[s[s]][s[s[s[s]][s]][s]]]][ or the binary number: i.e., in decimal: Represented in terms of numbers like this, we can plot all subexpressions which arise in the evolution of s[s][s][s[s]][s][s] (SSS(SS)SS): Making a combined picture for all combinator expressions up to size 8, one gets: There’s definitely some structure: one’s not just visiting every possible subexpression. But quite what the limiting form of this might be is not clear. Another type of question to ask is what the effect of a small change in a combinator expression is on its evolution. The result will inevitably be somewhat subtle—because there is both spacelike and treelike propagation of effects in the evolution. As one example, though, consider evolving s[s][s][s[s]][s][s] (SSS(SS)SS) for 20 steps (to get an expression of size 301). Now look at the effect of changing a single s in this expression to s[s], and then evolving the result. Here are the sizes of the expressions that are generated.
Combinators: A Centennial View 9.11 Equality and Theorem Proving for Combinators : How do you tell if two combinator expressions are equal? It depends what you mean by “equal”. The simplest definition—that we’ve implicitly used in constructing multiway graphs—is that expressions are equal only if they’re syntactically exactly the same (say they’re both s[k][s[s]]). But what about a more semantic definition, that takes into account the fact that one combinator expression can be transformed to another by the combinator rules? The obvious thing to say is that combinator expressions should be considered equal if they can somehow be transformed by the rules into expressions that are syntactically the same. And so long as the combinators evolve to fixed points this is in principle straightforward to tell. Like here are four syntactically different combinator expressions that all evolve to the same fixed point, and so in a semantic sense can be considered equal: One can think of the fixed point as representing a canonical form to which combinator expressions that are equal can be transformed. One can also think of the steps in the evolution as corresponding to steps in a proof of equality. But there’s already an issue—that’s associated with the fundamental fact that combinators are computation universal. Because in general there’s no upper bound on how many steps it can take for the evolution of a combinator expression to halt (and no general a priori way to even tell if it’ll halt at all). So that means that there’s also no upper bound on the “length of proof” needed to show by explicit computation that two combinators are equal. Yes, it might only take 12 steps to show that this is yet another combinator equal to s[k]: But it could also take 31 steps (and involve an intermediate expression of size 65): We know that if we use leftmost-outermost evaluation, then any combinator expression that has a fixed point will eventually evolve to it (even though we can’t in general know how long it will take). But what about combinator expressions that don’t have fixed points? How can we tell if they’re “equal” according to our definition? Basically we have to be able to tell if there are sequences of transformations under the combinator rules that cause the expressions to wind up syntactically the same. We can think of these sequences of transformations as being like possible paths of evolution. So then in effect what we’re asking is whether there are paths of evolution for different combinators that intersect. But how can we characterize what possible paths of evolution might exist for all possible evaluation schemes? Well, that’s what the multiway graph does. And in terms of multiway graphs there’s then a concrete way to ask about equality (or, really, equivalence) between combinator expressions. We basically just need to ask whether there is some appropriate path between the expressions in the multiway graph. There are lots of details, some of which we’ll discuss later. But what we’re basically dealing with is a quintessential example of the problem of theorem proving in a formal system. There are different ways to set things up. But as one example, we could take our system to define certain axioms that transform expressions. Applying these axioms in all possible ways generates a multiway graph with expressions as nodes. But then the statement that there’s a theorem that expression A is equal to expression B (in the sense that it can be transformed to it) becomes the statement that there’s a way to get from A to B in the graph—and giving a path can then be thought of as giving a proof of the theorem. As an example, consider the combinator expressions: s[s][s[s][s[s[s]]][k]][k[s[s][s[s[s]]][k]]] s[k[s[k][s[s[s]][k]]]][k[s[s][s[s[s]]][k]]] Constructing a multiway graph one can then find a path which corresponds to the proof that one can get from one of these expressions to the other: In this particular case, both expressions eventually reach a fixed point. But consider the expressions: s[s[s[s[s][s]]][k]][s[s[s[s[s][s]]][k]]][k[s[s[s[s][s]]][k]]] s[s[s[s][s]]][k][s[s[s[s[s][s]]][k]][k[s[s[s[s][s]]][k]]]] Neither of these expressions evolves to a fixed point. But there’s still a path in the (ultimately infinite) multiway graph between them corresponding to the equivalence proof: But with our definition, two combinator expressions can still be considered equal even if one of them can’t evolve into the other: it can just be that among the possible ancestors (or, equivalently for combinators, successors) of the expressions there’s somewhere an expression in common. (In physics terms, that their light cones somewhere overlap.) Consider the expressions: {s[s[s][s]][s][s[s][k]], s[s][k][s[s[s][k]]][k]} Neither terminates, but it still turns out that there are paths of evolution for each of them that lead to the same expression: If we draw a combined multiway graph starting from the two initial expressions, we can see the converging paths: But is there a more systematic way to think about relations between combinator expressions? Combinators are in a sense fundamentally computational constructs. But one can still try to connect them with traditional mathematics, and in particular with abstract algebra. And so, for example, it’s common in the literature of combinators to talk about “combinatory algebra”, and to write an expression like s[k][s[s[k[s[s]][s]][s]][k]][k[s[k]][s[s[k[s[s]][s]][s]][k]]] where now one imagines that • (“application”) is like an algebraic operator that “satisfies the relations” with “constants” S and K. To determine whether two combinator expressions are equal one then has to see if there’s a sequence of “algebraic” transformations that can go from one to the other. The setup is very similar to what we’ve discussed above, but the “two-way” character of the rules allows one to directly use standard equational logic theorem-proving methods (although because combinator evolution is confluent one never strictly has to use reversed rules). So, for example, to prove s[k[s]][k[k]][k]s[s][s][k][s][k] or one applies a series of transformations based on the S and K “axioms” to parts of the left- and right-hand sides to eventually reduce the original equation to a tautology: One can give the outline of this proof as a standard FindEquationalProof proof graph: The yellowish dots correspond to the “intermediate lemmas” listed above, and the dotted lines indicate which lemmas use which axioms. One can establish a theorem like with a slightly more complex proof: One feature of this proof is that because the combinator rules are confluent—so that different branches in the multiway system always merge—the proof never has to involve critical pair lemmas representing equivalences between branches in the multiway system, and so can consist purely of a sequence of “substitution lemmas”. There’s another tricky issue, though. And it has to do with taking “everyday” mathematical notions and connecting them with the precise symbolic structure that defines combinators and their evolution. As an example, let’s say you have combinators a and b. It might seem obvious that if a is to be considered equal to b, then it must follow that a[x]b[x] for all x. But actually saying this is true is telling us something about what we mean by “equal”, and to specify this precisely we have to add the statement as a new axiom. In our basic setup for proving anything to do with equality (or, for that matter, any equivalence relation), we’re already assuming the basic features of equivalence relations (reflexivity, symmetry, transitivity): In order to allow us to maintain equality while doing substitutions we also need the axiom: And now to specify that combinator expressions that are considered equal also “do the same thing” when applied to equal expressions, we need the “extensionality” axiom: The previous axioms all work in pure “equational logic”. But when we add the extensionality axiom we have to explicitly use full first-order logic—with the result that we get more complicated proofs, though the same basic methods apply.
Combinators: A Centennial View 9.12 Lemmas and the Structure of Combinator Space : One feature of the proofs we’ve seen above is that each intermediate lemma just involves direct use of one or other of the axioms. But in general, lemmas can use lemmas, and one can “recursively” build up a proof much more efficiently than just by always directly using the axioms. But which lemmas are best to use? If one’s doing ordinary human mathematics—and trying to make proofs intended for human consumption—one typically wants to use “famous lemmas” that help create a human-relatable narrative. But realistically there isn’t likely to be a “human-relatable narrative” for most combinator equivalence theorems (or, at least there won’t be until or unless thinking in terms of combinators somehow becomes commonplace). So then there’s a more “mechanical” criterion: what lemmas do best at reducing the lengths of as many proofs as much as possible? There’s some trickiness associated with translations between proofs of equalities and proofs that one expression can evolve into another. But roughly the question boils downs to this. When we construct a multiway graph of combinator evolution, each event—and thus each edge—is just the application of a single combinator “axiom”. But if instead we do transformations based on more sophisticated lemmas we can potentially get from one expression to another in fewer steps. In other words, if we “cache” certain combinator transformations, can we make finding paths in combinator multiway graphs systematically more efficient? To find all possible “combinator theorems” from a multiway system, we should start from all possible combinator expressions, then trace all possible paths to other expressions. It’s a little like what we did in the previous section—except now we want to consider multiway evolution with all possible evaluation orders. Here’s the complete multiway graph starting from all size-4 combinator expressions: Up to size 6, the graph is still finite (with each disconnected component in effect corresponding to a separate “fixed-point attractor”): For size 7 and above, it becomes infinite. Here’s the beginning of the graph for size-8 expressions involving only s: If one keeps only terminating cases, one gets for size 8: And for size 9: And for size 10: To assess the “most useful” transformations for “finding equations” there’s more to do: not only do we need to track what leads to what, but we also need to track causal relationships. And this leads to ideas like using lemmas that have the largest number of causal edges associated with them. But are there perhaps other ways to find relations between combinator expressions, and combinator theorems? Can we for example figure out what combinator expressions are “close to” what others? In a sense what we need is to define a “space of combinator expressions” with some appropriate notion of nearness. One approach would just be to look at “raw distances” between trees—say based on asking how many edits have to be made to one tree to get to another. But an approach that more closely reflects actual features of combinators is to think about the concept of branchial graphs and branchial space that comes from our Physics Project. Consider for example the multiway graph generated from s[s[s]][s][s[s]][s] (S(SS)S(SS)S): Now consider a foliation of this graph (and in general there will be many possible foliations that respect the partial order defined by the multiway graph): In each slice, we can then define—as in our Physics Project—a branchial graph in which nodes are joined when they have an immediate common ancestor in the multiway graph. In the case shown here, the branchial graphs in successive slices are: If we consider a combinator expression like s[s][s][s[s]][s][s] (SSS(SS)SS) that leads to infinite growth, we can ask what the “long-term” structure of the branchial graph will be. Here are the results after 18 and 19 steps: The largest connected components here contain respectively 1879 and 10,693 combinator expressions. But what can we say about their structure? One thing suggested by our Physics Project is to try to “fit them to continuous spaces”. And a first step in doing that is to estimate their effective dimension—which one can do by looking at the growth in the volume of a “geodesic ball” in the graph as a function of its radius: The result for distances small compared to the diameter of the graph is close to quadratic growth—suggesting that there is some sense in which the space of combinator expressions generated in this way may have a limiting 2D manifold structure. It’s worth pointing out that different foliations of the multiway graph (i.e. using different “reference frames”) will lead to different branchial graphs—but presumably the (suitably defined) causal invariance of combinator evolution will lead to relativistic-like invariance properties of the branchial graphs. Somewhat complementary to looking at foliations of the multiway graph is the idea of trying to find quantities that can be computed for combinator expressions to determine whether the combinator expressions can be equal. Can we in essence find hash codes for combinator expressions that are equal whenever the combinator expressions are equal? In general we’ve been looking at “purely symbolic” combinator expressions—like: But what if we consider S, K to have definite, say numerical, values, and • to be some kind of generalized multiplication operator that combines these values? We used this kind of approach above in finding a procedure for determining whether S combinator expressions will evolve to fixed points. And in general each possible choice of “multiplication functions” (and S, K “constant values”) can be viewed in mathematical terms as setting up a “model” (in the model-theoretic sense) for the “combinatory algebra”. As a simple example, let’s consider a finite model in which there are just 2 possible values, and the “multiplication table” for the • operator is: If we consider S combinator expressions of size 5, there are a total of 14 such expressions, in 10 equivalence classes, that evolve to different fixed points. If we now “evaluate the trees” according to our “model for •” we can see that within each equivalence class the value accumulated at the root of the tree is always the same, but differs between at least some of the equivalence classes: If we look at larger combinator expressions this all keeps working—until we get to two particular size-10 expressions, which have the same fixed point, but different “values”: Allowing 3 possible values, the longest-surviving models are but these both fail at size 13 (e.g. for s[s][s[s]][s[s[s[s[s]][s][s]]][s[s]]], s[s[s]][s[s[s[s[s]][s[s[s]]]]]][s[s]]). The fact that combinator equivalence is in general undecidable means we can’t expect to find a computationally finite “valuation procedure” that will distinguish all inequivalent combinator expressions. But it’s still conceivable that we could have a scheme to distinguish some classes of combinator expressions from others—in essence through the values of a kind of “conserved quantity for combinators”. Another approach is to consider directly “combinator axioms” like and simply ask if there are models of •, S and K that satisfy them. Assuming a finite “multiplication table”, there’s no way to do this for K, and thus for S and K together. For S alone, however, there are already 8 2-valued models, and 285 3-valued ones. The full story is more complicated, and has been the subject of a fair amount of academic work on combinators over the past half century. The main result is that there are models that are in principle known to exist, though they’re infinite and probably can’t be explicitly constructed. In the case of something like arithmetic, there are formal axioms (the Peano axioms). But we know that (even though Gödel’s theorem shows that there are inevitably also other, exotic, non-standard models) there’s a model of these axioms that is the ordinary integers. And our familiarity with these and their properties makes us feel that the Peano axioms aren’t just formal axioms; they’re axioms “about” something, namely integers. What are the combinator axioms “about”? There’s a perfectly good interpretation of them in terms of computational processes. But there doesn’t seem to be some “static” set of constructs—like the integers—that give one more insight about what combinators “really are”. Instead, it seems, combinators are in the end just through and through computational.
Combinators: A Centennial View 9.13 Empirical Computation Theory with Combinators : We’ve talked a lot here about what combinators “naturally do”. But what about getting combinators to do something specific—for example to perform a particular computation we want? As we saw by example at the beginning of this piece, it’s not difficult to take any symbolic structure and “compile it” to combinators. Let’s say we’re given: There’s then a recursive procedure that effectively builds “function invocations” out of s’s and “stops computations” with k’s. And using this we can “compile” our symbolic expression to the (slightly complicated) combinator expression: To “compute our original expression” we just have to take this combinator expression (“■”), form ■[f][x][y], then apply the combinator rules and find the fixed point: But is this the “best combinator way” to compute this result? There are various different things we could mean by “best”. Smallest program? Fastest program? Most memory-efficient program? Or said in terms of combinators: Smallest combinator expression? Smallest number of rule applications? Smallest intermediate expression growth? In computation theory one often talks theoretically about optimal programs and their characteristics. But when one’s used to studying programs “in the wild” one can start to do empirical studies of computation-theoretic questions—as I did, for example, with simple Turing machines in A New Kind of Science. Traditional computation theory tends to focus on asymptotic results about “all possible programs”. But in empirical computation theory one’s dealing with specific programs—and in practice there’s a limit to how many one can look at. But the crucial and surprising fact that comes from studying the computational universe of “programs in the wild” is that actually even very small programs can show highly complex behavior that’s in some sense typical of all possible programs. And that means that it’s realistic to get intuition—and results—about computation-theoretic questions just by doing empirical investigations of actual, small programs. So how does this work with combinators? An immediate question to ask is: if one wants a particular expression, what are all the possible combinator expressions that will generate it? Let’s start with a seemingly trivial case: x[x]. With the compilation procedure we used above we get the size-7 combinator expression which (with leftmost-outermost evaluation) generates x[x] in 6 steps: But what happens if we just start enumerating possible combinator expressions? Up to size 5, none compute x[x]. But at size 6, we have: So we can “save” one unit of program size, but at the “cost” of taking 9 steps, and having an intermediate expression of size 21. What if we look at size-7 programs? There are a total of 11 that work (including the one from our “compiler”): {s[s[s[s]]][s][s[k]], s[s[s]][s[k]][s[k]], s[s][s[k]][s[s[k]]], How do these compare in terms of “time” (i.e. number of steps) and “memory” (i.e. maximum intermediate expression size)? There are 4 distinct programs that all take the same time and memory, there are none that are faster, but there are others that are slowest (the slowest taking 12 steps): What happens with larger programs? Here’s a summary: Here are the distributions of times (dropping outliers)—implying (as the medians above suggest) that even a randomly picked program is likely to be fairly fast: And here’s the distribution of time vs. memory on a log-log scale: At size 10, the slowest and most memory-intensive program is s[s[s][k][s[s[s[s]]]]][s][k] (S(SSK(S(S(SS))))SK): There are so many other questions one can ask. For example: how similar are the various fastest programs? Do they all “work the same way”? At size 7 they pretty much seem to: At size 8 there are a few “different schemes” that start to appear: Then one can start to ask questions about how these fastest programs are laid out in the kind of “combinator space” we discussed in the last section—and whether there are good incremental (“evolutionary”) ways to find these fastest programs. Another type of question has to do with the running of our programs. In everything we’ve done so far in this section, we’ve used a definite evaluation scheme: leftmost outermost. And in using this definite scheme, we can think of ourselves as doing “deterministic combinator computation”. But we can also consider the complete multiway system of all possible updating sequences—which amounts to doing non-deterministic computation. Here’s the multiway graph for the size-6 case we considered above, highlighting the leftmost-outermost evaluation path: And, yes, in this case leftmost outermost happens to follow a fastest path here. Some other possible schemes are very slow in comparison—with the maximum time being 13 and the maximum intermediate expression size being 21. At size 7 the multiway graphs for all the leftmost-outermost-fastest programs are the same—and are very simple—among other things making it seem that in retrospect the size-6 case “only just makes it”: At size 8 there “two ideas” among the 16 cases: At size 9 there are “5 ideas” among 80 cases: And at size 10 things are starting to get more complicated: But if we don’t look at only at leftmost-outermost-fastest programs? At size 7 here are the multiway graphs for all combinator expressions that compute x[x]: So if one operates “non-deterministically”—i.e. one can follow any path in the multiway graph, not just the leftmost-outermost evaluation scheme one—can one compute the answer faster? The answer in this particular case is no. But what about at size 8? Of the 95 programs that compute x[x], in most cases the situation is like for size 7 and leftmost outermost gives the fastest result. But there are some wilder things that can happen. Consider for example s[s[s[s]]][k[s[k]]][s] Here’s the complete multiway graph in this case (with 477 nodes altogether): Two paths are indicated: the one in orange is the leftmost-outermost evaluation—which takes 12 steps in this case. But there’s also another path, shown in red—which has length 11. Here’s a comparison: To get a sense of the “amount of non-determinism” that can occur, we can look at the number of nodes in successive layers of the multiway graph—essentially the number of “parallel threads” present at each “non-deterministic step”: What about size-8 programs for x[x]? There are 9 more—similar to this one—where the non-deterministic computation is one step shorter. (Sometimes—as for s[s[s]][s][s[k[k][s]]]—the multiway graph is more complicated, in this case having 1661 nodes.) But there are some other things that happen. And a dramatic one is that there can be paths that just don’t terminate at all. s[s[s[s][s]]][s][s[k]] gives an example. Leftmost-outermost evaluation reaches a fixed point after 14 steps. But overall the multiway graph grows exponentially (already having size 24,705 after 14 steps)—yielding eventually an infinite number of infinite paths: non-deterministic threads that in a sense get “lost forever”. So far all we’ve talked about here is the computation of the one—seemingly trivial—object x[x]. But what about computing other things? Imagine we have a combinator expression ■ that we apply to x to form ■[x]. If when we “evaluate” this with the combinator rules it reaches a fixed point we can say this is the result of the computation. But a key point is that most of the time this “result” won’t just contain x; it’ll still have “innards of the computation”—in the form of S’s and K’s—in it. Out of all 2688 combinator expressions of size 6, 224 compute x. Only one (that we saw above) computes something more complicated: x[x]. At size 7, there are 11 programs that compute x[x], and 4 that compute x[x][x]. At size 8 the things that can be computed are: At size 9 the result is: In a sense what we’re seeing here are the expressions (or objects) of “low algorithmic information content” with respect to combinator computation: those for which the shortest combinator program that generates them is just of length 9. In addition to shortest program length, we can also ask about expressions generated within certain time or intermediate-expression-size constraints. What about the other way around? How large a program does one need to generate a certain object? We know that x[x] can be generated with a program of size 6. It turns out x[x[x]] needs a program of size 8: Here are the shortest programs for objects of size 4: Our original “straightforward compiler” generates considerably longer programs: to get an object involving only x’s of size n it produces a program of length 4n – 1 (i.e. 15 in this case). It’s interesting to compare the different situations here. x[x[x]][x[x]][x[x[x[x]][x[x]]]][x[x[x[x]][x[x]]]] (of size 17) can be generated by the program s[s[s]][s][s[s][s[k]]] (of size 8). But the shortest program that can generate x[x[x[x]]] (size 4) is of length 10. And what we’re seeing is that different objects can have very different levels of “algorithmic redundancy” under combinator computation. Clearly we could go on to investigate objects that involve not just x, but also y, etc. And in general there’s lots of empirical computation theory that one can expect to do with combinators. As one last example, one can ask how large a combinator expression is needed to “build to a certain size”, in the sense that the combinator expression evolves to a fixed point with that size. Here is the result for all sizes up to 100, both for S,K expressions, and for expressions with S alone (the dotted line is ): By the way, we can also ask about programs that involve only S, without K. If one wants ■[x] to evaluate to an expression involving only x this isn’t possible if one only uses S. But as we discussed above, it’s still perfectly possible to imagine “doing a computation” only using S: one just can’t expect to have the result delivered directly on its own. Instead, one must run some kind of procedure to extract the result from a “wrapper” that contains S’s. What about practical computations. The most obvious implementation of combinators on standard modern computer systems isn’t very efficient because it tends to involve extensive copying of expressions. But by using things like the DAG approach discussed above it’s perfectly possible to make it efficient. What about physical systems? Is there a way to do “intrinsically combinator” computation? As I discussed above, our model of fundamental physics doesn’t quite align with combinators. But closer would be computations that can be done with molecules. Imagine a molecule with a certain structure. Now imagine that another molecule reacts with it to produce a molecule with a new structure. If the molecules were tree-like dendrimers, it’s at least conceivable that one can get something like a combinator transformation process. I’ve been interested for decades in using ideas gleaned from exploring the computational universe to do molecular-scale computation. Combinators as such probably aren’t the best “raw material”, but understanding how computation works with combinators is likely to be helpful. And just for fun we can imagine taking actual expressions—say from the evolution of s[s][s][s[s]][s][s]—and converting them to “molecules” just using standard chemical SMILES strings (with C in place of S).
Combinators: A Centennial View 9.14 The Future of Combinators : S and K at first seem so simple, so basic. But as we’ve seen here, there’s an immense richness to what they can do. It’s a story I’ve seen played out many times across the computational universe. But in a sense it’s particularly remarkable for combinators because they were invented so early, and they seem so very simple. There’s little question that even a century after they were invented, combinators are still hard to get one’s head around. Perhaps if computation and computer technology had developed differently, we’d now find combinators easier to understand. Or perhaps the way our brains are made, they’re just intrinsically difficult. In a sense what makes combinators particularly difficult is the extent to which they’re both featureless and fundamentally dynamic in their structure. When we apply the ideas of combinators in practical “human-oriented” computing—for example in the Wolfram Language—we annotate what’s going on in a variety of ways. But with the Wolfram Physics Project we now have the idea that what happens at the lowest level in the physics of our universe is something much more like “raw combinators”. The details are different—we’re dealing with hypergraphs, not trees—but many of the concepts are remarkably similar. Yes, a universe made with combinators probably won’t have anything like space in the way we experience it. But a lot of ideas about updating processes and multiway systems are all there in combinators. For most of their history, combinators have been treated mainly as a kind of backstop for proofs. Yes, it is possible to avoid variables, construct everything symbolically, etc. But a century after they were invented, we can now see that combinators in their own right have much to contribute. What happens if we don’t just think about combinators in general, but actually look at what specific combinators do? What happens if we do experiments on combinators? In the past some elaborate behavior of a particular combinator expression might have just seemed like a curiosity. But now that we have the whole paradigm that I’ve developed from studying the computational universe we can see how such things fit in, and help build up a coherent story about the ways of computation. In A New Kind of Science I looked a bit at the behavior of combinators; here I’ve done more. But there’s still vastly more to explore in the combinator universe—and many surprises yet to uncover. Doing it will both advance the general science of the computational universe, and will give us a new palette of phenomena and intuition with which to think about other computational systems. There are things to learn for physics. There are things to learn for language design. There are things to learn about the theoretical foundations of computer science. There may also be things to learn for models of concrete systems in the natural and artificial world—and for the construction of useful technology. As we look at different kinds of computational systems, several stand out for their minimalism. Particularly notable in the past have been cellular automata, Turing machines and string substitution systems. And now there are also the systems from our Wolfram Physics Project—that seem destined to have all sorts of implications even far beyond physics. And there are also combinators. One can think of cellular automata, for example, as minimal systems that are intrinsically organized in space and time. The systems from our Wolfram Physics Project are minimal systems that purely capture relations between things. And combinators are in a sense minimal systems that are intrinsically about programs—and whose fundamental structure and operation revolve around the symbolic representation of programs. What can be done with such things? How should we think about them? Despite the passage of a century—and a substantial body of academic work—we’re still just at the beginning of understanding what can be done with combinators. There’s a rich and fertile future ahead, as we begin the second combinator century, now equipped with the ideas of symbolic computational language, the phenomena of the computational universe, and the computational character of fundamental physics.
Combinators: A Centennial View 9.15 Historical & Other Notes : I’m writing elsewhere about the origin of combinators, and about their interaction with the history of computation. But here let me make some remarks more specific to this piece. Combinators were invented in 1920 by Moses Schönfinkel (hence the centenary), and since the late 1920s there’s been continuous academic work on them—notably over more than half a century by Haskell Curry. A classic summary of combinators from a mathematical point of view is the book: Haskell B. Curry and Robert Feys, Combinatory Logic (1958). More recent treatments (also of lambda calculus) include: H. P. Barendregt, The Lambda Calculus (1981) and J. Roger Hindley and Jonathan P. Seldin, Lambda-Calculus and Combinators (1986). In the combinator literature, what I call “combinator expressions” are often called “terms” (as in “term rewriting systems”). The part of the expression that gets rewritten is often called the “redex”; the parts that get left over are sometimes called the “residuals”. The fixed point to which a combinator expression evolves is often called its “normal form”, and expressions that reach fixed points are called “normalizing”. Forms like a[b[a][c]] that I “immediately apply to arguments” are basically lambda expressions, written in Wolfram Language using Function. The procedure of “compiling” from lambda expressions to combinators is sometimes called bracket abstraction. As indicated by examples at the end of this piece, there are many possible methods for doing this. The scheme for doing arithmetic with combinators at the beginning of this piece is based on work by Alonzo Church in the 1930s, and uses so-called “Church numerals”. The idea of encoding logic by combinators was discussed by Schönfinkel in his original paper, though the specific minimal encoding I give was something I found by explicit computational search in just the past few weeks. Note that if one uses s[k] for True and k for False (as in the rule 110 cellular automaton encoding) the minimal forms for the Boolean operators are: The uniqueness of the fixed point for combinators is a consequence of the Church–Rosser property for combinators from 1941. It is closely related to the causal invariance property that appears in our model of physics. The Y combinator was notably used by Paul Graham in 2005 to name his Y Combinator startup accelerator. And perhaps channeling the aspirations of startups the “actual” Y combinator goes through many ups and downs but (with leftmost-outermost evaluation) reaches size 1 billion (“unicorn”) after 494 steps—and after 1284 steps reaches more-dollars-than-in-the-world size: 508,107,499,710,983. Empirical studies of the actual behavior of combinators “in the wild” have been pretty sparse. The vast majority of academic work on combinators has been done by hand, and without the overall framework of A New Kind of Science the detailed behavior of actual combinators mostly just seemed like a curiosity. I did fairly extensive computational exploration of combinators (and in general what I called “symbolic systems”) in the 1990s for A New Kind of Science. Page 712 summarized some combinator behavior I found (with /. evaluation): I don’t know to what extent the combinator results in A New Kind of Science were anticipated elsewhere. Longtime combinator enthusiast Henk Barendregt for example recently pointed me to a paper of his from 1976 mentioning non-termination in S combinator expressions: The procedure I describe for determining the termination of S combinator expression was invented by Johannes Waldmann at the end of the 1990s. (The detailed version that I used here came from Jörg Endrullis.) What we call multiway systems have been studied in different ways in different fields, under different names. In the case of combinators, they are basically Böhm trees (named after Corrado Böhm). I’ve concentrated here on the original S, K combinators; in recent livestreams, as in A New Kind of Science, I’ve also been exploring other combinator rules. Thanks, etc. Matthew Szudzik has helped me with combinator matters since 1998 (and has given a lecture on combinators almost every year for the past 18 years at our Wolfram Summer School). Roman Maeder did a demo implementation of combinators in Mathematica in 1988, and has now added CombinatorS etc. to Version 12.2 of Wolfram Language. I’ve had specific help on this piece from Jonathan Gorard, Jose Martin-Garcia, Eric Paul, Ed Pegg, Max Piskunov, and particularly Mano Namuduri, as well as Jeremy Davis, Sushma Kini, Amy Simpson and Jessica Wong. We’ve had recent interactions about combinators with a four-academic-generation sequence of combinator researchers: Henk Barendregt, Jan Willem Klop, Jörg Endrullis and Roy Overbeek. Thanks also to John Tromp for suggesting some combinator optimizations after this was first published.
Combinators and the Story of Computation 10.1 The Abstract Representation of Things : “In principle you could use combinators,” some footnote might say. But the implication tends to be “But you probably don’t want to.”  And, yes, combinators are deeply abstract—and in many ways hard to understand. But tracing their history over the hundred years since they were invented, I’ve come to realize just how critical they’ve actually been to the development of our modern conception of computation—and indeed my own contributions to it. The idea of representing things in a formal, symbolic way has a long history. In antiquity there was Aristotle’s logic and Euclid’s geometry. By the 1400s there was algebra, and in the 1840s Boolean algebra. Each of these was a formal system that allowed one to make deductions purely within the system. But each, in a sense, ultimately viewed itself as being set up to model something specific. Logic was for modeling the structure of arguments, Euclid’s geometry the properties of space, algebra the properties of numbers; Boolean algebra aspired to model the “laws of thought”. But was there perhaps some more general and fundamental infrastructure: some kind of abstract system that could ultimately model or represent anything? Today we understand that’s what computation is. And it’s becoming clear that the modern conception of computation is one of the single most powerful ideas in all of intellectual history—whose implications are only just beginning to unfold. But how did we finally get to it? Combinators had an important role to play, woven into a complex tapestry of ideas stretching across more than a century. The main part of the story begins in the 1800s. Through the course of the 1700s and 1800s mathematics had developed a more and more elaborate formal structure that seemed to be reaching ever further. But what really was mathematics? Was it a formal way of describing the world, or was it something else—perhaps something that could exist without any reference to the world? Developments like non-Euclidean geometry, group theory and transfinite numbers made it seem as if meaningful mathematics could indeed be done just by positing abstract axioms from scratch and then following a process of deduction. But could all of mathematics actually just be a story of deduction, perhaps even ultimately derivable from something seemingly lower level—like logic? But if so, what would things like numbers and arithmetic be? Somehow they would have to be “constructed out of pure logic”. Today we would recognize these efforts as “writing programs” for numbers and arithmetic in a “machine code” based on certain “instructions of logic”.  But back then, everything about this and the ideas around it had to be invented.
Combinators and the Story of Computation 10.2 What Is Mathematics—and Logic—Made Of? : Before one could really dig into the idea of “building mathematics from logic” one had to have ways to “write mathematics” and “write logic”. At first, everything was just words and ordinary language. But by the end of the 1600s mathematical notation like +, =, > had been established. For a while new concepts—like Boolean algebra—tended to just piggyback on existing notation. By the end of the 1800s, however, there was a clear need to extend and generalize how one wrote mathematics. In addition to algebraic variables like x, there was the notion of symbolic functions f, as in f(x). In logic, there had long been the idea of letters (p, q, …) standing for propositions (“it is raining now”). But now there needed to be notation for quantifiers (“for all x such-and-such”, or “there exists x such that…”). In addition, in analogy to symbolic functions in mathematics, there were symbolic logical predicates: not just explicit statements like x > y but also ones like p(x, y) for symbolic p. The first full effort to set up the necessary notation and come up with an actual scheme for constructing arithmetic from logic was Gottlob Frege’s 1879 Begriffsschrift (“concept script”): And, yes, it was not so easy to read, or to typeset—and at first it didn’t make much of an impression. But the notation got more streamlined with Giuseppe Peano’s Formulario project in the 1890s—which wasn’t so concerned with starting from logic as starting from some specified set of axioms (the “Peano axioms”): And then in 1910 Alfred Whitehead and Bertrand Russell began publishing their 2000-page Principia Mathematica—which pretty much by its sheer weight and ambition (and notwithstanding what I would today consider grotesque errors of language design)—popularized the possibility of building up “the complexity of mathematics” from “the simplicity of logic”: It was one thing to try to represent the content of mathematics, but there was also the question of representing the infrastructure and processes of mathematics. Let’s say one picks some axioms. How can one know if they’re consistent? What’s involved in proving everything one can prove from them? In the 1890s David Hilbert began to develop ideas about this, particularly in the context of tightening up the formalism of Euclid’s geometry and its axioms. And after Principia Mathematica, Hilbert turned more seriously to the use of logic-based ideas to develop “metamathematics”—notably leading to the formulation of things like the “decision problem” (Entscheidungsproblem) of asking whether, given an axiom system, there’s a definite procedure to prove or disprove any statement with respect to it. But while connections between logic and mathematics were of great interest to people concerned with the philosophy of mathematics, a more obviously mathematical development was universal algebra—in which axioms for different areas of mathematics were specified just by giving appropriate algebraic-like relations. (As it happens, universal algebra was launched under that name by the 1898 book A Treatise on Universal Algebra by Alfred Whitehead, later of Principia Mathematica fame.) But there was one area where ideas about algebra and logic intersected: the tightening up of Boolean algebra, and in particular the finding of simpler foundations for it. Logic had pretty much always been formulated in terms of And, Or and Not. But in 1912 Henry Sheffer—attempting to simplify Principia Mathematica—showed that just Nand (or Nor) were sufficient. (It turned out that Charles Peirce had already noted the same thing in the 1880s.) So that established that the notation of logic could be made basically as simple as one could imagine. But what about its actual structure, and axioms? Sheffer talked about needing five “algebra-style” axioms. But by going to axioms based on logical inferences Jean Nicod managed in 1917 to get it down to just one axiom. (And, as it happens, I finally finished the job in 2000 by finding the very simplest “algebra-style” axioms for logic—the single axiom: ((p·q)·r)·(p·((p·r)·p))r.) The big question had in a sense been “What is mathematics ultimately made of?”. Well, now it was known that ordinary propositional logic could be built up from very simple elements. So what about the other things used in mathematics—like functions and predicates? Was there a simple way of building these up too? People like Frege, Whitehead and Russell had all been concerned with constructing specific things—like sets or numbers—that would have immediate mathematical meaning. But Hilbert’s work in the late 1910s began to highlight the idea of looking instead at metamathematics and the “mechanism of mathematics”—and in effect at how the pure symbolic infrastructure of mathematics fits together (through proofs, etc.), independent of any immediate “external” mathematical meaning. Much as Aristotle and subsequent logicians had used (propositional) logic to define a “symbolic structure” for arguments, independent of their subject matter, so too did Hilbert’s program imagine a general “symbolic structure” for mathematics, independent of particular mathematical subject matter. And this is what finally set the stage for the invention of combinators.
Combinators and the Story of Computation 10.3 Combinators Arrive : We don’t know how long it took Moses Schönfinkel to come up with combinators. From what we know of his personal history, it could have been as long as a decade. But it could also have been as short as a few weeks. There’s no advanced math or advanced logic involved in defining combinators. But to drill through the layers of technical detail of mathematical logic to realize that it’s even conceivable that everything can be defined in terms of them is a supreme achievement of a kind of abstract reductionism. There is much we don’t know about Schönfinkel as a person. But the 11-page paper he wrote on the basis of his December 7, 1920, talk in which he introduced combinators is extremely clear. The paper is entitled “On the Building Blocks of Mathematical Logic” (in the original German, “Über die Bausteine der mathematischen Logik”.) In other words, its goal is to talk about “atoms” from which mathematical logic can be built. Schönfinkel explains that it’s “in the spirit of” Hilbert’s axiomatic method to build everything from as few notions as possible; then he says that what he wants to do is to “seek out those notions from which we shall best be able to construct all other notions of the branch of science in question”. His first step is to explain that Hilbert, Whitehead, Russell and Frege all set up mathematical logic in terms of standard And, Or, Not, etc. connectives—but that Sheffer had recently been able to show that just a single connective (indicated by a stroke “|”—and what we would now call Nand) was sufficient: The next couple of paragraphs talk about how the quantifiers “for all” (∀) and “there exists” (∃) can also be simplified in terms of the Sheffer stroke (i.e. Nand). But then comes the rallying cry: “The successes that we have encountered thus far… encourage us to attempt further progress.” And then he’s ready for the big idea—which he explains “at first glance certainly appears extremely bold”. He proposes to “eliminate by suitable reduction the remaining fundamental concepts of proposition, function and variable”. He explains that this only makes sense for “arbitrary, logically general propositions”, or, as we’d say now, for purely symbolic constructs without specific meanings yet assigned. In other words, his goal is to create a general framework for operating on arbitrary symbolic expressions independent of their interpretation. He explains that this is valuable both from a “methodological point of view” in achieving “the greatest possible conceptual uniformity”, but also from a certain philosophical or perhaps aesthetic point of view. And in a sense what he was explaining—back in 1920—was something that’s been a core part of the computational language design that I’ve done for the past 40 years: that everything can be represented as a symbolic expression, and that there’s tremendous value to this kind of uniformity. But as a “language designer” Schönfinkel was an ultimate minimalist. He wanted to get rid of as many notions as possible—and in particular he didn’t want variables, which he explained were “nothing but tokens that characterize certain argument places and operators as belonging together”; “mere auxiliary notions”. Today we have all sorts of mathematical notation that’s at least somewhat “variable free” (think coordinate-free notation, category theory, etc.)  But in 1920 mathematics as it was written was full of variables. And it needed a serious idea to see how to get rid of them. And that’s where Schönfinkel starts to go “even more symbolic”. He explains that he’s going to make a kind of “functional calculus” (Funktionalkalkül). He says that normally functions just define a certain correspondence between the domain of their arguments, and the domain of their values. But he says he’s going to generalize that—and allow (“disembodied”) functions to appear as arguments and values of functions. In other words, he’s inventing what we’d now call higher-order functions, where functions can operate “symbolically” on other functions. In the context of traditional calculus-and-algebra-style mathematics it’s a bizarre idea. But really it’s an idea about computation and computational structures—that’s more abstract and ultimately much more general than the mathematical objectives that inspired it. Again, it’s a bizarre idea—though actually Frege had had a similar idea many years earlier (and now the idea is usually called currying, after Haskell Curry, who we’ll be talking about later). But with his “functional calculus” set up, and all functions needing to take only one argument, Schönfinkel is ready for his big result. He’s effectively going to argue that by combining a small set of particular functions he can construct any possible symbolic function—or at least anything needed for predicate logic. He calls them a “sequence of particular functions of a very general nature”. Initially there are five of them: the identity function (Identitätsfunktion) I, the constancy function (Konstanzfunktion) C (which we now call K), the interchange function (Vertauschungsfunktion) T, the composition function (Zusammensetzungsfunktion) Z, and the fusion function (Verschmelzungsfunktion) S. And then he’s off and running defining what we now call combinators. The definitions look simple and direct. But to get to them Schönfinkel effectively had to cut away all sorts of conceptual baggage that had come with the historical development of logic and mathematics. Even talking about the identity combinator isn’t completely straightforward. Schönfinkel carefully explains that in I x = x, equality is direct symbolic or structural equality, or as he puts it “the equal sign is not to be taken to represent logical equivalence as it is ordi­narily defined in the propositional calculus of logic but signifies that the expressions on the left and on the right mean the same thing, that is, that the function value lx is always the same as the argument value x, whatever we may substitute for x.” He then adds parenthetically, “Thus, for instance, I I would be equal to I”. And, yes, to someone used to the mathematical idea that a function takes values like numbers, and gives back numbers, this is a bit mind-blowing. Next he explains the constancy combinator, that he called C (even though the German word for it starts with K), and that we now call K. He says “let us assume that the argument value is again arbitrary without restric­tion, while, regardless of what this value is, the function value will always be the fixed value a”. And when he says “arbitrary” he really means it: it’s not just a number or something; it’s what we would now think of as any symbolic expression. First he writes (C a)y = a, i.e. the value of the “constancy function C a operating on any y is a”, then he says to “let a be variable too”, and defines (C x)y = x or Cxy = x. Helpfully, almost as if he were writing computer documentation, he adds: “In practical applications C serves to permit the introduction of a quantity x as a ‘blind’ variable.” Finally comes the pièce de résistance: the S combinator (that Schönfinkel calls the “fusion function”): But Schönfinkel explains that for him “the practical use of the function S will be to enable us to reduce the number of occurrences of a variable—and to some extent also of a particular function—from several to a single one”. Setting up everything in terms of five basic objects I, C (now K), T, Z and S might already seem impressive and minimalist enough. But Schönfinkel realized that he could go even further: First, he says that actually I = SCC (or, in modern notation, s[k][k]). In other words, s[k][k][x] for symbolic x is just equal to x (since s[k][k][x] becomes k[x][k[x]] by using the definition of S, and this becomes x by using the definition of C). He notes that this particular reduction was communicated to him by a certain Alfred Boskowitz (who we know to have been a student at the time); he says that Paul Bernays (who was more of a colleague) had “some time before” noted that I = (SC)(CC) (i.e. s[k][k[k]]).  Today, of course, we can use a computer to just enumerate all possible combinator expressions of a particular size, and find what the smallest reduction is. But in Schönfinkel’s day, it would have been more like solving a puzzle by hand. Schönfinkel goes on, and proves that Z can also be reduced: Z = S(CS)C (i.e. s[k[s]][k]). And, yes, a very simple Wolfram Language program can verify in a few milliseconds that that is the simplest form. OK, what about T? Schönfinkel gives 8 steps of reduction to prove that T = S(ZZS)(CC) (i.e. s[s[k[s]][k][s[k[s]][k]][s]][k[k]]). But is this the simplest possible form for T? Well, no. But (with the very straightforward 2-line Wolfram Language program I wrote) it did take my modern computer a number of minutes to determine what the simplest form is. The answer is that it doesn’t have size 12, like Schönfinkel’s, but rather size 9. Actually, there are 6 cases of size 9 that all work: s[s[k[s]][s[k[k]][s]]][k[k]] (S(S(KS)(S(KK)S))(KK))) and five others. And, yes, it takes a few steps of reduction to prove that they work (the other size-9 cases S(SSK(K(SS(KK))))S, S(S(K(S(KS)K))S)(KK), S(K(S(S(KS)K)(KK)))S, S(K(SS(KK)))(S(KK)S), S(K(S(K(SS(KK)))K))S all have more complicated reductions): But, OK, what did Schönfinkel want to do with these objects he’d constructed? As the title of his paper suggests, he wanted to use them as building blocks for mathematical logic. He begins: “Let us now apply our results to a special case, that of the calculus of logic in which the basic elements are individuals and the functions are propositional functions.” I consider this sentence significant. Schönfinkel didn’t have a way to express it (the concept of universal computation hadn’t been invented yet), but he seems to have realized that what he’d done was quite general, and went even beyond being able to represent a particular kind of logic. Still, he went on to give his example. He’d explained at the beginning of the paper that the quantifiers we now call ∀ and ∃ could both be represented in terms of a kind of “quantified Nand” that he wrote : But now he wanted to “combinator-ify” everything. So he introduced a new combinator U, and defined it to represent his “quantified Nand”: Ufg = fx gx (he called U the “incompatibility function”—an interesting linguistic description of Nand): “It is a remarkable fact”, he says, “that every formula of logic can now be expressed by means… solely of C, S and U.” So he’s saying that any expression from mathematical logic can be written out as some combinator expression in terms of S, C (now K) and U.  He says that when there are quantifiers like “for all x…” it’s always possible to use combinators to get rid of the “bound variables” x, etc. He says that he “will not give the complete demonstration here”, but rather content himself with an example. (Unfortunately—for reasons of the trajectory of his life that are still quite unclear—he never published his “complete demonstration”.) But, OK, so what had he achieved? He’d basically shown that any expression that might appear in predicate logic (with logical connectives, quantifiers, variables, etc.) could be reduced to an expression purely in terms of the combinators S, C (now K) and U. Did he need the U? Not really. But he had to have some way to represent the thing with mathematical or logical “meaning” on which his combinators would be acting. Today the obvious thing to do would be to have a representation for true and false. And what’s more, to represent these purely in terms of combinators. For example, if we took K to represent true, and SK (s[k]) to represent false, then And can be represented as SSK (s[s][k]), Or as S(SS)S(SK) (s[s[s]][s][s[k]]) and Nand as S(S(K(S(SS(K(KK))))))S (s[s[k[s[s[s][k[k[k]]]]]]][s]). Schönfinkel got amazingly far in reducing everything to his “building blocks”. But, yes, he missed this final step. But given that he’d managed to reduce everything to S, C and U he figured he should try to go further. So he considered an object J that would be a single building block of S and C: JJ = S and J(JJ) = C. With S and K one can just point to any piece of an expression and see if it reduces. With J it’s a bit more complicated. In modern Wolfram Language terms one can state the rules as {j[j][x_][y_][z_]→x[z][y[z]], j[j[j]][x_][y_]→x} (where order matters) but to apply these requires pattern matching “clusters of J’s” rather than just looking at single S’s and K’s at a time. But even though—as Schönfinkel observed—this “final reduction” to J didn’t work out, getting everything down to S and K was already amazing. At the beginning of the paper, Schönfinkel had described his objectives. And then he says “It seems to me remarkable in the extreme that the goal we have just set can be realized also; as it happens, it can be done by a reduction to three fundamental signs.” (The paper does say three fundamental signs, presumably counting U as well as S and K.) I’m sure Schönfinkel expected that to reproduce all the richness of mathematical logic he’d need quite an elaborate set of building blocks. And certainly people like Frege, Whitehead and Russell had used what were eventually very complicated setups. Schönfinkel managed to cut through all the complexity to show that simple building blocks were all that were needed. But then he found something else: that actually just two building blocks (S and K) were enough. In modern terms, we’d say that Schönfinkel managed to construct a system capable of universal computation. And that’s amazing in itself. But even more amazing is that he found he could do it with such a simple setup. I’m sure Schönfinkel was extremely surprised. And here I personally feel a certain commonality with him. Because in my own explorations of the computational universe, what I’ve found over and over again is that it takes only remarkably simple systems to be capable of highly complex behavior—and of universal computation.  And even after exploring the computational universe for four decades, I’m still continually surprised at just how simple the systems can be. For me, this has turned into a general principle—the Principle of Computational Equivalence—and a whole conceptual framework around it. Schönfinkel didn’t have anything like that to think in terms of.  But he was in a sense a good enough scientist that he still managed to discover what he discovered—that many decades later we can see fits in as another piece of evidence for the Principle of Computational Equivalence. Looking at Schönfinkel’s paper a century later, it’s remarkable not only for what it discovers, but also for the clarity and simplicity with which it is presented. A little of the notation is now dated (and of course the original paper is written in German, which is no longer the kind of leading language of scholarship it once was). But for the most part, the paper still seems perfectly modern. Except, of course, that now it could be couched in terms of symbolic expressions and computation, rather than mathematical logic.
Combinators and the Story of Computation 10.4 What Is Their Mathematics? : Combinators are hard to understand, and it’s not clear how many people understood them when they were first introduced—let alone understood their implications.  It’s not a good sign that when Schönfinkel’s paper appeared in 1924 the person who helped prepare it for final publication (Heinrich Behmann) added his own three paragraphs at the end, that were quite confused. And Schönfinkel’s sole other published paper—coauthored with Paul Bernays in 1927—didn’t even mention combinators, even though they could have very profitably been used to discuss the subject at hand (decision problems in mathematical logic). But in 1927 combinators (if not perhaps Schönfinkel’s recognition for them) had a remarkable piece of good fortune. Schönfinkel’s paper was discovered by a certain Haskell Curry—who would then devote more than 50 years to studying what he named “combinators”, and to spreading the word about them. At some level I think one can view the main thrust of what Curry and his disciples did with combinators as an effort to “mathematicize” them. Schönfinkel had presented combinators in a rather straightforward “structural” way. But what was the mathematical interpretation of what he did, and of how combinators work in general? What mathematical formalism could capture Schönfinkel’s structural idea of substitution? Just what, for example, was the true notion of equality for combinators? In the end, combinators are fundamentally computational constructs, full of all the phenomena of “unbridled computation”—like undecidability and computational irreducibility. And it’s inevitable that mathematics as normally conceived can only go so far in “cracking” them. But back in the 1920s and 1930s the concept and power of computation was not yet understood, and it was assumed that the ideas and tools of mathematics would be the ones to use in analyzing a formal system like combinators. And it wasn’t that mathematical methods got absolutely nowhere with combinators. Unlike cellular automata, or even Turing machines, there’s a certain immediate structural complexity to combinators, with their elaborate tree structures, equivalences and so on. And so there was progress to be made—and years of work to be done—in untangling this, without having to face the raw features of full-scale computation, like computational irreducibility. In the end, combinators are full of computational irreducibility. But they also have layers of computational reducibility, some of which are aligned with the kinds of things mathematics and mathematical logic have been set up to handle. And in this there’s a curious resonance with our recent Physics Project. In our models based on hypergraph rewriting there’s also a kind of bedrock of computational irreducibility. But as with combinators, there’s a certain immediate structural complexity to what our models do. And there are layers of computational reducibility associated with this. But the remarkable thing with our models is that some of those layers—and the formalisms one can build to understand them—have an immediate interpretation: they are basically the core theories of twentieth-century physics, namely general relativity and quantum mechanics. Combinators work sufficiently differently that they don’t immediately align with that kind of interpretation. But it’s still true that one of the important properties discovered in combinators (namely confluence, related to our idea of causal invariance) turns out to be crucial to our models, their correspondence with physics, and in the end our whole ability to perceive regularity in the universe, even in the face of computational irreducibility. But let’s get back to the story of combinators as it played out after Schönfinkel’s paper. Schönfinkel had basically set things up in a novel, very direct, structural way. But Curry wanted to connect with more traditional ideas in mathematical logic, and mathematics in general. And after a first paper (published in 1929) which pretty much just recorded his first thoughts, and his efforts to understand what Schönfinkel had done, Curry was by 1930 starting to do things like formulate axioms for combinators, and hoping to prove general theorems about mathematical properties like equality. Without the understanding of universal computation and their relationship to it, it wasn’t clear yet how complicated it might ultimately be to deal with combinators. And Curry pushed forward, publishing more papers and trying to do things like define set theory using his axioms for combinators. But in 1934 disaster struck. It wasn’t something about computation or undecidability; instead it was that Stephen Kleene and J. Barkley Rosser showed the axioms Curry had come up with to try and “tighten up Schönfinkel” were just plain inconsistent. To Kleene and Rosser it provided more evidence of the need for Russell’s (originally quite hacky) idea of types—and led them to more complicated axiom systems, and away from combinators.  But Curry was undeterred. He revised his axiom system and continued—ultimately for many decades—to see what could be proved about combinators and things like them using mathematical methods. But already at the beginning of the 1930s there were bigger things afoot around mathematical logic—which would soon intersect with combinators.
Combinators and the Story of Computation 10.5 Gödel’s Theorem and Computability : How should one represent the fundamental constructs of mathematics?  Back in the 1920s nobody thought seriously about using combinators. And instead there were basically three “big brands”: Principia Mathematica, set theory and Hilbert’s program.  Relations were being found, details were being filled in, and issues were being found. But there was a general sense that progress was being made. Quite where the boundaries might lie wasn’t clear. For example, could one specify a way to “construct any function” from lower-level primitives? The basic idea of recursion was very old (think: Fibonacci). But by the early 1920s there was a fairly well-formalized notion of “primitive recursion” in which functions always found their values from earlier values. But could all “mathematical” functions be constructed this way? By 1926 it was known that this wouldn’t work: the Ackermann function was a reasonable “mathematical” function, but it wasn’t primitive recursive. It meant that definitions had to be generalized (e.g. to “general recursive functions” that didn’t just look back at earlier values, but could “look forward until…” as well). But there didn’t seem to be any fundamental problem with the idea that mathematics could just “mechanistically” be built out forever from appropriate primitives. But in 1931 came Gödel’s theorem. There’d been a long tradition of identifying paradoxes and inconsistencies, and finding ways to patch them by changing axioms. But Gödel’s theorem was based on Peano’s by-then-standard axioms for arithmetic (branded by Gödel as a fragment of Principia Mathematica). And it showed there was a fundamental problem. In essence, Gödel took the paradoxical statement “this statement is unprovable” and showed that it could be expressed purely as a statement of arithmetic—roughly a statement about the existence of solutions to appropriate integer equations. And basically what Gödel had to do to achieve this was to create a “compiler” capable of compiling things like “this statement is unprovable” into arithmetic. In his paper one can basically see him building up different capabilities (e.g. representing arbitrary expressions as numbers through Gödel numbering, checking conditions using general recursion, etc.)—eventually getting to a “high enough level” to represent the statement he wanted: What did Gödel’s theorem mean? For the foundations of mathematics it meant that the idea of mechanically proving “all true theorems of mathematics” wasn’t going to work. Because it showed that there was at least one statement that by its own admission couldn’t be proved, but was still a “statement about arithmetic”, in the sense that it could be “compiled into arithmetic”. That was a big deal for the foundations of mathematics. But actually there was something much more significant about Gödel’s theorem, even though it wasn’t recognized at the time. Gödel had used the primitives of number theory and logic to build what amounted to a computational system—in which one could take things like “this statement is unprovable”, and “run them in arithmetic”. What Gödel had, though, wasn’t exactly a streamlined general system (after all, it only really needed to handle one statement). But the immediate question then was: if there’s a problem with this statement in arithmetic, what about Hilbert’s general “decision problem” (Entscheidungsproblem) for any axiom system? To discuss the “general decision problem”, though, one needed some kind of general notion of how one could decide things. What ultimate primitives should one use? Schönfinkel (with Paul Bernays)—in his sole other published paper—wrote about a restricted case of the decision problem in 1927, but doesn’t seem to have had the idea of using combinators to study it. By 1934 Gödel was talking about general recursiveness (i.e. definability through general recursion). And Alonzo Church and Stephen Kleene were introducing λ definability. Then in 1936 Alan Turing introduced Turing machines. All these approaches involved setting up certain primitives, then showing that a large class of things could be “compiled” to those primitives. And that—in effect by thinking about having it compile itself—Hilbert’s Entscheidungsproblem couldn’t be solved. Perhaps no single result along these lines would have been so significant. But it was soon established that all three kinds of systems were exactly equivalent: the set of computations they could represent were the same, as established by showing that one system could emulate another. And from that discovery eventually emerged the modern notion of universal computation—and all its implications for technology and science. In the early days, though, there was actually a fourth equivalent kind of system—based on string rewriting—that had been invented by Emil Post in 1920–1. Oh, and then there were combinators.
Combinators and the Story of Computation 10.6 Lambda Calculus : What was the right “language” to use for setting up mathematical logic? There’d been gradual improvement since the complexities of Principia Mathematica. But around 1930 Alonzo Church wanted a new and cleaner setup. And he needed to have a way (as Frege and Principia Mathematica had done before him) to represent “pure functions”. And that’s how he came to invent λ. By the next year J. Barkley Rosser is trying to retool Curry’s “combinatory logic” with combinators of his own—and showing how they correspond to lambda expressions: Then in 1935 lambda calculus has its big “coming out” in Church’s “An Unsolvable Problem of Elementary Number Theory”, in which he introduces the idea that any “effectively calculable” function should be “λ definable”, then defines integers in terms of λ’s (“Church numerals”) and then shows that the problem of determining equivalence for λ expressions is undecidable. Very soon thereafter Turing publishes his “On Computable Numbers, with an Application to the Entscheidungsproblem” in which he introduces his much more manifestly mechanistic Turing machine model of computation. In the main part of the paper there are no lambdas—or combinators—to be seen. But by late 1936 Turing had gone to Princeton to be a student with Church—and added a note showing the correspondence between his Turing machines and Church’s lambda calculus. By the next year, when Turing is writing his rather abstruse “Systems of Logic Based on Ordinals” he’s using lambda calculus all over the place. Early in the document he writes I → λx[x], and soon he’s mixing lambdas and combinators with wild abandon—and in fact he’d already published a one-page paper which introduced the fixed-point combinator Θ (and, yes, the K in the title refers to Schönfinkel’s K combinator): In the end, combinators and lambda calculus are completely equivalent, and it’s quite easy to convert between them—but there’s a curious tradeoff. In lambda calculus one names variables, which is good for human readability, but can lead to problems at a formal level.  In combinators, things are formally much cleaner, but the expressions one gets can be completely incomprehensible to humans. And that’s part of why the correspondence between lambda calculus and combinators was important. With combinators there are no variables, and so no variable names to get tangled up. So if one can show that something can be converted to combinators—even if one never looks at the potentially very long and ugly combinator expression that’s generated—one knows one’s safe from issues about variable names. There are still plenty of other complicated issues, though. Prominent among them are questions about when combinator expressions can be considered equal. Let’s say you have a combinator expression, like s[s[s[s][k]]][k]. Well, you can repeatedly apply the rules for combinators to transform and reduce it. And it’ll often end up at a fixed point, where no rules apply anymore. But a basic question is whether it matters in which order the rules are applied. And in 1936 Church and Rosser proved it doesn’t. Actually, what they specifically proved was the analogous result for lambda calculus. They drew a picture to indicate different possible orders in which lambdas could be reduced out, and showed it didn’t matter which path one takes: This all might seem like a detail. But it turns out that generalizations of their result apply to all sorts of systems. In doing computations (or automatically proving theorems) it’s all about “it doesn’t matter what path you take; you’ll always get the same result”. And that’s important. But recently there’s been another important application that’s shown up. It turns out that a generalization of the “Church–Rosser property” is what we call causal invariance in our Physics Project. And it’s causal invariance that leads in our models to relativistic invariance, general covariance, objective reality in quantum mechanics, and other central features of physics.
Combinators and the Story of Computation 10.7 Practical Computation : In retrospect, one of the great achievements of the 1930s was the inception of what ended up being the idea of universal computation. But at the time what was done was couched in terms of mathematical logic and it was far from obvious that any of the theoretical structures being built would have any real application beyond thinking about the foundations of mathematics. But even as people like Hilbert were talking in theoretical terms about the mechanization of mathematics, more and more there were actual machines being built for doing mathematical calculations. We know that even in antiquity (at least one) simple gear-based mechanical calculational devices existed. In the mid-1600s arithmetic calculators started being constructed, and by the late 1800s they were in widespread use. At first they were mechanical, but by the 1930s most were electromechanical, and there started to be systems where units for carrying out different arithmetic operations could be chained together. And by the end of the 1940s fairly elaborate such systems based on electronics were being built. Already in the 1830s Charles Babbage had imagined an “analytical engine” which could do different operations depending on a “program” specified by punch cards—and Ada Lovelace had realized that such a machine had broad “computational” potential.  But by the 1930s a century had passed and nothing like this was connected to the theoretical developments that were going on—and the actual engineering of computational systems was done without any particular overarching theoretical framework. Still, as electronic devices got more complicated and scientific interest in psychology intensified, something else happened: there started to be the idea (sometimes associated with the name cybernetics) that somehow electronics might reproduce how things like brains work. In the mid-1930s Claude Shannon had shown that Boolean algebra could represent how switching circuits work, and in 1943 Warren McCulloch and Walter Pitts proposed a model of idealized neural networks formulated in something close to mathematical logic terms. Meanwhile by the mid-1940s John von Neumann—who had worked extensively on mathematical logic—had started suggesting math-like specifications for practical electronic computers, including the way their programs might be stored electronically. At first he made lots of brain-like references to “organs” and “inhibitory connections”, and essentially no mention of ideas from mathematical logic. But by the end of the 1940s von Neumann was talking at least conceptually about connections to Gödel’s theorem and Turing machines, Alan Turing had become involved with actual electronic computers, and there was the beginning of widespread understanding of the notion of general-purpose computers and universal computation. In the 1950s there was an explosion of interest in what would now be called the theory of computation—and great optimism about its relevance to artificial intelligence. There was all sorts of “interdisciplinary work” on fairly “concrete” models of computation, like finite automata, Turing machines, cellular automata and idealized neural networks. More “abstract” approaches, like recursive functions, lambda calculus—and combinators—remained, however, pretty much restricted to researchers in mathematical logic. When early programming languages started to appear in the latter part of the 1950s, thinking about practical computers began to become a bit more abstract. It was understood that the grammars of languages could be specified recursively—and actual recursion (of functions being able to call themselves) just snuck into the specification of ALGOL 60. But what about the structures on which programs operated? Most of the concentration was on arrays (sometimes rather elegantly, as in APL) and, occasionally, character strings. An issue in LISP was how to take “expressions” (which were viewed as representing things) and turn them into functions (which do things). And the basic plan was to use Church’s idea of λ notation. But when it came time to implement this, there was, of course, trouble with name collisions, which ended up getting handled in quite hacky ways. So did McCarthy know about combinators? The answer is yes, as his 1960 paper shows: I actually didn’t know until just now that McCarthy had ever even considered combinators, and in the years I knew him I don’t think I ever personally talked to him about them. But it seems that for McCarthy—as for Church—combinators were a kind of “comforting backstop” that ensured that it was OK to use lambdas, and that if things went too badly wrong with variable naming, there was at least in principle always a way to untangle everything. In the practical development of computers and computer languages, even lambdas—let alone combinators—weren’t really much heard from again (except in a small AI circle) until the 1980s. And even then it didn’t help that in an effort variously to stay close to hardware and to structure programs there tended to be a desire to give everything a “data type”—which was at odds with the “consume any expression” approach of standard combinators and lambdas. But beginning in the 1980s—particularly with the progressive rise of functional programming—lambdas, at least, have steadily gained in visibility and practical application. What of combinators? Occasionally as a proof of principle there’ll be a hardware system developed that natively implements Schönfinkel’s combinators. Or—particularly in modern times—there’ll be an esoteric language that uses combinators in some kind of purposeful effort at obfuscation. Still, a remarkable cross-section of notable people concerned with the foundations of computing have—at one time or another—taught about combinators or written a paper about them. And in recent years the term “combinator” has become more popular as a way to describe a “purely applicative” function. But by and large the important ideas that first arose with combinators ended up being absorbed into practical computing by quite circuitous routes, without direct reference to their origins, or to the specific structure of combinators.
Combinators and the Story of Computation 10.8 Combinators in Culture : For 100 years combinators have mostly been an obscure academic topic, studied particularly in connection with lambda calculus, at borders between theoretical computer science, mathematical logic and to some extent mathematical formalisms like category theory. Much of the work that’s been done can be traced in one way or another to the influence of Haskell Curry or Alonzo Church—particularly through their students, grandstudents, great-grandstudents, etc. Partly in the early years, most of the work was centered in the US, but by the 1960s there was a strong migration to Europe and especially the Netherlands. But even with all their abstractness and obscurity, on a few rare occasions combinators have broken into something closer to the mainstream. One such time was with the popular logic-puzzle book To Mock a Mockingbird, published in 1985 by Raymond Smullyan—a former student of Alonzo Church’s. It begins: “A certain enchanted forest is inhabited by talking birds” and goes on to tell a story that’s basically about combinators “dressed up” as birds calling each other (S is the “starling”, K the “kestrel”)—with a convenient “bird who’s who” at the end. The book is dedicated “To the memory of Haskell Curry—an early pioneer in combinatory logic and an avid bird-watcher”. And then there’s Y Combinator. The original Y combinator arose out of work that Curry did in the 1930s on the consistency of axiom systems for combinators, and it appeared explicitly in his 1958 classic book: He called it the “paradoxical combinator” because it was recursively defined in a kind of self-referential way analogous to various paradoxes. Its explicit form is SSK(S(K(SS(S(SSK))))K) and its most immediately notable feature is that under Schönfinkel’s combinator transformation rules it never settles down to a particular “value” but just keeps growing forever. Well, in 2005 Paul Graham—who had long been an enthusiast of functional programming and LISP—decided to name his new (and now very famous) startup accelerator “Y Combinator”. I remember asking him why he’d called it that. “Because,” he said, “nobody understands the Y combinator”. Looking in my own archives from that time I find an email I sent a combinator enthusiast who was working with me: Followed by, basically, “Yes our theorem prover can prove the basic property of the Y combinator” (V6 sounds so ancient; we’re now just about to release V12.2): I had another unexpected encounter with combinators last year. I had been given a book that was once owned by Alan Turing, and in it I found a piece of paper—that I recognized as being covered with none other than lambdas and combinators (but that’s not the Y combinator): It took quite a bit of sleuthing (that I wrote extensively about)—but I eventually discovered that the piece of paper was written by Turing’s student Robin Gandy. But I never figured out why he was doing combinators….
Combinators and the Story of Computation 10.9 Designing Symbolic Language : I think I first found out about combinators around 1979 by seeing Schönfinkel’s original paper in a book called From Frege to Gödel: A Source Book in Mathematical Logic (by a certain Jean van Heijenoort). How Schönfinkel’s paper ended up being in that book is an interesting question, which I’ll write about elsewhere. The spine of my copy of the book has long been broken at the location of Schönfinkel’s paper, and at different times I’ve come back to the paper, always thinking there was more to understand about it. But why was I even studying things like this back in 1979? I guess in retrospect I can say I was engaged in an activity that goes back to Frege or even Leibniz: I was trying to find a fundamental framework for representing mathematics and beyond. But my goal wasn’t a philosophical one; it was a very practical one: I was trying to build a computer language that could do general computations in mathematics and beyond. My immediate applications were in physics, and it was from physics that my main methodological experience came. And the result was that—like trying to understand the world in terms of elementary particles—I wanted to understand computation in terms of its most fundamental elements. But I also had lots of practical experience in using computers to do mathematical computation. And I soon developed a theory about how I thought computation could fundamentally be done. It started from the practical issue of transformations on algebraic expressions (turn sin(2x) into 2 sin(x) cos(x), etc.). But it soon became a general idea: compute by doing transformations on symbolic expressions. Was this going to work? I wanted to understand as fundamentally as possible what computation really was—and from that I was led to its history in mathematical logic. Much of what I saw in books and papers about mathematical logic I found abstruse and steeped in sometimes horrendous notational complexity. But what were these people really doing? It made it much easier that I had a definite theory, against which I could essentially do reductionist science. That stuff in Principia Mathematica? Those ideas about rewriting systems? Yup, I could see how to represent them as rules for transformations on symbolic expressions. And so it was that I came to design SMP: “A Symbolic Manipulation Program”—all based on transformation rules for symbolic expressions. It was easy to represent mathematical relations ($x is a pattern variable that would now in the Wolfram Language be x_ on the left-hand side only): Or basic logic: Or, for that matter, predicate logic of the kind Schönfinkel wanted to capture: And, yes, it could emulate a Turing machine (note the tape-as-transformation-rules representation that appears at the end): But the most important thing I realized is that it really worked to represent basically anything in terms of symbolic expressions, and transformation rules on them. Yes, it was quite often useful to think of “applying functions to things” (and SMP had its version of lambda, for example), but it was much more powerful to think about symbolic expressions as just “being there” (“x doesn’t have to have a value”)—like things in the world—with the language being able to define how things should transform. In retrospect this all seems awfully like the core idea of combinators, but with one important exception: that instead of everything being built from “purely structural elements” with names like S and K, there was a whole collection of “primitive objects” that were intended to have direct understandable meanings (like Plus, Times, etc.). And indeed I saw a large part of my task in language design as being to think about computations one might want to do, and then try to “drill down” to find the “elementary particles”—or primitive objects—from which these computations might be built up. Over time I’ve come to realize that doing this is less about what one can in principle use to construct computations, and more about making a bridge to the way humans think about things. It’s crucial that there’s an underlying structure—symbolic expressions—that can represent anything. But increasingly I’ve come to realize that what we need from a computational language is to have a way to encapsulate in precise computational form the kinds of things we humans think about—in a way that we humans can understand. And a crucial part of being able to do that is to leverage what has ultimately been at the core of making our whole intellectual development as a species possible: the idea of human language. Human language has given us a way to talk symbolically about the world: to give symbolic names to things, and then to build things up using these. In designing a computational language the goal is to leverage this: to use what humans already know and understand, but be able to represent it in a precise computational way that is amenable to actual computation that can be done automatically by computer. It’s probably no coincidence that the tree structure of symbolic expressions that I have found to be such a successful foundation for computational language is a bit like an idealized version of the kind of tree structure (think parse trees or sentence diagramming) that one can view human language as following. There are other ways to set up universal computation, but this is the one that seems to fit most directly with our way of thinking about things. And, yes, in the end all those symbolic expressions could be constructed like combinators from objects—like S and K—with no direct human meaning. But that would be like having a world without nouns—a world where there’s no name for anything—and the representation of everything has to be built from scratch. But the crucial idea that’s central to human language—and now to computational language—is to be able to have layers of abstraction, where one can name things and then refer to them just by name without having to think about how they’re built up “inside”. In some sense one can see the goal of people like Frege—and Schönfinkel—as being to “reduce out” what exists in mathematics (or the world) and turn it into something like “pure logic”. And the structural part of that is exactly what makes computational language possible. But in my conception of computational language the whole idea is to have content that relates to the world and the way we humans think about it. And over the decades I’ve continually been amazed at just how strong and successful the idea of representing things in terms of symbolic expressions and transformations on them is. Underneath everything that’s going on in the Wolfram Language—and in all the many systems that now use it—it’s all ultimately just symbolic expressions being transformed according to particular rules, and reaching fixed points that represent results of computations, just like in those examples in Schönfinkel’s original paper. And when Mathematica was first launched in 1988 I was charmed to see more than one person from mathematical logic immediately think of implementing combinators. Make the definitions: k[x_][y_] := x Then combinators “just work” (at least if they reach a fixed point): s[s[k[s]][s[k[k]][s[k[s]][k]]]][s[k[s[s[k][k]]]][k]][a][b][c] I don’t think most of us humans are particularly good at following this kind of chain of abstraction, at least without some kind of “guide rails”. And it’s been interesting for me to see over the years how we’ve been able to progressively build up guide rails for longer and longer chains of abstraction. First there were things like Function, Apply, Map. Then Nest, Fold, FixedPoint, MapThread. But only quite recently NestGraph, FoldPair, SubsetMap, etc. Even from the beginning there were direct “head manipulation” functions like Operate and Through. But unlike more “array-like” operations for list manipulation they’ve been slow to catch on. In a sense combinators are an ultimate story of “symbolic head manipulation”: everything can get applied to everything before it’s applied to anything. And, yes, it’s very hard to keep track of what’s going on—which is why “named guide rails” are so important, and also why they’re challenging to devise. But it seems as if, as we progressively evolve our understanding, we’re slowly able to get a little further, in effect building towards the kind of structure and power that combinators—in their very non-human-relatable way—first showed us was possible a century ago.
Combinators and the Story of Computation 10.10 Combinators in the Computational Universe : Combinators were invented for a definite purpose: to provide building blocks, as Schönfinkel put it, for logic. It was the same kind of thing with other models of what we now know of as computation. All of them were “constructed for a purpose”. But in the end computation—and programs—are abstract things, that can in principle be studied without reference to any particular purpose. One might have some particular reason to be looking at how fast programs of some kind can run, or what can be proved about them. But what about the analog of pure natural science: of studying what programs just “naturally do”? At the beginning of the 1980s I got very interested in what one can think of as the “natural science of programs”. My interest originally arose out of a question about ordinary natural science. One of the very noticeable features of the natural world is how much in it seems to us highly complex. But where does this complexity really come from? Through what kind of mechanism does nature produce it? I quickly realized that in trying to address that question, I needed as general a foundation for making models of things as possible. And for that I turned to programs, and began to study just what “programs in the wild” might do. Ever since the time of Galileo and Newton mathematical equations had been the main way that people ultimately imagined making models of nature. And on the face of it—with their real numbers and continuous character—these seemed quite different from the usual setup for computation, with its discrete elements and discrete choices. But perhaps in part through my own experience in doing mathematics symbolically on computers, I didn’t see a real conflict, and I began to think of programs as a kind of generalization of the traditional approach to modeling in science. But what kind of programs might nature use? I decided to just start exploring all the possibilities: the whole “computational universe” of programs—starting with the simplest. I came up with a particularly simple setup involving a row of cells with values 0 or 1 updated in parallel based on the values of their neighbors. I soon learned that systems like this had actually been studied under the name “cellular automata” in the 1950s (particularly in 2D) as potential models of computation, though had fallen out of favor mainly through not having seemed very “human programmable”. My initial assumption was that with simple programs I’d only see simple behavior. But with my cellular automata it was very easy to do actual computer experiments, and to visualize the results. And though in many cases what I saw was simple behavior, I also saw something very surprising: that in some cases—even though the rules were very simple—the behavior that was generated could be immensely complex: It took me years to come to terms with this phenomenon, and it’s gradually informed the way I think about science, computation and many other things. At first I studied it almost exclusively in cellular automata. I made connections to actual systems in nature that cellular automata could model. I tried to understand what existing mathematical and other methods could say about what I’d seen. And slowly I began to formulate general ideas to explain what was going on—like computational irreducibility and the Principle of Computational Equivalence. But at the beginning of the 1990s—now armed with what would become the Wolfram Language—I decided I should try to see just how the phenomenon I had found in cellular automata would play it in other kinds of computational systems. And my archives record that on April 4, 1992, I started looking at combinators. I seem to have come back to them several times, but in a notebook from July 10, 1994 (which, yes, still runs just fine), there it is: A randomly chosen combinator made of Schönfinkel’s S’s and K’s starting to show complex behavior. I seem to have a lot of notebooks that start with the simple combinator definitions—and then start exploring: There are what seem like they could be pages from a “computational naturalist’s field notebook”: Then there are attempts to visualize combinators in the same kind of way as cellular automata: But the end result was that, yes, like Turing machines, string substitution systems and all the other systems I explored in the computational universe, combinators did exactly the same kinds of things I’d originally discovered in cellular automata. Combinators weren’t just systems that could be set up to do things. Even “in the wild” they could spontaneously do very interesting and complex things. I included a few pages on what I called “symbolic systems” (essentially lambdas) at the end of my chapter on “The World of Simple Programs” in A New Kind of Science (and, yes, reading particularly the notes again now, I realize there are still many more things to explore…): Later in the book I talk specifically about Schönfinkel’s combinators in connection with the threshold of computation universality. But before showing examples of what they do, I remark: “Originally intended as an idealized way to represent structures of functions defined in logic, combinators were actually first introduced in 1920—sixteen years before Turing machines. But although they have been investigated somewhat over the past eighty years, they have for the most part been viewed as rather obscure and irrelevant constructs” How “irrelevant” should they be seen as being? Of course it depends on what for. As things to explore in the computational universe, cellular automata have the great advantage of allowing immediate visualization. With combinators it’s a challenge to find any way to translate their behavior at all faithfully into something suitable for human perception. And since the Principle of Computational Equivalence implies that general computational features won’t depend on the particulars of different systems, there’s a tendency to feel that even in studying the computational universe, combinators “aren’t worth the trouble”. Still, one thing that’s been prominently on display with cellular automata over the past 20 or so years is the idea that any sufficiently simple system will eventually end up being a useful model for something. Mollusc pigmentation. Catalysis processes. Road traffic flow. There are simple cellular automaton models for all of these. What about combinators? Without good visualization it’s harder to say “that looks like combinator behavior”. And even after 100 years they’re still a bit too unfamiliar. But when it comes to capturing some large-scale expression or tree behavior of some system, I won’t be surprised if combinators are a good fit. When one looks at the computational universe, one of the important ideas is “mining” it not just for programs that can serve as models for things, but also for programs that are somehow useful for some technological purpose. Yes, one can imagine specifically “compiling” some known program to combinators. But the question is whether “naturally occurring combinators” can somehow be identified as useful for some particular purpose. Could they deliver some new kind of distributed cryptographic protocol? Could they be helpful in mapping out distributed computing systems? Could they serve as a base for setting up molecular-scale computation, say with tree-like molecules? I don’t know. But it will be interesting to find out. And as combinators enter their second century they provide a unique kind of “computational raw material” to mine from the computational universe.
Combinators and the Story of Computation 10.11 Combinators All the Way Down? : What is the universe fundamentally made of? For a long time the assumption was that it must be described by something fundamentally mathematical. And indeed right around the time combinators were being invented the two great theories of general relativity and quantum mechanics were just developing. And in fact it seemed as if both physics and mathematics were going so well that people like David Hilbert imagined that perhaps both might be completely solved—and that there might be a mathematics-like axiomatic basis for physics that could be “mechanically explored” as he imagined mathematics could be. But it didn’t work out that way. Gödel’s theorem appeared to shatter the idea of a “complete mechanical exploration” of mathematics. And while there was immense technical progress in working out the consequences of general relativity and quantum mechanics little was discovered about what might lie underneath. Computers (including things like Mathematica) were certainly useful in exploring the existing theories of physics. But physics didn’t show any particular signs of being “fundamentally computational”, and indeed the existing theories seemed structurally not terribly compatible with computational processes. But as I explored the computational universe and saw just what rich and complex behavior could arise even from very simple rules, I began to wonder whether maybe, far below the level of existing physics, the universe might be fundamentally computational. I began to make specific models in which space and time were formed from an evolving network of discrete points.  And I realized that some of the ideas that had arisen in the study of things like combinators and lambda calculus from the 1930s and 1940s might have direct relevance. Like combinators (or lambda calculus) my models had the feature that they allowed many possible paths of evolution. And like combinators (or lambda calculus) at least some of my models had the remarkable feature that in some sense it didn’t matter what path one took; the final result would always be the same. For combinators this “Church–Rosser” or “confluence” feature was what allowed one to have a definite fixed point that could be considered the result of a computation. In my models of the universe that doesn’t just stop—things are a bit more subtle—but the generalization to what I call causal invariance is precisely what leads to relativistic invariance and the validity of general relativity. For many years my work on fundamental physics languished—a victim of other priorities and the uphill effort of introducing new paradigms into a well-established field. But just over a year ago—with help from two very talented young physicists—I started again, with unexpectedly spectacular results. I had never been quite satisfied with my idea of everything in the universe being represented as a particular kind of giant graph. But now I imagined that perhaps it was more like a giant symbolic expression, or, specifically, like an expression consisting of a huge collection of relations between elements—in effect, a certain kind of giant hypergraph. It was, in a way, a very combinator-like concept. At a technical level, it’s not the same as a general combinator expression: it’s basically just a single layer, not a tree. And in fact that’s what seems to allow the physical universe to consist of something that approximates uniform (manifold-like) space, rather than showing some kind of hierarchical tree-like structure everywhere. But when it comes to the progression of the universe through time, it’s basically just like the transformation of combinator expressions. And what’s become clear is that the existence of different paths—and their ultimate equivalences—is exactly what’s responsible not only for the phenomena of relativity, but also for quantum mechanics. And what’s remarkable is that many of the concepts that were first discovered in the context of combinators and lambda calculus now directly inform the theory of physics. Normal forms (basically fixed points) are related to black holes where “time stops”. Critical pair lemmas are related to measurement in quantum mechanics. And so on. In practical computing, and in the creation of computational language, it was the addition of “meaningful names” to the raw structure of combinators that turned them into the powerful symbolic expressions we use. But in understanding the “data structure of the universe” we’re in a sense going back to something much more like “raw combinators”. Because now all those “atoms of space” that make up the universe don’t have meaningful names; they’re more like S’s and K’s in a giant combinator expression, distinct but yet all the same. In the traditional, mathematical view of physics, there was always some sense that by “appropriately clever mathematics” it would be possible to “figure out what will happen” in any physical system. But once one imagines that physics is fundamentally computational, that’s not what one can expect. And just like combinators—with their capability for universal computation—can’t in a sense be “cracked” using mathematics, so also that’ll be true of the universe. And indeed in our model that’s what the progress of time is about: it’s the inexorable, irreducible process of computation, associated with the repeated transformation of the symbolic expression that represents the universe. When Hilbert first imagined that physics could be reduced to mathematics he probably thought that meant that physics could be “solved”. But with Gödel’s theorem—which is a reflection of universal computation—it became clear that mathematics itself couldn’t just be “solved”. But now in effect we have a theory that “reduces physics to mathematics”, and the result of the Gödel’s theorem phenomenon is something very important in our universe: it’s what leads to a meaningful notion of time. Moses Schönfinkel imagined that with combinators he was finding “building blocks for logic”. And perhaps the very simplicity of what he came up with makes it almost inevitable that it wasn’t just about logic: it was something much more general. Something that can represent computations. Something that has the germ of how we can represent the “machine code” of the physical universe. It took in a sense “humanizing” combinators to make them useful for things like computational language whose very purpose is to connect with humans. But there are other places where inevitably we’re dealing with something more like large-scale “combinators in the raw”. Physics is one of them. But there are others. In distributed computing. And perhaps in biology, in economics and in other places. There are specific issues of whether one’s dealing with trees (like combinators), or hypergraphs (like our model of physics), or something else. But what’s important is that many of the ideas—particularly around what we call multiway systems—show up with combinators. And yes, combinators often aren’t the easiest places for us humans to understand the ideas in. But the remarkable fact is that they exist in combinators—and that combinators are now a century old. I’m not sure if there’ll ever be a significant area where combinators alone will be the dominant force. But combinators have—for a century—had the essence of many important ideas. Maybe as such they are at some level destined forever to be footnotes. But in sense they are also seeds or roots—from which remarkable things have grown. And as combinators enter their second century it seems quite certain that there is still much more that will grow from them.
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.1 : December 7, 1920 On Tuesday, December 7, 1920, the Göttingen Mathematics Society held its regular weekly meeting—at which a 32-year-old local mathematician named Moses Schönfinkel with no known previous mathematical publications gave a talk entitled “Elemente der Logik” (“Elements of Logic”). A hundred years later what was presented in that talk still seems in many ways alien and futuristic—and for most people almost irreducibly abstract. But we now realize that that talk gave the first complete formalism for what is probably the single most important idea of this past century: the idea of universal computation. Sixteen years later would come Turing machines (and lambda calculus). But in 1920 Moses Schönfinkel presented what he called “building blocks of logic”—or what we now call “combinators”—and then proceeded to show that by appropriately combining them one could effectively define any function, or, in modern terms, that they could be used to do universal computation. Looking back a century it’s remarkable enough that Moses Schönfinkel conceptualized a formal system that could effectively capture the abstract notion of computation. And it’s more remarkable still that he formulated what amounts to the idea of universal computation, and showed that his system achieved it. But for me the most amazing thing is that not only did he invent the first complete formalism for universal computation, but his formalism is probably in some sense minimal. I’ve personally spent years trying to work out just how simple the structure of systems that support universal computation can be—and for example with Turing machines it took from 1936 until 2007 for us to find the minimal case. But back in his 1920 talk Moses Schönfinkel—presenting a formalism for universal computation for the very first time—gave something that is probably already in his context minimal. Moses Schönfinkel described the result of his 1920 talk in an 11-page paper published in 1924 entitled “Über die Bausteine der mathematischen Logik” (“On the Building Blocks of Mathematical Logic”). The paper is a model of clarity. It starts by saying that in the “axiomatic method” for mathematics it makes sense to try to keep the number of “fundamental notions” as small as possible. It reports that in 1913 Henry Sheffer managed to show that basic logic requires only one connective, that we now call Nand. But then it begins to go further. And already within a couple of paragraphs it’s saying that “We are led to [an] idea, which at first glance certainly appears extremely bold”. But by the end of the introduction it’s reporting, with surprise, the big news: “It seems to me remarkable in the extreme that the goal we have just set can be realized… [and]; as it happens, it can be done by a reduction to three fundamental signs”. Those “three fundamental signs”, of which he only really needs two, are what we now call the S and K combinators (he called them S and C). In concept they’re remarkably simple, but their actual operation is in many ways brain-twistingly complex. But there they were—already a century ago—just as they are today: minimal elements for universal computation, somehow conjured up from the mind of Moses Schönfinkel.
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.2 Who Was Moses Schönfinkel? : So who was this person, who managed so long ago to see so far? The complete known published output of Moses Schönfinkel consists of just two papers: his 1924 “On the Building Blocks of Mathematical Logic”, and another, 31-page paper from 1927, coauthored with Paul Bernays, entitled “Zum Entscheidungsproblem der mathematischen Logik” (“On the Decision Problem of Mathematical Logic”). And somehow Schönfinkel has always been in the shadows—appearing at best only as a kind of footnote to a footnote. Turing machines have taken the limelight as models of computation—with combinators, hard to understand as they are, being mentioned at most only in obscure footnotes. And even within the study of combinators—often called “combinatory logic”—even as S and K have remained ubiquitous, Schönfinkel’s invention of them typically garners at most a footnote. About Schönfinkel as a person, three things are commonly said. First, that he was somehow connected with the mathematician David Hilbert in Göttingen. Second, that he spent time in a psychiatric institution. And third, that he died in poverty in Moscow, probably around 1940 or 1942. But of course there has to be more to the story. And in recognition of the centenary of Schönfinkel’s announcement of combinators, I decided to try to see what I could find out. I don’t think I’ve got all the answers. But it’s been an interesting, if at times unsettling, trek through the Europe—and mathematics—of a century or so ago.  And at the end of it I feel I’ve come to know and understand at least a little more about the triumph and tragedy of Moses Schönfinkel.
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.3 The Beginning of the Story : It’s a strange and sad resonance with Moses Schönfinkel’s life… but there’s a 1953 song by Tom Lehrer about plagiarism in mathematics—where the protagonist explains his chain of intellectual theft: “I have a friend in Minsk/Who has a friend in Pinsk/Whose friend in Omsk”… “/Whose friend somehow/Is solving now/The problem in Dnepropetrovsk”. Well, Dnepropetrovsk is where Moses Schönfinkel was born. Except, confusingly, at the time it was called (after Catherine the Great or maybe her namesake saint) Ekaterinoslav (Екатеринослáв)—and it’s now called Dnipro. It’s one of the larger cities in Ukraine, roughly in the center of the country, about 250 miles down the river Dnieper from Kiev. And at the time when Schönfinkel was born, Ukraine was part of the Russian Empire. So what traces are there of Moses Schönfinkel in Ekaterinoslav (AKA Dnipro) today? 132 years later it wasn’t so easy to find (especially during a pandemic)… but here’s a record of his birth: a certificate from the Ekaterinoslav Public Rabbi stating that entry 272 of the Birth Register for Jews from 1888 records that on September 7, 1888, a son Moses was born to the Ekaterinoslav citizen Ilya Schönfinkel and his wife Masha: This seems straightforward enough. But immediately there’s a subtlety. When exactly was Moses Schönfinkel born? What is that date? At the time the Russian Empire—which had the Russian Orthodox Church, which eschewed Pope Gregory’s 1582 revision of the calendar—was still using the Julian calendar introduced by Julius Caesar. (The calendar was switched in 1918 after the Russian Revolution, although the Orthodox Church plans to go on celebrating Christmas on January 7 until 2100.) So to know a correct modern (i.e. Gregorian calendar) date of birth we have to do a conversion. And from this we’d conclude that Moses Schönfinkel was born on September 19, 1888. But it turns out that’s not the end of the story. There are several other documents associated with Schönfinkel’s college years that also list his date of birth as September 7, 1888. But the state archives of the Dnepropetrovsk region contain the actual, original register from the synagogue in Ekaterinoslav. And here’s entry 272—and it records the birth of Moses Schönfinkel, but on September 17, not September 7: So the official certificate is wrong! Someone left a digit out. And there’s a check: the Birth Register also gives the date in the Jewish calendar: 24 Tishrei–which for 1888 is the Julian date September 17. So converting to modern Gregorian form, the correct date of birth for Moses Schönfinkel is September 29, 1888. OK, now what about his name? In Russian it’s given as Моисей Шейнфинкель (or, including the patronymic, with the most common transliteration from Hebrew, Моисей Эльевич Шейнфинкель). But how should his last name be transliterated? Well, there are several possibilities. We’re using Schönfinkel—but other possibilities are Sheinfinkel and Sheynfinkel—and these show up almost randomly in different documents. What else can we learn from Moses Schönfinkel’s “birth certificate”? Well, it describes his father Эльева (Ilya) as an Ekaterinoslav мещанина. But what is that word? It’s often translated “bourgeoisie”, but seems to have basically meant “middle-class city dweller”. And in other documents from the time, Ilya Schönfinkel is described as a “merchant of the 2nd guild” (i.e. not the “top 5%” 1st guild, nor the lower 3rd guild). Apparently, however, his fortunes improved. The 1905 “Index of Active Enterprises Incorporated in the [Russian] Empire” lists him as a “merchant of the 1st guild” and records that in 1894 he co-founded the company of “Lurie & Sheinfinkel” (with a paid-in capital of 10,000 rubles, or about $150k today) that was engaged in the grocery trade: Lurie & Sheinfinkel seems to have had multiple wine and grocery stores. Between 1901 and 1904 its “store #2” was next to a homeopathic pharmacy in a building that probably looked at the time much like it does today: And for store #1 there are actually contemporary photographs (note the -инкель for the end of “Schönfinkel” visible on the bottom left; this particular building was destroyed in World War II): There seems to have been a close connection between the Schönfinkels and the Luries—who were a prominent Ekaterinoslav family involved in a variety of enterprises. Moses Schönfinkel’s mother Maria (Masha) was originally a Lurie (actually, she was one of the 8 siblings of Ilya Schönfinkel’s business partner Aron Lurie). Ilya Schönfinkel is listed from 1894 to 1897 as “treasurer of the Lurie Synagogue”. And in 1906 Moses Schönfinkel listed his mailing address in Ekaterinoslav as Lurie House, Ostrozhnaya Square. (By 1906 that square sported an upscale park—though a century earlier it had housed a prison that was referenced in a poem by Pushkin. Now it’s the site of an opera house.) Accounts of Schönfinkel sometimes describe him as coming from a “village in Ukraine”. In actuality, at the turn of the twentieth century Ekaterinoslav was a bustling metropolis, that for example had just become the third city in the whole Russian Empire to have electric trams. Schönfinkel’s family also seems to have been quite well to do. Some pictures of Ekaterinoslav from the time give a sense of the environment (this building was actually the site of a Lurie candy factory): As the name “Moses” might suggest, Moses Schönfinkel was Jewish, and at the time he was born there was a large Jewish population in the southern part of Ukraine. Many Jews had come to Ekaterinoslav from Moscow, and in fact 40% of the whole population of the town was identified as Jewish. Moses Schönfinkel went to the main high school in town (the “Ekaterinoslav classical gymnasium”)—and graduated in 1906, shortly before turning 18. Here’s his diploma: The diploma shows that he got 5/5 in all subjects—the subjects being theology, Russian, logic, Latin, Greek, mathematics, geodesy (“mathematical geography”), physics, history, geography, French, German and drawing. So, yes, he did well in high school. And in fact the diploma goes on to say: “In view of his excellent behavior and diligence and excellent success in the sciences, especially in mathematics, the Pedagogical Council decided to award him the Gold Medal…”.
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.4 Going to College in Odessa : Having graduated from high school, Moses Schönfinkel wanted to go (“for purely family reasons”, he said) to the University of Kiev. But being told that Ekaterinoslav was in the wrong district for that, he instead asked to enroll at Novorossiysk University in Odessa. He wrote a letter—in rather neat handwriting—to unscramble a bureaucratic issue, giving various excuses along the way: But in the fall of 1906, there he was: a student in the Faculty of Physics and Mathematics Faculty of Novorossiysk University, in the rather upscale and cosmopolitan town of Odessa, on the Black Sea. The Imperial Novorossiya University, as it was then officially called, had been created out of an earlier institution by Tsar Alexander II in 1865. It was a distinguished university, with for example Dmitri Mendeleev (of periodic table fame) having taught there. In Soviet times it would be renamed after the discoverer of macrophages, Élie Metchnikoff (who worked there). Nowadays it is usually known as Odessa University. And conveniently, it has maintained its archives well—so that, still there, 114 years later, is Moses Schönfinkel’s student file: It’s amazing how “modern” a lot of what’s in it seems. First, there are documents Moses Schönfinkel sent so he could register (confirming them by telegram on September 1, 1906). There’s his high-school diploma and birth certificate—and there’s a document from the Ekaterinoslav City Council certifying his “citizen rank” (see above). The cover sheet also records a couple of other documents, one of which is presumably some kind of deferment of military service. And then in the file there are two “photo cards” giving us pictures of the young Moses Schönfinkel, wearing the uniform of the Imperial Russian Army: (These pictures actually seem to come from 1908; the style of uniform was a standard one issued after 1907; the [presumably] white collar tabs indicate the 3rd regiment of whatever division he was assigned to.) Nowadays it would all be online, but in his physical file there is a “lecture book” listing courses (yes, every document is numbered, to correspond to a line in a central ledger): Here are the courses Moses Schönfinkel took in his first semester in college (fall 1906): Introduction to Analysis (6 hrs), Introduction to Determinant Theory (2 hrs), Analytical Geometry 1 (2 hrs), Chemistry (5 hrs), Physics 1 (3 hrs), Elementary Number Theory (2 hrs): a total of 20 hours. Here’s the bill for these courses: pretty good value at 1 ruble per course-hour, or a total of 20 rubles, which is about $300 today: Subsequent semesters list many very familiar courses: Differential Calculus, Integrals (parts 1 and 2), and Higher Algebra, as well as “Calculus of Probabilities” (presumably probability theory) and “Determinant Theory” (essentially differently branded “linear algebra”). There are some “distribution” courses, like Astronomy (and Spherical Astronomy) and Physical Geography (or is that Geodesy?). And by 1908, there are also courses like Functions of a Complex Variable, Integro-Differential Equations (yeah, differential equations definitely pulled ahead of integral equations over the past century), Calculus of Variations and Infinite Series. And—perhaps presaging Schönfinkel’s next life move—another course that makes an appearance in 1908 is German (and it’s Schönfinkel’s only non-science course during his whole university career). In Schönfinkel’s “lecture book” many of the courses also have names of professors listed. For example, there’s “Kagan”, who’s listed as teaching Foundations of Geometry (as well as Higher Algebra, Determinant Theory and Integro-Differential Equations). That’s Benjamin Kagan, who was then a young lecturer, but would later become a leader in differential geometry in Moscow—and also someone who studied the axiomatic foundations of geometry (as well as writing about the somewhat tragic life of Lobachevsky). Another professor—listed as teaching Schönfinkel Introduction to Analysis and Theory of Algebraic Equation Solving—is “Shatunovsky”. And (at least according to Shatunovsky’s later student Sofya Yanovskaya, of whom we’ll hear more later), Samuil Shatunovsky was basically Schönfinkel’s undergraduate advisor. Shatunovsky had been the 9th child of a poor Jewish family (actually) from a village in Ukraine. He was never able to enroll at a university, but for some years did manage to go to lectures by people around Pafnuty Chebyshev in Saint Petersburg. For quite a few years he then made a living as an itinerant math tutor (notably in Ekaterinoslav) but papers he wrote were eventually noticed by people at the university in Odessa, and, finally, in 1905, at the age of 46, he ended up as a lecturer at the university—where the following year he taught Schönfinkel. Shatunovsky (who stayed in Odessa until his death in 1929) was apparently an energetic but precise lecturer. He seems to have been quite axiomatically oriented, creating axiomatic systems for geometry, algebraic fields, and notably, for order relations. (He was also quite a constructivist, opposed to the indiscriminate use of the Law of Excluded Middle.) The lectures from his Introduction to Analysis course (which Schönfinkel took in 1906) were published in 1923 (by the local publishing company Mathesis in which he and Kagan were involved). Another of Schönfinkel’s professors (from whom he took Differential Calculus and “Calculus of Probabilities”) was a certain Ivan (or Jan) Śleszyński, who had worked with Karl Weierstrass on things like continued fractions, but by 1906 was in his early 50s and increasingly transitioning to working on logic. In 1911 he moved to Poland, where he sowed some of the seeds for the Polish school of mathematical logic, in 1923 writing a book called On the Significance of Logic for Mathematics (notably with no mention of Schönfinkel), and in 1925 one on proof theory. It’s not clear how much mathematical logic Moses Schönfinkel picked up in college, but in any case, in 1910, he was ready to graduate. Here’s his final student ID (what are those pieces of string for?): There’s a certificate confirming that on April 6, 1910, Moses Schönfinkel had no books that needed returning to the library. And he sent a letter asking to graduate (with slightly-less-neat handwriting than in 1906): The letter closes with his signature (Моисей Шейнфинкель).
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.5 Göttingen, Center of the Mathematical Universe : After Moses Schönfinkel graduated college in 1910 he probably went into four years of military service (perhaps as an engineer) in the Russian Imperial Army. World War I began on July 28, 1914—and Russia mobilized on July 30. But in one of his few pieces of good luck Moses Schönfinkel was not called up, having arrived in Göttingen, Germany, on June 1, 1914 (just four weeks before the event that would trigger World War I), to study mathematics. Göttingen was at the time a top place for mathematics. In fact, it was sufficiently much of a “math town” that around that time postcards of local mathematicians were for sale there. And the biggest star was David Hilbert—which is who Schönfinkel went to Göttingen hoping to work with. Hilbert had grown up in Prussia and started his career in Königsberg. His big break came in 1888 at age 26 when he got a major result in representation theory (then called “invariant theory”)—using then-shocking non-constructive techniques. And it was soon after this that Felix Klein recruited Hilbert to Göttingen—where he remained for the rest of his life. In 1900 Hilbert gave his famous address to the International Congress of Mathematicians where he first listed his (ultimately 23) problems that he thought should be important in the future of mathematics. Almost all the problems are what anyone would call “mathematical”. But problem 6 has always stuck out for me: “Mathematical Treatment of the Axioms of Physics”: Hilbert somehow wanted to axiomatize physics as Euclid had axiomatized geometry. And he didn’t just talk about this; he spent nearly 20 years working on it. He brought in physicists to teach him, and he worked on things like gravitation theory (“Einstein–Hilbert action”) and kinetic theory—and wanted for example to derive the existence of the electron from something like Maxwell’s equations. (He was particularly interested in the way atomistic processes limit to continua—a problem that I now believe is deeply connected to computational irreducibility, in effect implying another appearance of undecidability, like in Hilbert’s 1st, 2nd and 10th problems.) Hilbert seemed to feel that physics was a crucial source of raw material for mathematics. But yet he developed a whole program of research based on doing mathematics in a completely formalistic way—where one just writes down axioms and somehow “mechanically” generates all true theorems from them. (He seems to have drawn some distinction between “merely mathematical” questions, and questions about physics, apparently noting—in a certain resonance with my life’s work—that in the latter case “the physicist has the great calculating machine, Nature”.) In 1899 Hilbert had written down more precise and formal axioms for Euclid’s geometry, and he wanted to go on and figure out how to formulate other areas of mathematics in this kind of axiomatic way. But for more than a decade he seems to have spent most of his time on physics—finally returning to questions about the foundations of mathematics around 1917, giving lectures about “logical calculus” in the winter session of 1920. By 1920, World War I had come and gone, with comparatively little effect on mathematical life in Göttingen (the nearest battle was in Belgium 200 miles to the west). Hilbert was 58 years old, and had apparently lost quite a bit of his earlier energy (not least as a result of having contracted pernicious anemia [autoimmune vitamin B12 deficiency], whose cure was found only a few years later). But Hilbert was still a celebrity around Göttingen, and generating mathematical excitement. (Among “celebrity gossip” mentioned in a letter home by young Russian topologist Pavel Urysohn is that Hilbert was a huge fan of the gramophone, and that even at his advanced age, in the summer, he would sit in a tree to study.) I have been able to find out almost nothing about Schönfinkel’s interaction with Hilbert. However, from April to August 1920 Hilbert gave weekly lectures entitled “Problems of Mathematical Logic” which summarized the standard formalism of the field—and the official notes for those lectures were put together by Moses Schönfinkel and Paul Bernays (the “N” initial for Schönfinkel is a typo): A few months after these lectures came, at least from our perspective today, the highlight of Schönfinkel’s time in Göttingen: the talk he gave on December 7, 1920. The venue was the weekly meeting of the Göttingen Mathematics Society, held at 6pm on Tuesdays. The society wasn’t officially part of the university, but it met in the same university “Auditorium Building” that at the time housed the math institute: The talks at the Göttingen Mathematics Society were listed in the Annual Report of the German Mathematicians Association: There’s quite a lineup. November 9, Ludwig Neder (student of Edmund Landau): “Trigonometric Series”. November 16, Erich Bessel-Hagen (student of Carathéodory): “Discontinuous Solutions of Variational Problems”. November 23, Carl Runge (of Runge–Kutta fame, then a Göttingen professor): “American Work on Star Clusters in the Milky Way”. November 30 Gottfried Rückle (assistant of van der Waals): “Explanations of Natural Laws Using a Statistical Mechanics Basis”. And then: December 7: Moses Schönfinkel, “Elements of Logic”. The next week, December 14, Paul Bernays, who worked with Hilbert and interacted with Schönfinkel, spoke about “Probability, the Arrow of Time and Causality” (yes, there was still a lot of interest around Hilbert in the foundations of physics). January 10+11, Joseph Petzoldt (philosopher of science): “The Epistemological Basis of Special and General Relativity”. January 25, Emmy Noether (of Noether’s theorem fame): “Elementary Divisors and General Ideal Theory”. February 1+8, Richard Courant (of PDE etc. fame) & Paul Bernays: “About the New Arithmetic Theories of Weyl and Brouwer”. February 22, David Hilbert: “On a New Basis for the Meaning of a Number” (yes, that’s foundations of math). What in detail happened at Schönfinkel’s talk, or as a result of it? We don’t know. But he seems to have been close enough to Hilbert that just over a year later he was in a picture taken for David Hilbert’s 60th birthday on January 23, 1922: There are all sorts of well-known mathematicians in the picture (Richard Courant, Hermann Minkowski, Edmund Landau, …) as well as some physicists (Peter Debye, Theodore von Kármán, Ludwig Prandtl, …). And there near the top left is Moses Schönfinkel, sporting a somewhat surprised expression. For his 60th birthday Hilbert was given a photo album—with 44 pages of pictures of altogether about 200 mathematicians (and physicists). And there on page 22 is Moses Schönfinkel: Göttingen University, Cod. Ms. D. Hilbert 754 Göttingen University, Cod. Ms. D. Hilbert 754, Bl. 22 Who are the other people on the page with him? Adolf Kratzer (1893–1983) was a student of Arnold Sommerfeld, and a “physics assistant” to Hilbert. Hermann Vermeil (1889–1959) was an assistant to Hermann Weyl, who worked on differential geometry for general relativity. Heinrich Behmann (1891–1970) was a student of Hilbert and worked on mathematical logic, and we’ll encounter him again later. Finally, Carl Ludwig Siegel (1896–1981) had been a student of Landau and would become a well-known number theorist. Problems Are Brewing There’s a lot that’s still mysterious about Moses Schönfinkel’s time in Göttingen. But we have one (undated) letter written by Nathan Schönfinkel, Moses’s younger brother, presumably in 1921 or 1922 (yes, he romanizes his name “Scheinfinkel” rather than “Schönfinkel”): Göttingen University, Cod. Ms. D. Hilbert 455: 9 Dear Professor! I received a letter from Rabbi Dr. Behrens in which he wrote that my brother was in need, that he was completely malnourished. It was very difficult for me to read these lines, even more so because I cannot help my brother. I haven’t received any messages or money myself for two years. Thanks to the good people where I live, I am protected from severe hardship. I am able to continue my studies. I hope to finish my PhD in 6 months. A few weeks ago I received a letter from my cousin stating that our parents and relatives are healthy. My cousin is in Kishinev (Bessarabia), now in Romania. He received the letter from our parents who live in Ekaterinoslav. Our parents want to help us but cannot do so because the postal connections are nonexistent. I hope these difficulties will not last long. My brother is helpless and impractical in this material world. He is a victim of his great love for science. Even as a 12 year old boy he loved mathematics, and all window frames and doors were painted with mathematical formulas by him. As a high school student, he devoted all his free time to mathematics. When he was studying at the university in Odessa, he was not satisfied with the knowledge there, and his striving and ideal was Göttingen and the king of mathematics, Prof. Hilbert. When he was accepted in Göttingen, he once wrote to me the following: “My dear brother, it seems to me as if I am dreaming but this is reality: I am in Göttingen, I saw Prof. Hilbert, I spoke to Prof. Hilbert.” The war came and with it suffering. My brother, who is helpless, has suffered more than anyone else. But he did not write to me so as not to worry me. He has a good heart. I ask you, dear Professor, for a few months until the connections with our city are established, to help him by finding a suitable (not harmful to his health) job for him. I will be very grateful to you, dear Professor, if you will answer me. Sincerely. N. Scheinfinkel We’ll talk more about Nathan Schönfinkel later. But suffice it to say here that when he wrote the letter he was a physiology graduate student at the University of Bern—and he would get his PhD in 1922, and later became a professor. But the letter he wrote is probably our best single surviving source of information about the situation and personality of Moses Schönfinkel. Obviously he was a serious math enthusiast from a young age. And the letter implies that he’d wanted to work with Hilbert for some time (presumably hence the German classes in college). It also implies that he was financially supported in Göttingen by his parents—until this was disrupted by World War I. (And we learn that his parents were OK in the Russian Revolution.)  (By the way, the rabbi mentioned is probably a certain Siegfried Behrens, who left Göttingen in 1922.) There’s no record of any reply to Nathan Schönfinkel’s letter from Hilbert. But at least by the time of Hilbert’s 60th birthday in 1922 Moses Schönfinkel was (as we saw above) enough in the inner circle to be invited to the birthday party. What else is there in the university archives in Göttingen about Moses Schönfinkel? There’s just one document, but it’s very telling: Göttingen University, Unia GÖ, Sek. 335.55 It’s dated 18 March 1924. And it’s a carbon copy of a reference for Schönfinkel. It’s rather cold and formal, and reads: “The Russian privatdozent [private lecturer] in mathematics, Mr. Scheinfinkel, is hereby certified to have worked in mathematics for ten years with Prof. Hilbert in Göttingen.” It’s signed (with a stylized “S”) by the “University Secretary”, a certain Ludwig Gossmann, who we’ll be talking about later. And it’s being sent to Ms. Raissa Neuburger, at Bühlplatz 5, Bern. That address is where the Physiology Institute at the University of Bern is now, and also was in 1924. And Raissa Neuberger either was then, or soon would become, Nathan Schönfinkel’s wife. But there’s one more thing, handwritten in black ink at the bottom of the document. Dated March 20, it’s another note from the University Secretary. It’s annotated “a.a.”, i.e. ad acta—for the records. And in German it reads: Gott sei Dank, dass Sch weg ist which translates in English as: Thank goodness Sch is gone Hmm. So for some reason at least the university secretary was happy to see Schönfinkel go.  (Or perhaps it was a German 1920s version of an HR notation: “not eligible for rehire”.) But let’s analyze this document in a little more detail. It says Schönfinkel worked with Hilbert for 10 years. That agrees with him having arrived in Göttingen in 1914 (which is a date we know for other reasons, as we’ll see below). But now there’s a mystery. The reference describes Schönfinkel as a “privatdozent”. That’s a definite position at a German university, with definite rules, that in 1924 one would expect to have been rigidly enforced. The basic career track was (and largely still is): first, spend 2–5 years getting a PhD. Then perhaps get recruited for a professorship, or if not, continue doing research, and write a habilitation, after which the university may issue what amounts to an official government “license to teach”, making someone a privatdozent, able to give lectures. Being a privatdozent wasn’t as such a paid gig. But it could be combined with a job like being an assistant to a professor—or something outside the university, like tutoring, teaching high school or working at a company. So if Schönfinkel was a privatdozent in 1924, where is the record of his PhD, or his habilitation? To get a PhD required “formally publishing” a thesis, and printing (as in, on a printing press) at least 20 or so copies of the thesis. A habilitation was typically a substantial, published research paper. But there’s absolutely no record of any of these things for Schönfinkel. And that’s very surprising. Because there are detailed records for other people (like Paul Bernays) who were around at the time, and were indeed privatdozents. And what’s more the Annual Report of the German Mathematicians Association—which listed Schönfinkel’s 1920 talk—seems to have listed mathematical goings-on in meticulous detail. Who gave what talk. Who wrote what paper. And most definitely who got a PhD, did a habilitation or became a privatdozent. (And becoming a privatdozent also required an action of the university senate, which was carefully recorded.) But going through all the annual reports of the German Mathematicians Association we find only four mentions of Schönfinkel. There’s his 1920 talk, and also a 1921 talk with Paul Bernays that we’ll discuss later. There’s the publication of his papers in 1924 and 1927. And there’s a single other entry, which says that on November 4, 1924, Richard Courant gave a report to the Göttingen Mathematical Society about a conference in Innsbruck, where Heinrich Behmann reported on “published work by M. Schönfinkel”. (It describes the work as follows: “It is a continuation of Sheffer’s [1913] idea of replacing the elementary operations of symbolic logic with a single one. By means of a certain function calculus, all logical statements (including the mathematical ones) are represented by three basic signs alone.”) So, it seems, the university secretary wasn’t telling it straight. Schönfinkel might have worked with Hilbert for 10 years.  But he wasn’t a privatdozent. And actually it doesn’t seem as if he had any “official status” at all. So how do we even know that Schönfinkel was in Göttingen from 1914 to 1924? Well, he was Russian, and so in Germany he was an “alien”, and as such he was required to register his address with the local police (no doubt even more so from 1914 to 1918 when Germany was, after all, at war with Russia).  And the remarkable thing is that even after all these years, Schönfinkel’s registration card is still right there in the municipal archives of the city of Göttingen: Stadtarchiv Göttingen, Meldekartei So that means we have all Schönfinkel’s addresses during his time in Göttingen. Of course, there are confusions. There’s yet another birthdate for Schönfinkel: September 4, 1889. Wrong year. Perhaps a wrongly done correction from the Julian calendar. Perhaps “adjusted” for some reason of military service obligations. But, in any case, the document says that Moses Schönfinkel from Ekaterinoslav arrived in Göttingen on June 1, 1914, and started living at 6 Lindenstraße (now Felix-Klein-Strasse). He moved pretty often (11 times in 10 years), not at particularly systematic times of year. It’s not clear exactly what the setup was in all these places, but at least at the end (and in another document) it lists addresses and “with Frau….”, presumably indicating that he was renting a room in someone’s house. Where were all those addresses? Well, here’s a map of Göttingen circa 1920, with all of them plotted (along with a red “M” for the location of the math institute): Stadtarchiv Göttingen, D 2, V a 62 The last item on the registration card says that on March 18, 1924, he departed Göttingen, and went to Moscow. And the note on the copy of the reference saying “thank goodness [he’s] gone” is dated March 20, so that all ties together. But let’s come back to the reference. Who was this “University Secretary” who seems to have made up the claim that Schönfinkel was a privatdozent? It was fairly easy to find out that his name was Ludwig Gossmann. But the big surprise was to find out that the university archives in Göttingen have nearly 500 pages about him—primarily in connection with a “criminal investigation”. Here’s the story. Ludwig Gossmann was born in 1878 (so he was 10 years older than Schönfinkel). He grew up in Göttingen, where his father was a janitor at the university. He finished high school but didn’t go to college and started working for the local government. Then in 1906 (at age 28) he was hired by the university as its “secretary”. The position of “university secretary” was a high-level one. It reported directly to the vice-rector of the university, and was responsible for “general administrative matters” for the university, including, notably, the supervision of international students (of whom there were many, Schönfinkel being one). Ludwig Gossmann held the position of university secretary for 27 years—even while the university had a different rector (normally a distinguished academic) every year. But Mr. Gossmann also had a sideline: he was involved in real estate. In the 1910s he started building houses (borrowing money from, among others, various university professors). And by the 1920s he had significant real estate holdings—and a business where he rented to international visitors and students at the university. Years went by. But then, on January 24, 1933, the newspaper headline announced: “Sensational arrest: senior university official Gossmann arrested on suspicion of treason—communist revolution material [Zersetzungsschrift] confiscated from his apartment”. It was said that perhaps it was a setup, and that he’d been targeted because he was gay (though, a year earlier, at age 54, he did marry a woman named Elfriede). Göttingen University, Kur 3730, Sek 356 2 This was a bad time to be accused of being a communist (Hitler would become chancellor less than a week later, on January 30, 1933, in part propelled by fears of communism). Gossmann was taken to Hanover “for questioning”, but was then allowed back to Göttingen “under house arrest”. He’d had health problems for several years, and died of a heart attack on February 24, 1933. But none of this really helps us understand why Gossmann would go out on a limb to falsify the reference for Schönfinkel. We can’t specifically find an address match, but perhaps Schönfinkel had at least at some point been a tenant of Gossmann’s. Perhaps he still owed rent. Perhaps he was just difficult in dealing with the university administration. It’s not clear. It’s also not clear why the reference Gossmann wrote was sent to Schönfinkel’s brother in Bern, even though Schönfinkel himself was going to Moscow. Or why it wasn’t just handed to Schönfinkel before he left Göttingen.
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.6 The 1924 Paper : Whatever was going on with Schönfinkel in Göttingen in 1924, we know one thing for sure: it was then that he published his remarkable paper about what are now called combinators. Let’s talk in a bit more detail about the paper—though the technicalities I’m discussing elsewhere. First, there’s some timing. At the end of the paper, it says it was received by the journal on March 15, 1924, i.e. just three days before the date of Ludwig Gossmann’s reference for Schönfinkel.  And then at the top of the paper, there’s something else: under Schönfinkel’s name it says “in Moskau”, i.e. at least as far as the journal was concerned, Schönfinkel was in Moscow, Russia, at the time the article was published: There’s also a footnote on the first page of the paper: “The following thoughts were presented by the author to the Mathematical Society in Göttingen on December 7, 1920. Their formal and stylistic processing for this publication was done by H. Behmann in Göttingen.” The paper itself is written in a nice, clear and mathematically mature way. Its big result (as I’ve discussed elsewhere) is the introduction of what would later be called combinators: two abstract constructs from which arbitrary functions and computations can be built up. Schönfinkel names one of them S, after the German word “Schmelzen” for “fusion”. The other has become known as K, although Schönfinkel calls it C, even though the German word for “constancy” (which is what would naturally describe it) is “Konstantheit”, which starts with a K. The paper ends with three paragraphs, footnoted with “The considerations that follow are the editor’s” (i.e. Behmann’s). They’re not as clear as the rest of the paper, and contain a confused mistake. The main part of the paper is “just math” (or computation, or whatever). But here’s the page where S and K (called C here) are first used: And now there’s something more people-oriented: a footnote to the combinator equation I = SCC saying “This reduction was communicated to me by Mr. Boskowitz; some time before that, Mr. Bernays had called the somewhat less simple one (SC)(CC) to my attention.” In other words, even if nothing else, Schönfinkel had talked to Boskowitz and Bernays about what he was doing. OK, so we’ve got three people—in addition to David Hilbert—somehow connected to Moses Schönfinkel. Let’s start with Heinrich Behmann—the person footnoted as “processing” Schönfinkel’s paper for publication: He was born in Bremen, Germany, in 1891, making him a couple of years younger than Schönfinkel. He arrived in Göttingen as a student in 1911, and by 1914 was giving a talk about Whitehead and Russell’s Principia Mathematica (which had been published in 1910). When World War I started he volunteered for military service, and in 1915 he was wounded in action in Poland (receiving an Iron Cross)—but in 1916 he was back in Göttingen studying under Hilbert, and in 1918 he wrote his PhD thesis on “The Antinomy of the Transfinite Number and Its Resolution by the Theory of Russell and Whitehead” (i.e. using the idea of types to deal with paradoxes associated with infinity). Behmann continued in the standard academic track (i.e. what Schönfinkel apparently didn’t do)—and in 1921 he got his habilitation with the thesis “Contributions to the Algebra of Logic, in Particular to the Entscheidungsproblem [Decision Problem]”. There’d been other decision problems discussed before, but Behmann said what he meant was a “procedure [giving] complete instructions for determining whether a [logical or mathematical] assertion is true or false by a deterministic calculation after finitely many steps”. And, yes, Alan Turing’s 1936 paper “On Computable Numbers, with an Application to the Entscheidungsproblem” was what finally established that the halting problem, and therefore the Entscheidungsproblem, was undecidable. Curiously, in principle, there should have been enough in Schönfinkel’s paper that this could have been figured out back in 1921 if Behmann or others had been thinking about it in the right way (which might have been difficult before Gödel’s work). So what happened to Behmann? He continued to work on mathematical logic and the philosophy of mathematics. After his habilitation in 1921 he became a privatdozent at Göttingen (with a job as an assistant in the applied math institute), and then in 1925 got a professorship in Halle in applied math—though having been an active member of the Nazi Party since 1937, lost this professorship in 1945 and became a librarian. He died in 1970. (By the way, even though in 1920 “PM” [Principia Mathematica] was hot—and Behmann was promoting it—Schönfinkel had what in my opinion was the good taste to not explicitly mention it in his paper, referring only to Hilbert’s much-less-muddy ideas about the formalization of mathematics.) OK, so what about Boskowitz, credited in the footnote with having discovered the classic combinator result I = SKK? That was Alfred Boskowitz, in 1920 a 23-year-old Jewish student at Göttingen, who came from Budapest, Hungary, and worked with Paul Bernays on set theory. Boskowitz is notable for having contributed far more corrections (nearly 200) to Principia Mathematica than anyone else, and being acknowledged (along with Behmann) in a footnote in the (1925–27) second edition. (This edition also gives a reference to Schönwinkel’s [sic] paper at the end of a list of 14 “other contributions to mathematical logic” since the first edition.) In the mid-1920s Boskowitz returned to Budapest. In 1936 he wrote to Behmann that anti-Jewish sentiment there made him concerned for his safety. There’s one more known communication from him in 1942, then no further trace. The third person mentioned in Schönfinkel’s paper is Paul Bernays, who ended up living a long and productive life, mostly in Switzerland. But we’ll come to him later. So where was Schönfinkel’s paper published? It was in a journal called Mathematische Annalen (Annals of Mathematics)—probably the top math journal of the time. Here’s its rather swank masthead, with quite a collection of famous names (including physicists like Einstein, Born and Sommerfeld): The “instructions to contributors” on the inside cover of each issue had a statement from the “Editorial Office” about not changing things at the proof stage because “according to a calculation they [cost] 6% of the price of a volume”. The instructions then go on to tell people to submit papers to the editors—at their various home addresses (it seems David Hilbert lived just down the street from Felix Klein…): Here’s the complete table of contents for the volume in which Schönfinkel’s paper appears: There are a variety of famous names here. But particularly notable for our purposes are Aleksandr Khintchine (of Khinchin constant fame) and the topologists Pavel Alexandroff and Pavel Urysohn, who were all from Moscow State University, and who are all indicated, like Schönfinkel, as being “in Moscow”. There’s a little bit of timing information here. Schönfinkel’s paper was indicated as having been received by the journal on March 15, 1924. The “thank goodness [he’s] gone [from Göttingen]” comment is dated March 20. Meanwhile, the actual issue of the journal with Schönfinkel’s article (number 3 of 4) was published September 15, with table of contents: But note the ominous † next to Urysohn’s name. Turns out his fatal swimming accident was August 17, so—notwithstanding their admonitions—the journal must have added the † quite quickly at the proof stage.
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.7 The “1927” Paper : Beyond his 1924 paper on combinators, there’s only one other known piece of published output from Moses Schönfinkel: a paper coauthored with Paul Bernays “On the Decision Problem of Mathematical Logic”: It’s actually much more widely cited than Schönfinkel’s 1924 combinator paper, but it’s vastly less visionary and ultimately much less significant; it’s really about a technical point in mathematical logic. About halfway through the paper it has a note: “The following thoughts were inspired by Hilbert’s lectures on mathematical logic and date back several years. The decision procedure for a single function F(x, y) was derived by M. Schönfinkel, who first tackled the problem; P. Bernays extended the method to several logical functions, and also wrote the current paper.” The paper was submitted on March 24, 1927. But in the records of the German Mathematicians Association we find a listing of another talk at the Göttingen Mathematical Society: December 6, 1921, P. Bernays and M. Schönfinkel, “Das Entscheidungsproblem im Logikkalkul”.  So the paper had a long gestation period, and (as the note in the paper suggests) it basically seems to have fallen to Bernays to get it written, quite likely with little or no communication with Schönfinkel. So what else do we know about it? Well, remarkably enough, the Bernays archive contains two notebooks (the paper kind!) by Moses Schönfinkel that are basically an early draft of the paper (with the title already being the same as it finally was, but with Schönfinkel alone listed as the author): ETH Zurich, Bernays Archive, Hs. 974: 282 These notebooks are basically our best window into the front lines of Moses Schönfinkel’s work. They aren’t dated as such, but at the end of the second notebook there’s a byline of sorts, that lists his street address in Göttingen—and we know he lived at that address from September 1922 until March 1924: OK, so what’s in the notebooks? The first page might indicate that the notebooks were originally intended for a different purpose. It’s just a timetable of lectures: “Hilbert lectures: Monday: Mathematical foundations of quantum theory; Thursday: Hilbert–Bernays: Foundations of arithmetic; Saturday: Hilbert: Knowledge and mathematical thinking”. (There’s also a slightly unreadable note that seems to say “Hoppe. 6–8… electricity”, perhaps referring to Edmund Hoppe, who taught physics in Göttingen, and wrote a history of electricity.) But then we’re into 15 pages (plus 6 in the other notebook) of content, written in essentially perfect German, but with lots of parentheticals of different possible word choices: The final paper as coauthored with Bernays begins: “The central problem of mathematical logic, which is also closely connected to its axiomatic foundations, is the decision problem [Entscheidungsproblem]. And it deals with the following. We have logical formulas which contain logic functions, predicates, …” Schönfinkel’s version begins considerably more philosophically (here with a little editing for clarity): “Generality has always been the main goal—the ideal of the mathematician. Generality in the solution, in the method, in the concept and formulation of the theorem, in the problem and question. This tendency is even more pronounced and clearer with modern mathematicians than with earlier ones, and reaches its high point in the work of Hilbert and Ms. Noether. Such an ideal finds its most extreme expression when one faces the problem of ‘solving all problems’—at least all mathematical problems, because everything else after is easy, as soon as this ‘Gordian Knot’ is cut (because the world is written in ‘mathematical letters’ according to Hilbert). In just the previous century mathematicians would have been extremely skeptical and even averse to such fantasies… But today’s mathematician has already been trained and tested in the formal achievements of modern mathematics and Hilbert’s axiomatics, and nowadays one has the courage and the boldness to dare to touch this question as well. We owe to mathematical logic the fact that we are able to have such a question at all. From Leibniz’s bold conjectures, the great logician-mathematicians went step by step in pursuit of this goal, in the systematic structure of mathematical logic: Boole (discoverer of the logical calculus), (Bolzano?), Ernst Schröder, Frege, Peano, Ms. Ladd-Franklin, the two Peirces, Sheffer, Whitehead, Couturat, Huntington, Padoa, Shatunovsky, Sleshinsky, Kagan, Poretsky, Löwenheim, Skolem, … and their numerous students, collaborators and contemporaries … until in 1910–1914 ‘the system’ by Bertrand Russell and Whitehead appeared—the famous ‘Principia Mathematica’—a mighty titanic work, a large system. Finally came our knowledge of logic from Hilbert’s lectures on (the algebra of) logic (-calculus) and, following on from this, the groundbreaking work of Hilbert’s students: Bernays and Behmann. The investigations of all these scholars and researchers have led (in no uncertain terms) to the fact that it has become clear that actual mathematics represents a branch of logic. … This emerges most clearly from the treatment and conception of mathematical logic that Hilbert has given. And now, thanks to Hilbert’s approach, we can (satisfactorily) formulate the great decision problem of mathematical logic.” We learn quite a bit about Schönfinkel from this. Perhaps the most obvious thing is that he was a serious fan of Hilbert and his approach to mathematics (with a definite shout-out to “Ms. Noether”). It’s also interesting that he refers to Bernays and Behmann as “students” of Hilbert. That’s pretty much correct for Behmann. But Bernays (as we’ll see soon) was more an assistant or colleague of Hilbert’s than a student. It gives interesting context to see Schönfinkel rattle off a sequence of contributors to what he saw as the modern view of mathematical logic. He begins—quite rightly I think—mentioning “Leibniz’s bold conjectures”. He’s not sure whether Bernard Bolzano fits (and neither am I). Then he lists Schröder, Frege and Peano—all pretty standard choices, involved in building up the formal structure of mathematical logic. Next he mentions Christine Ladd-Franklin. At least these days, she’s not particularly well known, but she had been a mathematical logic student of Charles Peirce, and in 1881 she’d written a paper about the “Algebra of Logic” which included a truth table, a solid 40 years before Post or Wittgenstein. (In 1891 she had also worked in Göttingen on color vision with the experimental psychologist Georg Müller—who was still there in 1921.) It’s notable that Schönfinkel mentions Ladd-Franklin ahead of the father-and-son Peirces. Next we see Sheffer, who Schönfinkel quotes in connection with Nand in his combinator paper. (No doubt unbeknownst to Schönfinkel, Henry Sheffer—who spent most of his life in the US—was also born in Ukraine [“near Odessa”, his documents said], and was also Jewish, and was just 6 years older than Schönfinkel.) I’m guessing Schönfinkel mentions Whitehead next in connection with universal algebra, rather than his later collaboration with Russell. Next comes Louis Couturat, who frankly wouldn’t have made my list for mathematical logic, but was another “algebra of logic” person, as well as a Leibniz fan, and developer of the Ido language offshoot from Esperanto. Huntington was involved in the axiomatization of Boolean algebra; Padoa was connected to Peano’s program. Shatunovsky, Sleshinsky and Kagan were all professors of Schönfinkel’s in Odessa (as mentioned above), concerned in various ways with foundations of mathematics. Platon Poretsky I must say I had never heard of before; he seems to have done fairly technical work on propositional logic. And finally Schönfinkel lists Löwenheim and Skolem, both of whom are well known in mathematical logic today. I consider it rather wonderful that Schönfinkel refers to Whitehead and Russell’s Principia Mathematica as a “titanic work” (Titanenwerk). The showy and “overconfident” Titanic had come to grief on its iceberg in 1912, somehow reminiscent of Principia Mathematica, eventually coming to grief on Gödel’s theorem. At first it might just seem charming—particularly in view of his brother’s comment that “[Moses] is helpless and impractical in this material world”—to see Schönfinkel talk about how after one’s solved all mathematical problems, then solving all problems will be easy, explaining that, after all, Hilbert has said that “the world is written in ‘mathematical letters’”. He says that in the previous century mathematicians wouldn’t have seriously considered “solving everything”, but now, because of progress in mathematical logic, “one has the courage and the boldness to dare to touch this question”. It’s very easy to see this as naive and unworldly—the writing of someone who knew only about mathematics. But though he didn’t have the right way to express it, Schönfinkel was actually onto something, and something very big. He talks at the beginning of his piece about generality, and about how recent advances in mathematical logic embolden one to pursue it. And in a sense he was very right about this. Because mathematical logic—through work like his—is what led us to the modern conception of computation, which really is successful in “talking about everything”. Of course, after Schönfinkel’s time we learned about Gödel’s theorem and computational irreducibility, which tell us that even though we may be able to talk about everything, we can never expect to “solve every problem” about everything. But back to Schönfinkel’s life and times. The remainder of Schönfinkel’s notebooks give the technical details of his solution to a particular case of the decision problem. Bernays obviously worked through these, adding more examples as well as some generalization. And Bernays cut out Schönfinkel’s philosophical introduction, no doubt on the (probably correct) assumption that it would seem too airy-fairy for the paper’s intended technical audience.
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.8 So who was Paul Bernays? : Here’s a picture of him from 1928: Bernays was almost exactly the same age as Schönfinkel (he was born on October 17, 1888—in London, where there was no calendar issue to worry about). He came from an international business family, was a Swiss citizen and grew up in Paris and Berlin. He studied math, physics and philosophy with a distinguished roster of professors in Berlin and Göttingen, getting his PhD in 1912 with a thesis on analytic number theory. After his PhD he went to the University of Zurich, where he wrote a habilitation (on complex analysis), and became a privatdozent (yes, with the usual documentation, that can still be found), and an assistant to Ernst Zermelo (of ZFC set theory fame). But in 1917 Hilbert visited Zurich and soon recruited Bernays to return to Göttingen. In Göttingen, for apparently bureaucratic reasons, Bernays wrote a second habilitation, this time on the axiomatic structure of Principia Mathematica (again, all the documentation can still be found). Bernays was also hired to work as a “foundations of math assistant” to Hilbert. And it was presumably in that capacity that he—along with Moses Schönfinkel—wrote the notes for Hilbert’s 1920 course on mathematical logic. Unlike Schönfinkel, Bernays followed a fairly standard—and successful—academic track. He became a professor in Göttingen in 1922, staying there until he was dismissed (because of partially Jewish ancestry) in 1933—after which he moved back to Zurich, where he stayed and worked very productively, mostly in mathematical logic (von Neumann–Bernays–Gödel set theory, etc.), until he died in 1977. Back when he was in Göttingen one of the things Bernays did with Hilbert was to produce the two-volume classic Grundlagen der Mathematik (Foundations of Mathematics). So did the Grundlagen mention Schönfinkel? It has one mention of the Bernays–Schönfinkel paper, but no direct mention of combinators. However, there is one curious footnote: This starts “A system of axioms that is sufficient to derive all true implicational formulas was first set up by M. Schönfinkel…”, then goes on to discuss work by Alfred Tarski. So do we have evidence of something else Schönfinkel worked on? Probably. In ordinary logic, one starts from an axiom system that gives relations, say about And, Or and Not. But, as Sheffer established in 1910, it’s also possible to give an axiom system purely in terms of Nand (and, yes, I’m proud to say that I found the very simplest such axiom system in 2000). Well, it’s also possible to use other bases for logic. And this footnote is about using Implies as the basis. Actually, it’s implicational calculus, which isn’t as strong as ordinary logic, in the sense that it only lets you prove some of the theorems. But there’s a question again: what are the possible axioms for implicational calculus? Well, it seems that Schönfinkel found a possible set of such axioms, though we’re not told what they were; only that Tarski later found a simpler set. (And, yes, I looked for the simpler axiom systems for implicational calculus in 2000, but didn’t find any.) So again we see Schönfinkel in effect trying to explore the lowest-level foundations of mathematical logic, though we don’t know any details. So what other interactions did Bernays have with Schönfinkel? There seems to be no other information in Bernays’s archives. But I have been able to get a tiny bit more information. In a strange chain of connections, someone who’s worked on Mathematica and Wolfram Language since 1987 is Roman Maeder. And Roman’s thesis advisor (at ETH Zurich) was Erwin Engeler—who was a student of Paul Bernays. Engeler (who is now in his 90s) worked for many years on combinators, so of course I had to ask him what Bernays might have told him about Schönfinkel. He told me he recalled only two conversations. He told me he had the impression that Bernays found Schönfinkel a difficult person. He also said he believed that the last time Bernays saw Schönfinkel it was in Berlin, and that Schönfinkel was somehow in difficult circumstances. Any such meeting in Berlin would have had to be before 1933. But try as we might to track it down, we haven’t succeeded.
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.9 To Moscow and Beyond… : In the space of three days in March 1924 Moses Schönfinkel—by then 35 years old—got his paper on combinators submitted to Mathematische Annalen, got a reference for himself sent out, and left for Moscow. But why did he go to Moscow? We simply don’t know. A few things are clear, though. First, it wasn’t difficult to get to Moscow from Göttingen at that time; there was pretty much a direct train there. Second, Schönfinkel presumably had a valid Russian passport (and, one assumes, didn’t have any difficulties from not having served in the Russian military during World War I). One also knows that there was a fair amount of intellectual exchange and travel between Göttingen and Moscow. The very same volume of Mathematische Annalen in which Schönfinkel’s paper was published has three (out of 19) authors in addition to Schönfinkel listed as being in Moscow: Pavel Alexandroff, Pavel Urysohn and Aleksandr Khintchine. Interestingly, all of these people were at Moscow State University. And we know there was more exchange with that university. Nikolai Luzin, for example, got his PhD in Göttingen in 1915, and went on to be a leader in mathematics at Moscow State University (until he was effectively dismissed by Stalin in 1936). And we know that for example in 1930, Andrei Kolmogorov, having just graduated from Moscow State University, came to visit Hilbert. Did Schönfinkel go to Moscow State University? We don’t know (though we haven’t yet been able to access any archives that may be there). Did Schönfinkel go to Moscow because he was interested in communism? Again, we don’t know. It’s not uncommon to find mathematicians ideologically sympathetic to at least the theory of communism. But communism doesn’t seem to have particularly been a thing in the mathematics or general university community in Göttingen. And indeed when Ludwig Gossmann was arrested in 1933, investigations of who he might have recruited into communism didn’t find anything of substance. Still, as I’ll discuss later, there is a tenuous reason to think that Schönfinkel might have had some connection to Leon Trotsky’s circle, so perhaps that had something to do with him going to Moscow—though it would have been a bad time to be involved with Trotsky, since by 1925 he was already out of favor with Stalin. A final theory is that Schönfinkel might have had relatives in Moscow; at least it looks as if some of his Lurie cousins ended up there. But realistically we don’t know. And beyond the bylines on the journals, we don’t really have any documentary evidence that Schönfinkel was in Moscow. However, there is one more data point, from November 1927 (8 months after the submission of Schönfinkel’s paper with Bernays). Pavel Alexandroff was visiting Princeton University, and when Haskell Curry (who we’ll meet later) asked him about Schönfinkel he was apparently told that “Schönfinkel has… gone insane and is now in a sanatorium & will probably not be able to work any more.” Ugh! What happened? Once again, we don’t know. Schönfinkel doesn’t seem to have ever been “in a sanatorium” while he was in Göttingen; after all, we have all his addresses, and none of them were sanatoria. Maybe there’s a hint of something in Schönfinkel’s brother’s letter to Hilbert. But are we really sure that Schönfinkel actually suffered from mental illness? There’s a bunch of hearsay that says he did. But then it’s a common claim that logicians who do highly abstract work are prone to mental illness (and, well, yes, there are a disappointingly large number of historical examples). Mental illness wasn’t handled very well in the 1920s. Hilbert’s only child, his son Franz (who was about five years younger than Schönfinkel), suffered from mental illness, and after a delusional episode that ended up with him in a clinic, David Hilbert simply said “From now on I have to consider myself as someone who does not have a son”. In Moscow in the 1920s—despite some political rhetoric—conditions in psychiatric institutions were probably quite poor, and there was for example quite a bit of use of primitive shock therapy (though not yet electroshock). It’s notable, by the way, that Curry reports that Alexandroff described Schönfinkel as being “in a sanatorium”. But while at that time the word “sanatorium” was being used in the US as a better term for “insane asylum”, in Russia it still had more the meaning of a place for a rest cure. So this still doesn’t tell us if Schönfinkel was in fact “institutionalized”—or just “resting”. (By the way, if there was mental illness involved, another connection for Schönfinkel that doesn’t seem to have been made is that Paul Bernays’s first cousin once removed was Martha Bernays, wife of Sigmund Freud.) Whether or not he was mentally ill, what would it have been like for Schönfinkel in what was then the Soviet Union in the 1920s? One thing is that in the Soviet system, everyone was supposed to have a job. So Schönfinkel was presumably employed doing something—though we have no idea what. Schönfinkel had presumably been at least somewhat involved with the synagogue in Göttingen (which is how the rabbi there knew to tell his brother he was in bad shape). There was a large and growing Jewish population in Moscow in the 1920s, complete with things like Yiddish newspapers. But by the mid 1930s it was no longer so comfortable to be Jewish in Moscow, and Jewish cultural organizations were being shut down. By the way, in the unlikely event that Schönfinkel was involved with Trotsky, there could have been trouble even by 1925, and certainly by 1929. And it’s notable that it was a common tactic for Stalin (and others) to claim that their various opponents were “insane”. So what else do we know about Schönfinkel in Moscow? It’s said that he died there in 1940 or 1942, aged 52–54. Conditions in Moscow wouldn’t have been good then; the so-called Battle of Moscow occurred in the winter of 1941. And there are various stories told about Schönfinkel’s situation at that time. The closest to a primary source seems to be a summary of mathematical logic in the Soviet Union, written by Sofya Yanovskaya in 1948. Yanovskaya was born in 1896 (so 8 years after Schönfinkel), and grew up in Odessa. She attended the same university there as Schönfinkel, studying mathematics, though arrived five years after Schönfinkel graduated. She had many of the same professors as Schönfinkel, and, probably like Schönfinkel, was particularly influenced by Shatunovsky. When the Russian Revolution happened, Yanovskaya went “all in”, becoming a serious party operative, but eventually began to teach, first at the Institute of Red Professors, and then from 1925 at Moscow State University—where she became a major figure in mathematical logic, and was eventually awarded the Order of Lenin. One might perhaps have thought that mathematical logic would be pretty much immune to political issues. But the founders of communism had talked about mathematics, and there was a complex debate about the relationship between Marxist–Leninist ideology and formal ideas in mathematics, notably the Law of Excluded Middle. Sofya Yanovskaya was deeply involved, initially in trying to “bring mathematics to heel”, but later in defending it as a discipline, as well as in editing Karl Marx’s mathematical writings. It’s not clear to what extent her historical writings were censored or influenced by party considerations, but they certainly contain lots of good information, and in 1948 she wrote a paragraph about Schönfinkel: “The work of M. I. Sheinfinkel played a substantial role in the further development of mathematical logic. This brilliant student of S. O. Shatunovsky, unfortunately, left us early. (After getting mentally ill [заболев душевно], M. I. Sheinfinkel passed away in Moscow in 1942.) He did the work mentioned here in 1920, but only published it in 1924, edited by Behmann.” Unless she was hiding things, this quote doesn’t make it sound as if Yanovskaya knew much about Schönfinkel. (By the way, her own son was apparently severely mentally ill.) A student of Jean van Heijenoort (who we’ll encounter later) named Irving Anellis did apparently in the 1990s ask a student of Yanovskaya’s whether Yanovskaya had known Schönfinkel. Apparently he responded that unfortunately nobody had thought to ask her that question before she died in 1966. What else do we know? Nothing substantial. The most extensively embellished story I’ve seen about Schönfinkel appears in an anonymous comment on the talk page for the Wikipedia entry about Schönfinkel: “William Hatcher, while spending time in St Petersburg during the 1990s, was told by Soviet mathematicians that Schönfinkel died in wretched poverty, having no job and but one room in a collective apartment. After his death, the rough ordinary people who shared his apartment burned his manuscripts for fuel (WWII was raging). The few Soviet mathematicians around 1940 who had any discussions with Schönfinkel later said that those mss reinvented a great deal of 20th century mathematical logic. Schönfinkel had no way of accessing the work of Turing, Church, and Tarski, but had derived their results for himself. Stalin did not order Schönfinkel shot or deported to Siberia, but blame for Schönfinkel’s death and inability to publish in his final years can be placed on Stalin’s doorstep.  202.36.179.65 06:50, 25 February 2006 (UTC)” William Hatcher was a mathematician and philosopher who wrote extensively about the Baháʼí Faith and did indeed spend time at the Steklov Institute of Mathematics in Saint Petersburg in the 1990s—and mentioned Schönfinkel’s technical work in his writings. People I’ve asked at the Steklov Institute do remember Hatcher, but don’t know anything about what it’s claimed he was told about Schönfinkel. (Hatcher died in 2005, and I haven’t been successful at getting any material from his archives.) So are there any other leads? I did notice that the IP address that originated the Wikipedia comment is registered to the University of Canterbury in New Zealand. So I asked people there and in the New Zealand foundations of math scene. But despite a few “maybe so-and-so wrote that” ideas, nobody shed any light. OK, so what about at least a death certificate for Schönfinkel?  Well, there’s some evidence that the registry office in Moscow has one. But they tell us that in Russia only direct relatives can access death certificates….
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.10 Other Schönfinkels… : So far as we know, Moses Schönfinkel never married, and didn’t have children. But he did have a brother, Nathan, who we encountered earlier in connection with the letter he wrote about Moses to David Hilbert. And in fact we know quite a bit about Nathan Scheinfinkel (as he normally styled himself). Here’s a biographical summary from 1932: Deutsches Biographisches Archiv, II 1137, 103 The basic story is that he was about five years younger than Moses, and went to study medicine at the University of Bern in Switzerland in April 1914 (i.e. just before World War I began).  He got his MD in 1920, then got his PhD on “Gas Exchange and Metamorphosis of Amphibian Larvae after Feeding on the Thyroid Gland or Substances Containing Iodine” in 1922. He did subsequent research on the electrochemistry of the nervous system, and in 1929 became a privatdozent—with official “license to teach” documentation: Staatsarchiv des Kantons Bern, BB IIIb 557 Scheinfinkel N. (In a piece of bizarre small-worldness, my grandfather, Max Wolfram, also got a PhD in the physiology [veterinary medicine] department at the University of Bern [studying the function of the thymus gland], though that was in 1909, and presumably he had left before Nathan Scheinfinkel arrived.) But in any case, Nathan Scheinfinkel stayed at Bern, eventually becoming a professor, and publishing extensively, including in English. He became a Swiss citizen in 1932, with the official notice stating: “Scheinfinkel, Nathan. Son of Ilia Gerschow and Mascha [born] Lurie, born in Yekaterinoslav, Russia, September 13, 1893 (old style). Doctor of medicine, residing in Bern, Neufeldstrasse 5a, husband of Raissa [born] Neuburger.” In 1947, however, he moved to become a founding professor in a new medical school in Ankara, Turkey. (Note that Turkey, like Switzerland, had been neutral in World War II.) In 1958 he moved again, this time to found the Institute of Physiology at Ege University in Izmir, Turkey, and then at age 67, in 1961, he retired and returned to Switzerland. Did Nathan Scheinfinkel have children (whose descendents, at least, might know something about “Uncle Moses”)? It doesn’t seem so. We tracked down Nuran Harirî, now an emeritus professor, but in the 1950s a young physiology resident at Ege University responsible for translating Nathan Scheinfinkel’s lectures into Turkish. She said that Nathan Scheinfinkel was at that point living in campus housing with his wife, but she never heard mention of any children, or indeed of any other family members. What about any other siblings? Amazingly, looking through handwritten birth records from Ekaterinoslav, we found one! Debora Schönfinkel, born December 22, 1889 (i.e. January 3, 1890, in the modern calendar): So Moses Schönfinkel had a younger sister, as well as a younger brother. And we even know that his sister graduated from high school in June 1907. But we don’t know anything else about her, or about other siblings. We know that Schönfinkel’s mother died in 1936, at the age of 74. Might there have been other Schönfinkel relatives in Ekaterinoslav? Perhaps, but it’s unlikely they survived World War II—because in one of those shocking and tragic pieces of history, over a four-day period in February 1942 almost the whole Jewish population of 30,000 was killed. Could there be other Schönfinkels elsewhere? The name is not common, but it does show up (with various spellings and transliterations), both before and after Moses Schönfinkel. There’s a Scheinfinkel Russian revolutionary buried in the Kremlin Wall; there was a Lovers of Zion delegate Scheinfinkel from Ekaterinoslav. There was a Benjamin Scheinfinkel in New York City in the 1940s; a Shlomo Scheinfinkel in Haifa in the 1930s. There was even a certain curiously named Bas Saul Haskell Scheinfinkel born in 1875. But despite quite a bit of effort, I’ve been unable to locate any living relative of Moses Schönfinkel. At least so far.
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.11 Haskell Curry : What happened with combinators after Schönfinkel published his 1924 paper? Initially, so far as one can tell, nothing. That is, until Haskell Curry found Schönfinkel’s paper in the library at Princeton University in November 1927—and launched into a lifetime of work on combinators. Who was Haskell Curry? And why did he know to care about Schönfinkel’s paper? Haskell Brooks Curry was born on September 12, 1900, in a small town near Boston, MA. His parents were both elocution educators, who by the time Haskell Curry was born were running the School of Expression (which had evolved from his mother’s Boston-based School of Elocution and Expression). (Many years later, the School of Expression would evolve into Curry College in Waltham, Massachusetts—which happens to be where for several years we held our Wolfram Summer School, often noting the “coincidence” of names when combinators came up.) Haskell Curry went to college at Harvard, graduating in mathematics in 1920. After a couple of years doing electrical engineering, he went back to Harvard, initially working with Percy Bridgman, who was primarily an experimental physicist, but was writing a philosophy of science book entitled The Logic of Modern Physics. And perhaps through this Curry got introduced to Whitehead and Russell’s Principia Mathematica. But in any case, there’s a note in his archive about Principia Mathematica dated May 20, 1922: Haskell P. Curry papers, PSUA 222, Special Collections Library, Pennsylvania State University Curry seems—perhaps like an electrical engineer or a “pre-programmer”—to have been very interested in the actual process of mathematical logic, starting his notes with: “No logical process is possible without the phenomenon of substitution.” He continued, trying to break down the process of substitution. But then his notes end, more philosophically, and perhaps with “expression” influence: “Phylogenetic origin of logic: 1. Sensation; 2. Association: Red hot poker–law of permanence”. At Harvard Curry started working with George Birkhoff towards a PhD on differential equations. But by 1927–8 he had decided to switch to logic, and was spending a year as an instructor at Princeton. And it was there—in November 1927—that he found Schönfinkel’s paper. Preserved in his archives are the notes he made: Haskell P. Curry papers, PSUA 222, Special Collections Library, Pennsylvania State University At the top there’s a date stamp of November 28, 1927. Then Curry writes: “This paper anticipates much of what I have done”—then launches into a formal summary of Schönfinkel’s paper (charmingly using f@x to indicate function application—just as we do in Wolfram Language, except his is left associative…). He ends his “report” with “In criticism I might say that no formal development have been undertaken in the above. Equality is taken intuitively and such things as universality, and proofs of identity are shown on the principle that if for every z, x@z : y@z then x=y ….” But then there’s another piece: “On discovery of this paper I saw Prof. Veblen. Schönfinkel’s paper said ‘in Moskau’. Accordingly we sought out Paul Alexandroff. The latter says Schönfinkel has since gone insane and is now in a sanatorium & will probably not be able to work any more. The paper was written with help of Paul Bernays and Behman [sic]; who would presumably be the only people in the world who would write on that subject.” What was the backstory to this? Oswald Veblen was a math professor at Princeton who had worked on the axiomatization of geometry and was by then working on topology. Pavel Alexandroff (who we encountered earlier) was visiting from Moscow State University for the year, working on topology with Hopf, Lefschetz, Veblen and Alexander. I’m not quite sure why Curry thought Bernays and Behmann “would be the only people in the world who would write on that subject”; I don’t see how he could have known. Curry continues: “It was suggested I write to Bernays, who is außerord. prof. [long-term lecturer] at Göttingen.”  But then he adds—in depressingly familiar academic form: “Prof. Veblen thought it unwise until I had something definite ready to publish.” “A footnote to Schönfinkel’s paper said the ideas were presented before Math Gesellschaft in Göttingen on Dec. 7, 1920 and that its formal and elegant [sic] write up was due to H. Behman”. “Elegant” is a peculiar translation of “stilistische” that probably gives Behmann too much credit; a more obvious translation might be “stylistic”. Curry continues: “Alexandroff’s statements, as I interpret them, are to the effect that Bernays, Behman, Ackermann, von Neumann, Schönfinkel & some others form a small school of math logicians working on this & similar topics in Göttingen.” And so it was that Curry resolved to study in Göttingen, and do his PhD in logic there. But before he left for Göttingen, Curry wrote a paper (published in 1929): Already there’s something interesting in the table of contents: the use of the word “combinatory”, which, yes, in Curry’s care is going to turn into “combinator”. The paper starts off reading a bit like a student essay, and one’s not encouraged by a footnote a few pages in: “In writing the foregoing account I have naturally made use of any ideas I may have gleaned from reading the literature. The writings of Hilbert are fundamental in this connection. I hope that I have added clearness to certain points where the existing treatments are obscure.” [“Clearness” not “clarity”?] Then, towards the end of the “Preliminary Discussion” is this: And the footnote says: “See the paper of Schönfinkel cited below”. It’s (so far as I know) the first-ever citation to Schönfinkel’s paper! On the next page Curry starts to give details. Curry starts talking about substitution, then says (in an echo of modern symbolic language design) this relates to the idea of “transformation of functions”: At first he’s off talking about all the various combinatorial arrangements of variables, etc. But then he introduces Schönfinkel—and starts trying to explain in a formal way what Schönfinkel did. And even though he says he’s talking about what one assumes is structural substitution, he seems very concerned about what equality means, and how Schönfinkel didn’t quite define that. (And, of course, in the end, with universal computation, undecidability, etc. we know that the definition of equality wasn’t really accessible in the 1920s.) By the next page, here we are, S and K (Curry renamed Schönfinkel’s C): The rest of the paper is basically concerned with setting up combinators that can successively represent permutations—and it certainly would have been much easier if Curry had had a computer (and one could imagine minimal “combinator sorters” like minimal sorting networks): After writing this paper, Curry went to Göttingen—where he worked with Bernays. I must say that I’m curious what Bernays said to Curry about Schönfinkel (was it more than to Erwin Engeler?), and whether other people around Göttingen even remembered Schönfinkel, who by then had been gone for more than four years. In 1928, travel in Europe was open enough that Curry should have had no trouble going, for example, to Moscow, but there’s no evidence he made any effort to reach out to Schönfinkel. But in any case, in Göttingen he worked on combinators, and over the course of a year produced his first official paper on “combinatory logic”: Strangely, the paper was published in an American journal—as the only paper not in English in that volume. The paper is more straightforward, and in many ways more “Schönfinkel like”. But it was just the first of many papers that Curry wrote about combinators over the course of nearly 50 years. Curry was particularly concerned with the “mathematicization” of combinators, finding and fixing problems with axioms invented for them, connecting to other formalisms (notably Church’s lambda calculus), and generally trying to prove theorems about what combinators do. But more than that, Curry spread the word about combinators far and wide. And before long most people viewed him as “Mr. Combinator”, with Schönfinkel at most a footnote. In 1958, when Haskell Curry and Robert Feys wrote their book on Combinatory Logic, there’s a historical footnote—that gives the impression that Curry “almost” had Schönfinkel’s ideas before he saw Schönfinkel’s paper in 1927: I have to say that I don’t think that’s a correct impression. What Schönfinkel did was much more singular than that. It’s plausible to think that others (and particularly Curry) could have had the idea that there could be a way to go “below the operations of mathematical logic” and find more fundamental building blocks based on understanding things like the process of substitution. But the actuality of how Schönfinkel did it is something quite different—and something quite unique. And when one sees Schönfinkel’s S combinator: what mind could have come up with such a thing? Even Curry says he didn’t really understand the significance of the S combinator until the 1940s. I suppose if one’s just thinking of combinatory logic as a formal system with a certain general structure then it might not seem to matter that things as simple as S and K can be the ultimate building blocks. But the whole point of what Schönfinkel was trying to do (as the title of his paper says) was to find the “building blocks of logic”. And the fact that he was able to do it—especially in terms of things as simple as S and K—was a great and unique achievement. And not something that (despite all the good he did for combinators) Curry did.
Where Did Combinators Come From? Hunting the Story of Moses Schönfinkel 11.12 Schönfinkel : Rediscovered In the decade or so after Schönfinkel’s paper appeared, Curry occasionally referenced it, as did Church and a few other closely connected people. But soon Schönfinkel’s paper—and Schönfinkel himself—disappeared completely from view, and standard databases list no citations. But in 1967 Schönfinkel’s paper was seen again—now even translated into English. The venue was a book called From Frege to Gödel: A Source Book in Mathematical Logic, 1879–1931. And there, sandwiched between von Neumann on transfinite numbers and Hilbert on “the infinite”, is Schönfinkel’s paper, in English, with a couple of pages of introduction by Willard Van Orman Quine. (And indeed it was from this book that I myself first became aware of Schönfinkel and his work.) But how did Schönfinkel’s paper get into the book? And do we learn anything about Schönfinkel from its appearance there?  Maybe. The person who put the book together was a certain Jean van Heijenoort, who himself had a colorful history. Born in 1912, he grew up mostly in France, and went to college to study mathematics—but soon became obsessed with communism, and in 1932 left to spend what ended up being nearly ten years working as a kind of combination PR person and bodyguard for Leon Trotsky, initially in Turkey but eventually in Mexico. Having married an American, van Heijenoort moved to New York City, eventually enrolling in a math PhD program, and becoming a professor doing mathematical logic (though with some colorful papers along the way, with titles like “The Algebra of Revolution”). Why is this relevant? Well, the question is: how did van Heijenoort know about Schönfinkel? Perhaps it was just through careful scholarship. But just maybe it was through Trotsky. There’s no real evidence, although it is known that during his time in Mexico, Trotsky did request a copy of Principia Mathematica (or was it his “PR person”?). But at least if there was a Trotsky connection it could help explain Schönfinkel’s strange move to Moscow. But in the end we just don’t know. What Should We Make of Schönfinkel? When one reads about the history of science, there’s a great tendency to get the impression that big ideas come suddenly to people. But my historical research—and my personal experience—suggest that that’s essentially never what happens. Instead, there’s usually a period of many years in which some methodology or conceptual framework gradually develops, and only then can the great idea emerge. So with Schönfinkel it’s extremely frustrating that we just can’t see that long period of development. The records we have just tell us that Schönfinkel announced combinators on December 7, 1920. But how long had he been working towards them? We just don’t know. On the face of it, his paper seems simple—the kind of thing that could have been dashed off in a few weeks. But I think it’s much more likely that it was the result of a decade of development—of which, through foibles of history, we now have no trace. Yes, what Schönfinkel finally came up with is simple to explain. But to get to it, he had to cut through a whole thicket of technicality—and see the essence of what lay beneath. My life as a computational language designer has often involved doing very much this same kind of thing. And at the end of it, what you come up with may seem in retrospect “obvious”. But to get there often requires a lot of hard intellectual work. And in a sense what Schönfinkel did was the most impressive possible version of this. There were no computers. There was no ambient knowledge of computation as a concept. Yet Schönfinkel managed to come up with a system that captures the core of those ideas. And while he didn’t quite have the language to describe it, I think he did have a sense of what he was doing—and the significance it could have. What was the personal environment in which Schönfinkel did all this? We just don’t know. We know he was in Göttingen. We don’t think he was involved in any particularly official way with the university. Most likely he was just someone who was “around”. Clearly he had some interaction with people like Hilbert and Bernays. But we don’t know how much.  And we don’t really know if they ever thought they understood what Schönfinkel was doing. Even when Curry picked up the idea of combinators—and did so much with it—I don’t think he really saw the essence of what Schönfinkel was trying to do. Combinators and Schönfinkel are a strange episode in intellectual history. A seed sown far ahead of its time by a person who left surprisingly few traces, and about whom we know personally so little. But much as combinators represent a way of getting at the essence of computation, perhaps in combinators we have the essence of Moses Schönfinkel: years of a life compressed to two “signs” (as he would call them) S and K. And maybe if the operation we now call currying needs a symbol we should be using the “sha” character Ш from the beginning of Schönfinkel’s name to remind us of a person about whom we know so little, but who planted a seed that gave us so much. Thanks Many people and organizations have helped in doing research and providing material for this piece. Thanks particularly to Hatem Elshatlawy (fieldwork in Göttingen, etc.), Erwin Engeler (first-person history), Unal Goktas (Turkish material), Vitaliy Kaurov (locating Ukraine + Russia material), Anna & Oleg Marichev (interpreting old Russian handwriting), Nik Murzin (fieldwork in Moscow), Eila Stiegler (German translations), Michael Trott (interpreting German). Thanks also for input from Henk Barendregt, Semih Baskan, Metin Baştuğ, Cem Bozşahin, Jason Cawley, Jack Copeland, Nuran Hariri, Ersin Koylu, Alexander Kuzichev, Yuri Matiyasevich, Roman Maeder, Volker Peckhaus, Jonathan Seldin, Vladimir Shalack, Matthew Szudzik, Christian Thiel, Richard Zach. Particular thanks to the following archives and staff: Berlin State Library [Gabriele Kaiser], Bern University Archive [Niklaus Bütikofer], ETHZ (Bernays) Archive [Flavia Lanini, Johannes Wahl], Göttingen City Archive [Lena Uffelmann], Göttingen University [Katarzyna Chmielewska, Bärbel Mund, Petra Vintrová, Dietlind Willer].
Multiway Turing Machines—Wolfram Institute Bulletins 12.1 : Over the years I’ve studied the simplest ordinary Turing machines quite a bit, but I’ve barely looked at multiway Turing machines (also known as nondeterministic Turing machines or NDTMs). Recently, though, I realized that multiway Turing machines can be thought of as “maximally minimal” models both of concurrent computing and of the way we think about quantum mechanics in our Physics Project. So now this piece is my attempt to “do the obvious explorations” of multiway Turing machines. And as I’ve found so often in the computational universe, even cases with some of the very simplest possible rules yield some significant surprises…. Ordinary vs. Multiway Turing Machines An ordinary Turing machine has a rule such as that specifies a unique successor for each configuration of the system (here shown going down the page starting from an initial condition consisting of a blank tape): In a multiway Turing machine more than one possible outcome can be specified for a particular case in the rule: The result is that there can be many possible paths of evolution for the system—as represented by a multiway graph that connects successive possible configurations: The evolution according to the ordinary Turing machine rule corresponds to a particular path in the multiway graph: Continuing for a few more steps, one begins to see a fairly complex pattern of branching and merging in the multiway graph: My purpose here is to explore what multiway Turing machines with simple rules can do. The basic setup for my multiway Turing machines is the same as for what are usually called “nondeterministic Turing machines” (NDTMs), but in NDTMs one is usually interested in whether single paths have particular properties, while we will be interested in the complete multiway structure of all possible paths. (And by using “multiway” rather than “nondeterministic” we avoid the confusion that we might be thinking about probabilistic or random paths—and emphasize that we’re studying the structure of all possible paths.) In our Physics Project, multiway systems are what lead to quantum mechanics, and our multiway Turing machines here correspond quite directly to quantum Turing machines, with the magnitudes of quantum amplitudes being determined by path weighting, and their phases being determined by locations in the branchial space that emerges from transverse slices of the multiway graph.
Multiway Turing Machines—Wolfram Institute Bulletins 12.2 Turing Machines with Simple Rules : There are 4096 possible ordinary Turing machines with s = 2 head states and k = 2 colors. All these machines ultimately behave in simple ways, with examples of the most complex behavior being (the last case is a binary counter with logarithmic growth): Things get only slightly more complex with s = 3, k = 2, but with s = 2, k = 3 one finds the simplest Turing machine with complex behavior (and this behavior occurs even with a blank initial tape): As suggested by the Principle of Computational Equivalence, this Turing machine turns out be computation universal (making it the very simplest possible universal Turing machine). So what this means is that the threshold for universality in ordinary Turing machines is s = 2, k = 3: i.e. rules in which there are 6 cases specified. But one question we can then ask is: what is the threshold for universality in multiway Turing machines? To say that our s = 2, k = 3 ordinary Turing machine is universal is to say that with appropriate initial conditions the evolution of this Turing machine can emulate the evolution of any other Turing machine (at least with a suitable encoding, implementable by a bounded computation). The closest analogous definition of computation universality for multiway Turing machines is to say that with appropriate initial conditions the multiway evolution of a Turing machine can emulate (up to suitable encoding) the multiway evolution of any other Turing machine. (As we will discuss later, however, the “nondeterministic interpretation” suggests a nominally different definition based on whether initial conditions can be set up to make corresponding individual paths with particular identifying features exist.) To specify the rule for an ordinary Turing machine, one normally just defines the outcome for each of the s k possible “inputs” such as But what if one omits some of these inputs? In analogy to our usual treatment of string or hypergraph rewriting systems it is natural to just say that if one of the omitted inputs is reached in the evolution of the system, then no rewrite is performed, or—in standard Turing machine terms—the system “halts”. With s = 2, k = 2, the longest that machines that will eventually halt survive (starting from a blank tape) is just 6 steps—and there are 8 distinct machines (up to reflection) which do this, 3 with only 2 cases out of 4 in their rules, and 5 with 3 cases: In a sense each of these rules can be thought of as using some subset of all 2 s2 k2 possible rule cases, which for k = 2, s = 2 is: And in general any multiway Turing machine rule corresponds to some such subset. If all possible inputs appear exactly once, one gets an ordinary Turing machine that does not halt. If inputs appear at most once, but some do not appear at all, the Turing machine may halt (though if it never reaches that input it won’t halt). But if any input appears more than once, the Turing machine may exhibit nontrivial multiway evolution. If all inputs appear at least once, no branch of the multiway evolution can halt. But if some inputs appear more than once, while some do not appear at all, some branches of the multiway evolution may halt, while others may continue forever. In general, there are 22s2k2 possible multiway Turing machine rules. In talking about rulial space and rulial multiway systems I previously discussed the particular example of the (“rulial”) multiway Turing machine rule in which every single case is included (i.e. all 32 cases for s = 2, k = 2), so that the Turing machine is in a sense “as multiway as possible”. But here we are concerned with multiway Turing machines whose rules are instead as simple as possible—and “far from the rulial limit”. A very simple example is the rule which generates tape configurations containing all binary numbers and gives a multiway system which forms an infinite binary tree: A slightly less trivial example is the rule which produces a multiway graph in which branches stop as soon as the head is on a non-blank cell: Continuing this, the overall structure of the multiway graph is (where halting states are indicated by red dots): As another example, consider the rule: Once again, the system halts if the head gets onto a non-blank cell, but this happens in a slightly more elaborate way, giving a slightly more complicated multiway graph: In the example we’ve just seen, there is explicit halting, in the sense that the Turing machine can reach a state in which none of its rules apply. Another thing that can happen is that Turing machines can get into infinite loops—and since in our multiway graphs identical states are merged, such loops show up as actual loops in the multiway graph, as in this example: As soon as one allows 3 cases in the rules, things rapidly get more complicated. Consider for example the rule (which can never halt): The first few steps in its multiway graph are: This example exposes a somewhat subtle issue. In our earlier examples, all the states generated on a particular step could consistently be arranged in a single layer in the multiway graph. But here the state is generated both at step 2 and at step 4—so no such “global layering” is possible (at least, assuming, as we do, that we combine identical copies of this state). And a consequence of this lack of global layering is that if we compute for a given number of steps we can end up with “dangling states”—like —that appear in our rendering well above the “final layer” shown. After another step, the previous “dangling states” now have successors, but there are new dangling states: Continuing for more steps, a somewhat elaborate but nevertheless fairly regular multiway begins to develop: Here’s another example of a multiway Turing machine with 3 cases in its rule: The first few steps in its multiway graph are: Continuing for another step, things get more complicated, and, notably, we see a loop: Continuing for a few more steps we get: (Note that since in this rule there is no halting, the dangling ends visible here must show additional growth if the evolution is continued.) It’s perfectly possible to get both rapid growth, and halting. Here’s an example or, continuing for longer.
Multiway Turing Machines—Wolfram Institute Bulletins 12.3 Visualization and Multispace : It’s fairly easy to get a sense of the behavior of an ordinary Turing machine just by displaying its successive configurations down the page. But what about for a multiway Turing machine? The multiway graph shows one the relationships defined by evolution between complete configurations (states) of the system: But such a picture does not make clear the relationships between the detailed forms of different configurations, and for example the “spatial motion” of the head. So how can we do this? One possibility is in effect to display all the different possible configurations at a given step “overlaid on top of each other”. For the machine above the new configurations obtained at successive steps are (though note that these do not appear on specific layers in the rendering of the multiway graph above): Putting these “on top of each other”, and “averaging colors” we get: We can see the lattice of paths more explicitly if we join head positions obtained at successive steps of evolution: What does this approach reveal about other multiway Turing machines we’ve discussed? Here’s one of our first examples: And here’s another of our examples: But generally this kind of averaged picture is not particularly helpful. So is there an alternative? Basically we need some way to simultaneously visualize both multiway structure, and the detailed forms of individual configurations. In analogy to our Physics Project, this is essentially the problem of simultaneously visualizing both branchial and spatial structure. So where should we start? Since our everyday experience is with ordinary space, not branchial space, it seems potentially better to start with ordinary space, and then add in the branchial component. And thinking about this led to the concept of multispace: a generalization of space in which there is in effect at every point a stack of multiway possibilities, with these different possibilities being connected at different places in branchial space. In our Physics Project, even visualizing ordinary space itself is difficult, because it’s built up from lower-level structures corresponding to hypergraphs. But for Turing machines the basic structure of space is constrained to be very simple: there is just a one-dimensional tape with a head that can go backwards and forwards on it. But in understanding multispace, it is probably easier to start from the branchial side. Let’s for now ignore the values written on the tape of a Turing machine, and consider only the position of the head, say relative to where it started. Every state in the multiway graph can then be labeled with the position of the head in that configuration of the Turing machine. So for the first rule above we get: But now let’s imagine setting this up in 3D, with the timelike direction down the page, the branchlike direction across the page, and the spacelike direction into the page: But if we now rotate this, we can see the interplay of branchial and spatial structure: In this particular case, we see that the main branches visible in the multiway system (i.e. in the branchlike direction) correspond to progressively more separated positions of the head in the spacelike direction. Things are not always this simple. Here’s the head-position-annotated multiway graph for the Turing machine from the beginning of this section: And now here’s a 3D view of the evolution of this Turing machine in multispace. First we’re effectively projecting into the branchtime plane. Note however that because connections can be made in 3D, the layout chosen for the rendering is different from what gets chosen when the multiway graph is rendered in 2D: Rotating in multispace we can see the interplay between branchlike and spacelike structure: Even for this comparatively simple multiway Turing machine, it’s already quite difficult to tell what’s going on. But continuing for a few more steps, one can see definite patterns in multispace (the narrowing at the bottom reflects the fact that this is run only for a finite number of steps; it would fill in if one ran it for longer): In our Physics Project, there is no intrinsic coordinatization of either physical space or branchial space—so any coordinatization must be purely emergent. But the basic structure of Turing machines implies an immediate definition of space and time coordinates—which is what we use in the pictures above to place nodes in the spacelike and timelike directions. (The timelike direction is slightly subtle: we’re assigning a coordinate based on where a state appears in the layering of the multiway graph, which may or may not be the “time step” at which it first appears.) But in Turing machines there is no immediate definition of branchial coordinates—and in the pictures above we’re deriving branchial coordinates from the (somewhat arbitrary) layout used in the 2D rendering of the multiway graph. So is there a better alternative? In effect, what one has to do is to find some good way to coordinatize the complete state of the Turing machine, including the configuration of its tape. One possibility is just to treat the configuration of the tape as defining a base-k number—and then for example to compute a coordinate from the log of this number. (One can also imagine shifting the number so that its “decimal” point is at the position of the head, but the head position is in a sense already “handled” through the spatial coordinate.) Here’s the result from doing this for the Turing machine above: Here are a couple of other examples: Thinking about tape configurations suggests another visualization: just stack up all tape configurations that can be generated on successive steps. In the case of the rule one just gets at step t the binary digits of successive integers 0 through 2t–1 on the tape: But even for a rule like: the result is more complicated: where now the grayed rows correspond to states on which the Turing machine has already halted. When we construct a multiway graph—and render it in multispace or elsewhere—we are directly representing the evolution of states of something like a multiway Turing machine. But particularly the study of quantum mechanics in our Physics Project emphasizes the importance of a complementary view—of looking not at the evolution of individual states, but rather at relationships (or “entanglements”) between states defined by their common ancestry in the multiway graph. Consider the rule: that generates a multiway graph: Now make a foliation of this graph: If we look at the last slice here, there are several pairs of nodes that have common ancestors—each coming from the use of different cases in the original rule. The branchial graph that joins configurations (nodes) with immediate common ancestors is, however, rather trivial: But even for the rule: with multiway graph: the branchial graphs on successive steps are slightly less straightforward: One can also construct “thickened branchial graphs” in which one looks not just at immediate common ancestors, but say at common ancestors up to τ steps back. For τ > 1 the result is a hypergraph, and even in the first case here, it can be somewhat nontrivial.
Multiway Turing Machines—Wolfram Institute Bulletins 12.4 The World of Simple Multiway Turing Machines : Unless an ordinary Turing machine has both at least s = 2 states and k = 2 colors it will always have essentially trivial behavior. But what about multiway Turing machines? With k = 1 color the system in effect cannot store anything on the tape, and it must always “do the same dance” at whatever position the head is in. Consider for example the s = 2 rule: The states of this system can be represented in the form {σ, i} where σ is the head state, and i is the position of the head. The multiway graph for the evolution of the system after 3 steps starting from a blank tape with the head in head state 1 can then be drawn as (with the coordinates of each state being determined from {σ, i}): Continuing longer we get: and we can see that eventually the multiway graph is effectively a repeating braid—though with “slightly frayed” ends at any given step. The braid can be constructed from a sequence of “motifs”, here consisting of 3 “vectors”, where each vector is directly determined by a case in the underlying rule, or in this case: If we look at all s = 2 multiway Turing machines with 3 cases in their rules, these are the multiway graph structures we get: In all cases, the effective repetition length for any path is 1, 2 or 4. With s = 3 possible head states, slightly more elaborate multiway graphs become possible. For example gives the “8-cycle braid”: To get beyond “pure-braid” multiway graphs, we must consider k ≥ 2 colors. But what happens if we use just one head state s = 1 (so we basically have a “mobile automaton” with rules of range 0)? For s = 1, k = 2, there are 8 possible cases that can occur in the rule: and thus 256 possible multiway Turing machines, of which 231 are not “purely deterministic”. If we allow only p = 2 possible cases in the rule, there are 28 possible machines, of which 12 are not purely deterministic, and the distinct possible nontrivial forms of multiway graphs that we can get are just (where in the first rule the second color is not used): With p = 3 possible cases in the rule, there are 56 possible machines, none purely deterministic. The distinct multiway graphs they generate are: Some of these we have already seen above. But it is remarkable how complex these graphs appear to be. Laying the graphs out in multispace, however, some show very clear regularities: If we look at the complicated cases for more steps, we still see considerable complexity: The number of states reached at successive steps for the first rule is: while for the other two it is: Both of these appear to grow exponentially, but with slightly different bases, both near 1.46. Looking at the collections of states generated also does not give immediate insight into the behavior: So what happens if we allow p > 3 cases in the rule? With p = 4 cases, there are 70 possible machines, none purely deterministic. Here are some examples of the more complicated behavior that is seen: We can go on and look at machines with larger p. Here are examples with p = 5: We can also ask what happens if we allow s = 2 instead of s = 1, with k = 2. With p = 3 cases in the rule, the most complicated multiway graphs are just: With p = 4 cases in the rule, the behavior can be slightly more complicated, but does not appear to go beyond what we already saw with a single head state s = 1.
Multiway Turing Machines—Wolfram Institute Bulletins 12.5 What Is the Simplest Universal Multiway Turing Machine? : We might have assumed that to get complicated behavior in a multiway Turing machine it would need complicated rules, and that as the rules got more complicated the behavior would somehow get correspondingly more complicated. But that’s not what we saw in the previous section. Instead, even multiway Turing machines with very simple rules were already capable of apparently complex behavior, and the behavior didn’t seem to get fundamentally more complicated as the rules got more complicated. This might seem surprising. But it’s actually very much the same as we’ve seen all over the computational universe—and indeed it’s just what my Principle of Computational Equivalence implies should happen. Because what the principle says is that as soon as one goes beyond systems with obviously simple behavior, one gets to systems that are equivalent in the sophistication of the computations they perform. Or, in other words, that above a very low threshold, one sees systems that all achieve a maximal level of computational sophistication that is always the same. The Principle of Computational Equivalence talks about “typical computations” that a system does, and implies, for example, that these computations will tend to be computationally irreducible, in the sense that one cannot determine their outcomes except by computations that are essentially just as long as the ones the system itself is performing. But another consequence of the Principle of Computational Equivalence is that as soon as the behavior of a system is not obviously simple, it is likely to be computation universal. Or, in other words, once a system can exhibit complicated behavior, it is essentially always possible to find a particular initial condition that will make it emulate any other system—at least up to a translation given by some kind of bounded computation. And we have good evidence that this is how things work in simple cellular automata, and in ordinary, deterministic Turing machines. With s = 2, k = 2 such Turing machines always show simple, highly regular behavior, and no such machines are capable of universal computation. But with s = 2, k = 3 there are a few machines that show more complex behavior. And we know that these machines are indeed computation universal, establishing that among ordinary Turing machines the threshold of universality is in a sense as low as it could be. So what about multiway Turing machines? Where might the threshold for universality for such machines be? Based on our experience with ordinary Turing machines (and many other kinds of systems) the results in the previous section might tend to suggest that s = 1, k = 2, p = 3 would already be sufficient. With ordinary Turing machines, universality is achieved when s = 2, k = 3, corresponding to 6 cases in the rule. Here universality seems possible even with just 3 cases in the rule—and with s = 1, k = 2. Perhaps the most surprising part of this is the implication that universality might be possible even with just a single head state s = 1. And this implies that we do not even need to have a Turing machine with any head states at all; we can just use a mobile automaton, and actually one with “radius 0” (i.e. with rules that are not directly affected by any neighbors)—that we can indicate for example as: Turing machines are often viewed as extensions of finite automata, in which a tape is added to provide potentially infinite memory. Ordinary deterministic Turing machines are then extensions of deterministic finite automata (DFAs), and multiway Turing machines of nondeterministic finite automata (NDFAs). And finite automata with just a single state s = 1 are always trivial—whether they are deterministic or nondeterministic. We also know that ordinary deterministic Turing machines with s = 1 are always trivial, regardless of how many tape colors k they have; s = 2 is the minimum to achieve nontrivial behavior (and indeed we know that s = 2, k = 3 is sufficient to achieve in a sense maximally complex behavior). But what we saw above is that multiway Turing machines can show nontrivial behavior even when s = 1. Quite likely there is a direct construction that can show how an s = 1 multiway Turing machine can emulate an s > 1 one. But here is some intuition about why this might be possible. In an ordinary Turing machine, the head moves “in space” along the tape, and the head state carries state information with it. But in a multiway Turing machine, there is in effect motion not only in space, but also in branchial space. And even though without multiple head states the head may not be able to carry information in ordinary space, the collection of nearby configurations in branchial space can potentially carry information in branchial space. In other words, in branchial space one can potentially think of there being a “cloud” of nearby configurations that represent some kind of “super head” that can in effect assume multiple possible head states. Crucial to this picture is the fact that multiway systems are set up to merge identical states. If it were not for this merging, the multiway graph for a multiway Turing machine would just be a tree. But the merging “knits together” branchial space, and allows one to make sense of concepts like distance and motion in it. In an ordinary Turing machine, one can think of the evolution as progressively transforming the state of the system. In a multiway Turing machine it may be better to think of the evolution as knitting together elementary pieces of the multiway graph—with the result that it matters less what the individual states of the Turing machine are like, and more how different states are related in branchial space, or in the multiway system. But what exactly do we mean by universality anyway? In an ordinary deterministic system the basic definition is that a universal system—if given appropriate initial conditions—must be able to emulate any other computational system (say any Turing machine). Inevitably, there will be encoding and decoding required. Given a specification of the system one is trying to emulate (say, the rules for a Turing machine), one must have a procedure for setting up appropriate initial conditions—or, in effect, for “compiling” the rules to the appropriate “code” for the universal machine. Then when one runs the universal machine, one must have a way of “decoding” its evolution to identify the steps in the evolution of the system one is emulating. For the idea of universality to be meaningful, it’s important of course that the processes of encoding and decoding don’t do too much computation. In particular, the amount of computation they need to do must be bounded: so even if the “main computation” is computationally irreducible and needs to run progressively longer, these do not. Or, put another way, any feature of the encoding and decoding should be decidable, whereas the main computation can show computational irreducibility, and undecidability. We have talked here about universality being a story of one machine emulating another. And ultimately this is what it is about. But in traditional computation theory (which was developed before actual digital computers were commonplace) there are typically two additional features of the setup. First, one often thinks about the Turing machine as just “computing a function”, so that when fed certain input it gives certain output—and we don’t ask “what it’s doing inside”. And second, one thinks of the Turing machine as “halting” when it has produced its answer. But particularly when one wants to get intuition about computational processes, it’s crucial to be able to “see the process”. And in keeping with the operation of modern digital computers, one is less concerning with “halting”, and more just with knowing how to decode the behavior (say by looking for “signals” that indicate that something should be “considered to be a result”). (Even beyond this, Turing machines are sometimes thought of purely as “decision machines”: given a particular input they eventually give the result “true” or “false”, rather than generating different forms of result.) In studying multiway Turing machines I think the best way to define universality is the one that is most directly analogous to what I’ve used for ordinary Turing machines: the universal machine must be able to emulate the multiway graph for any other machine. In other words, given a universal multiway Turing machine there must be a way to set up the initial conditions so that the multiway graph it generates can be “decoded” to be the multiway graph for any other system (or, in particular, for any other multiway Turing machine). It is worth realizing that the “initial conditions” here may not just be a single Turing machine configuration; they can be an ensemble of configurations. In an ordinary deterministic Turing machine one gives initial conditions which are “spread out in space”, in the sense that they specify colors of cells at different positions on the tape. In a multiway Turing machine one can also give initial conditions which are “spread out in branchial space”, i.e. involve different configurations that will initiate different branches (which might merge) in the multiway system. In an ordinary Turing machine—in physics terms—it is as if one specifies one’s initial data on a “spacelike hypersurface”. In a multiway Turing machine, one also specifies one’s initial data on a “branchlike hypersurface”. By the way, needless to say, a universal deterministic Turing machine can always emulate a multiway Turing machine (which is why we can run multiway Turing machines on practical computers). But at least in the most obvious approach it can potentially require exponentially many steps of the ordinary Turing machine to follow all the branches of the multiway graph for the multiway Turing machine. But what is the multiway analog of an ordinary Turing machine “computing a function”? The most direct possibility is that a multiway Turing machine defines a mapping from some set of configurations to some other set. (There is some subtlety to this, associated with the foliation of the multiway graph—and specifying just what set of configurations should be considered to be “the result”.) But then we just require that with appropriate encoding and decoding a universal multiway Turing machine should be able to compute any function that any multiway Turing machine can compute. There is another possibility, however, suggested by the term “nondeterministic Turing machine”: require just that there exists some branch of the multiway system that has the output one wants. For example, if one is trying to determine the answer to a decision problem, just see if there is any “yes” branch, and, if so, conclude that the answer is yes. As a simple analog, consider the string substitution with transformations {“()”→“”,”()”→“|”}. One can compute whether a given sequence of parentheses is balanced by seeing whether its evolution will eventually generate an empty string (“”) state—but even if this state is generated, all sorts of “irrelevant” states will also be generated: There is presumably always a way to translate the concept of universality from our “emulate the whole multiway evolution” to the “see if there’s any path that leads to some specific configuration”. But for the purposes of intuition and empirical investigation, it seems much better to look at the whole evolution, which can for example be visualized as a graph. So what is the simplest universal multiway Turing machine? I am not certain, but I think it quite likely that it has s = 1, k = 2, p = 3. Two candidates are: To prove universality one would have to show that there exists a procedure for setting up initial conditions that will make a particular rule here yield a multiway graph that corresponds (after decoding) to the multiway graph for any other specified system, say any other multiway Turing machine. To get some sense of how this might be possible, here are the somewhat diverse multispace graphs generated by the first rule above, starting from a single initial configuration whose tape contains the digits of successive binary numbers.
Multiway Turing Machines—Wolfram Institute Bulletins 12.6 The Halting Problem and Busy Beavers : If a system is computation universal, it is inevitable that there will be computational irreducibility associated with determining its behavior, and that to answer questions about what it will do after an arbitrarily long time may require unbounded amounts of computation, and must therefore in general be considered formally undecidable. A classic example is the halting problem—of asking whether a Turing machine starting from a particular initial condition will ever reach a particular “halt” state. Most often, this question is formulated for ordinary deterministic Turing machines, and one asks whether these machines can reach a special “halting” head state. But in the context of multiway Turing machines, a more natural version of this question is just to ask, as we have done above, whether the multiway system will reach a configuration where none of its possible rules apply. And in the simplest case, we can consider “deterministic but incomplete rules”, in which there is never more than one possible successor for a given state, but there may be no successor at all. An example is the rule which contains no case for . Starting this rule from a blank tape it can evolve for 5 steps, but then reaches a state where none of its rules apply: As we saw above, for s = 2, k = 2 rules, 6 steps is the longest any ordinary Turing machine rule will “survive” starting from a blank tape. For s = 3, k = 2 it is 21 steps, for s = 2, k = 3 it is 38 and for s = 4, k = 2 it is 107: For larger s and k the survival times for the best-known “busy beaver” machines rapidly become very long; for s = 3, k = 3 it is known to be at least 1017. So what about multiway Turing machines in general—where we allow more than one successor for a given state (leading to branching in the multiway graph)? For s = 1 or k = 1 the results are always trivial. But for s = 2, k = 2 things start to be more interesting. With only p = 2 possible cases in the rule, the longest halting time is achieved by the deterministic rule which halts in 4 steps as represented by the (single-path) multiway graph: The p = 3 case includes standard deterministic s = 2, k = 2 Turing machines with a single halt state, and the longest halting time is achieved by the deterministic machine we saw above: But if we look at slightly shorter halting times, we start seeing branching in the multiway graph: When we think in terms of multiway graphs, it is not so natural to distinguish “no-rule-applies” halting from other situations that lead to finite multiway graphs. An example is which is a deterministic machine that after 4 steps ends up in a loop: With p = 4 total cases in the rule we can have a “full deterministic s = 2, k = 2 Turing machine”, with no possible cases omitted. The longest “halting time” (of 8 steps) is achieved by a machine which enters a loop: But if we consider shorter maximal halting times, then we get both “genuine halting” and branching in the multiway system: Note that unlike in the deterministic case where there is a single, definite halting time from a given initial state, a multiway Turing machine can have different halting times on different branches in its multiway evolution. So this means that there are multiple ways to define the multiway analog of the busy beaver problem for deterministic Turing machines. One approach perhaps the closest to the spirit of the deterministic case is to ask for machines that maximize the maximal finite halting time obtained by following any branch. But one can also ask for machines which maximize the minimal halting time. Or one can ask for machines that give multiway graphs that involve as many states as possible while still being finite (because all their branches either halt or cycle). (Yet another criterion could be to ask for the maximum finite number of distinct halting states.) For the p = 2 case, the maximum-halting-time and maximum-states criteria turn out to be satisfied by the same deterministic machine shown above (with 4 total states). For p = 3, though, the maximum-halting-time criterion is satisfied by the first machine we showed, which evolves through 6 states, while the maximum-states-criterion is instead satisfied by the second machine, which has 7 states in its multiway graph. For the k = 2, s = 2, p = 4 case above the maximum-halting-time machine is again deterministic (and goes through 7 states), but maximum-states machines have 11 states: Even for ordinary deterministic Turing machines, finding busy beavers involves direct confrontation with the halting problem, and computational irreducibility. Let’s say a machine has been running for a while. If it halts, well, then one knows it halts. But what if it doesn’t halt? If its behavior is sufficiently simple, then one may be able to recognize—or somehow prove—that it can never halt. But if the behavior is more complex one may simply not be able to tell what will happen. And the problem is even worse for a multiway Turing machine. Because now it is not just a question of evolving one configuration; instead there may be a whole, potentially growing, multiway graph of configurations one has to consider. Usually when the multiway graph gets big, it has a simple enough structure that one can readily determine that it will never “terminate”, and just grow forever. But when the multiway graph gets complex it can be extremely difficult to be sure what will eventually happen. Still, at least in most cases, one can be fairly certain of the results. With p = 5, the rule whose behavior allows the longest halting (or, in this case, cycling) time (8 steps) is: With p = 5, many other types of “finite behavior” can also occur, such as: With p = 6 there is basically just more of the same—with no extension in the maximum halting time, though with larger total numbers of states: The maximum halting times and finite numbers of states reached by s = 2, k = 2 machines with p cases is as follows and the distributions of halting times and finite numbers of steps (for p for up 7) are: What if one considers, say, s = 3, k = 2? For p = 2 and p = 3 there are not enough cases in the rule to sample the rule range of head states, so the longest-surviving rules are basically the same as for s = 2, k = 2. Meanwhile, for p = 5 one has the 21-step “deterministic” busy beaver shown above, while for p = 4 there is a 17-step-halting-time rule: In the p = 5 case, the maximum-state machine is the following, with 30 states: There are several interesting potential variants of the things we have discussed here. For example, instead of considering multiway Turing machines where all branches halt (or cycle), we can consider ones where some branches continue, perhaps even branching forever—but where either, say, one or all of the branches that do halt survive longest. We can also consider Turing machines that start from non-blank tapes. And—in analogy to what one might study in computational complexity theory—we can ask how the number (or size of region) of non-blank cells affects halting times. (We can also study the “functions” computed by Turing machines by looking at the transformations they imply between initial tapes and final outputs..
Multiway Turing Machines—Wolfram Institute Bulletins 12.7 Causal Graphs : Our main concern here so far has been in mapping out the successions of states that can be obtained in the evolution of multiway Turing machines. But one of the discoveries of our Physics Project is that there is a complementary way to understand the behaviors of systems, by looking not at successions of states but at causal relationships between events that update these states. And it is the causal graph of these relationships that is of most relevance if one wants to understand what observers embedded within a system can perceive. In our multiway Turing machines, we can think of each application of a Turing machine rule as an “event”. Each such event takes certain “input” (the state of the head and the color of the cell under it), and generates certain “output” (the new state of the head, the color under it, and its position). The causal graph maps out what outputs from other events the input to a given event requires, or, in other words, what the causal dependence is of one event on others. For example, for the ordinary Turing machine evolution the network of causal relationships between updating events is just yielding the causal graph: Continuing longer gives: or with a different graph rendering just: One can also define causal graphs for multiway Turing machines. As an example, look at a rule we considered above: The multiway graph that describes possible successions of states in this case is: Now let’s explicitly show the update event that transforms each state to its successor: Just as for the deterministic case, we can identify the causal relationships between these update events, here indicated by orange edges: Keeping only the events and their causal relationships we get the final multiway causal graph: Continuing for more steps we get: Informed by our Physics Project, we can think of each edge in the causal graph as representing a causal relationship between two events which follow each other in time. In the causal graph for an ordinary “single-way” Turing machine these two events may also be separated in space, so that in effect the causal graph defines how space and time are knitted together. In the (multiway) causal graph for a multiway Turing machine, events can be separated not only in space, but also in branchial space (or, in other words, they can occur on different branches in the evolution)—so the multiway causal graph can be thought of as defining how space, branchial space and time are knitted together. We discussed above how states in a multiway graph can be thought of as being laid out in “multispace” which includes both ordinary space and branchial space coordinates. One can do more or less the same for events in a multiway causal graph—suggesting a “multispace rendering of a multiway causal graph”: Different multiway Turing machines can have quite different multiway causal graphs. Here are some samples for various rules we considered above: These multiway causal graphs in a sense capture all causal relationships within and between different possible paths followed in the multiway graph. If we pick a particular path in the multiway graph (corresponding to a particular sequence of choices about which case in the multiway Turing machine rule to apply at each step) then this will yield a “deterministic” evolution. And this evolution will have a corresponding causal graph—that must always be a subgraph of the full multiway causal graph.
Multiway Turing Machines—Wolfram Institute Bulletins 12.8 Causal Invariance : Whenever there is more than one possible successor for a given state in the evolution of a multiway Turing machine, this will lead to multiple branches in the multiway graph. Different possible “paths of history” (with different choices about which case in the rule to apply at each step) then correspond to following different branches. And in a case like this once one has picked a branch one is “committed”: one can never subsequently reach any other branches—and paths that diverge never converge again. But in a case like this it’s a different story because here—at least if one goes far enough in generating the multiway graph—every pair of paths that diverge must eventually converge again. In other words, if one takes a “wrong turn”, one can always recover. Or, put another way, whatever sequence of rules one applies, it is always possible to reach eventual consistency in the results one gets. This property is closely related to a property that’s very important in our Physics Project: causal invariance. When causal invariance is present, it implies that the causal graphs generated by following any possible path must always be the same. In other words, even though the multiway system in a sense allows many possible histories, the network of causal relationships obtained in each case will always be the same—so that with respect to causal relationships there is essentially just one “objective reality” about how the system behaves. (By the way, the multiway causal graph contains more information than the individual causal graphs, because it also describes how all the causal graphs—from all possible histories—“knit together” across space, time and branchial space. There’s a subtlety which we will not explore in detail here. Whether one considers branches in the multiway graph to have converged depends on how one defines equivalence of states. In the multiway graphs we have drawn, we have done this just by looking at whether the instantaneous states of Turing machines are the same. And in doing this, the merging of branches is related to a property often called confluence. But to ensure that we get full causal invariance we can instead consider states to be equivalent if in addition to having the same tape configurations, the causal graphs that lead to them are isomorphic (so that in a sense we’re considering a “causal multiway graph”). So what multiway Turing machines are causal invariant (or at least confluent)? If a multiway Turing machine has rules that make it deterministic (without halting), and thus effectively “single way”, then it is trivially causal invariant. If a multiway Turing machine has multiple halting states it is inevitably not causal invariant—because if it “falls into” any one of the halting states, then it can never “get out” and reach another. On the other hand, if there is just a single halting state, and all possible histories lead to it, then a multiway Turing machine will at least be confluent (and in terms of its computation can be thought of as always “reaching a final unique answer”, or “normal form”). Finally, in the “rulial limit” where the multiway Turing machine can use any possible rule, causal invariance is inevitable. In general, causal invariance is not particularly rare among multiway Turing machines. For s = 1, k = 1 rules, it is inevitable. For s = 1, k = 2 rules here are some examples of multiway graphs and multiway causal graphs for rules that appear to be at least confluent and ones that do not: (Note that in general it can be undecidable if a given multiway Turing machine is causal invariant—or confluent—or not. For even if two branches eventually merge, there is no a priori upper bound on how many steps this will take—though in practice for simple rules it typically seems to resolve quite quickly..
Multiway Turing Machines—Wolfram Institute Bulletins 12.9 Finite Tapes : So far we’ve always assumed that the tapes in our Turing machines are unbounded. But just as we did in our earlier project of studying the rulial space of Turing machines, we can also consider the case of Turing machines with bounded—say cyclic—tapes with a finite number of “cells” n. Such a Turing machine has a total of n s kn possible complete states—and for a given n we can construct a complete state transition graph. For an ordinary deterministic Turing machine, these graphs always have a single outgoing edge from each state node (though possibly several incoming edges). So, for example, with the rule the state transition graph for a length-3 tape is: Here are some other s = 2, k = 2 examples (all for n = 3): In all cases, what we see are certain cycles of states that are visited repeatedly, fed by transients containing other states. For multiway Turing machines such as the structure of the state transition graph can be different, with nodes having both more or less than 1 outgoing edge: Starting from the node corresponding to a particular state, the subgraph reached from that state is the multiway graph corresponding to the evolution from that state. Halting occurs when there is outdegree 0 at a node—and the 3 nodes at the bottom of the image correspond to states where the Turing machine has immediately halted. Here are results for some other multiway Turing machines, here with tape size n = 5, indicating halting states in red: What happens when there are more cases p in the multiway Turing machine rule? If all possible cases are allowed, so that p = 2 s2 k2, then we get the full rulial multiway graph. For size n = 3 this is while for n = 5 it is: This “rulial-limit” multiway graph has the property that it is in a sense completely uniform: the graph is vertex transitive, so that the neighborhood of every node is the same. (The graph corresponds to the Cayley graph of a finite “Turing machine group”, here \!\( But while this “rulial-limit” graph in a sense involves maximal nondeterminism and maximal branching, it also shows maximal merging, and in fact starting from any node, paths that branch must always be able to merge again—as in this example for the n = 3 case (here shown with two different graph renderings): So what this means is that—just as in the infinite-tape case discussed above—multiway Turing machines must always be causal invariant in the rulial limit. So what about “below the rulial limit”? When is there causal invariance? For s = 1, k = 1 it is inevitable: For s = 1, k = 2 it is somewhat rarer; here is an example where paths can branch and not merge: When we discussed causal invariance (and confluence) in the case of infinite tapes, we did so in the context of starting from a particular configuration (such as a blank tape), and then seeing whether all paths from this node in the multiway graph eventually merge. We can do something similar in the case of finite tapes, say asking about paths starting from the node corresponding to a blank tape. Doing this for tapes of sizes between 2 and 4, we get the following results for the number of s = 1, k = 2 rules (excluding purely deterministic ones) that exhibit confluence as a function of the number of cases p in the rule: As expected, it is more common to see confluence with shorter tapes, since there is then less opportunity for branching in the multiway graph. (By the way, even for arbitrarily large n, cyclic tapes make more machines confluent than infinite tapes do, because they allow merging of branches associated with the head “going all the way around”. If instead of cyclic boundary conditions, one uses boundary conditions that “reflect the head”, fewer machines are confluent.) With finite tapes, it is possible to consider starting not just from a particular initial condition, but from all possible initial conditions—leading to slightly fewer rules that show “full confluence”: Here are examples of multiway graphs for rules that show full confluence and ones that do not: Typically non-confluent cases look more “tree like” while confluent ones look more “cyclic”. But sometimes the overall forms can be very similar, with the only significant difference being the directionality of edges. (Note that if a multiway graph is Hamiltonian, then it inevitably corresponds to a system that exhibits confluence.) What I call multiway Turing machines are definitionally the same as what are usually called nondeterministic Turing machines (NDTMs) in the theory of computation literature. I use “multiway” rather than “nondeterministic”, however, to indicate that my interest is in the whole multiway graph of possible evolutions, rather than in whether a particular outcome can “nondeterministically” be reached. The idea of nondeterminism seems to have diffused quite gradually into the theory of computation, with the specific concept of nondeterministic Turing machines emerging around 1970, and promptly being used to formulate the class of NP computations. The original Turing machines from 1936 were purely deterministic. However, although they were not yet well understood, combinators introduced in 1920 already involved what we now think of as nondeterministic reductions, as did lambda calculus. Many mathematical problems are of the form “Does there exist a ___ that does ___?”, and the concept of looking “nondeterministically” for a particular solution has arisen many times. In a specifically computational context it seems to have been emphasized when formal grammars became established in 1956, and it was common to ask whether a particular string was in a given formal language, in the sense that some parse tree (or equivalent) could be found for it. Nondeterministic finite automata seem to have first been explicitly discussed in 1959, and various other forms of nondeterministic language descriptions appeared in the course of the 1960s. In the early 1980s, quantum generalizations of Turing machines began to be considered (and I in fact studied them a little at that time). The obvious basic setup was structurally the same as for nondeterministic Turing machines (and now for multiway Turing machines), except that in the quantum case different “nondeterministic states” were identified as quantum states, assigned quantum amplitudes, and viewed as being combined in superpositions. (There was also significant complexity in imagining a physical implementation, with an actual Hamiltonian, etc.) Turing machines are most often used as theoretical constructs, rather than being investigated at the level of specific simple rules. But while some work on specific ordinary Turing machines has been done (for example by me), almost nothing seems to have been done on specific nondeterministic (i.e. multiway) Turing machines. The fact that two head states are sufficient for universality in ordinary Turing machines was established by Claude Shannon in 1956 (and we now know that s = 2, k = 3 is sufficient), but I know of no similar results for nondeterministic Turing machines. Note that in my formulation of multiway Turing machines halting occurs just because there are no rules that apply. Following Alan Turing, most treatments of Turing machines introduce an explicit special “halt” state—but in my formulation this is not needed. I have considered only Turing machines with single heads. If multiple heads are introduced in ordinary Turing machines (or mobile automata) there typically have to be special rules added to define how they should interact. But with my formulation of multiway Turing machines, there is no need to specify this; heads that would “collide in space” just end up at different places in branchial space. A basic multiway generalization of the TuringMachine function in Wolfram Language is (note that the specification of configurations is slightly different from what is used in A New Kind of Science): Running this for 2 steps from a blank tape gives: The number of distinct states increases as: This creates a multiway graph for the evolution: A more complete multiway Turing machine function is in the Wolfram Function Repository.
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.1 “[Despite] Considerable Effort… [It Proved] Intractable” : In the early years of the twentieth century it looked as if—if only the right approach could be found—all of mathematics might somehow systematically be solved. In 1910 Whitehead and Russell had published their monumental Principia Mathematica showing (rather awkwardly) how all sorts of mathematics could be represented in terms of logic. But Emil Post wanted to go further. In what seems now like a rather modern idea (with certain similarities to the core structure of the Wolfram Language, and very much like the string multiway systems in our Physics Project), he wanted to represent the logic expressions of Principia Mathematica as strings of characters, and then have possible operations correspond to transformations on these strings. In the summer of 1920 it was all going rather well, and Emil Post as a freshly minted math PhD from Columbia arrived in Princeton to take up a prestigious fellowship. But there was one final problem. Having converted everything to string transformations, Post needed to have a theory of what such transformations could do. He progressively simplified things, until he reached what he called the problem of “tag”. Take a string of 0s and 1s. Drop its first ν elements. Look at the first dropped element. If it’s a 0 add a certain block of elements at the end of the string, and if it’s a 1 add another block. Post solved several cases of this problem. But then he came across the one he described as 0→00, 1→1101 with ν=3. Here’s an example of its behavior: After a few steps it just ends up in a simple loop, alternating forever between two strings. Here’s another example, starting now from a different string: Again this ends up in a loop, now involving 6 possible strings. But what happens in general? To Post, solving this problem was a seemingly simple stepping stone to his program of solving all of mathematics. And he began on it in the early summer of 1921, no doubt expecting that such a simple-to-state problem would have a correspondingly simple solution. But rather than finding a simple solution, he instead discovered that he could make little real progress. And after months of work he finally decided that the problem was in fact, as he later said, “hopeless”—and as a result, he concluded, so was his whole approach to “solving mathematics”. What had happened? Well, Post had seen a glimpse of a completely unanticipated but fundamental feature of what we now call computation. A decade later what was going on became a little clearer when Kurt Gödel discovered Gödel’s theorem and undecidability. (As Post later put it: “I would have discovered Gödel’s theorem in 1921—if I had been Gödel.”) Then as the years went by, and Turing machines and other kinds of computational systems were introduced, tag systems began to seem more about computation than about mathematics, and in 1961 Marvin Minsky proved that in fact a suitably constructed tag system could be made to do any computation that any Turing machine could do. But what about Post’s particular, very simple tag system? It still seemed very surprising that something so simple could behave in such complicated ways. But sixty years after Post’s work, when I started to systematically explore the computational universe of simple programs, it began to seem a lot less surprising. For—as my Principle of Computational Equivalence implies—throughout the computational universe, above some very low threshold, even in systems with very simple rules, I was seeing the phenomenon of computational irreducibility, and great complexity of behavior. But now a century has passed since Emil Post battled with his tag system. So armed with all our discoveries—and all our modern tools and technology—what can we now say about it? Can we finally crack Post’s problem of tag? Or—simple as it is—will it use the force of computational irreducibility to resist all our efforts? This is the story of my recent efforts to wage my own battle against Post’s tag system.
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.2 The Basic Setup : The Wolfram Language can be seen in part as a descendent of Post’s idea of representing everything in terms of transformation rules (though for symbolic expressions rather than strings). So it’s no surprise that Post’s problem of tag is very simple to set up in the Wolfram Language: Given the initial string, the complete behavior is always determined. But what can happen? In the examples above, what we saw is that after some “transient” the system falls into a cycle which repeats forever. Here’s a plot for all possible initial strings up to length 7. In each case there’s a transient and a cycle, with lengths shown in the plot (with cycle length stacked on top of transient length): (Note that if the system reaches 00—or another string with less than 3 characters—one can either say that it has a cycle of length 1, or that it stops completely, effectively with a cycle of length 0.) For initial strings up to length 7, the nontrivial cycles observed are of lengths 2 and 6. Starting from 10010 as above, we can show the behavior directly—or we can try to compensate for the removal of elements from the front at each step by rotating at each step: We can also show only successive “generations” in which the rule has effectively “gone through the whole string”: Let’s continue to longer initial sequences. Here are the lengths of transients and cycles for initial sequences up to length 12: All the cycles are quite short—in fact they’re all of lengths 0, 2, 4, 6 or 10. And for initial strings up to length 11, the transients (which we can think of as “halting times”) are at most of length 28. But at length 12 the string 100100100000 suddenly gives a transient of length 419, before finally evolving to the string 00. Here’s a plot of the sequence of lengths of intermediate strings produced in this case (the maximum length is 56): And, by the way, this gives an indication of why Post called this the “problem of tag” (at the suggestion of his colleague Bennington Gill). Elements keep on getting removed from the “head” of the string, and added to its “tail”. But will the head catch up with the tail? When it does, it’s like someone winning a game of tag, by being able to “reach the last person”. Here’s a picture of the detailed behavior in the case above: And here’s the “generational” plot, now flipped around to go from left to right: By the way, we can represent the complete history of the tag system just by concatenating the original string with all the blocks of elements that are added to it, never removing blocks of elements at the beginning. In this case this is the length-1260 string we get: Plotting the “walk” obtained by going up at each 1 and down at each 0 we get (and not surprisingly, this is basically the same curve as the sequence of total string lengths above): How “random” is the sequence of 0s and 1s? There are a total of 615 1s and 645 0s in the whole sequence—so roughly equal. For length-2 blocks, there are only about 80% as many 01s and 10s as 00s and 11s. For length-3 blocks, the disparities are larger, with only 30% as many 001 blocks occurring as 000 blocks. And then at length 4, there is something new: none of the blocks ever appear at all, and 0010 appears only twice, both at the beginning of the sequence. Looking at the rule, it’s easy to see why, for example, 1111 can never occur—because no sequence of the 00s and 1101s inserted by the rule can ever produce it. (We’ll discuss block occurrences more below.) OK, so we’ve found some fairly complicated behavior even with initial strings of length 12. But what about longer strings? What can happen with them? Before exploring this, it’s useful to look in a little more detail at the structure of the underlying problem.
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.3 The Space of Possible States : To find out what can happen in our tag system, we’ve enumerated all possible initial strings up to certain lengths. But it turns out that there’s a lot of redundancy in this—as our plots of “halting times” above might suggest. And the reason is that the way the tag system operates, only every third element in the initial string actually ever matters. As far as the rule is concerned we can just fill in _ for the other elements: The _’s will steadily be “eaten up”, and whether they were originally filled in with 0s or 1s will never matter. So given this, we don’t lose any information by using a compressed representation of the strings, in which we specify only every third element: But actually this isn’t quite enough. We also need to say the “phase” of the end of the string: the number of trailing elements after the last block of 3 elements (i.e. the length of the original string mod 3). So now we can start enumerating non-redundant possible initial strings, specifying them in the compressed representation: Given a string in compressed form, we can explicitly compute its evolution. The effective rules are a little more complicated than for the underlying uncompressed string, but for example the following will apply one step of evolution to any compressed string (represented in the form {phase, elements}): Can we reconstruct an uncompressed string from a compressed one? Well, no, not uniquely. Because the “intermediate” elements that will be ignored by the rule aren’t specified in the compressed form. Given, say, the compressed string 10:2 we know the uncompressed string must be of the form 1__0_ but the _’s aren’t determined. However, if we actually run the rule, we get so that the blanks in effect quickly resolve. (By the way, given a compressed string s:0 the uncompressed one is __, for s:1 it is just , and for s:2 it is , with the uncompressed string length mod 3 being equal to the phase.) So taking all compressed strings up to length 4 here is the sequence of transient and cycle lengths obtained: The first case that is cut off in the plot has halting time 419; it corresponds to the compressed string 1110:0. We can think of compressed strings as corresponding to possible non-redundant “states” of the tag system. And then we can represent the global evolution of the system by constructing a state transition graph that connects each state to its successor in the evolution. Here is the result starting from distinct length-3 strings (here shown in uncompressed form; the size of each node reflects the length of the string): There is a length-2 cycle, indicated in red, and also a “terminating state” indicated in yellow. Here’s the state transition graph starting with all length-1 compressed strings (i.e. non-redundant uncompressed strings with lengths between 3 and 5)—with nodes now labeled just with the (uncompressed) length of the string that they represent: We see the same length-2 cycle and terminating state as we saw before. But now there is also a length-6 cycle. The original “feeder” for this length-6 cycle is the string 10010 (compressed: 11:2), which takes 16 steps to reach the cycle. Here are the corresponding results for compressed initial strings up to successively greater lengths n, with the lengths of cycles labeled: A notable feature of these graphs is that at compressed length 4, a long “highway” appears that goes for about 400 steps. The highway basically represents the long transient first seen for the initial string 11:2. There is one “on-ramp” for this string, but then there is also a tree of other states that enter the same highway. Why is there a “highway” in the first place? Basically because the length-419 transient involves strings that are long compared to any we are starting from—so nothing can feed into it after the beginning, and it basically just has to “work itself through” until it reaches whatever cycle it ends up in. When we allow initial strings with compressed length up to 6 a new highway appears, dwarfing the previous one (by the way, most of the wiggliness we see is an artifact of the graph layout): The first initial state to reach this highway is 111010:0 (uncompressed: 100100100000100000)—which after 2141 steps evolves to a cycle of length 28. Here are the lengths of the intermediate strings along this highway (note the cycle at the end): And here are the “generational states” reached (note that looking only at generations makes the final 28-cycle show up as a 1-cycle): Or looking at “compressed strings” (i.e. including only every third element of each string): If we consider all initial strings up to compressed length 6, we get the following transient+cycle lengths: And what we see is that there are particular lengths of transients—corresponding to the highways in the state transition graph above—to which certain strings evolve. If we plot the distribution of halting (i.e. transient) times for all the strings we find, then, as expected, it peaks around the lengths of the main highways: So given a particular “on-ramp to a highway”—or, for that matter, a state on a cycle—what states will evolve to it? In general there’ll be a tree of states in the state transition graph that are the “predecessors” of a given state—in effect forming its “basin of attraction”. For any particular string the rule gives a unique successor. But we can also imagine “running the rule backwards”. And if we do this, it turns out that any given compressed string can have 0, 1 or 2 immediate predecessors. For example, 000:0 has the unique predecessor 0000:1. But 001:0 has both 0001:1 and 100:2 as predecessors. And for example 001:1 has no predecessors. (For uncompressed strings, there are always either 0 or 4 immediate predecessors.) Any state that has no predecessors can occur only as the initial string; it can never be generated in the evolution. (There are similar results for substrings, as we’ll discuss later.) And if we start from a state that does have at least one predecessor, we can in general construct a whole tree of “successively further back” predecessors. Here, for example, is the 10-step tree for 000:2: Here it is after 30 steps, in two different renderings: If we continue this particular tree we’ll basically get a state transition graph for all states that eventually terminate. Not surprisingly, there’s considerable complexity in this tree—though the number of states after t steps does grow roughly exponentially (apparently like ): By the way, there are plenty of states that have finite predecessor trees. For example 1100:0 yields a tree which grows only for 21 steps, then stops.
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.4 The Cycle Structure : At least in all the cases we’ve seen so far, our tag system always evolves to a cycle (or terminates in a trivial state). But what cycles are possible? In effect any cycle state S must be a solution to a “tag eigenvalue equation” of the form S = S for some p, where T is the “tag evolution operator”. Starting with compressed strings of length 1, only one cycle can ever be reached: Starting with compressed strings of length 2 a 6-cycle appears (here shown labeled respectively with uncompressed and with compressed strings): No new cycles appear until one has initial strings of compressed length 4, but then one gets (where now the states are labeled with their uncompressed lengths): The actual cycles are as follows while the ones from length-5 initial strings are: What larger cycles can occur? It is fairly easy to see that a compressed string consisting of any sequence of the blocks 01 and 1100 will yield a state on a cycle. To find out about uncompressed strings on cycles, we can just apply the rule 0→00, 1→1101, with the result that we conclude that any sequence of the length-6 and length-12 blocks 001101 and 110111010000 will give a state on a cycle. If we plot the periods of cycles against the lengths of their “seed” strings, we get: If we generate cycles from sequences of, say, b of our 01, 1100 blocks, how many of the cycles we get will be distinct? Here are the periods of the distinct cycles for successive b: The total number of cycles turns out to be: We can also ask an inverse question: of all 2n (uncompressed) strings of length n, how many of them lie on cycles of the kind we have identified? The answer is the same as the number of distinct “cyclic necklaces” with n beads, each 0 or 1, with no pair of 0s adjacent: Asymptotically this is about —implying that of all strings of length n, only a fraction ≈ of them will be on cycles, so that for large n the overwhelming majority of strings will not be on cycles, at least of this kind. But are there other kinds of cycles? It turns out there are, though they do not seem to be common or plentiful. One family—always of period 6—are seeded by strings obtained from 00111(000111m) by applying the rule 0 → 00, 1 → 1101 (with length 16 + 18m): But there are other cases too. The first example appears with initial compressed strings of length 9. The length-13 compressed string 0011111110100 (with uncompressed length 39) yields the period-40 cycle (with uncompressed string lengths between 37 and 44): The next example occurs with an initial compressed string of length 15, and a compressed “seed” of length 24—and has period 282: And I’ve found one more example (that arises from an initial compressed string of length 18) and has period 66: If we look at these cycles in “generational” terms, they are of lengths 3, 11 and 14, respectively (note that the second two pictures above start with “incomplete generations”).
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.5 Exploring Further : I don’t know how far Emil Post got in exploring his tag system by hand a century ago. And I rather suspect that we’ve already gone a lot further here than he ever did. But what we’ve seen has just deepened the mystery of what tag systems can do. So far, every initial string we’ve tried has evolved to a cycle (or just terminated). But will this always happen? And how long can it take? So far, the longest transient we’ve seen is 2141 steps—from the length-6 compressed string 111010:0. Length-7 and length-8 strings at most just “follow the same highway” in the state transition graph, and don’t give longer transients. But at length 9 something different happens: 111111010:0 takes 24,552 steps to evolve a 6-cycle (with string length 12), with the lengths of intermediate (compressed) strings being: Plotting (from left to right) the actual elements in compressed strings in each “generation” this shows in more detail what’s “going on inside”: In systematically exploring what can happen in tag systems, it’s convenient to specify initial compressed strings by converting their sequences of 1s and 0s to decimal numbers—but because our strings can have leading 0s we have to include the length, say as a prefix. So with this setup our length-9 “halting time winner” 111111010:0 becomes 9:506:0. The next “winner” is 12:3962:0, which takes 253,456 steps to evolve to a 6-cycle: In generational form the explicit evolution in this case is: The first case to take over a million steps is 15:30166:0—which terminates after 20,858,103 steps: The first case to take over a billion steps is 20:718458:0—which leads to a 6-cycle after 2,586,944,112 steps: Here’s table of all the “longest-so-far” winners through compressed initial length-28 strings (i.e. covering all ≈ 2 × 1025 ordinary initial strings up to length 84): And here are their “size traces”: One notable thing here—that we’ll come back to—is that after the first few cases, it’s very difficult to tell the overall scale of these pictures. On the first row, the longest x axis is about 20,000 steps; on the last row it is about 600 billion. But probably the most remarkable thing is that we now know that for all (uncompressed) initial strings up to length 75, the system always eventually evolves to a cycle (or terminates).
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.6 Are They Like Random Walks? : Could the sequences of lengths in our tag system be like random walks? Obviously they can’t strictly be random walks because given an initial string, each entire “walk” is completely determined, and nothing probabilistic or random is introduced. But what if we look at a large collection of initial conditions? Could the ensemble of observed walks somehow statistically be like random walks? From the basic construction of the tag system we know that at each step the (uncompressed) string either increases or decreases in length by one element depending on whether its first element is 1 or 0. But if we just picked increase or decrease at random here are two typical examples of ordinary random walks we’d get: One very obvious difference from our tag system case is these walks can go below 0, whereas in the tag system case once one’s reached something at least close to 0 (corresponding to a cycle), the walk stops. (In a market analogy, the time series ends if there’s “bankruptcy” where the price hits 0.) An important fact about random walks (at least in one dimension) is that with probability 1 they always eventually reach any particular value, like 0. So if our tag system behaved enough like a random walk, we might have an argument that it must “terminate with probability 1” (whatever that might mean given its discrete set of possible initial conditions). But how similar can the sequence generated by a tag system actually be to an ordinary random walk? An important fact is that—beyond its initial condition—any tag system sequence must always consist purely of concatenations of the blocks 00 and 1101, or in other words, the sequence must be defined by a path through the finite automaton: And from this we can see that—while all 2-grams and 3-grams can occur—the 4-grams 1111,1100, 0101 and 0010 can never occur. In addition, if we assume that 0s and 1s occur with equal probability at the beginning of the string, then the blocks 00 and 1101 occur with equal probability, but the 3-grams 000, 011 occur with double the probabilities of the others. In general the numbers of possible m-grams for successive m are 2, 4, 8, 12, 15, 20, 25, 33, 41, …  or for all m ≥ 3: Asymptotically this is —implying a limiting set entropy of per element. The relative frequencies of m-grams that appear (other than 0000…) are always of the form . The following lists for each m the number of m-grams that appear at given multiplicities (as obtained from Flatten[DeBruijnSequence[{{0,0},{1,1,0,1}},m]]): (This implies a “p log p” measure entropy of below 0.1.) So what happens in actual tag system sequences? Once clear of the initial conditions, they seem to quite accurately follow these probabilistic (“mean-field theory”) estimates, though with various fluctuations. In general, the results are quite different from a pure ordinary random walk with every element independent, but in agreement with the estimates for a “00, 1101 random walk”. Another difference from an ordinary random walk is that our walks end whenever they reach a cycle—and we saw above that there are an infinite number of cycles, of progressively greater sizes. But the density of such “trap” states is small: among all size-n strings, only perhaps of them lie on cycles. The standard theory of random walks says, however, that in the limit of infinitely large strings and long walks, if there is indeed a random process underneath, these things will not matter: we’ll have something that is in the same universality class as the ordinary ±1 random walk, with the same large-scale statistical properties. But what about our tag systems that survive billions of steps before hitting 0? Could genuine random walks plausibly survive that long? The standard theory of first passage times (or “stopping times”) tells us that the probability for a random walk starting at 0 to first reach x (or, equivalently, for a walk starting at x to reach 0) at time t is: P(t) = (x exp(-(x^2/(2 t))))/Sqrt[2 \[Pi] t^3] This shows the probability of starting from x and first reaching 0 as a function of the number of steps: The most likely stopping time is , but there is a long tail, and the probability of surviving for a time longer than t is: erf(x/Sqrt[2 t]) \[TildeTilde] Sqrt[2/(\[Pi] t)] x How does this potentially apply to our systems? Assume we start from a string of (compressed) length n. This implies that the probability to survive for t steps (before “reaching x = 0”) is about . But there are 3 × 2n possible strings of length n. So we can roughly estimate that one of them might survive for about steps, or at least a number of steps that increases roughly exponentially with n. And our results for “longest-so-far winners” above do in fact show roughly exponential increase with n (the dotted line is ≈ ): We can do a more detailed comparison with random walks by looking at the complete distribution of halting (AKA stopping) times for tag systems. Here are the results for all n = 15 and 25 initial strings: Plotting these on a log scale we get showing at least a rough approximation to the behavior expected for a random walk. In making distributions like these, we’re putting together all the initial strings of length n, and asking about the statistical properties of this ensemble. But we can also imagine seeing whether initial strings with particular properties consistently behave differently from others. This shows the distribution of halting times as a function of the number of 1s in the initial string; no strong correlations are seen (here for n = 20), even though at least at the beginning the presence of 1s leads to growth:.
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.7 Analogies & Expectations : How should we think about what we’re seeing? To me it in many ways just seems a typical manifestation of the ubiquitous phenomenon of computational irreducibility. Plenty of systems show what seems like random walk behavior. Even in rule 30, for example, the dividing line between regularity and randomness appears to follow a (biased) random walk: If we changed the initial conditions, we’d get a different random walk. But in all cases, we can think of the evolution of rule 30 as intrinsically generating apparent randomness, “seeded” by its initial conditions. Even more directly analogous to our tag system are cellular automata whose boundaries show apparent randomness. An example is the k = 2, r = 3/2 rule 7076: Will this pattern go on growing forever, or will it eventually become very narrow, and either enter a cycle or terminate entirely? This is analogous to asking whether our tag system will halt. There are other cellular automata that show even more obvious examples of these kinds of questions. Consider the k = 3, r = 1 totalistic code 1329 cellular automaton. Here is its behavior for a sequence of simple initial conditions. In some cases the pattern dies out (“it halts”); in some cases it evolves to a (rather elaborate) period-78 cycle. And in one case here it evolves to a period-7 cycle: But is this basically all that can happen? No. Here are the various persistent structures that occur with the first 10,000 initial conditions—and we see that in addition to getting ordinary “cycles”, we also get “shift cycles”: But if we go a little further, there’s another surprise: initial condition 54,889 leads to a structure that just keeps growing forever—while initial condition 97,439 also does this, but in a much more trivial way: In our tag system, the analog of these might be particular strings that produce patterns that “obviously grow forever”. One might think that there could be a fundamental difference between a cellular automaton and a tag system. In a cellular automaton the rules operate in parallel, in effect connecting a whole grid of neighboring cells, while in a tag system the rules only specifically operate on the very beginning and end of each string. But to see a closer analogy we can consider every update in the tag system as an “event”, then draw a causal graph that shows the relationships between these events. Here is a simple case: Extracting the pure causal graph we get: For the string 4:14:0 which takes 419 steps to terminate, the causal graph is: Or laid out differently, and marking expansion (1→1101) and contraction (0→00) events with red and blue: Here is the causal graph for the 2141-step evolution of 6:58:0 and what is notable is that despite the “spatial localization” of the underlying operation of the tag system, the causal graph in effect connects events in something closer to a uniform mesh.
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.8 Connecting to Number Theory : When Emil Post was first studying tag systems a hundred years ago he saw them as the last hurdle in finding a systematic way to “solve all of mathematics”, and in particular to solve all problems in number theory. Of course, they turned out to be a very big hurdle. But having now seen how complex tag systems can be, it’s interesting to go back and connect again with number theory. It’s straightforward to convert a tag system into something more obviously number theoretical. For example, if one represents each string of length n by a pair of integers {n,i} in which the binary digits of i give the elements of the string, then each step in the evolution can be obtained from: Starting from the 4:14:0 initial condition (here represented in uncompressed form by {12, 2336}) the first few steps are then: For compressed strings, the corresponding form is: There are different number theoretical formulations one can imagine, but a core feature is that at each step the tag system is making a choice between two arithmetic forms, based on some essentially arithmetic property of the number obtained so far. (Note that the type of condition we have given here can be further “compiled” into “pure arithmetic” by extracting it as a solution to a Diophantine equation.) A widely studied system similar to this is the Collatz or 3n + 1 problem, which generates successive integers by applying the function: Starting, say, from 27, the sequence of numbers obtained is 27, 82, 41, 124, 62, 31, ... where after 110 steps the system reaches the cycle 4, 2, 1, 4, 2, 1, .... As a closer analog to the plots for tag systems that we made above we can instead plot the lengths of the successive integers, represented in base 2: The state transition graph starting from integers up to 10 is and up to 1000 it is: Unlike for Post’s tag system, there is only one connected component (and one final cycle), and the “highways” are much shorter. For example, among the first billion initial conditions, the longest transient is just 986 steps. It occurs for the initial integer 670617279—which yields the following sequence of integer lengths: Despite a fair amount of investigation since the 1930s, it’s still not known whether the 3n + 1 problem always terminates on its standard cycle—though this is known to be the case for all integers up to . For Post’s tag system the most obvious probabilistic estimate suggests that the sequence of string lengths should follow an unbiased random walk. For the 3n + 1 problem, a similar analysis suggests a random walk with an average bias of binary digits per step, as suggested by this collection of walks from initial conditions + k: The rule (discussed in A New Kind of Science) instead implies a bias of +0.11 digits per step, and indeed most initial conditions lead to growth: But there are still some that—even though they grow for a while—have “fluctuations” that cause them to “crash” and end up in cycles: What is the “most unbiased” a n + b system? If we consider mod 3 instead of mod 2, we have systems like: We need to be divisible by 3 when n = i mod 3. In our approximation, the bias will be . This is closest to zero (with value +0.05) when ai are 4 and 7. An example of a possible iteration is then: Starting from a sequence of initial conditions this clearly shows less bias than the 3n + 1 case: Here are the halting times for initial conditions up to 1000: Most initial conditions quickly evolve to cycles of length 5 or 20. But initial condition 101 takes 2604 steps to reach the 20-cycle: And initial condition 469 does not appear to reach a cycle at all—and instead appears to systematically grow at about 0.018 bits per step: In other words, unlike the 3n + 1 problem—or our tag system—this iteration usually leads to a cycle, but just sometimes appears to “escape” and continue to increase, presumably forever. (In general, for modulus m, the minimum bias will typically be , and the “smoothest” iterations will be ones whose multipliers involve similar-sized factors of numbers close to . For m = 4, for example, {n, 3n – 3, 5n – 2, 17n + 1} is the best.) One might wonder how similar our tag system---or the 3n + 1 problem---is to classic unsolved problems in number theory, like the Riemann Hypothesis. In essence the Riemann Hypothesis is an assertion about the statistical randomness of primes, normally stated in terms of complex zeroes of the Riemann zeta function, or equivalently, that all the maxima of RiemannSiegelZ[t] (for any value of t) lie above the axis: But it’s known (thanks to extensive work by Yuri Matiyasevich) that an equivalent—much more obviously integer-related—statement is that is positive for all positive n. And this then turns out to be equivalent to the surprisingly simple statement that the iteration will never terminate. For successive n the quantity above is given by: At least at the beginning the numbers are definitely positive, as the Riemann Hypothesis would suggest. But if we ask about the long-term behavior we can see something of the complexity involved by looking at the differences in successive ratios: The Riemann Hypothesis effectively says that there aren’t too many negative differences here.
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.9 Other Tag Systems : So far we’ve been talking specifically about Emil Post’s particular 00, 1101 tag system. But as Post himself observed, one can define plenty of other tag systems—including ones that involve not just 0 and 1 but any number of possible elements (Post called the number of possible elements μ, but I’ll call it k), and delete not just 3 but any number of elements at each step (Post called this ν, but I’ll call it r). It’s easy to see that rules which delete only one element at each step (r = 1) cannot involve real “communication” (or causal connections) between different parts of the string, and must be equivalent to neighbor-independent substitution systems—so that they either have trivial behavior, or grow without bound to produce at most highly regular nested sequences. (0→01, 1→10 will generate the Thue–Morse string, while 0→01, 1→0 will generate the Fibonacci string.) Things immediately get more complicated when two elements are deleted at each step (r = 2). Post correctly observed that with just 0 and 1 (k = 2) there are no rules that show the kind of sometimes-expanding, sometimes-contracting behavior of his 00, 1101 rule. But back in 2007—as part of a live experiment at our annual Summer School—I looked at the r = 2 rule 0→1, 1→110. Here’s what it does starting with 10: And here’s how the sequence of string lengths behaves: If we assume that 0 and 1 appear randomly with certain probabilities, then a simple calculation shows that 1 should occur about times as often as 0, and the string should grow an average of elements at each step. So “detrending” by this, we get: Continuing for more steps we see a close approximation to a random walk: So just like with Post’s 00, 1101 rule—and, of course, with rule 30 and all sorts of other systems in the computational universe—we have here a completely deterministic system that generates what seems like randomness. And indeed among tag systems of the type we’re discussing here this appears to be the very simplest rule that shows this kind of behavior. But does this rule show the same kind of growth from all initial conditions? It can show different random sequences, for example here for initial conditions 5:17 and 7:80: And sometimes it just immediately enters a cycle. But it has some “surprises” too. Like with initial condition 9:511 (i.e. 111111111) it grows not linearly, but like (shown here without any detrending): But what about a tag system that doesn’t seem to “typically grow forever”? When I was working on A New Kind of Science I studied generalized tag systems that don’t just look at their first elements, but instead use the whole block of elements they’re deleting to determine what elements to add at the end (and so work in a somewhat more “cellular-automaton-style” way). One particular rule that I showed in A New Kind of Science (as case (c) on page 94) is: Starting with 11 this rule gives and grows for a while—but then terminates after 289 steps: The corresponding generational evolution is: (Note that the kind of “phase decomposition” that we did for Post’s tag system doesn’t make sense for a block tag system like this.) Here are the lengths of the transients+cycles for possible initial conditions up to size 7: This looks more irregular—and “livelier”—than the corresponding plot for Post’s tag system, but not fundamentally different. At size 5 the initial string 11010 (denoted 5:12) yields which terminates after 706 steps in a length-8 cycle. Going further one sees a sequence of progressively longer transients: But like with Post’s tag system, the system always eventually reaches a cycle (or terminates)—at least for all initial strings up to size 17. But what will happen for the longest initial strings is not clear, and the greater “liveliness” of this system relative to Post’s suggests that if exotic behavior occurs, it will potentially do so for smaller initial strings than in Post’s system. Another way to generalize Post’s 00, 1101 tag system is to consider not just elements 0, 1, but, say, 0, 1, 2 (i.e. k = 3). And in this case there is already complex behavior even with rules that consider just the first element, and delete two elements at each step (r = 2). As an example, consider the rule: Starting, say, with 101 this gives which terminates after 74 steps: Here are the lengths of transients+cycles for this rule up to length-6 initial (ternary) strings: The initial string 202020 (denoted 6:546, where now this indicates ternary rather than binary) terminates after 6627 steps with (phase-reduced) generational evolution: And once again, the overall features of the behavior are very similar to Post’s system, with the longest halting times seen up to strings of length 14 being: But what about other possible rules? As an example, we can look at all 90 possible k = 3, r = 2 rules of the form 0→_, 1→__, 2→___ in which the right-hand sides are “balanced” in the sense that in total they all contain two 0s, 1s and 2s. This shows the evolution (for 100 steps) for each of these rules that has the longest transient for any initial string with less than 7 elements: Many lead quickly to cycles or termination. Others after 100 steps seem to be growing irregularly, but all the specific evolutions shown here eventually halt. There are peculiar cases, like 0→0, 1→02, 2→112 which precisely repeats the initial string 20 after 18,255 steps: And then there are cases like 0→0, 1→01, 2→212, say starting from 200020, which either halt quickly, or generate strings of ever-increasing length (here like ) and can easily be seen never to halt: (By the way, the situation with “non-balanced” k = 3 rules is not fundamentally different from balanced ones; 0→0, 1→22, 2→102, for example, shows very “Post-like” behavior.) The tag systems we’ve been discussing are pretty simple. But an even simpler version considered in A New Kind of Science are what I called cyclic tag systems. In a cyclic tag system one removes the first element of the string at each step. On successive steps, one cycles through a collection of possible blocks to add, adding one if the deleted element was a 1 (and otherwise adding nothing). If the possible blocks to add are 111 and 0, then the behavior starting from the string 1 is as follows with the lengths “detrended by t/2” behaving once again like an approximate random walk: With cycles of just 2 blocks, one typically sees either quick cycling or termination, or what seems like obvious infinite growth. But if one allows a cycle of 3 blocks, more complicated halting behavior becomes possible. Consider for example 01, 0, 011. Starting from 0111 one gets with the system halting after 169 steps: Here are the transient+cycle times for initial strings up to size 8 (the system usually just terminates, but for example 001111 goes into a cycle of length 18): The behavior of the longest-to-halt-so-far “winners” are again similar to what we have seen before---except perhaps for the rather huge jump in halting time at length 13---that isn’t surpassed until size 16:.
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.10 What Can It Compute? : When Post originally invented tag systems in 1920 he intended them as a string-based idealization of the operations in mathematical proofs. But a decade and a half later, once Turing machines were known, it started to be clear that tag systems were better framed as being computational systems. And by the 1940s it was known that at least in principle string-rewriting systems of the kind Post used were capable of doing exactly the same types of computations as Turing machines—or, as we would say now, that they were computation universal. At first what was proved was that a fairly general string-rewriting system was computation universal. But by the early 1960s it was known that a tag system that looks only at its first element is also universal. And in fact it’s not too difficult to write a “compiler” that takes any Turing machine rule and converts it to a tag system rule—and page 670 of A New Kind of Science is devoted to showing a pictorial example of how this works: For example we can take the simplest universal Turing machine (which has 2 states and 3 colors) and compile it into a 2-element-deletion tag system with 32 possible elements (the ones above 9 represented by letters) and rules: But what about a tag system like Post’s 00, 1101 one—with much simpler rules? Could it also be universal? Our practical experience with computers might make us think that to get universality we would necessarily have to have a system with complicated rules. But the surprising conclusion suggested by the Principle of Computational Equivalence is that this is not correct—and that instead essentially any system whose behavior is not obviously simple will actually be capable of universal computation. For any particular system it’s usually extremely difficult to prove this. But we now have several examples that seem to validate the Principle of Computational Equivalence—in particular the rule 110 cellular automaton and the 2,3 Turing machine. And this leads us to the conjecture that even tag systems with very simple rules (at least ones whose overall behavior is not obviously simple) should also be computation universal. How can we get evidence for this? We might imagine that we could see a particular tag system “scanning over” a wide range of computations as we change its initial conditions. Of course, computation universality just says that it must be possible to construct an initial condition that performs any given computation. And it could be that to perform any decently sophisticated computation would require an immensely complex initial condition, that would never be "found naturally" by scanning over possible initial conditions. But the Principle of Computational Equivalence actually goes further than just saying that all sorts of systems can in principle do sophisticated computations; it says that such computations should be quite ubiquitous among possible initial conditions. There may be some special initial conditions that lead to simple behavior. But other initial conditions should produce behavior that corresponds to a computation that is in a sense as sophisticated as any other computation. And a consequence of this is that the behavior we see will typically be computationally irreducible: that in general there will be no way to compute its outcome much more efficiently than just by following each of its steps. Or, in other words, when we observe the system, we will have no way to computationally reduce it—and so its behavior will seem to us complex. So when we find behavior in tag systems that seems to us complex—and that we do not appear able to analyze or predict—the expectation is that it must correspond to a sophisticated computation, and be a sign that the tag system follows the Principle of Computational Equivalence and is computation universal. But what actual computations do particular tag systems do? Clearly they do the computations that are defined by their rules. But the question is whether we can somehow also interpret the overall computations they do in terms of familiar concepts, say in mathematics or computer science. Consider for example the 2-element-deletion tag system with rules 1→111. Starting it off with 11 we get and we can see that the tag in effect just “counts up in unary”. (The 1-element-deletion rule 1→11 does the same thing.) Now consider the tag system with rules: Starting it with 11 we get or more pictorially (red is 1, blue is 2): But now look at steps where strings of only 1s appear. The number of 1s in these strings forms the sequence of successive powers of 2. (The 1-element-deletion rule 1→2, 2→11 gives the same sequence..
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.11 The rule : starting from 11 yields instead and now the lengths of the sequences of 1s form the sequence: This sequence is not as familiar as powers of 2, but it still has a fairly traditional “mathematical interpretation”: it is the result of iterating (and this same iteration applies for any initial string of 1s of any length). But consider now the rule: Here is what it does starting with sequences of 1s of different lengths: In effect it is taking the initial number of 1s n and computing the function: But what “is” this function? In effect it depends on the binary digits of n, and turns out to be given (for n > 1) by: What other “identifiable functions” can simple tag systems produce? Consider the rules: Starting with a string of five 1s this gives (3 is white) in effect running for 21 steps and then terminating. If one looks at the string of 1s produced here, their sequence of lengths is 5, 8, 4, 2, 1, and in general the sequence is determined by the iteration except that if n reaches 1 the tag system terminates, while the iteration keeps going. So if we ask what this tag system is “doing”, we can say it’s computing 3n + 1 problem iterations, and we can explicitly “see it doing the computation”. Here it’s starting with n = 7 and here it’s starting with successive values of n: Does the tag system always eventually halt? This is exactly the 3n + 1 problem—which has been unsolved for the better part of a century. It might seem remarkable that even such a simple tag system rule can in effect give us such a difficult mathematical problem. But the Principle of Computational Equivalence makes this seem much less surprising—and in fact it tells us that we should expect tag systems to quickly “ascend out of” the range of computations to which we can readily assign traditional mathematical interpretations. Changing the rule to yields instead the iteration which again is “interpretable” as corresponding to the iteration: But what if we consider all possible rules, say with the very simple form 1→__, 2→___? Here is what each of the 32 of these does starting from 1111: For some of these we’ve been able to identify “traditional mathematical interpretations”, but for many we have not. And if we go even further and look at the very simplest nontrivial rules—of the form 1→_, 2→___—here is what happens starting from a string of 10 1s: One of these rules we already discussed above and we found that it seems to lead to infinite irregular growth (here shown “detrended” by ): But even in the case of which appears always to halt the differences between halting times with successive sizes of initial strings form a surprisingly complex sequence that does not seem to have any simple traditional mathematical interpretation. (By the way, in a case like this it’s perfectly possible that there will be some kind of “mathematical interpretation”—though it might be like the page of weird definitions that I found for halting times of Turing machine 600720 in A New Kind of Science..
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.12 So Does It Always Halt? : When Emil Post was studying his tag system back in 1921, one of his big questions was: “Does it always halt?” Frustratingly enough, I must report that even a century later I still haven’t been able to answer this question. Running Post’s tag system on my computer I’m able to work out what it does billions of times faster than Post could. And I’ve been able to look at billions of possible initial strings. And I’ve found that it can take a very long time—like half a trillion steps—for the system to halt: But so far—even with all the computation I’ve done—I haven’t found a single example where it doesn’t eventually halt. If we were doing ordinary natural science, billions of examples that all ultimately work the same would normally be far more than enough to convince us of something. But from studying the computational universe we know that this kind of “scientific inference” won’t always be correct. Gödel’s theorem from 1931 introduced the idea of undecidability (and it was sharpened by Turing machines, etc.). And that’s what can bite us in the computational universe. Because one of the consequences of undecidability as we now understand it is that there can be questions where there may be no bound on how much computation will be needed to answer them. So this means that even if we have failed to see something in billions of examples that doesn't mean it’s impossible; it may just be that we haven’t done enough computation to see it. In practice it’s tended to be assumed, though, that undecidability is something rare and exotic, that one will only run into if one asks some kind of awkward—or “meta”—question. But my explorations in the computational universe—and in particular my Principle of Computational Equivalence—imply that this is not correct, and that instead undecidability is quite ubiquitous, and occurs essentially whenever a system can behave in ways that are not obviously simple. And this means that—despite the simplicity of its construction—it’s actually to be expected that something like the 00, 1101 tag system could show undecidability, and so that questions about it could require arbitrary amounts of computational effort to answer. But there’s something of a catch. Because the way one normally proves the presence of undecidability is by proving computation universality. But at least in the usual way of thinking about computation universality, a universal system cannot always halt—since otherwise it wouldn’t be able to emulate systems that themselves don’t halt. So with this connection between halting and computation universality, we have the conclusion that if the 00, 1101 tag system always halts it cannot be computation universal. So from our failure to find a non-halting example the most obvious conclusion might be that our tag system does in fact always halt, and is not universal. And this could then be taken as evidence against the Principle of Computational Equivalence, or at least its application to this case. But I believe strongly enough in the Principle of Computational Equivalence that I would tend to draw the opposite conclusion: that actually the 00, 1101 tag system is universal, and won’t always halt, and it’s just that we haven’t gone far enough in investigating it to see a non-halting example yet. But how far should we have to go? Undecidability says we can’t be sure. But we can still potentially use experience from studying other systems to get some sense. And this in fact tends to suggest that we might have to go a long way to get our first non-halting example. We saw above an example of cellular automata in which unbounded growth (a rough analog of non-halting) does occur, but we have to look through nearly 100,000 initial conditions before we find it. A New Kind of Science contains many other examples. And in number theory, it is quite routine to have Diophantine equations where the smallest solutions are very large. How should we think about these kinds of things? In essence, we are taking computation universal systems and trying to “program them” (by setting up appropriate initial conditions) to have a particular form of behavior, say non-halting. But there is nothing to say these programs have to be short. Yes, non-halting might seem to us like a simple objective. And, yes, the universal system should in the end be able to achieve it. But given the particular components of the universal system, it may be complicated to get. Let me offer two analogies. The first has to do with mathematical proofs. Having found the very simplest possible axiom system for Boolean algebra ((p · q) · r) · (p · ((p · r) · p)) = = r, we know that in principle we can prove any theorem in Boolean algebra. But even something like p · q = q · p—that might seem simple to us—can take hundreds of elaborate steps to prove given our particular axiom system. As a more whimsical example, consider the process of self-reproduction. It seems simple enough to describe this objective, yet to achieve it, say with the components of molecular biology, may be complex. And maybe on the early Earth it was only because there were so many molecules, and so much time, that self-reproduction could ever be “discovered”. One might think that, yes, it could be difficult to find something (like a non-halting initial condition, or a configuration with particular behavior in a cellular automaton) by pure search, but that it would still be possible to systematically “engineer” one. And indeed there may be ways to “engineer” initial conditions for the 00, 1101 tag system. But in general it is another consequence of the Principle of Computational Equivalence (and computational irreducibility) that there is no guarantee that there will be any “simple engineering path” to reach any particular capability. By the way, one impression from looking at tag systems and many other kinds of systems is that as one increases the sizes of initial conditions, one crosses a sequence of thresholds for different behaviors. Only at size 14, for example, might some long “highway” in our tag system’s state transition graph appear. And then nothing longer might appear until size 17. Or some particular period of final cycle might only appear at size-15 initial conditions. It’s as if there’s a “minimum program length” needed to achieve a particular objective, in a particular system. And perhaps similarly there’s a minimum initial string length necessary to achieve non-halting in our tag system—that we just don’t happen to have reached yet. (I’ve done random searches in longer initial conditions, though, so we at least know it’s not common there.) OK, but let’s try a different tack. Let’s ask what would be involved in proving that the tag system doesn’t always halt. We’re trying to prove essentially the following statement: “There exists an initial condition i such that for all steps t the tag system has not halted”. In the language of mathematical logic this is a ∃∀ statement, that is at the level in the arithmetic hierarchy. One way to prove it is just explicitly to find a string whose evolution doesn’t halt. But how would one show that the evolution doesn’t halt? It might be obvious: there might for example just be something like a fixed block that is getting added in a simple cycle of some kind, as in: But it also might not be obvious. It could be like some of our examples above where there seems to be systematic growth, but where there are small fluctuations: Will these fluctuations suddenly become big and lead the system to halt? Or will they always stay somehow small enough that that cannot happen? There are plenty of questions like this that arise in number theory. And sometimes (as, for example, with the Skewes number associated with the distribution of primes) there can be surprises, with very long-term trends getting reversed only in exceptionally large cases. By the way, even identifying “halting” can be difficult, especially if (as we do for our tag system) we define “halting” to include going into a cycle. For example, we saw above a tag system that does cycle, but takes more than 18,000 steps to do so: Conversely, just because something takes a long time to halt doesn’t mean that it will be difficult to show this. For example, it is quite common to see Turing machines that take a huge number of steps to halt, but behave in basically systematic and predictable ways (this one takes 47,176,870 steps): But to “explain why something halts” we might want to have something like a mathematical proof: a sequence of steps consistent with a certain set of axioms that derives the fact that the system halts. In effect the proof is a higher-level (“symbolic”) way of representing aspects of what the system is doing. Instead of looking at all the individual values at each step in the evolution of the system we’re just calling things x and y (or whatever) and deriving relationships between them at some kind of symbolic level. And given a particular axiom system it may or may not be possible to construct this kind of symbolic proof of any given fact. It could be that the axiom system just doesn’t have the “derivational power” to represent faithfully enough what the system we are studying is doing. So what does this mean for tag systems? It means, for example, that it could perfectly well be that a given tag system evolution doesn’t halt—but that we couldn’t prove that using, say, the axiom system of Peano Arithmetic. And in fact as soon as we have a system that is computation universal it turns out that any finite axiom system must eventually fail to be able to give a finite proof of some fact about the system. We can think of the axioms as defining certain relations about the system. But computational irreducibility implies that eventually the system will be able to do things which cannot be “reduced” by any finite set of relations. Peano Arithmetic contains as an axiom the statement that mathematical induction works, in the sense that if a statement s[0] is true, and s[n] implies s[n + 1], then any statement s[n] must be true. But it’s possible to come up with statements that entail for example nested collections of recursions that effectively grow too quickly for this axiom alone to be able to describe symbolically “in one go” what they can do. If one uses a stronger axiom system, however, then one will be able to do this. And, for example, Zermelo–Fraenkel set theory—which allows not only ordinary induction but also transfinite induction—may succeed in being able to give a proof even when Peano Arithmetic fails. But in the end any finitely specified axiom system will fail to be able to prove everything about a computationally irreducible system. Intuitively this is because making proofs is a form of computational reduction, and it is inevitable that this can only go so far. But more formally, one can imagine using a computational system to encode the possible steps that can be made with a given axiom system. Then one would construct a program in the computational system that would systematically enumerate all theorems in the axiom system. (It may be easier to think of first creating a multiway system in which each possible application of the axiom rules is made, and then “unrolling” the multiway system to be “run sequentially”.) And for example we could set things up so that the computational system halts if it ever finds an inconsistency in the theorems derived from the axiom system. But then we know that we won’t be able to prove that the computational system does not halt from within the axiom system because (by Gödel’s second incompleteness theorem) no nontrivial axiom system can prove its own consistency. So if we chose to work, say, purely within Peano Arithmetic, then it might be that Post’s original question is simply unanswerable. We might have no way to prove or disprove that his tag system always halts. To know that might require a finer level of analysis—or, in effect, a higher degree of reduction—than Peano Arithmetic can provide. (Picking a particular model of Peano Arithmetic would resolve the question, but to home in on a particular model can in effect require infinite computational effort.) If we have a tag system that we know is universal then it’s inevitable that certain things about it will not be provable within Peano Arithmetic, or any other finitely specified axiom system. But for any given property of the system it may be very difficult to determine whether that property is provable within Peano Arithmetic. The problem is similar to proving computation universality: in effect one has to see how to encode some specified structure within a particular formal system—and that can be arbitrarily difficult to do. So just as it may be very hard to prove that the 00, 1101 tag system is computation universal, it may also be very difficult to prove that some particular property of it is not “accessible” through Peano Arithmetic. Could it be undecidable whether the 00, 1101 tag system always halts? And if we could prove this, would this actually have proved that it in fact doesn’t halt? Recall that above we mentioned that at least the obvious statement of the problem is at the level in the arithmetic hierarchy. And it turns out that statements at this level don’t have “default truth values”, so proving undecidability wouldn’t immediately give us a conclusion. But there’s nothing to say that some clever reformulation might not reduce the problem to or , at which point proving undecidability would lead to a definite conclusion. (Something like this in fact happened with the Riemann Hypothesis. At first this seemed like a statement, but it was reformulated as a statement---and eventually reduced to the specific statement several sections above that a particular computation should not terminate. But now if the termination of this is proved undecidable, it must in fact not terminate, and the Riemann Hypothesis must be true.) Can one prove undecidability without proving computation universality? There are in principle systems that show “intermediate degrees”: they exhibit undecidability but cannot directly be used to do universal computation (and Post was in fact the person who suggested that this might be possible). But actual examples of systems with intermediate degree still seem to involve having computation universality “inside”, but then limiting the input-output capabilities to prevent the universality from being accessed, beyond making certain properties undecidable. The most satisfying (and ultimately satisfactory) way to prove universality for the 00, 1101 tag system would simply be to construct a compiler that takes a specification of some other system that is known to support universality (say a particular known-to-be-universal tag system, or the set of all possible tag systems) and then turns this into an initial string for the 00, 1101 tag system. The tag system would then “run” the string, and generate something that could readily be “decoded” as the result of the original computation. But there are ways one might imagine establishing what amounts to universality, that could be enough to prove halting properties, even though they might not be as “practical” as actual ways to do computations. (Yes, one could conceivably imagine a molecular-scale computer that works just like a tag system.) In the current proofs of universality for the simplest cellular automata and Turing machines, for example, one assumes that their initial configurations contain “background” periodic patterns, with the specific input for a particular computation being a finite-size perturbation to this background. For a cellular automaton or Turing machine it seems fairly unremarkable to imagine such a background: even though it extends infinitely across the cells of the system it somehow does not seem to be adding more than a small amount of “new information” to the system. But for a tag system it’s more complicated to imagine an infinite periodic “background”, because at every step the string the system is dealing with is finite. One could consider modifying the rules of the tag system so that, for example, there is some fixed background that acts as a “mask” every time the block of elements is added at the end of the string. (For example, the mask could flip the value of every element, relative to a fixed “coordinate system”.) But with the original tag system rules the only way to have an infinite background seems to be to have an infinite string. But how could this work? The rules of the tag system add elements at the end of the string, and if the string is infinitely long, it will take an infinite number of steps before the values of these elements ever matter to the actual behavior of the system. There is one slightly exotic possibility, however, which is to think about transfinite versions of the tag system. Imagine that the string in the tag system has a length given by a transfinite number, say the ordinal ω. Then it is perfectly meaningful in the context of transfinite arithmetic to imagine additional elements being added at positions ω + 1 etc. And if the tag system then runs for ω steps, its behavior can start to depend on these added elements. Needless to say, the various issues I’ve discussed above about provability in particular axiom systems may come into play. But there may still be cases where definite results about computation universality could be established “symbolically” about transfinite tag systems. And conceivably such results could then be “projected down” to imply undecidability or other results about tag systems with finite initial strings. Clearly the question of proving (or disproving) halting for the 00, 1101 tag system is a complicated one. We might be lucky, and be able to find with our computers (or conceivably engineer) an initial string that we can see doesn’t halt. Or we might be able to construct a symbolic representation in which we can carry out a proof. But ultimately we are in a sense at the mercy of the Principle of Computational Equivalence. There is presumably computational irreducibility in the 00, 1101 tag system that we can’t systematically outrun. Yes, the trace of the tag system seems to be a good approximation to a random walk. And, yes, as a random walk it will halt with probability 1. But in reality it’s not a “truly random” random walk; it’s a walk determined by a specific computational process. We can turn our questions about halting to questions about the randomness of the walk (and to do so may provide interesting connections with the foundations of probability theory). But in the end we’re back to the same issues, and we’re still confronted by computational irreducibility.
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.13 More about the History : Tag systems are simple enough that it’s conceivable they might have arisen in something like games even millennia ago. But for us tag systems—and particularly the specific 00, 1101 tag system we’ve mostly been studying—were the invention of Emil Post, in 1921. Emil Post lived most of his life in New York City, though he was born (into a Jewish family) in 1897 in Augustow, Poland (then part of the Russian Empire). (And, yes, it’s truly remarkable how many of the notable contributors to mathematical logic in the early part of the 20th century were born to Jewish families in a fairly small region of what’s now eastern Poland and western Ukraine.) As a child, Post seems to have at first wanted to be an astronomer, but having lost his left arm in a freak car-related street accident at age 12 he was told this was impractical—and turned instead to mathematics. Post went to a public high school for gifted students and then attended City College of New York, graduating with a bachelor’s degree in math in 1917. Perhaps presaging a lifelong interest in generalization, he wrote his first paper while in college (though it wasn’t published until 15+ years later), on the subject of fractional differentiation. He enrolled in the math PhD program at Columbia, where he got involved in a seminar studying Whitehead and Russell’s recently published Principia Mathematica, run by Cassius Keyser, who was one of the early American mathematicians interested in the foundations of math (and who wrote many books on history and philosophy around mathematics; a typical example being his 1922 Mathematical Philosophy, a Study of Fate and Freedom). Early in graduate school, Post wrote a paper about functional equations for the gamma function (related to fractional differentiation), but soon he turned to logic, and his thesis—written in 1920—included early versions of what became his signature ideas. Post’s main objective in his thesis was to simplify, streamline and further formalize Principia Mathematica. He started by looking at propositional calculus, and tried to “drill down” to find out more of what logic was really about. He invented truth tables (as several other people also independently did) and used them to prove completeness and consistency results. He investigated how different logic functions could be built up from one another through composition, classifying different elements of what’s now called the Post lattice. (He commented on Nand and an early simple axiom system for it—and might well have gone further with it if he’d known the minimal axiom system for Nand that I finally discovered in 2000. In another small-intellectual-world story, I realize now his lattice is also similar to my “cellular automaton emulation network”.) Going in the direction of “what’s logic really about” Post also considered multivalued logic, and algebraic structures around it. Post published the core of his thesis in 1921 as “Introduction to a General Theory of Elementary Propositions”, but—in an unfortunate and recurring theme—didn’t publish the whole thing for another 20 years. But even in 1920 Post had what he called “generalization by postulation” and this quickly turned into the idea that all operations in Principia Mathematica (or mathematics in general) could ultimately be represented as transformations (“production rules”) on strings of characters. When he finally ended up publishing this in 1943 he called the resulting formal structures “canonical systems”. And already by 1920 he’d discovered that not all possible production rules were needed; it was sufficient to have only ones in “normal form” g$→$h, where $ is a “pattern variable”. (The idea of $ representing a pattern became common in early computer string-manipulation systems, and in fact I used it for expression patterns in my SMP system in 1979—probably without at the time knowing it came from Post.) Post was close to the concept of universal computation, and the notion that anything (in his case, any string transformation) could be built up from a fixed set of primitives. And in 1920 —in the effort to “reduce his primitives” he came up with tag systems. At the time—11 years before Gödel’s theorem—Post and others still thought that it might somehow be possible to “solve mathematics” in some finite way. Post felt he had good evidence that Principia Mathematica could be reduced to string rewriting, so now he just had to solve that. One basic question was how to tell when two strings should be considered equivalent under the string rewriting rules. And in formulating a simple case of this Post came up with tag systems. In particular, he wanted to determine whether the “iterative process [of tag] was terminating, periodic, or divergent”. And Post made “the problem of ‘tag’... the major project of [his] tenure of a Procter fellowship in mathematics at Princeton during the academic year 1920–21.” Post later reported that a “major success of the project was the complete solution of the problem for all bases in which μ and ν were both 2”, though stated that “even this special case... involved considerable labor”. But then, as he later wrote, “while considerable effort was expanded [sic] on the case μ = 2, ν > 2... little progress resulted... [with] such a simple basis as 0→00, 1→1101, ν = 3, proving intractable”. Post makes a footnote “Numerous initial sequences... tried [always] led... to termination or periodicity, usually the latter.” Then he added, reflecting our random walk observations, “It might be noted that an easily derived probability ‘prognostication’ suggested... that periodicity was to be expected.” (I’m curious how he could tell it should be periodicity rather than termination.) But by the end of the summer of 1921, Post had concluded that “the solution of the general problem of ‘tag’ appeared hopeless, and with it [his] entire program of the solution of finiteness problems”. In other words, the seemingly simple problem of tag had derailed Post’s whole program of “solving mathematics”. In 1920 Princeton had a top American mathematics department, and Post went there on a prestigious fellowship (recently endowed by the Procter of Procter & Gamble). But—like the problem of tag—things did not work out so well there for Post, and in 1921 he had the first of what would become a sequence of “runaway mind” manic episodes, in what appears to have been a cycle of what was then called manic depression. It's strange to think that the problem of tag might have "driven Post crazy", and probably the timing of the onset of manic depression had more to do with his age—though Post later seems to have believed that the excitement of research could trigger manic episodes (which often involved talking intensely about streams of poorly connected ideas, like the "psychic ether" from which new ideas come, discovering a new star named "Post", etc.) But in any case, in late 1921 Post—who had by then returned to Columbia—was institutionalized. By 1924 he had recovered enough to take up an instructorship at Cornell, but then relapsed. Over the years that followed he supported himself by teaching high school in New York, but continued to have mental health issues. He married in 1929, had a daughter in 1932, and in 1935 finally became a professor at City College, where he remained for the rest of his life. Post published nothing from the early 1920s until 1936. But in 1936—with Gödel’s theorem known, and Alonzo Church’s “An Unsolvable Problem of Elementary Number Theory” recently published—Post published a 3-page paper entitled “Finite Combinatory Processes—Formulation 1”. Post comes incredibly close to defining Turing machines (he talks about “workers” interacting with a potentially infinite sequence of “marked” and “unmarked boxes”). And he says that he “expects [his] formulation to be logically equivalent to recursiveness in the sense of the Gödel–Church development”, adding “Its purpose, however, is not only to present a system of a certain logical potency but also, in its restricted field, of psychological fidelity”. Post doesn’t get too specific, but he does make the comment (rather resonating with my own work, and particularly our Physics Project) that the hypothesis of global success of these formalisms would be “not so much... a definition or an axiom but... a natural law”. In 1936 Post also published his longest-ever paper: 142 pages on what he called “polyadic groups”. It’s basically about abstract algebra, but in typical Post style, it’s a generalization, involving looking not at binary “multiplication” operations but for example ternary ones. It’s not been a popular topic, though, curiously, I also independently got interested in it in the 1990s, eventually discovering Post’s work on it. By 1941 Post was publishing more, including several now-classic papers in mathematical logic, covering things like degrees of unsolvability, the unsolvability of the word problem for semigroups, and what’s now called the Post Correspondence Problem. He managed his time in a very precise way, following a grueling teaching schedule (with intense and precise lectures planned to the minute) and---apparently to maintain his psychological wellbeing---restricting his research activities to three specific hours each day (interspersed with walks). But by then he was a respected professor, and logic had become a more popular field, giving him more of an audience. In 1943, largely summarizing his earlier work, Post published “Formal Reductions of the General Combinatorial Decision Problem”, and in it, the “problem of tag” makes its first published appearance: Post notes that “the little progress made in [its] solution” makes it a “candidate for unsolvability”. (Notice the correction in Post’s handwriting “intensely” → “intensively” in the copy of his paper reproduced in his collected works.) Through all this, however, Post continued to struggle with mental illness. But by the time he reached the age of 50 in 1947 he began to improve, and even loosened up on his rigid schedule. But in 1954 depression was back, and after receiving electroshock therapy (which he thought had helped him in the past), he died of a heart attack at the age of 57. His former undergraduate student, Martin Davis, eventually published Post’s “Absolutely Undecidable Problems”, subtitled “Account of an Anticipation”, which describes the arc of Post’s work—including more detail on the story of tag systems. And in hindsight we can see how close Post came to discovering Gödel’s theorem and inventing the idea of universal computation. If instead of turning away from the complexity he found in tag systems he had embraced and explored it, I suspect he would have discovered not only foundational ideas of the 1930s, but also some of what I found half a century later in my by-then-computer-assisted explorations of the computational universe. When Post died, he left many unpublished notes. A considerable volume of them concern a major project he launched in 1938 that he planned to call “Creative Logic”. He seemed to feel that “extreme abstraction” as a way of exploring mathematics would give way to something in which it’s recognized that “processes of deduction are themselves essentially physical and hence subject to formulations in a physical science”. And, yes, there’s a strange resonance here with my own current efforts—informed by our Physics Project—to “physicalize” metamathematics. And perhaps I’ll discover that here too Post anticipated what was to come. So what happened to tag systems? By the mid-1950s Post’s idea of string rewriting (“production systems”) was making its way into many things, notably both the development of generative grammars in linguistics, and formal specifications of early computer languages. But tag systems—which Post had mentioned only once in his published works, and then as a kind of aside—were still basically unknown. Post had come to his string rewriting systems—much as Turing had come to his Turing machines—as a way to idealize the processes of mathematics. But by the 1950s there was increasing interest in using such abstract systems as a way to represent “general computations”, as well as brains. And one person drawn in this direction was Marvin Minsky. After a math PhD in 1954 at Princeton on what amounted to analog artificial neural networks, he started exploring more discrete systems, initially finite automata, essentially searching for the simplest elements that would support universal computation (and, he hoped, thinking-like behavior). Near the end of the 1950s he looked at Turing machines—and in trying to find the simplest form of them that would be universal started looking at their correspondence with Post’s string rewriting systems. Marvin Minsky knew Martin Davis from their time together at the Bronx High School of Science in New York, and by 1958 Davis was fully launched in mathematical logic, with a recently published book entitled Computability and Unsolvability. As Davis tells it now, Minsky phoned him about some unsolvability results he had about Post’s systems, asking if they were of interest. Davis told him about tag systems, and that Post had thought they might be universal. Minsky found that indeed they were, publishing the result in 1960 in “Recursive Unsolvability of Post's Problem of ‘Tag’ and Other Topics in Theory of Turing Machines”. Minsky had recently joined the faculty at MIT, but also had a position at MIT’s Lincoln Laboratory, where in working on computing for the Air Force there was a collaboration with IBM. And it was probably through this that Minsky met John Cocke, a lifelong computer designer (and general inventor) at IBM (who in later years was instrumental in the development of RISC architecture). The result was that in 1963 Minsky and Cocke published a paper entitled “Universality of Tag Systems with P=2” that dramatically simplified Minsky’s construction and showed (essentially by compiling to a Turing machine) that universality could be achieved with tag systems that delete only 2 elements at each step. (One might think of it as an ultimate RISC architecture.) For several years, Minsky had been trying to find out what the simplest universal Turing machine might be, and in 1962 he used the results Cocke and he had about tag systems to construct a 7-state, 4-color universal machine. That machine remained the record holder for the simplest known universal Turing machine for more than 40 years, though finally now we know the very simplest possible universal machine: a 2,3 machine that I discovered and conjectured would be universal—and that was proved so by Alex Smith in 2007 (thereby winning a prize I offered). But back in 1967, the visibility of tag systems got a big boost. Minsky wrote an influential book entitled Computation: Finite and Infinite Machines—and the last part of the book was devoted to “Symbol-Manipulation Systems and Computability”, with Post’s string rewriting systems a centerpiece. But my favorite part of Minsky’s book was always the very last chapter: “Very Simple Bases for Computability”. And there on page 267 is Post’s tag system: Minsky reports that “Post found this (00, 1101) problem ‘intractable’, and so did I, even with the help of a computer”. But then he adds, in a style very characteristic of the Marvin Minsky I knew for nearly 40 years: “Of course, unless one has a theory, one cannot expect much help from a computer (unless it has a theory)...” He goes on to say that “if the reader tries to study the behavior of 100100100100100100 without [the aid of a computer] he will be sorry”. Well, I guess computers have gotten a lot faster since the early 1960s; for me now it’s trivial to determine that this case evolves to a 10-cycle after 47 steps: (By the way, I recently asked Martin Davis if Post had ever run a tag system on a computer. He responded: “Goodness! When Post died von Neumann still thought that a dozen computers should suffice for America’s needs.  I guess I could have programmed [the tag system] for the [Institute for Advanced Study] computer, but it never occurred to me to do so.” Notably, in 1954 Davis did start programming logic theorem-proving algorithms on that computer.) After their appearance in Minsky’s book, tag systems became “known”, but they hardly became famous, and only a very few papers appeared about them. In 1972, at least their name got some visibility, when Alan Cobham, a longtime IBMer then working on coding theory, published a paper entitled “Uniform Tag Sequences”. Yes, this was about tag systems, but now with just one element being deleted at each step, which meant there couldn’t really be any interaction between elements. The mathematics was much more tractable (this was one of several inventions of neighbor-independent substitution systems generating purely nested behavior), but it didn’t really say anything about Post’s “problem of tag”.
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.14 Actually, I’ve Been Here Before... : When I started working on A New Kind of Science in 1991 I wanted to explore the computational universe of simple programs as widely as I could—to find out just how general (or not) the surprising phenomena I’d seen in cellular automata in the 1980s actually were. And almost from the beginning in the table of contents for my chapter on “The World of Simple Programs”, nestled between substitution systems and register machines, were tag systems (I had actually first mentioned tag systems in a paper in 1985): In the main text, I only spent two pages on them: And I did what I have done so many times for so many kinds of systems: I searched and found remarkably simple rules that generate complex behavior. And then on these pages I showed my favorite examples. (I generalized Post’s specific tag systems by allowing dependence on more than just the first element.) Did I look at Post’s specific 00, 1101 system? A New Kind of Science includes the note: And, yes, it mentions Post’s 00, 1101 tag system, then comments that “at least for all the initial conditions up to length 28, the rule eventually just leads to behavior that repeats”. An innocuous-looking statement, in very small print, tucked at the back of my very big book. But like so many such statements in the book, there was quite a lot behind it. (By the way, “length 28” then is what I would consider [compressed] length 9 now.) A quick search of my filesystem quickly reveals (.ma is an earlier format for notebooks that, yes, we can still read over a third of a century later): I open one of the notebook files (and, yes, windows—and screens—were tiny in those days): And there it is! Post’s 00, 1101 tag system, along with many others I was studying. And it seems I couldn’t let go of this; in 1994 I was running a standalone program to try to find infinitely growing cases. Here’s the output: So that’s where I got my statement about “up to size 28” (now size 9) from. I don’t know how long this took to run; “pyrethrum” was at the time the fastest computer at our company—with a newfangled 64-bit CPU (a DEC Alpha) running at the now-snail-sounding clock speed of 150 MHz. My archives from the early 1990s record a fair amount of additional “traffic” about tag systems. Interactions with Marvin Minsky. Interactions with my then-research-assistant about what I ended up calling “cyclic tag systems” (I originally called them “cyclic substitution systems”). For nearly 15 years there’s not much. That is, until June 25, 2007. It’s been my tradition since we started our Wolfram Summer School back in 2003 that on the first day I do a “live experiment”, and try to discover something. Well, that day I decided to look at tag systems. Here’s how I began: Right there, it’s Post’s 00, 1101 system. And I think I took it further than I’d ever done before. Pretty soon I was finding “long survivors” (I even got one that lasted more than 200,000 steps): I was drawing state transition graphs: But I obviously decided that I couldn’t get further with the 00, 1101 system that day. So I turned to “variants” and quickly found the 2-element-deletion 1, 110 rule that I’ve described above. I happened to write a piece about this particular live experiment (“Science: Live and in Public”), and right then I made a mental note: let me look at Post’s tag system again before its centenary, in 2021. So here we are....
After 100 Years, Can We Finally Crack Post’s Problem of Tag? A Story of Computational Irreducibility, and More 13.15 The Path Forward : Emil Post didn’t manage to crack his 00, 1101 tag system back in 1921 with hand calculations. But we might imagine that a century later—with the equivalent of tens of billions times more computational power we’d be able to do. But so far I haven’t managed it. For Post, the failure to crack his system derailed his whole intellectual worldview. For me now, the failure to crack Post’s system in a sense just bolsters my worldview—providing yet more indication of the strength and ubiquity of computational irreducibility and the Principle of Computational Equivalence. After spending several weeks throwing hundreds of modern computers and all sorts of computational methods at Post’s 00, 1101 tag system, what do we know? Here’s a summary:   All initial strings up to (uncompressed) length 84 lead either to cycles or termination   The time to termination or cycling can be as long as 643 billion steps   The sequence of lengths of strings generated seems to always behave much like a random walk   The sequences of 0s and 1s generated seem effectively random, apart from about 31% statistical redundancy   Most cycles are in definite families, but there are also some sporadic ones What’s missing here? Post wanted to know whether the system would halt, and so do we. But now the Principle of Computational Equivalence makes a definite prediction. It predicts that the system should be capable of universal computation. And this basically has the implication that the system can’t always halt: there has to be some initial string that will make it grow forever. In natural science it’s standard for theories to make predictions that can be investigated by doing experiments in the physical world. But the kind of predictions that the Principle of Computational Equivalence makes are more general; they’re not just about particular systems in the natural world, but about all possible abstract systems, and in a sense all conceivable universes. But it’s still possible to do experiments about them, though the experiments are now not physical ones, but abstract ones, carried out in the computational universe of possible programs. And with Post’s tag system we have an example of one particular such experiment: can we find non-halting behavior that will validate the prediction that the system can support universal computation? To do so would be another piece of evidence for the breadth of applicability of the Principle of Computational Equivalence. But what’s going to be involved in doing it? Computational irreducibility tells us that we can’t know. Traditional mathematical science has tended to make the assumption that once you know an abstract theory for something, then you can work out anything you want about it. But computational irreducibility shows that isn’t true. And in fact it shows how there are fundamental limitations to science that intrinsically arise from within science itself. And our difficulty in analyzing Post’s tag system is in a sense just an “in your face” example of how strong these limitations can be. But the Principle of Computational Equivalence says that somewhere we’ll see non-halting behavior. It doesn’t tell us exactly what that behavior will be like, or how difficult it’ll be for us to interpret what we see. But it says that the “simple conclusion” of “always halting” shouldn’t continue forever. I’ve so far done nearly a quintillion iterations of Post’s tag system in all. But that hasn’t been enough. I’ve been able to optimize the computations a bit. But fundamentally I’ve been left with what seems to be raw computational irreducibility. And to make progress I seem to need more time and more computers. Will a million of today’s computers be enough? Will it take a billion? I don’t know. Maybe it requires a new level of computational speed. Maybe to resolve the question requires more steps of computation than the physical universe has ever done. I don’t know for sure. But I’m optimistic that it’s within the current computational capabilities of the world to find that little string of bits for the tag system that will allow us to see more about the general Principle of Computational Equivalence and what it predicts. In the future there will be ever more that we will want and need to explore in the computational universe. And in a sense the problem of tag is a dry run for the kinds of things that we will see more and more often. But with the distinction of a century of history it’s a good place to rally our efforts and learn more about what’s involved. So far it’s only been my computers that have been working on this. But we’ll be setting things up so that anyone can join the project. I don’t know if it’ll get solved in a month, a year or a century. But with the Principle of Computational Equivalence as my guide I’m confident there’s something interesting to discover. And a century after Emil Post defined the problem I, for one, want to see it resolved. The main tag-system-related functions used are in the Wolfram Function Repository, as TagSystemEvolve, TagSystemEvolveList, TagSystemConvert, CyclicTagSystemEvolveList. A list of t steps in the evolution of the tag system from an (uncompressed) initial list init can be achieved with giving for example: The list of lengths can be obtained from giving for example: The output from t steps of evolution can be obtained from: A version of this using a low-level queue data structure is: The compressed {p, values} form of a tag system state can be obtained with while an uncompressed form can be recovered with: Each step in evolution in compressed form is obtained from The largest-scale computations done here made use of further-optimized code (available in the Wolfram Function Repository), in which the state of the tag system is stored in a bit-packed array, with 8 updates being done at a time by having a table of results for all 256 cases and using the first byte of the bit-packed array to index into this. This approach routinely achieves a quarter billion updates per second on current hardware. (Larger update tables no longer fit in L1 cache and so typically do not help.) As I’ve mentioned, there isn’t a particularly large literature on the specific behavior of tag systems. In 1963 Shigeru Watanabe described the basic families of cycles for Post’s 00, 1101 tag system (though did not discover the “sporadic cases”). After A New Kind of Science in 2002, I’m aware of one extensive series of papers (partly using computer experiment methods) written by Liesbeth De Mol following her 2007 PhD thesis. Carlos Martin (a student at the Wolfram Summer School) also wrote about probabilistic methods for predicting tag system evolution. Thanks, etc. Thanks to Max Piskunov and Mano Namuduri for help with tag system implementations, Ed Pegg for tag system analysis (and for joining me in some tag system “hunting expeditions”), Matthew Szudzik and Jonathan Gorard for clarifying metamathematical issues, and Catherine Wolfram for help on the theory of random walks. Thanks also to Martin Davis and Margaret Minsky for clarifying some historical issues (and Dana Scott for having also done so long ago).
What Is Consciousness? Some New Perspectives from Our Physics Project 14.1 “What about Consciousness?” : For years I’ve batted it away. I’ll be talking about my discoveries in the computational universe, and computational irreducibility, and my Principle of Computational Equivalence, and people will ask “So what does this mean about consciousness?” And I’ll say “that’s a slippery topic”. And I’ll start talking about the sequence: life, intelligence, consciousness. I’ll ask “What is the abstract definition of life?” We know about the case of life on Earth, with all its RNA and proteins and other implementation details. But how do we generalize? What is life generally? And I’ll argue that it’s really just computational sophistication, which the Principle of Computational Equivalence says happens all over the place. Then I’ll talk about intelligence. And I’ll argue it’s the same kind of thing. We know the case of human intelligence. But if we generalize, it’s just computational sophistication—and it’s ubiquitous. And so it’s perfectly reasonable to say that “the weather has a mind of its own”; it just happens to be a mind whose details and “purposes” aren’t aligned with our existing human experience. I’ve always implicitly assumed that consciousness is just a continuation of the same story: something that, if thought about in enough generality, is just a feature of computational sophistication, and therefore quite ubiquitous. But from our Physics Project—and particularly from thinking about its implications for the foundations of quantum mechanics—I’ve begun to realize that at its core consciousness is actually something rather different. Yes, its implementation involves computational sophistication. But its essence is not so much about what can happen as about having ways to integrate what’s happening to make it somehow coherent and to allow what we might see as “definite thoughts” to be formed about it. And rather than consciousness being somehow beyond “generalized intelligence” or general computational sophistication, I now instead see it as a kind of “step down”—as something associated with simplified descriptions of the universe based on using only bounded amounts of computation. At the outset, it’s not obvious that a notion of consciousness defined in this way could consistently exist in our universe. And indeed the possibility of it seems to be related to deep features of the formal system that underlies physics. In the end, there’s a lot going on in the universe that’s in a sense “beyond consciousness”. But the core notion of consciousness is crucial to our whole way of seeing and describing the universe—and at a very fundamental level it’s what makes the universe seem to us to have the kinds of laws and behavior it does. Consciousness is a topic that’s been discussed and debated for centuries. But the surprise to me is that with what we’ve learned from exploring the computational universe and especially from our recent Physics Project it seems there may be new perspectives to be had, which most significantly seem to have the potential to connect questions about consciousness to concrete, formal scientific ideas. Inevitably the discussion of consciousness—and especially its connection to our new foundations of physics—is quite conceptually complex, and all I’ll try to do here is sketch some preliminary ideas. No doubt quite a bit of what I say can be connected to existing philosophical and other thinking, but so far I’ve only had a chance to explore the ideas themselves, and haven’t yet tried to study their historical context.
What Is Consciousness? Some New Perspectives from Our Physics Project 14.2 Observers and Their Physics : The universe in our models is full of sophisticated computation, all the way down. At the lowest level it’s just a giant collection of “atoms of space”, whose relationships are continually being updated according to a computational rule. And inevitably much of that process is computationally irreducible, in the sense that there’s no general way to “figure out what’s going to happen” except, in effect, by just running each step. But given that, how come the universe doesn’t just seem to us arbitrarily complex and unpredictable? How come there’s order and regularity that we can perceive in it? There’s still plenty of computational irreducibility. But somehow there are also pockets of reducibility that we manage to leverage to form a simpler description of the world, that we can successfully and coherently make use of. And a fundamental discovery of our Physics Project is that the two great pillars of twentieth-century physics—general relativity and quantum mechanics—correspond precisely to two such pockets of reducibility. There’s an immediate analog—that actually ends up being an example of the same fundamental computational phenomenon. Consider a gas, like air. Ultimately the gas consists of lots of molecules bouncing around in a complicated way that’s full of computational irreducibility. But it’s a central fact of statistical mechanics that if we look at the gas on a large scale, we can get a useful description of what it does just in terms of properties like temperature and pressure. And in effect this reflects a pocket of computational reducibility, that allows us to operate without engaging with all the computational irreducibility underneath. How should we think about this? An idea that will generalize is that as “observers” of the gas, we’re conflating lots of different microscopic configurations of molecules, and just paying attention to overall aggregate properties. In the language of statistical mechanics, it’s effectively a story of “coarse graining”. But within our computational approach, there’s now a clear, computational way to characterize this. At the level of individual molecules there’s an irreducible computation happening. And to “understand what’s going on” the observer is doing a computation. But the crucial point is that if there’s a certain boundedness to that computation then this has immediate consequences for the effective behavior the observer will perceive. And in the case of something like a gas, it turns out to directly imply the Second Law of Thermodynamics. In the past there’s been a certain amount of mystery around the origin and validity of the Second Law. But now we can see it as a consequence of the interplay between underlying computational irreducibility and the computational boundedness of observers. If the observer kept track of all the computationally irreducible motions of individual molecules, they wouldn’t see Second Law behavior. The Second Law depends on a pocket of computational reducibility that in effect emerges only when there’s a constraint on the observer that amounts to the requirement that the observer has a “coherent view” of what’s going on. So what about physical space? The traditional view had been that space was something that could to a large extent just be described as a coherent mathematical object. But in our models of physics, space is actually made of an immense number of discrete elements whose pattern of interconnections evolves in a complex and computationally irreducible way. But it’s much like with the gas molecules. If an observer is going to form a coherent view of what’s going on, and if they have bounded computational capabilities, then this puts definite constraints on what behavior they will perceive. And it turns out that those constraints yield exactly relativity. In other words, for the “atoms of space”, relativity is the result of the interplay between underlying computational irreducibility and the requirement that the observer has a coherent view of what’s going on. It may be helpful to fill in a little more of the technical details. Our underlying theory basically says that each elementary element of space follows computational rules that will yield computationally irreducible behavior. But if that was all there was to it, the universe would seem like a completely incoherent place, with every part of it doing irreducibly unpredictable things. But imagine there’s an observer who perceives coherence in the universe. And who, for example, views there as being a definite coherent notion of “space”. What can we say about such an observer? The first thing is that since our model is supposed to describe everything in the universe, it must in particular include our observer. The observer must be an embedded part of the system—made up of the same atoms of space, and following the same rules, as everything else. And there’s an immediate consequence to this. From “inside” the system there are only certain things about the system that the observer can perceive. Let’s say, for example, that in the whole universe there’s only one point at which anything is updated at any given time, but that that “update point” zips around the universe (in “Turing machine style”), sometimes updating a piece of the observer, and sometimes updating something they were observing. If one traces through scenarios like this, one realizes that from “inside the system” the only thing the observer can ever perceive is causal relationships between events. They can’t tell “specifically when” any given event happens; all they can tell is what event has to happen before what other one, or in other words, what the causal relationships between events are. And this is the beginning of what makes relativity inevitable in our models. But there are two other pieces. If the observer is going to have a coherent description of “space” they can’t in effect be tracking each atom separately; they’ll have to fit them into some overall framework, say by assigning each of them particular “coordinates”, or, in the language of relativity, defining a “reference frame” that conflates many different points in space. But if the observer is computationally bounded, then this puts constraints on the structure of the reference frame: it can’t for example be so wild that it separately traces the computationally irreducible behavior of individual atoms of space. But let’s say an observer has successfully picked some reference frame. What’s to say that as the universe evolves it’s still possible to consistently maintain that reference frame? Well, this relies on a fundamental property that we believe either directly or effectively defines the operation of our universe: what we call “causal invariance”. The underlying rules just describe possible ways that the connections between atoms of space can be updated. But causal invariance implies that whatever actual sequence of updatings is used, there must always be the same graph of causal relationships. And it’s this that gives observers the ability to pick different reference frames, and still have the same consistent and coherent perception of the behavior of the universe. And in the end, we have a definite result: that if there’s underlying computational irreducibility—plus causal invariance—then any observer who forms their perception of the universe in a computationally bounded way must inevitably perceive the universe to follow the laws of general relativity. But—much like with the Second Law—this conclusion relies on having an observer who forms a coherent perception of the universe. If the observer could separately track every atom of space they won’t “see general relativity”; that only emerges for an observer who forms a coherent perception of the universe.
What Is Consciousness? Some New Perspectives from Our Physics Project 14.3 The Quantum Observer : OK, so what about quantum mechanics? How does that relate to observers? The story is actually surprisingly similar to both the Second Law and general relativity: quantum mechanics is again something that emerges as a result of trying to form a coherent perception of the universe. In ordinary classical physics one considers everything that happens in the universe to happen in a definite way, in effect defining a single thread of history. But the essence of quantum mechanics is that actually there are many threads of history that are followed. And an important feature of our models is that this is inevitable. The underlying rules define how local patterns of connections between atoms of space should be updated. But in the hypergraph of connections that represents the universe there will in general be many different places where the rules can be applied. And if we trace all the possibilities we get a multiway graph that includes many possible threads of history, sometimes branching and sometimes merging. So how will an observer perceive all this? The crucial point is that the observer is themselves part of this multiway system. So in other words, if the universe is branching, so is the observer. And in essence the question becomes how a “branching brain” will perceive a branching universe. It’s fairly easy to imagine how an observer who is “spatially large” compared to individual molecules in a gas—or atoms of space—could conflate their view of these elements so as to perceive only some aggregate property. Well, it seems like very much the same kind of thing is going on with observers in quantum mechanics. It’s just that instead of being extended in physical space, they’re extended in what we call branchial space. Consider a multiway graph representing possible histories for a system. Now imagine slicing through this graph at a particular level that in effect corresponds to a particular time. In that slice there will be a certain set of nodes of the multiway graph, representing possible states of the system. And the structure of the multiway graph then defines relationships between these states (say through common ancestry). And in a large-scale limit we can say that the states are laid out in branchial space. In the language of quantum mechanics, the geometry of branchial space in effect defines a map of entanglements between quantum states, and coordinates in branchial space are like phases of quantum amplitudes. In the evolution of a quantum system, one might start from a certain bundle of quantum states, then follow their threads of history, looking at where in branchial space they go. But what would a quantum observer perceive about this? Even if they didn’t start that way, over time a quantum observer will inevitably become spread out in branchial space. And so they’ll always end up sampling a whole region in branchial space, or a whole bundle of “threads of history” in the multiway graph. What will they make of them? If they considered each of them separately no coherent picture would emerge, not least since the underlying evolution of individual threads of history can be expected to be computationally irreducible. But what if the observer just defines their way of viewing things to be one that systematically organizes different threads of history, say by conflating “computationally nearby” ones? It’s similar to setting up a reference frame in relativity, except that now the coherent representation that this “quantum frame” defines is of branchial space rather than physical space. But what will this coherent representation be like? Well, it seems to be exactly quantum mechanics as it was developed over the past century. In other words, just like general relativity emerges as an aggregate description of physical space formed by a computationally bounded observer, so quantum mechanics emerges as an aggregate description of branchial space. Does the observer “create” the quantum mechanics? In some sense, yes. Just as in the spacetime case, the multiway graph has all sorts of computationally irreducible things going on. But if there’s an observer with a coherent description of what’s going on, then their description must follow the laws of quantum mechanics. Of course, there are lots of other things going on too—but they don’t fit into this coherent description. OK, but let’s say that we have an observer who’s set up a quantum frame that conflates different threads of history to get a coherent description of what’s going on. How will their description correlate with what another observer—with a different quantum frame—would perceive? In the traditional formalism of quantum mechanics it’s always been difficult to explain why different observers—making different measurements—still fundamentally perceive the universe to be working the same. In our model, there’s a clear answer: just like in the spacetime case, if the underlying rules show causal invariance, then regardless of the frame one uses, the basic perceived behavior will always be the same. Or, in other words, causal invariance guarantees the consistency of the behavior deduced by different observers. There are many technical details to this. The traditional formalism of quantum mechanics has two separate parts. First, the time evolution of quantum amplitudes, and second, the process of measurement. In our models, there’s a very beautiful correspondence between the phenomenon of motion in space and the evolution of quantum amplitudes. In essence, both are associated with the deflection of (geodesic) paths by the presence of energy-momentum. But in the case of motion this deflection (that we identify as the effect of gravity) happens in physical space, while in the quantum case the deflection (that we identify as the phase change specified by the path integral) happens in branchial space. (In other words, the Feynman path integral is basically just the direct analog in branchial space of the Einstein equations in physical space.) OK, so what about quantum measurement? Doing a quantum measurement involves somehow taking many threads of history (corresponding to a superposition of many quantum states) and effectively reducing them to a single thread that coherently represents the “outcome”. A quantum frame defines a way to do this—in effect specifying the pattern of threads of history that should be conflated. In and of itself, a quantum frame—like a relativistic reference frame—isn’t a physical thing; it just defines a way of describing what’s going on. But as a way of probing possible coherent representations that an observer can form, one can consider what happens if one formally conflates things according to a particular quantum frame. In an analogy where the multiway graph defines inferences between propositions in a formal system, conflating things is like “performing certain completions”. And each completion is then like an elementary step in the act of measurement. And by looking at the effect of all necessary completions one gets the “Completion Interpretation of Quantum Mechanics” suggested by Jonathan Gorard. Assuming that the underlying rule for the universe ultimately shows causal invariance, doing these completions is never fundamentally necessary, because different threads of history will always eventually give the same results for what can be perceived within the system. But if we want to get a “possible snapshot” of what the system is doing, we can pick a quantum frame and formally do the completions it defines. Doing this doesn’t actually “change the system” in a way that we would “see from outside”. It’s only that we’re in effect “doing a formal projection” to see how things would be perceived by an observer who’s picked a particular quantum frame. And if the observer is going to have a coherent perception of what’s going on, they in effect have to have picked some specific quantum frame. But then from the “point of view of the observer” the completions associated with that frame in some sense “seem real” because they’re the way the observer is accessing what’s going on. Or, in other words, the way a computationally bounded “branching brain” can have a coherent perception of a “branching universe” is by looking at things in terms of quantum frames and completions, and effectively picking off a computationally reducible slice of the whole computationally irreducible evolution of the universe—where it then turns out that the slice must necessarily follow the laws of quantum mechanics. So, once again, for a computationally bounded observer to get a coherent perception of the universe—with all its underlying computational irreducibility—there’s a strong constraint on what that perception can be. And what we’ve discovered is that it turns out to basically have to follow the two great core theories of twentieth-century physics: general relativity and quantum mechanics. It’s not immediately obvious that there has to be any way to get a coherent perception of the universe. But what we now know is that if there is, it essentially forces specific major results about physics. And, of course, if there wasn’t any way to get a coherent perception of the universe there wouldn’t really be systematic overall laws, or, for that matter, anything like physics, or science as we know it.
What Is Consciousness? Some New Perspectives from Our Physics Project 14.4 So, What Is Consciousness? : What’s special about the way we humans experience the world? At some level, the very fact that we even have a notion of “experiencing” it at all is special. The world is doing what it does, with all sorts of computational irreducibility. But somehow even with the computationally bounded resources of our brains (or minds) we’re able to form some kind of coherent model of what’s going on, so that, in a sense, we’re able to meaningfully “form coherent thoughts” about the universe. And just as we can form coherent thoughts about the universe, so also we can form coherent thoughts about that small part of the universe that corresponds to our brains—or to the computations that represent the operation of our minds. But what does it mean to say that we “form coherent thoughts”? There’s a general notion of computation, which the Principle of Computational Equivalence tells us is quite ubiquitous. But it seems that what it means to “form coherent thoughts” is that computations are being “concentrated down” to the point where a coherent stream of “definite thoughts” can be identified in them. At the outset it’s certainly not obvious that our brains—with their billions of neurons operating in parallel—should achieve anything like this. But in fact it seems that our brains have a quite specific neural architecture—presumably produced by biological evolution—that in effect attempts to “integrate and sequentialize” everything. In our cortex we bring together sensory data we collect, then process it with a definite thread of attention. And indeed in medical settings observed deficits in this are what are normally used to identify absence of levels of consciousness. There may still be neurons firing but without integration and sequentialization there doesn’t really seem to be what we normally consider consciousness. These are biological details. But they seem to point to a fundamental feature of consciousness. Consciousness is not about the general computation that brains—or, for that matter, many other things—can do. It’s about the particular feature of our brains that causes us to have a coherent thread of experience. But what we have now realized is that the notion of having a coherent thread of experience has deep consequences that far transcend the details of brains or biology. Because in particular what we’ve seen is that it defines the laws of physics, or at least what we consider the laws of physics to be. Consciousness—like intelligence—is something of which we only have a clear sense in the single case of humans. But just as we’ve seen that the notion of intelligence can be generalized to the notion of arbitrary sophisticated computation, so now it seems that the notion of consciousness can be generalized to the notion of forming a coherent thread of representation for computations. Operationally, there’s potentially a rather straightforward way to think about this, though it depends on our recent understanding of the concept of time. In the past, time in fundamental physics was usually viewed as being another dimension, much like space. But in our models of fundamental physics, time is something quite different from space. Space corresponds to the hypergraph of connections between the elements that we can consider as “atoms of space”. But time is instead associated with the inexorable and irreducible computational process of repeatedly updating these connections in all possible ways. There are definite causal relationships between these updating events (ultimately defined by the multiway causal graph), but one can think of many of the events as happening “in parallel” in different parts of space or on different threads of history. But this kind of parallelism is in a sense antithetical to the concept of a coherent thread of experience. And as we’ve discussed above, the formalism of physics—whether reference frames in relativity or quantum mechanics—is specifically set up to conflate things to the point where there is a single thread of evolution in time. So one way to think about this is that we’re setting things up so we only have to do sequential computation, like a Turing machine. We don’t have multiple elements getting updated in parallel like in a cellular automaton, and we don’t have multiple threads of history like in a multiway (or nondeterministic) Turing machine. The operation of the universe may be fundamentally parallel, but our “parsing” and “experience” of it is somehow sequential. As we’ve discussed above, it’s not obvious that such a “sequentialization” would be consistent. But if it’s done with frames and so on, the interplay between causal invariance and underlying computational irreducibility ensures that it will be—and that the behavior of the universe that we’ll perceive will follow the core features of twentieth-century physics, namely general relativity and quantum mechanics. But do we really “sequentialize” everything? Experience with artificial neural networks seems to give us a fairly good sense of the basic operation of brains. And, yes, something like initial processing of visual scenes is definitely handled in parallel. But the closer we get to things we might realistically describe as “thoughts” the more sequential things seem to get. And a notable feature is that what seems to be our richest way to communicate thoughts, namely language, is decidedly sequential. When people talk about consciousness, something often mentioned is “self-awareness” or the ability to “think about one’s own processes of thinking”. Without the conceptual framework of computation, this might seem quite mysterious. But the idea of universal computation instead makes it seem almost inevitable. The whole point of a universal computer is that it can be made to emulate any computational system—even itself. And that is why, for example, we can write the evaluator for Wolfram Language in Wolfram Language itself. The Principle of Computational Equivalence implies that universal computation is ubiquitous, and that both brains and minds, as well as the universe at large, have it. Yes, the emulated version of something will usually take more time to execute than the original. But the point is that the emulation is possible. But consider a mind in effect thinking about itself. When a mind thinks about the world at large, its process of perception involves essentially making a model of what’s out there (and, as we’ve discussed, typically a sequentialized one). So when the mind thinks about itself, it will again make a model. Our experiences may start by making models of the “outside world”. But then we’ll recursively make models of the models we make, perhaps barely distinguishing between “raw material” that comes from “inside” and “outside”. The connection between sequentialization and consciousness gives one a way to understand why there can be different consciousnesses, say associated with different people, that have different “experiences”. Essentially it’s just that one can pick different frames and so on that lead to different “sequentialized” accounts of what’s going on. Why should they end up eventually being consistent, and eventually agreeing on an objective reality? Essentially for the same reason that relativity works, namely that causal invariance implies that whatever frame one picks, the causal graph that’s eventually traced out is always the same. If it wasn’t for all the interactions continually going on in the universe, there’d be no reason for the experience of different consciousnesses to get aligned. But the interactions—with their underlying computational irreducibility and overall causal invariance—lead to the consistency that’s needed, and, as we’ve discussed, something else too: particular effective laws of physics, that turn out to be just the relativity and quantum mechanics we know.
What Is Consciousness? Some New Perspectives from Our Physics Project 14.5 Other Consciousnesses : The view of consciousness that we’ve discussed is in a sense focused on the primacy of time: it’s about reducing the “parallelism” associated with space—and branchial space—to allow the formation of a coherent thread of experience, that in effect occurs sequentially in time. And it’s undoubtedly no coincidence that we humans are in effect well placed in the universe to be able to do this. In large part this has to do with the physical sizes of things—and with the (undoubtedly not coincidental) fact that human scales are intermediate between those at which the effects of either relativity or quantum mechanics become extreme. Why can we “ignore space” to the point where we can just discuss things happening “wherever” at a sequence of moments in time? Basically it’s because the speed of light is large compared to human scales. In our everyday lives the important parts of our visual environment tend to be at most tens of meters away—so it takes light only tens of nanoseconds to reach us. Yet our brains process information on timescales measured in milliseconds. And this means that as far as our experience is concerned, we can just “combine together” things at different places in space, and consider a sequence of instantaneous states in time. If we were the size of planets, though, this would no longer work. Because—assuming our brains still ran at the same speed—we’d inevitably end up with a fragmented visual experience, that we wouldn’t be able to think about as a single thread about which we can say “this happened, then that happened”. Even at standard human scale, we’d have somewhat the same experience if we used for example smell as our source of information about the world (as, say, dogs to a large extent do). Because in effect the “speed of smell” is quite slow compared to brain processing. And this would make it much less useful to identify our usual notion of “space” as a coherent concept. So instead we might invent some “other physics”, perhaps labeling things in terms of the paths of air currents that deliver smells to us, then inventing some elaborate gauge-field-like construct to talk about the relations between different paths. In thinking about our “place in the universe” there’s also another important effect: our brains are small and slow enough that they’re not limited by the speed of light, which is why it’s possible for them to “form coherent thoughts” in the first place. If our brains were the size of planets, it would necessarily take far longer than milliseconds to “come to equilibrium”, so if we insisted on operating on those timescales there’d be no way—at least “from the outside”—to ensure a consistent thread of experience. From “inside”, though, a planet-size brain might simply assume that it has a consistent thread of experience. And in doing this it would in a sense try to force a different physics on the universe. Would it work? Based on what we currently know, not without at least significantly changing the notions of space and time that we use. By the way, the situation would be even more extreme if different parts of a brain were separated by permanent event horizons. And it seems as if the only way to maintain a consistent thread of experience in this case would be in effect to “freeze experience” before the event horizons formed. What if we and our brains were much smaller than they actually are? As it is, our brains may contain perhaps 10300 atoms of space. But what if they contained, say, only a few hundred? Probably it would be hard to avoid computational irreducibility—and we’d never even be able to imagine that there were overall laws, or generally predictable features of the universe, and we’d never be able to build up the kind of coherent experience needed for our view of consciousness. What about our extent in branchial space? In effect, our perception that “definite things happen even despite quantum mechanics” implies a conflation of the different threads of history that exist in the region of branchial space that we occupy. But how much effect does this have on the rest of the universe? It’s much like the story with the speed of light, except now what’s relevant is a new quantity that appears in our models: the maximum entanglement speed. And somehow this is large enough that over “everyday scales” in branchial space it’s adequate for us just to pick a quantum frame and treat it as something that can be considered to have a definite state at any given instant in time—so that we can indeed consistently maintain a “single thread of experience”. OK, so now we have a sense of why with our particular human scale and characteristics our view of consciousness might be possible. But where else might consciousness be possible? It’s a tricky and challenging thing to ask. To achieve our view of consciousness we need to be able to build up something that “viewed from the inside” represents a coherent thread of experience. But the issue is that we’re in effect “on the outside”. We know about our human thread of experience. And we know about the physics that effectively follows from it. And we can ask how we might experience that if, for example, our sensory systems were different. But to truly “get inside” we have to be able to imagine something very alien. Not only different sensory data and different “patterns of thinking”, but also different implied physics. An obvious place to start in thinking about “other consciousnesses” is with animals and other organisms. But immediately we have the issue of communication. And it’s a fundamental one. Perhaps one day there’ll be ways for various animals to fluidly express themselves through something like human-relatable videogames. But as of now we have surprisingly little idea how animals “think about things”, and, for example, what their experience of the world is. We can guess that there will be many differences from ours. At the simplest level, there are organisms that use different sensory modalities to probe the world, whether those be smell, sound, electrical, thermal, pressure, or other. There are “hive mind” organisms, where whatever integrated experience of the world there may be is built up through slow communication between different individuals. There are organisms like plants, which are (quite literally) rooted to one place in space. There are also things like viruses where anything akin to an “integrated thread of experience” can presumably only emerge at the level of something like the progress of an epidemic. Meanwhile, even in us, there are things like the immune system, which in effect have some kind of “thread of experience” though with rather different input and output than our brains. Even if it seems bizarre to attribute something like consciousness to the immune system, it is interesting to try to imagine what its “implied physics” would be. One can go even further afield, and think about things like the complete tree of life on Earth, or, for that matter, the geological history of the Earth, or the weather. But how can these have anything like consciousness? The Principle of Computational Equivalence implies that all of them have just the same fundamental computational sophistication as our brains. But, as we have discussed, consciousness seems to require something else as well: a kind of coherent integration and sequentialization. Take the weather as an example. Yes, there is lots of computational sophistication in the patterns of fluid flow in the atmosphere. But—like fundamental processes in physics—it seems to be happening all over the place, with nothing, it seems, to define anything like a coherent thread of experience. Coming a little closer to home, we can consider software and AI systems. One might expect that to “achieve consciousness” one would have to go further than ever before and inject some special “human-like spark”. But I suspect that the true story is rather different. If one wants the systems to make the richest use of what the computational universe has to offer, then they should behave a bit like fundamental physics (or nature in general), with all sorts of components and all sorts of computationally irreducible behavior. But to have something like our view of consciousness requires taking a step down, and effectively forcing simpler behavior in which things are integrated to produce a “sequentialized” experience. And in the end, it may not be that different from picking out of the computational universe of possibilities just what can be expressed in a definite computational language of the kind the Wolfram Language provides. Again we can ask about the “implied physics” of such a setup. But since the Wolfram Language is modeled on picking out the computational essence of human thinking it’s basically inevitable that its implied physics will be largely the same as the ordinary physics that is derived from ordinary human thinking. One feature of having a fundamental model for physics is that it “reduces physics to mathematics”, in the sense that it provides a purely formal system that describes the universe. So this raises the question of whether one can think about consciousness in a formal system, like mathematics. For example, imagine a formal analog of the universe constructed by applying axioms of mathematics. One would build up an elaborate network of theorems, that in effect populate “metamathematical space”. This setup leads to some fascinating analogies between physics and metamathematics. The notion of time effectively remains as always, but here represents the progressive proving of new mathematical theorems. The analog of our spatial hypergraph is a structure that represents all theorems proved up to a given time. (And there’s also an analog of the multiway graph that yields quantum mechanics, but in which different paths now in effect represent different possible proofs of a theorem.) So what about things like reference frames? Well, just as in physics, a reference frame is something associated with an observer. But here the observer is observing not physical space, but metamathematical space. And in a sense any given observer is “discovering mathematics in a particular order”. It could be that all the different “points in metamathematical space” (i.e. theorems) are behaving in completely incoherent—and computationally irreducible—ways. But just as in physics, it seems that there’s a certain computational reducibility: causal invariance implies that different reference frames will in a sense ultimately always “see the same mathematics”. There’s an analog of the speed of light: the speed at which a new theorem can affect theorems that are progressively further away in metamathematical space. And relativistic invariance then becomes the statement that “there’s only one mathematics”—but it can just be explored in different ways. How does this relate to “mathematical consciousness”? The whole idea of setting up reference frames in effect relies on the notion that one can “sequentialize metamathematical space”. And this in turn relies on a notion of “mathematical perception”. The situation is a bit like in physics. But now one has a formalized mathematician whose mind stretches over a certain region of metamathematical space. In current formalized approaches to mathematics, a typical “human-scale mathematical theorem” might correspond to perhaps 105 lowest-level mathematical propositions. Meanwhile, the “mathematician” might “integrate into their experience” some small fraction of the metamathematical universe (which, for human mathematics, is currently perhaps 3 × 106 theorems). And it’s this setup—which amounts to defining a “sequentialized mathematical consciousness”—that means it makes sense to do analysis using reference frames, etc. So, just as in physics, it’s ultimately the characteristics of our consciousness that lead to the physics we attribute to the universe, so something similar seems to happen in mathematics. Clearly we’ve now reached a quite high level of abstraction, so perhaps it’s worth mentioning one more wrinkle that involves an even higher level of abstraction. We’ve talked about applying a rule to update the abstract structure that represents the universe. And we’ve discussed the fact that the rule can be applied at different places, and on different threads of history. But there’s another freedom: we don’t have to consider a specific rule; we can consider all possible rules. The result is a rulial multiway graph of possible states of the universe. On different paths, different specific rules are followed. And if you slice across the graph you can get a map of states laid out in rulial space, with different positions corresponding to the outcomes of applying different rules to the universe. An important fact is then that at the level of the rulial multiway graph there is always causal invariance. So this means that different “rulial reference frames” must always ultimately give equivalent results. Or, in other words, even if one attributes the evolution of the universe to different rules, there is always fundamental equivalence in the results. In a sense, this can be viewed as a reflection of the Principle of Computational Equivalence and the fundamental idea that the universe is computational. In essence it is saying that since whatever rules one uses to “construct the universe” are almost inevitably computation universal, one can always use them to emulate any other rules. How does this relate to consciousness? Well, one feature of different rulial reference frames is that they can lead to utterly and incoherently different basic descriptions of the universe. One of them could be our hypergraph-rewriting-based setup, with a representation of space that corresponds well with what emerged in twentieth-century physics. But another could be a Turing machine, in which one views the updating of the universe as being done by a single head zipping around to different places. We’ve talked about some possible systems in which consciousness could occur. But one we haven’t yet mentioned—but which has often been considered—is “extraterrestrial intelligences”. Before our Physics Project one might reasonably have assumed that even if there was little else in common with such “alien intelligences”, at least they would be “experiencing the same physics”. But it’s now clear that this absolutely does not need to be the case. An alien intelligence could perfectly well be experiencing the universe in a different rulial reference frame, utterly incoherent with the one we use. Is there anything “sequentializable” in a different rulial reference frame? Presumably it’s possible to find at least something sequentializable in any rulial reference frame. But the question of whether the alien intelligence can be thought of as sampling it is a quite different one. Does there need to be a “sequentializable consciousness” to imply “meaningful laws of physics”? Presumably meaningful laws have to somehow be associated with computational reducibility; certainly that would be true if they were going to be useful to a “computationally bounded” alien intelligence. But it’s undoubtedly the case that “sequentializability” is not the only way to access computational reducibility. In a mathematical analogy, using sequentializability is a bit like using ordinary mathematical induction. But there are other axiomatic setups (like transfinite induction) that define other ways to do things like prove theorems. Yes, human-like consciousness might involve sequentializability. But if the general idea of consciousness is to have a way of “experiencing the universe” that accesses computational reducibility then there are no doubt other ways. It’s a kind of “second-order alienness”: in addition to using a different rulial reference frame, it’s using a different scheme for accessing reducibility. And the implied physics of such a setup is likely to be very different from anything we currently think of as physics. Could we ever expect to identify what some of these “alien possibilities” are? The Principle of Computational Equivalence at least implies that we can in principle expect to be able to set up any possible computational rule. But if we start doing experiments we can’t have an expectation that scientific induction will work, and it is potentially arbitrarily difficult to identify computational reducibility. Yes, we might recognize some form of prediction or regularity that we are familiar with. But to recognize an arbitrary form of computational reducibility in effect relies on some analog of a definition of consciousness, which is what we were looking for in the first place.
What Is Consciousness? Some New Perspectives from Our Physics Project 14.6 What Now? : Consciousness is a difficult topic, that has vexed philosophers and others for centuries. But with what we know now from our Physics Project it at least seems possible to cast it in a new light much more closely connected to the traditions of formal science. And although I haven’t done it here, I fully anticipate that it’ll be possible to take the ideas I’ve discussed and use them to create formal models that can answer questions about consciousness and capture its connections, particularly to physics. It’s not clear how much realistic physics there will need to be in models to make them useful. Perhaps one will already be able to get worthwhile information about how branching brains perceive a branching universe by looking at some simple case of a multiway Turing machine. Perhaps some combinator system will already reveal something about how different versions of physics could be set up. In a sense what’s important is that it seems we may have a realistic way to formalize issues about consciousness, and to turn questions about consciousness into what amount to concrete questions about mathematics, computation, logic or whatever that can be formally and rigorously explored. But ultimately the way to tether the discussion—and to have it not for example devolve into debates about the meaning of words—is to connect it to actionable issues and applications. As a first example, let’s discuss distributed computing. How should we think about computations that—like those in our model of physics—take place in parallel across many different elements? Well—except in very simple or structured cases—it’s hard, at least for us humans. And from what we’ve discussed about consciousness, perhaps we can now understand why. The basic issue is that consciousness seems to be all about forming a definite “sequentialized” thread of experience of the world, which is directly at odds with the idea of parallelism. But so how can we proceed if we need to do distributed computing? Following what we believe about consciousness, I suspect a good approach will be to essentially mirror what we do in parsing the physical universe—and for example to pick reference frames in which to view and integrate the computation. Distributed computing is difficult enough for us humans to “wrap our brains around”. Multiway or nondeterministic computing tends to be even harder. And once again I suspect this is because of the “limitations imposed by consciousness”. And that the way to handle it will be to use ideas that come from physics, and from the interaction of consciousness with quantum mechanics. A few years ago at an AI ethics conference I raised the question of what would make us think AIs should have rights and responsibilities. “When they have consciousness!” said an enthusiastic philosopher. Of course, that begs the question of what it would mean for AIs to have consciousness. But the point is that attributing consciousness to something has potential consequences, say for ethics. And it’s interesting to see how the connection might work. Consider a system that’s doing all sorts of sophisticated and irreducible computation. Already we might reasonably say that the system is showing a generalization of intelligence. But to achieve what we’re viewing as consciousness the system also has to integrate this computation into some kind of single thread of experience. And somehow it seems much more appropriate to attribute “responsibility” to that single thread that we can somehow “point to” than to a whole incoherent distributed computation. In addition, it seems much “more wrong” to imagine “killing” a single thread, probably because it feels much more unique and special. In a generic computational system there are many ways to “move forward”. But if there’s a single thread of experience it’s more like there’s only one. And perhaps it’s like the death of a human consciousness. Inevitably the history around that consciousness has affected all sorts of things in the physical universe that will survive its disappearance. But it’s the thread of consciousness that ties it all together that seems significant to us, particularly as we try to make a “summary” of the universe to create our own coherent thread of experience. And, by the way, when we talk about “explaining AI” what it tends to come down to is being able not just to say “that’s the computation that ran”, but being able to “tell a story” about what happened, which typically begins with making it “sequential enough” that we can relate to it like “another consciousness”. I’ve often noted that the Principle of Computational Equivalence has important implications for understanding our “place in the universe”. We might have thought that with our life and intelligence there must be something fundamentally special about us. But what we’ve realized is that the essence of these is just computational sophistication—and the Principle of Computational Equivalence implies that that’s actually quite ubiquitous and generic. So in a sense this promotes the importance of our human details—because that’s ultimately all that’s special about us. So what about consciousness? In full generality it too has a certain genericity. Because it can potentially “plug into” any pocket of reducibility of which there are inevitably infinitely many—even though we humans would not yet recognize most of them. But for our particular version of consciousness the idea of sequentialization seems to be central. And, yes, we might have hoped that our consciousness would be something that even at an abstract level would put us “above” other parts of the physical universe. So the idea that this vaunted feature of ours is ultimately associated with what amounts to a restriction on computation might seem disappointing. But I view this as just part of the story that what’s special about us are not big, abstract things, but specific things that reflect all that specific irreducible computation that has gone into creating our biology, our civilization and our lives. In a sense the story of science is a story of struggle between computational irreducibility and computational reducibility. The richness of what we see is a reflection of computational irreducibility, but if we are to understand it we must find computational reducibility in it. And from what we have discussed here we now see how consciousness—which seems so core to our existence—might fundamentally relate to the computational reducibility we need for science, and might ultimately drive our actual scientific laws. How does this all relate to what philosophers (and others) have said before? It will take significant work to figure that out, and I haven’t done it. But it’ll surely be valuable. Of course it’ll be fun to know if Leibniz or Kant or Plato already figured out—or guessed—this or that, even centuries or millennia before we discovered some feature of computation or physics. But what’s more important is that if there’s overlap with some existing body of work then this provides the potential to make a connection with other aspects of that work, and to show, for example, how what I discuss might relate, say, to other areas of philosophy or other questions in philosophy. My mother, Sybil Wolfram, was a longtime philosophy professor at Oxford University, and I was introduced to philosophical discourse at a very young age. I always said, though, that if there was one thing I’d never do when I was grown up, it’s philosophy; it just seemed too crazy to still be arguing about the same issues after two thousand years. But after more than half a century of “detour” in science, here I am, arguably, doing philosophy after all…. Some of the early development of the ideas here were captured in the livestream: A Discussion about Physics Built by Alien Intelligences (June 25, 2020). Thanks particularly to Jeff Arle, Jonathan Gorard and Alexander Wolfram for discussions.
The Wolfram Physics Project: A One-Year Update 15.1 How’s It Going? : When we launched the Wolfram Physics Project a year ago today, I was fairly certain that—to my great surprise—we’d finally found a path to a truly fundamental theory of physics, and it was beautiful. A year later it’s looking even better. We’ve been steadily understanding more and more about the structure and implications of our models—and they continue to fit beautifully with what we already know about physics, particularly connecting with some of the most elegant existing approaches, strengthening and extending them, and involving the communities that have developed them. And if fundamental physics wasn’t enough, it’s also become clear that our models and formalism can be applied even beyond physics—suggesting major new approaches to several other fields, as well as allowing ideas and intuition from those fields to be brought to bear on understanding physics. Needless to say, there is much hard work still to be done. But a year into the process I’m completely certain that we’re “climbing the right mountain”. And the view from where we are so far is already quite spectacular. We’re still mostly at the stage of exploring the very rich structure of our models and their connections to existing theoretical frameworks. But we’re on a path to being able to make direct experimental predictions, even if it’ll be challenging to find ones accessible to present-day experiments. But quite independent of this, what we’ve done right now is already practical and useful—providing new streamlined methods for computing several important existing kinds of physics results. The way I see what we’ve achieved so far is that it seems as if we’ve successfully found a structure for the “machine code” of the universe—the lowest-level processes from which all the richness of physics and everything else emerges. It certainly wasn’t obvious that any such “machine code” would exist. But I think we can now be confident that it does, and that in a sense our universe is fundamentally computational all the way down. But even though the foundations are different, the remarkable thing is that what emerges aligns with important mathematical structures we already know, enhancing and generalizing them. From four decades of exploring the computational universe of possible programs, my most fundamental takeaway has been that even simple programs can produce immensely complex behavior, and that this behavior is usually computationally irreducible, in the sense that it can’t be predicted by anything much less than just running the explicit computation that produced it. And at the level of the machine code our models very much suggest that our universe will be full of such computational irreducibility. But an important part of the way I now understand our Physics Project is that it’s about what a computationally bounded observer (like us) can see in all this computational irreducibility. And the key point is that within the computational irreducibility there are inevitably slices of computational reducibility. And, remarkably, the three such slices we know correspond exactly to the great theories of existing physics: general relativity, quantum mechanics and statistical mechanics. And in a sense, over the past year, I’ve increasingly come to view the whole fundamental story of science as being about the interplay between computational irreducibility and computational reducibility. The computational nature of things inevitably leads to computational irreducibility. But there are slices of computational reducibility that inevitably exist on top of this irreducibility that are what make it possible for us—as computationally bounded entities—to identify meaningful scientific laws and to do science. There’s a part of this that leads quite directly to specific formal development, and for example specific mathematics. But there’s also a part that leads to a fundamentally new way of thinking about things, that for example provides new perspectives on issues like the nature of consciousness, that have in the past seemed largely in the domain of philosophy rather than science.
The Wolfram Physics Project: A One-Year Update 15.2 What Is Our Universe Made Of? : Spatial hypergraphs. Causal graphs. Multiway graphs. Branchial graphs. A year ago we had the basic structure of our models and we could see how both general relativity and quantum mechanics could arise from them. And it could have been that as we went further—and filled in more details—we’d start seeing issues and inconsistencies. But nothing of the sort has happened. Instead, it seems as if at every turn more and more seems to fit beautifully together—and more and more of the phenomena we know in physics seem to inevitably emerge as simple and elegant consequences of our models. It all starts—very abstractly—with collections of elements and relations. And as I’ve got more comfortable with our models, I’ve started referring to those elements by what might almost have been an ancient Greek term: atoms of space. The core concept is then that space as we know it is made up from a very large number of these atoms of space, connected by a network of relations that can be represented by a hypergraph. And in our models there’s in a sense nothing in the universe except space: all the matter and everything else that “exists in space” is just encoded in the details of the hypergraph that corresponds to space. Time in our models is—at least initially—something fundamentally different from space: it corresponds to the computational process of successively applying rules that transform the structure of the hypergraph. And in a sense the application of these rules represents the fundamental operation of the universe. And a key point is that this will inevitably show the phenomenon of computational irreducibility—making the progress of time an inexorable and irreducible computational process. A striking feature of our models is that at the lowest level there’s nothing constant in our universe. At every moment even space is continually being remade by the action of the underlying rules—and indeed it is precisely this action that knits together the whole structure of spacetime.  And though it still surprises me that it can be said so directly, it’s possible to identify energy as essentially just the amount of activity in space, with mass in effect being the “inertia” or persistence of this activity. At the lowest level everything is just atoms of space “doing their thing”. But the crucial result is that—with certain assumptions—there’s large-scale collective behavior that corresponds exactly to general relativity and the observed continuum structure of spacetime. Over the course of the year, the derivation of this result has become progressively more streamlined. And it’s clear it’s all about what a computationally bounded observer will be able to conclude about underlying computationally irreducible processes. But there’s then an amazing unification here. Because at a formal level the setup is basically the same as for molecular dynamics in something like a gas. Again there’s computational irreducibility in the underlying behavior. And there’s a computationally bounded observer, usually thought of in terms of “coarse graining”. And for that observer—in direct analogy to an observer in spacetime—one then derives the Second Law of Thermodynamics, and the equations of continuum fluid behavior. But there’s an important feature of both these derivations: they’re somehow generic, in the sense that they don’t depend on underlying details like the precise nature of the molecules in the gas, or the atoms of space. And what this means is that both thermodynamics and relativity are general emergent laws. Regardless of what the precise underlying rules are, they’ll basically always be what one gets in a large-scale limit. It’s quite remarkable that relativity in a sense formally comes from the same place as thermodynamics. But it’s the genericity of general relativity that’s particularly crucial in thinking about our models. Because it implies that we can make large-scale conclusions about physics without having to know what specific rule is being applied at the level of the underlying hypergraph. Much like hypersonic flow in a gas, however, there will nevertheless be extreme situations in which one will be able to “see beneath” the generic continuum behavior—and tell that there are discrete atoms of space with particular behavior. Or in other words, that one will be able to see corrections to Einstein’s equations—that depend on the fact that space is actually a hypergraph with definite rules, rather than a continuous manifold. One important feature of our spatial hypergraph is that—unlike our ordinary experience of space—it doesn’t intrinsically have any particular dimension. Dimension is an emergent large-scale feature of the hypergraph—and it can be an integer, or not, and it can, for example, vary with position and time. So one of the unexpected implications of our models is that there can be dimension fluctuations in our universe. And in fact it seems likely that our universe started essentially infinite-dimensional, only gradually “cooling” to become basically three-dimensional. And though we haven’t yet worked it out, we expect there’ll be a “dimension-changing cosmology” that may well have definite predictions for the observed large-scale structure of our universe. The underlying discreteness—and variable dimension—of space in our models has many other implications. Traditional general relativity suggests certain exotic phenomena in spacetime, like event horizons and black holes—but ultimately it’s limited by its reliance on describing spacetime in terms of a continuous manifold. In our models, there are all sorts of possible new exotic phenomena—like change in spacetime topology, space tunnels and dynamic disconnection of the hypergraph. What happens if one sets up a black hole that spins too rapidly? In our models, a piece of spacetime simply disconnects. And it’s been interesting to see how much more direct our models allow one to be in analyzing the structure of spacetime, even in cases where traditional general relativity gives one a hint of what happens. Calculus has been a starting point for almost all traditional mathematical physics. But our models in a sense require a fundamental generalization of calculus. We have to go beyond the notion of an integer number of “variables” corresponding to particular dimensions, to construct a kind of “hypercalculus” that can for example generalize differential geometry to fractional dimensional space. It’s a challenging direction in mathematics, but the concreteness of our models helps greatly in defining and exploring what to do—and in seeing what it means to go “below whole variables” and build everything up from fragmentary discrete connections. And one of the things that’s happened over the past year is that we’ve been steadily recapitulating the history of calculus-like mathematics, progressively defining generalizations of notions like tangent spaces, tensors, parallel transport, fiber bundles, homotopy classes, Lie group actions and so on, that apply to limits of our hypergraphs and to the kind of space to which they correspond. One of the ironies of practical investigations of traditional general relativity is that even though the theory is set up in terms of continuous manifolds and continuous partial differential equations, actual computations normally involve doing “numerical relativity” that uses discrete approximations suitable for digital computers. But our models are “born digital” so nothing like this has to be done. Of course, the actual number of atoms of space in our real universe is immensely larger than anything we can simulate. But we’ve recently found that even much more modest hypergraphs are already sufficient to reproduce the same kind of results that are normally found with numerical relativity. And so for example we can directly see in our models things like the ring-down of merging black holes. And what’s more, as a matter of practical computation, our models seem potentially more efficient at generating results than numerical relativity. So that means that even if one isn’t interested in models of fundamental physics and in the “underlying machine code” of the universe, our project is already useful—in delivering a new and promising method for doing practical computations in general relativity. And, by the way, the method isn’t limited to general relativity: it looks as if it can be applied to other kinds of systems based on PDEs—like stress analysis and biological growth. Normally one thinks of taking some region of space, and approximating it by a discrete mesh, that one might adapt and subdivide. But with our method, the hypergraphs—with their variable dimensions—provide a richer way to approximate space, in which subdivision is done “automatically” through the actual dynamics of the hypergraph evolution.
The Wolfram Physics Project: A One-Year Update 15.3 Can We Finally Understand Quantum Mechanics? : I already consider it very impressive and significant that our models can start from simple abstract rules and end up with the structure of space and time as we know them in some sense inevitably emerging. But what I consider yet more impressive and significant is that these very same models also inevitably yield quantum mechanics. It’s often been said (for example by my late friend Richard Feynman) that “nobody really understands quantum mechanics”.  But I’m excited to be able to say that—particularly after this past year—I think that we are finally beginning to actually truly understand quantum mechanics. Some aspects of it are at first somewhat mind-bending, but given our new understanding we’re in a position to develop more and more accessible ways of thinking about it. And with our new understanding comes a formalism that can actually be applied in many other places—and from these applications we can expect that in time what now seem like bizarre features of quantum mechanics will eventually seem much more familiar. In ordinary classical physics, the typical setup is to imagine that definite things happen, and that in a sense every system follows a definite thread of behavior through time.  But the key idea of quantum mechanics is to imagine that many threads of possible behavior are followed, with a definite outcome being found only through a measurement made by an observer. And in our models this picture is not just conceivable, but inevitable. The rules that operate on our underlying spatial hypergraph specify that a particular configuration of elements and relations will be transformed into some other one. But typically there will be many different places in the spatial hypergraphs where any such transformation can be applied. And each possible sequence of such updating events defines a particular possible “thread of history” for the system. A key idea of our models is to consider all those possible threads of history—and to represent these in a single object that we call a multiway graph. In the most straightforward way of setting this up, each node in the multiway graph is a complete state of the universe, joined to whatever states are reached from it by all possible updating events that can occur in it. A particular possible history for the universe then corresponds to a particular path through the multiway graph. And the crucial point is that there is branching—and merging—in the multiway graph leading in general to a complicated interweaving of possible threads of history. But now imagine slicing across the multiway graph—in a sense sampling many threads of history at some particular stage in their evolution. If we were to look at these threads of history separately there might not seem to be any relation between them. But the way they’re embedded in the multiway graph inevitably defines relations between them. And for example we can imagine just saying that any two states in a particular slice of the multiway graph are related if they have a common ancestor, and are each just a result of a different event occurring in that ancestor state. And by connecting such states we form what we call a branchial graph—a graph that captures the relations between multiway branches. But just like we imagine our spatial hypergraphs limit to something like ordinary continuous physical space, so also we can imagine that our branchial graphs limit to something we can call branchial space. And in our models branchial space corresponds to a space of quantum states, with the branchial graph in effect providing a map of the entanglements between those states. In ordinary physical space we know that we can define coordinates that label different positions. And one of the things we’re understanding with progressively more clarity is also how to set up coordinatizations of branchial space—so that instead of just talking individually about “points in branchial space” we can talk more systematically about what happens “as a function of position” in branchial space. But what is the interpretation of “position” in branchial space? It turns out that it is essentially the phase of a quantum amplitude. In the traditional formalism of quantum mechanics, every different state has a certain complex number associated with it that is its quantum amplitude. In our models, that complex number should be thought of in two parts. Its magnitude is associated with a combinatorial counting of possible paths in the multiway graph. But its phase is “position in branchial space”. Once one has a notion of position, one is led to talk about motion. And in classical mechanics and general relativity a key concept is that things in physical space move by following shortest paths (“geodesics”) between different positions. When space is flat these paths are ordinary straight lines, but when there is curvature in space—corresponding in general relativity to the presence of gravity—the paths are deflected. But what the Einstein equations then say is that curvature in space is associated with the presence of energy-momentum. And in our models, this is exactly what happens: energy-momentum is associated with the presence of update events in the spatial hypergraph, and these lead to curvature and a deflection of geodesics. So what about motion in branchial space? Here we are interested in how “bundles of nearby histories” progress through time in the multiway graph. And it turns out that once again we are dealing with geodesics that are deflected by the presence of update events that we can interpret as energy-momentum. But now this deflection is not in physical space but in branchial space. The fundamental underlying mathematical structure is the same in both cases. But the interpretation in terms of traditional physics is different. And in what to me is a singularly beautiful result of our models it turns out that what gives the Einstein equations in physical space gives the Feynman path integral in branchial space. Or in other words, quantum mechanics is the same as general relativity, except in branchial space rather than physical space. But, OK, so how do we assign positions in branchial space? It’s a mathematically complicated thing to do. Nearly a year ago we found a kind of trick way to do it for a standard simple quantum setup: the double-slit experiment. But over the course of the year, we’ve developed a much more systematic approach based on category theory and categorical quantum mechanics. In its usual applications in mathematics, category theory talks about things like the patterns of mappings (morphisms) between definite named kinds of objects. But in our models what we want is just the “bulk structure” of category theory, and the general idea of patterns of connections between arbitrary unnamed objects. It’s very much like what we do in setting up our spatial hypergraph. There are symbolic expressions—like in the Wolfram Language—that define structures associated with named kinds of things, and on which transformations can be applied. But we can also consider “bulk symbolic expressions” that don’t in effect “name every element of space”, and where we just consider their overall structure. It’s an abstract and elaborate mathematical story. But the key point is that in the end our multiway formalism can be shown to correspond to the formalism that has been developed for categorical quantum mechanics—which in turn is known to be equivalent to the standard formalism of quantum mechanics. So what this means is that we can take a description of a quantum system—say a quantum circuit—and in effect “compile” it into an equivalent multiway system. One thing is that we can think of this as a “proof by compilation”: we know our models reproduce standard quantum mechanics, because standard quantum mechanics can in effect just be systematically compiled into our models. But in practice there’s something more: by really getting at the essence of quantum mechanics, our models can provide more efficient ways to do actual computations in quantum mechanics. And for example we’ve got recent results on using automated theorem proving methods within our models to more efficiently optimize practical quantum circuits. Much as in the case of general relativity, it seems that by “going underneath” the standard formalism of physics, we’re able to come up with more efficient ways to do computations, even for standard physics. And what’s more, the formalism we have potentially applies to things other than physics. I’ll talk more about this later. But here let me mention a simple example that I’ve tried to use to build intuition about quantum mechanics. If you have something like tic-tac-toe, you can think of all possible games that can be played as paths through a multiway graph in which the nodes are possible configurations of the tic-tac-toe board. Much like in the case of quantum mechanics, one can define a branchial graph—and then one can start thinking about the analogs of all kinds of “quantum” effects, and how there are just a few final “classical” outcomes for the game. Most practical computations in quantum mechanics are done at the level of quantum amplitudes—which in our setup corresponds essentially to working out the evolution of densities in branchial space. But in a sense this just tells us that there are lots of different threads of history that a particular system could follow. So how is it then that we come to perceive definite things as happening in the world? The traditional formalism of quantum mechanics essentially by fiat introduces the so-called Born rule which in effect says how densities in branchial space can be converted to probabilities of different specific outcomes. But in our models we can “go inside” this “process of measurement”. The key idea—which has become clearer over the course of this year—is at first a bit mind-bending. Remember that our models are supposed to be models for everything in the universe, including us as observers of the universe. In thinking about space and time we might at first imagine that we could just independently trace the individual time evolution of, for example, different atoms of space. But if we’re inside the system no such “absolute tracing” is possible; instead all we can ever perceive is the graph of causal relationships of different events that occur. In a sense we’re only “plugged into” the universe through the causal effects that the universe has on us. OK, so what about the quantum case? We want to tell what’s going on in the multiway graph of all possible histories.  But we’re part of that graph, with many possible histories ourselves. So in a sense what we have to think about is how a “branching brain” perceives a “branching universe”.  People have often imagined that somehow having a “conscious observer” is crucial to “making measurements” in quantum mechanics. And I think we can now understand how that works. It seems as if the essence of being a “conscious observer” is precisely having a “single thread of experience”—or in other words conflating the different histories in different branches. Of course, it is not at all obvious that doing this will be consistent. But in our models there is the notion of causal invariance. In the end this doesn’t have to be an intrinsic feature of specific low-level rules one attributes to the universe; as I’ll talk about a bit later, it seems to be an inevitable emergent feature of the structure of what we call rulial space. But what’s important about causal invariance is that it implies that different possible threads of history must in effect in the end always have the same causal structure—and the same observable causal graph that describes what happens in the universe. It’s causal invariance that makes different reference frames in physical space (corresponding, for example, to different states of motion) work the same, and that leads to relativistic invariance. And it’s also causal invariance (or at least eventual causal invariance) that makes the conflation of quantum histories be consistent—and makes there be a meaningful notion of objective reality in quantum mechanics, shared by different observers. There’s more to do in working out the detailed mechanics of how threads of history can be conflated. It can be thought of as closely related to the addition of “completion lemmas” in automated theorem proving. Some aspects of it can be thought of as a “convention”—analogous to a choice of reference frame. But the structure of the model implies certain important “physical constraints”. We’ve often been asked: “What does all this mean for quantum computing?” The basic idea of quantum computing—captured in a minimal form by something like a multiway Turing machine—is to do different computations in parallel along different possible threads of history. But the key issue (that I’ve actually wondered about since the early 1980s) is then how to corral those threads of history together to figure out a definite answer for the computation. And our models give us ways to look “inside” that process, and see what’s involved, and how much time it should take. We’re still not sure about the answer, but the preliminary indication is that at least at a formal level, quantum computers aren’t going to come out ahead. (In practice, of course, investigating physical processes other than traditional semiconductor electronics will surely lead to even perhaps dramatically faster computers, even if they’re not “officially quantum”.) One of the surprises to me this year has been just how far we can get in exploring quantum mechanics without ever having to talk about actual particles like electrons or photons. Actual quantum experiments usually involve particles that are somehow localized to particular positions in space. But it seems as if the essentials of quantum mechanics can actually be captured without depending on particles, or space. What are particles in our models? Like everything else in the universe, they can be thought of as features of space. The general picture is that in the spatial hypergraph there are continual updates going on, but most of them are basically just concerned with “maintaining the structure of space”. But within that structure, we imagine that there can be localized pieces that have a certain stability that allows them to “move largely unchanged through space” (even as “space itself” is continually getting remade). And these correspond to particles. Analogous to things like vortices in fluids, or black holes in spacetime, we can view particles in our models as some kind of “topological obstructions” that prevent features of the hypergraph from “readily unraveling”. We’ve made some progress this year in understanding what these topological obstructions might be like, and how their structure might be related to things like the quantization of particle spin, and in general the existence of discrete quantum numbers. It’s an interesting thing to have both “external space” and “internal quantum numbers” encoded together in the structure of the spatial hypergraph. But we’ve been making progress at seeing how to tease apart different features of things like homotopy and geometry in the limit of large hypergraphs, and how to understand the relations between things like foliations and fibrations in the multiway graph describing hypergraph evolution. We haven’t “found the electron” yet, but we’re definitely getting closer. And one of the things we’ve started to identify is how a fiber bundle structure can emerge in the evolution of the hypergraph—and how local gauge invariance can arise. In a discrete hypergraph it’s not immediately obvious even how something like limiting rotational symmetry would work. We have a pretty good idea how hypergraphs can limit on a large scale to continuous “spatial” manifolds. And it’s now becoming clearer how things like the correspondences between collections of geodesics from a single point can limit to things like continuous symmetry groups. What’s very nice about all of this is how generic it’s turning out to be. It doesn’t depend on the specifics of the underlying rules. Yes, it’s difficult to untangle, and to set up the appropriate mathematics. But once one’s done that, the results are very robust. But how far will that go? What will be generic, and what not? Spatial isotropy—and the corresponding spherical symmetry—will no doubt be generic. But what about local gauge symmetry? The SU(3)×SU(2)×U(1) that appears in the Standard Model of particle physics seems on the face of it quite arbitrary. But it would be very satisfying if we were to find that our models inevitably imply a gauge group that is, say, a subgroup of E(8). We haven’t finished the job yet, but we’ve started understanding features of particle physics like CPT invariance (P and T are space and time inversion, and we suspect that charge conjugation operation C is “branchial inversion”). Another promising possibility relates to the distinction between fermions and bosons. We’re not sure yet, but it seems as if Fermi–Dirac statistics may be associated with multiway graphs where we see only non-merging branches, while Bose–Einstein statistics may be associated with ones where we see all branches merging. Spinors may then turn out to be as straightforward as being associated with directed rather than undirected spatial hypergraphs. It’s not yet clear how much we’re going to have to understand particles in order to see things like the spin-statistics connection, or whether—like in basic quantum mechanics—we’re going to be able to largely “factor out” the “spatial details” of actual particles. And as we begin to think about quantum field theory, it’s again looking as if there’ll be a lot that can be said in the “bulk” case, without having to get specific about particles. And just as we’ve been able to do for spacetime and general relativity, we’re hoping it’ll be possible to do computations in quantum field theory directly from our models, providing, for example, an alternative to things like lattice gauge theory (presumably with a more realistic treatment of time). When we mix spatial hypergraphs with multiway graphs we inevitably end up with pretty complex structures—and ones that at least in the first instance tend to be full of redundancy. In the most obvious “global” multiway graph, each multiway graph node is in effect a complete state of the universe, and one’s always (at least conceptually) “copying” every part of this state (i.e. every spatial hypergraph node) at every update, even though only a tiny part of the state will actually be affected by the update. So one thing we’ve been working on this year is defining more local versions of multiway systems. One version of this is based on what I call “multispace”, in which one effectively “starts from space”, then lets parts of it “bow out” where there are differences between different multiway branches. But a more scalable approach is to make a multiway graph not from whole states, but instead from a mixture of update events and individual “tokens” that knit together to form states. There’s a definite tradeoff, though. One can set up a “token-event graph” that pretty much completely avoids redundancy. But the cost is that it can be very difficult to reassemble complete states. The full problem of reassembly no doubt runs into the computational irreducibility of the underlying evolution. But presumably there’s some limited form of reassembly that captures actual physical measurements, and that can be done by computationally bounded observers.
The Wolfram Physics Project: A One-Year Update 15.4 Towards Experimental Implications : In assessing a scientific theory the core question to ask is whether you get out more than you put in. It’s a bad sign if you carefully set up some very detailed model, and it still can’t tell you much. It’s a good sign if you just set up a simple model, and it can tell you lots of things. Well, by this measure, our models are the most spectacular I have ever seen. A year ago, it was already clear that the models had a rich set of implications. But over the course of this year, it feels as if more and more implications have been gushing out. And the amazing thing is that they all seem to align with what we know from physics. There’s been no tweaking involved. Yes, it’s often challenging to work out what the models imply. But when we do, it always seems to agree with physics. And that’s what makes me now so confident that our models really do actually represent a correct fundamental theory of physics. It’s been very interesting to see the methodology of “proof by compilation”. Do our models correctly reproduce general relativity? We can “compile” questions in general relativity into our models—then effectively run at the level of our “machine code”, and generate results. And what we’ve found is that, yes, compiling into our models works, giving the same results as we would get in the traditional theory, though, as it happens, potentially more efficiently.  We’ve found the same thing for quantum mechanics. And maybe we’ll find the same thing also for quantum field theory (where the traditional computations are much harder). We’ve also been looking at specific effects and phenomena in existing physics—and we’re having excellent success not only in reproducing them in our models (and finding ways to calculate them) but also in (often for the first time) fundamentally understanding them. But what about new effects and phenomena that aren’t seen or expected in existing physics? Especially surprising ones? It’s already very significant when a theory can efficiently explain things that are already known. But it’s a wonderful “magic trick” if a theory can say “This is what you’ll see”, and then that’s what’s seen in some actual experiment. Needless to say, it can be very difficult to figure out detailed predictions from a theory (and historically it’s often taken decades or even centuries). And when you’re dealing with something that’s never been seen before, it’s often difficult to know if you’ve included everything you need to get the right answer, both in working out theoretical predictions, and in making experimental measurements. But one of the interesting things about our models is how structurally different they are from existing physics. And even before we manage to make detailed quantitative predictions, the very structure of our models implies the possibility of a variety of unexpected and often bizarre phenomena. One class of such phenomena relate to the fact that in our models the dimension of space is dynamic, and does not just have a fixed integer value. Our expectation is that in the very early universe, the dimension of space was effectively infinite, gradually “cooling” to approximately 3. And in this setup, there should have been “dimension fluctuations”, which could perhaps have left a recognizable imprint on the cosmic microwave background, or other large-scale features of the universe. It’s also possible that there could be dimension fluctuations still in our universe today, either as relics from the early universe, or as the result of gravitational processes. And if photons propagate through such dimension fluctuations, we can expect strange optical effects, though the details are still to be worked out. (One can also imagine things like pulsar timing anomalies, or effects on gravitational waves—or just straight local deviations from the inverse square law. Conceivably quantum field theoretic phenomena like anomalous magnetic moments of leptons could be sensitive dimension probes—though on small scales it’s difficult to distinguish dimension change from curvature. Or maybe there would be anomalies or magnetic monopoles made possible by noninteger dimensionality.) A core concept of our models is that space (and time) are fundamentally discrete. So how might we see signs of this discreteness? There’s really only one fundamental unknown free parameter in our models (at least at a generic level), and there are many seemingly very different experiments that could determine it. But without having the value of this parameter, we don’t ultimately know the scale of discreteness in our models. We have a (somewhat unreliable) estimate, however, that the elementary length might be around 10-90 meters (and the elementary time around 10-100 seconds). But these are nearly 70 orders of magnitude smaller than anything directly probed by present-day experiments. So can we imagine any way to detect discreteness on such scales? Conceivably there could be effects left over from a time when the whole universe was very small.  In the current universe there could be a signature of momentum discreteness in “maximum boosts” for sufficiently light particles. Or maybe there could be “shot noise” in the propagation of particles. But the best hope for detecting discreteness of spacetime seems to be in connection with large gravitational fields. Eventually our models must imply corrections to Einstein’s equations. But at least in the most obvious estimates these would only become significant when the scale of curvature is comparable to the elementary length. Of course, it’s conceivable that there could be situations where, for example, there could be, say, a logarithmic signature of discreteness, allowing a more effective “gravitational microscope” to be constructed. In current studies of general relativity, the potentially most accessible “extreme situation” is a spinning black hole close to critical angular momentum. And in our models, we already have direct simulations of this. And what we see is that as we approach criticality there starts to be a region of space that’s knitted into the rest of space by fewer and fewer updating events. And conceivably when this happens there would be “shot noise”, say visible in gravitational waves. There are other effects too. In a kind of spacetime analog of vacuum polarization, the discreteness of spacetime should lead to a “black hole wind” of outgoing momentum from an event horizon—though the effect is probably only significant for elementary-length-scale black holes. (Such effects might lead to energy loss from black holes through a different “mode of spacetime deformation” than ordinary gravitational radiation.) Another effect of having a discrete structure to space is that information transmission rates are only “statistically” limited to the speed of light, and so fluctuations are conceivable, though again most likely only on elementary-length-type scales. In general the discreteness of spacetime leads to all sorts of exotic structures and singularities in spacetime not present in ordinary general relativity. Notable potential features include dynamic topology change, “space tunnels”, “dimension anomalies” and spatial disconnection. We imagine that in our models particles are some kind of topological obstructions in the spatial hypergraph. And perhaps we will find even quite generic results for the “spectrum” of such obstructions. But it’s also quite possible that there will be “topologically stable” structures that aren’t just like point particles, but are something more exotic. By the way, in computing things like the cosmological constant—or features of dark energy—we need to compare the “total visible particle content” with the total activity in the spatial hypergraph, and there may be generic results to be had about this. One feature of our models is that they imply that things like electrons are not intrinsically of zero size—but in fact are potentially quite large compared to the elementary length. Their actual size is far out of range of any anticipated experiments, but the fact that they involve so many elements in the underlying spatial hypergraph suggests that there might be particles—that I’ve called oligons—that involve many fewer, and that might have measurable cosmological or astrophysical effects, or even be directly detectable as some kind of very-low-mass dark matter. In thinking about particles, our models also make one think about some potential highly exotic possibilities. For example, perhaps not every photon in the universe with given energy-momentum and polarization is actually identical. Maybe they have the same “overall topological structure”, but different detailed configuration of (say) the multiway causal graph. And maybe such differences would have detectable effects on sufficiently large coherent collections of photons. (It may be more plausible, however, that particles act a bit like tiny black holes, with their “internal state” not evident outside.) When it comes to quantum mechanics, our models again have some generic predictions—the most obvious of which is the existence of a maximum entanglement speed ζ, that is the analog of the speed of light, but in branchial space. In our models, the scale of ζ is directly connected to the scale of the elementary length, so measuring one would determine the other—and with our (rather unreliable) estimate for the elementary length ζ might be around 105 solar masses per second. There are a host of “relativity-analog” effects associated with ζ, an example being the quantum Zeno effect that is effectively time dilation associated with rapidly repeated measurement. And conceivably there is some kind of atomic-scale (or gravitational-wave-detector-deformation-scale) “measurement from the environment” that could be sensitive to this—perhaps associated with what might be considered “noise” for a quantum computer. (By the way, ζ potentially also defines limitations on the effectiveness of quantum computing, but it’s not clear how one would disentangle “engineering issues”.) Then there are potential interactions between quantum mechanics and the structure of spacetime—perhaps for example effects of features of spacetime on quantum coherence. But probably the most dramatic effects will be associated with things like black holes, where for example the maximum entanglement speed should represent an additional limitation on black hole formation—that with our estimate for ζ might actually be observable in the near term. Historically, general relativity was fortunate enough to imply effects that did not depend on any unknown scales (like the cosmological constant). The most obvious candidates for similar effects in our models involve things like the quantum behavior of photons orbiting a black hole. But there’s lots of detailed physics to do to actually work any such things out. In the end, a fundamental model for physics in our setup involves some definite underlying rule. And some of our conclusions and predictions about physics will surely depend on the details of that rule. But one of the continuing surprises in our models is how many implied features of physics are actually generic to a large class of rules. Still, there are things like the masses of elementary particles that at least feel like they must be specific to particular rules. Although—who knows—maybe overall symmetries are determined by the basic structure of the model, maybe the number of generations of fermions is connected to the effective dimensionality of space, etc. These are some of the kinds of things it looks conceivable that we’ll begin to know in the next few years.
The Wolfram Physics Project: A One-Year Update 15.5 Beyond Physics : When I first started developing what people have been calling “Wolfram models”, my primary motivation was to understand fundamental physics. But it was quickly clear that the models were interesting in their own right, independent of their potential connection to physics, and that they might have applications even outside of physics. And I suppose one of the big surprises this year has been just how true that is. I feel like our models have introduced a whole new paradigm, that allows us to think about all kinds of fields in fundamentally new ways, and potentially solve longstanding foundational problems in them. The general exploration of the computational universe—that I began more than forty years ago—has brought us phenomena like computational irreducibility and has led to all sorts of important insights. But I feel that with our new models we’ve entered a new phase of understanding the computational universe, in particular seeing the subtle but robust interplay between computational reducibility and computational irreducibility that’s associated with the introduction of computationally bounded observers or measurements. I hadn’t really known how to fit the successes of physics into the framework of what I’d seen in the computational universe. But now it’s becoming clear. And the result is not only that we understand more about the foundations of physics, but also that we can import the successes of physics into our thinking about the computational universe, and all its various applications. At a very pragmatic level, cellular automata (my longtime favorite examples in the computational universe) provide minimal models for systems in which arbitrary local rules operate on a fixed array in space and time. Our new models now provide minimal models for systems that have no such definite structure in space and time. Cellular automata are minimal models of “array parallel” computational processes; our new models are minimal models of distributed, asynchronous computational processes. In something like a cellular automaton—with its very organized structure for space and time—it’s straightforward to see “what leads to what”. But in our new models it can be much more complicated—and to represent the causal relationships between different events we need to construct causal graphs. And for me one consequence of studying our models has been that whenever I’m studying anything I now routinely start asking about causal graphs—and in all sorts of cases this has turned out to be very illuminating. But beyond causal graphs, one feature of our new models is their essentially inevitable multiway character. There isn’t just one “thread of history” for the evolution of the system, there’s a whole multiway graph of them. In the past, there’ve been plenty of probabilistic or nondeterministic models for all sorts of systems. But in a sense I’ve always found them unsatisfactory, because they end up talking about making an arbitrary choice “from outside the system”. A multiway graph doesn’t do that. Instead, it tells our story purely from within the system. But it’s the whole story: “in one gulp” it’s capturing the whole dynamic collection of all possibilities. And now that the formalism of our models has gotten me used to multiway graphs, I see them everywhere. And all sorts of systems that I thought somehow weren’t well enough defined to be able to study in a systematic way I now realize are amenable to “multiway analysis”. One might think that a multiway graph that captures all possibilities would inevitably be too complicated to be useful. But this is another key observation from our Physics Project: particularly with the phenomenon of causal invariance, there are generic statements that can be made, without dealing with all the details. And one of the important directions we’ve pursued over the course of this year is to get a better understanding—sometimes using methods from category theory—of the general theory of multiway systems. But, OK, so what can we apply the formalism of our models to? Lots of things. Some that we’ve at least started to think seriously about are: distributed computing, mathematics and metamathematics, chemistry, biology and economics.  And in each case it’s not just a question of having some kind of “add-on” model; it seems like our formalism allows one to start talking about deep, foundational questions in each of these fields. In distributed computing, I feel like we’re just getting started. For decades I’ve wondered how to think about organizing distributed computing so that we humans can understand it. And now within our formalism, I’ve both understood why that’s hard, and begun to get ideas about how we might do it.  A crucial part is getting intuition from physics: thinking about “programming in a reference frame”, causal invariance as a source of eventual consistency, quantum effects as ambiguities of outcome, and so on.  But it’s also been important over the past year to study specific systems—like multiway Turing machines and combinators—and be able to see how things work out in these simpler cases. As an “exercise”, we’ve been looking at using ideas from our formalism to develop a distributed analog of blockchain—in which “intentional events” introduced from outside the system are “knitted together” by large numbers of “autonomous events”, in much the same way as consistent “classical” space arises in our models of physics. (The analog of “forcing consensus” or coming to a definite conclusion is essentially like the process of quantum measurement.) It’s interesting to try to apply “causal” and “multiway” thinking to practical computation, for example in the Wolfram Language. What is the causal graph of a computation? It’s a kind of dependency trace. And after years of looking for a way to get a good manipulable symbolic representation of program execution this may finally show us how to do it. What about the multiway graph? We’re used to thinking about computations that get done on “data structures”, like lists. But how should we think of a “multiway computation” that can produce a whole bundle of outputs? (In something like logic programming, one starts with a multiway concept, but then typically picks out a single path; what seems really interesting is to see how to systematically “compute at the multiway level”.) OK, so what about mathematics? There’s an immediate correspondence between multiway graphs and the networks obtained by applying axioms or laws of inference to generate all possible theorems in a given mathematical theory. But now our study of physics makes a suggestion: what would happen if—like in physics—we take a limit of this process? What is “bulk” or “continuum” metamathematics like? In the history of human mathematics, there’ve been a few million theorems published—defining in a sense the “human geography” of metamathematical space.  But what about the “intrinsic geometry”? Is there a theory of this, perhaps analogous to our theory of physics? A “physicalized metamathematics”? And what does it tell us about the “infinite-time limit” of mathematics, or the general nature of mathematics? If we try to fully formalize mathematics, we typically end up with a very “non-human” “machine code”. In physics there might be a hundred orders of magnitude between the atoms of space and our typical experience. In present-day formalized mathematics, there might be 4 or 5 orders of magnitude from the “machine code” to typical statements of theorems that humans would deal with. At the level of the machine code, there’s all sorts of computational irreducibility and undecidability, just like in physics. But somehow at the “human level” there’s enough computational reducibility that one can meaningfully “do mathematics”. I used to think that this was some kind of historical accident. But I now suspect that—just like with physics—it’s a fundamental feature of the involvement of computationally bounded human “observers”.  And with the correspondence of formalism, one’s led to ask things like what the analog of relativity—or quantum mechanics—is in “bulk metamathematics”, and, for example, how it might relate to things like “computationally bounded category theory”. And, yes, this is interesting in terms of understanding the nature of mathematics. But mathematics also has its own deep stack of results and intuition, and in studying mathematics using the same formalism as physics, we also get to use this in our efforts to understand physics. How could all this be relevant to chemistry? Well, a network of all possible chemical reactions is once again a multiway graph. In chemical synthesis one’s usually interested in just picking out one particular “pathway”. But what if we think “multiway style” about all the possibilities? Branchial space is a map of chemical species. And we now have to understand what kind of laws a “computationally bounded chemical sensor” might “perceive” in it. Imagine we were trying to “do a computation with molecules”. The “events” in the computation could be thought of as chemical reactions. But now instead of just imagining “getting a single molecular result”, consider using the whole multiway system “as the computation”. It’s basically the same story as distributed computing. And while we don’t yet have a good way to “program” like this, our Physics Project now gives us a definite direction. (Yes, it’s ironic that this kind of molecular-scale computation might work using the same formalism as quantum mechanics—even though the actual processes involved don’t have to be “quantum” in the underlying physics sense.) When we look at biological systems, it’s always been a bit of a mystery how one should think about the complex collections of chemical processes they involve. In the case of genetics we have the organizing idea of digital information and DNA. But in the general case of systems biology we don’t seem to have overarching principles. And I certainly wonder whether what’s missing is “multiway thinking” and whether using ideas from our Physics Project we might be able to get a more global understanding—like a “general relativity” of systems biology. It’s worth pointing out that the detailed techniques of hypergraph evolution are probably applicable to biological morphogenesis. Yes, one can do a certain amount with things like continuum reaction-diffusion equations. But in the end biological tissue—like, we now believe, physical space—is made of discrete elements. And particularly when it comes to topology-changing phenomena (like gastrulation) that’s probably pretty important. Biology hasn’t generally been a field that’s big on formal theories—with the one exception of the theory of natural selection. But beyond specific few-whole-species-dynamics results, it’s been difficult to get global results about natural selection. Might the formalism of our models help. Perhaps we’d be able to start thinking about individual organisms a bit like we think about atoms of space, then potentially derive large-scale “relativity-style” results, conceivably about general features of “species space” that really haven’t been addressed before. In the long list of potential areas where our models and formalism could be applied, there’s also economics. A bit like in the natural selection case, the potential idea is to think about in effect modeling every individual event or “transaction” in an economy. The causal graph then gives some kind of generalized supply chain. But what is the effect of all those transactions? The important point is that there’s almost inevitably lots of computational irreducibility. Or, in other words, much like in the Second Law of Thermodynamics, the transactions rapidly start to not be “unwindable” by a computationally bounded agent, but have robust overall “equilibrium” properties, that in the economic case might represent “meaningful value”—so that the robustness of the notion of monetary value might correspond to the robustness with which thermodynamic systems can be characterized as having certain amounts of heat. But with this view of economics, the question still remains: are there “physics-like” laws to be found? Are there economic analogs of reference frames? (In an economy with geographically local transactions one might even expect to see effects analogous to relativistic time dilation.) To me, the most remarkable thing is that the formalism we’ve developed for thinking about fundamental physics seems to give us such a rich new framework for discussing so many other kinds of areas—and for pooling the results and intuitions of these areas. And, yes, we can keep going. We can imagine thinking about machine learning—for example considering the multiway graph of all possible learning processes. We can imagine thinking about linguistics—starting from every elementary “event” of, say, a word being said by one person to another.  We even think about questions in traditional physics—like one of my old favorites, the hard-sphere gas—analyzing them not with correlation functions and partition functions but with causal graphs and multiway graphs.
The Wolfram Physics Project: A One-Year Update 15.6 Towards Ultimate Abstraction : A year ago, as we approached the launch of the Wolfram Physics Project, we felt increasingly confident that we’d found the correct general formalism for the “machine code” of the universe, we’d built intuition by looking at billions of possible specific rules, and we’d discovered that in our models many features of physics are actually quite generic, and independent of specific rules. But we still assumed that in the end there must be some specific rule for our particular universe. We thought about how we might find it. And then we thought about what would happen if we found it, and how we might imagine answering the question “Why this rule, and not another?” But then we realized: actually, the universe does not have to be based on just one particular rule; in some sense it can be running all possible rules, and it is merely through our perception that we attribute a specific rule to what we see about the universe. We already had the concept of a multiway graph, generated by applying all possible update events, and tracing out the different histories to which they lead. In an ordinary multiway graph, the different possible update events occur at different places in the spatial hypergraph. But we imagined generalizing this to a rulial multiway graph, generated by applying not just updates occurring in all possible places, but also updates occurring with all possible rules. At first one might assume that if one used all possible rules, nothing definite could come out. But the fact that different rules can potentially lead to identical states causes a definite rulial multiway graph to be knitted together—including all possible histories, based on all possible sequences of rules. What could an observer embedded in such a rulial multiway graph perceive? Just as for causal graphs or ordinary multiway graphs, one can imagine defining a reference frame—here a “rulial frame”—that makes the observer perceive the universe as evolving through a series of slices in rulial space, or in effect operating according to certain rules. In other words, the universe follows all possible rules, but an observer in a particular rulial frame describes its operation according to particular rules. And the critical point is then that this is consistent because the evolution in the rulial multiway graph inevitably shows causal invariance. At first this all might seem quite surprising. But the thing to realize is that the Principle of Computational Equivalence implies that collections of rules will generically show computation universality. And this means that whatever rulial frame one picks—and whatever rules one uses to describe the evolution of the universe—it’ll always be possible to use those rules to emulate any other possible rules. There is a certain ultimate abstraction and unification in all this. In a sense it says that the only thing one ultimately needs to know about our universe is that it is “computational”—and from there the whole formal structure of our models takes over. It also tells us that there is ultimately only one universe—though different rulial frames may describe it differently. How should we think about the limiting rulial multiway graph? It turns out that something like it has also appeared in the upper reaches of pure mathematics in connection with higher category theory. We can think of our basic multiway graphs as related to (weak versions of) ordinary categories. It’s a little different from how categorical quantum mechanics works in our models. But when we add in equivalences between branches in the multiway system we get a 2-category. And if we keep adding higher-and-higher-order equivalences, we get higher and higher categories. But in the infinite limit it turns out the structure we get is exactly the rulial multiway graph—so that now we can identify this as an infinity category, or more specifically an infinity groupoid. Grothendieck’s conjecture suggests that there is in a sense inevitable geometry in the infinity groupoid, and it’s ultimately this structure that seems to “trickle down” from the rulial multiway graph to everything else we look at, and imply, for example, that there can be meaningful notions of physical and branchial space. We can think of the limiting multiway graph as a representation of physics and the universe. But the exact same structure can also be thought of as a kind of metamathematical limit of all possible mathematics—in a sense fundamentally tying together the foundations of physics and mathematics. There are many details and implications to this, that we’re just beginning to work out. The ultimate formation of the rulial multiway graph depends on identifying when states or objects can be treated as the same, and merged. In the case of physics, this can be seen as a feature of the observer, and the reference frames they define. In the case of mathematics, it can be seen as a feature of the underlying axiomatic framework used, with the univalence axiom of homotopy type theory being one possible choice. The whole concept of rulial space raises the question of why we perceive the kind of laws of physics we do, rather than other ones. And the important recent realization is that it seems deeply connected to what we define as consciousness. I must say that I’ve always been suspicious about attempts to make a scientific framework for consciousness.  But what’s recently become clear is that in our approach to physics there’s both a potential way to do it, and in a sense it’s fundamentally needed to explain what we see. Long ago I realized that as soon as you go beyond humans, the only viable general definition of intelligence is the ability to do sophisticated computation—which the Principle of Computational Equivalence says is quite ubiquitous. One might have thought that consciousness is an “add-on” to intelligence, but actually it seems instead to be a “step down”. Because it seems that the key element of what we consider consciousness is the notion of having a definite “thread of experience” through time—or, in other words, a sequential way to experience the universe. In our models the universe is doing all sorts of complicated things, and showing all sorts of computational irreducibility. But if we’re going to sample it in the way consciousness does, we’ll inevitably pick out only certain computationally reducible slices. And that’s precisely what the laws of physics we know—embodied in general relativity and quantum mechanics—correspond to.  In some sense, therefore, we see physics as we do because we are observing the universe through the sequential thread of experience that we associate with consciousness. Let me not go deeper into this here, but suffice it to say that from our science we seem to have reached an interesting philosophical conclusion about the way that we effectively “create” our description of the universe as a result of our own sensory and cognitive capabilities. And, yes, that means that “aliens” with different capabilities (or even just different extents in physical or branchial space) could have descriptions of the universe that are utterly incoherent with our own. But, OK, so what can we say about rulial space? With a particular description of the universe we’re effectively stuck in a particular location or frame in rulial space. But we can imagine “moving” by changing our point of view about how the universe works. We can always make a translation, but that inevitably takes time. And in the end, just like with light cones in physical space, or entanglement cones in branchial space, there’s a limit to how fast a particular translation distance can be covered, defined by a “translation cone”. And there’s a “maximum translation speed” ρ, analogous to the speed of light c in space or the maximum entanglement speed ζ in branchial space. And in a sense ρ defines the ultimate “processor speed” for the universe. In defining the speed of light we have to introduce units for length in space. In defining ρ we have to introduce units for the length of descriptions of programs or rules—so, for example, ρ could be measured, say, in units of “Wolfram Language tokens per second”. We don’t know the value of ρ, but an unreliable estimate might be 10450 WLT/second. And just like in general relativity and quantum mechanics one can expect that there will be all sorts of effects scaled by ρ that occur in rulial space. (One example might be a “quantum-like uncertainty” that provides limits on inductive inference by not letting one distinguish “theories of the universe” until they’ve “diverged far enough” in rulial space.) The concept of rulial space is a very general one. It applies to physics. It applies to mathematics. And it also applies to pure computation. In a sense rulial space provides a map of the computational universe. It can be “coordinatized” by representing computations in terms of Turing machines, cellular automata, Wolfram models, or whatever. But in general we can ask about its limiting geometrical and topological structure. And here we see a remarkable convergence with fundamental questions in theoretical computer science. For example, particular geodesic paths in rulial space correspond to maximally efficient deterministic computations that follow a single rule. Geodesic balls correspond to maximally efficient nondeterministic computations that can follow a sequence of rules. So then something like the P vs. NP question becomes what amounts to a geometrical or topological question about rulial space. In our Physics Project we set out to find a fundamental theory for physics. But what’s become clear is that in thinking about physics we’re uncovering a formal structure that applies to much more than just physics. We already had the concept of computation in all its generality—with implications like the Principle of Computational Equivalence and computational irreducibility. But what we’ve now uncovered is unification at a different level, not about all computation, but about computation as perceived by computationally bounded observers, and about the kinds of things about which we can expect to make theories as powerful as the ones we know in physics. For each field what’s key is to identify the right question. What is the analog of space, or time, or quantum measurement, or whatever? But once we know that, we can start to use the machinery our formalism provides. And the result is a remarkable new level of unification and power to apply to science and beyond.
The Wolfram Physics Project: A One-Year Update 15.7 The Process of the Project: New Ways to Do Science : How should one set about finding a fundamental theory of physics? There was no roadmap for the science to do. And there was no roadmap for how the science should be done. And part of the unfolding story of the Wolfram Physics Project is about its process, and about new ways of doing science. Part of what has made the Wolfram Physics Project possible is ideas.  But part of it is also tools, and in particular the tall tower of technology that is the Wolfram Language.  In a sense the whole four decades of history behind the Wolfram Language has led us to this point. The general conception of computational language built to represent everything, including, it now seems, the whole universe. And the extremely broad yet tightly integrated capabilities of the language that make it possible to so fluidly and efficiently pursue each different piece of research that is needed. For me, the Wolfram Physics Project is an exciting journey that, yes, is going much better than I ever imagined. From the start we were keen to share this journey as widely as possible. We certainly hoped to enlist help. But we also wanted to open things up so that as many people as possible could experience and participate in this unique adventure at the frontiers of science. And a year later I think I can say that our approach to open science has been a great and accelerating success. An increasing number of talented researchers have become involved in the project, and have been able to make progress with great synergy and effectiveness. And by opening up what we’re doing, we’ve also been able to engage with—and hopefully inspire—a very wide range of people even outside of professional science. One core part of what’s moving the project forward is our tools and the way we’re using them. The idea of computational language—as the Wolfram Language uniquely embodies—is to have a way to represent things in computational terms, and be able to communicate them like that. And that’s what’s happening all the time in the Wolfram Physics Project. There’s an idea or direction. And it gets expressed in Wolfram Language. And that means it can explicitly and repeatably be understood, run and explored—by anyone. We’re posting our Wolfram Language working notebooks all the time—altogether 895 of them over the past year. And we’re packaging functions we write into the Wolfram Function Repository—130 of them over the past year—all with source code, all documented, and all instantly and openly usable in any Wolfram Language system.  It’s become a rhythm for our research. First explore in working notebooks, adding explanations where appropriate to make them readable as computational essays. Then organize important functions and submit them to the Function Repository, then use these functions to take the next steps in the research. This whole setup means that when people write about their results, there’s immediately runnable computational language code. And in fact, at least in what I’ve personally written, I’ve had the rule that for any picture or result I show (so far 2385 of them) it must be possible to just click it, and immediately get code that will reproduce it.  It might sound like a small thing, but this kind of fluid immediacy to being able to reproduce and build on what’s been done has turned out to be tremendously important and powerful. There are so many details—that in a sense come as second nature given our long experience with production software development. Being careful and consistent about the design of functions. Knowing when it makes sense to optimize at the cost of having less flexible code. Developing robust standardized visualizations. There are lots of what seem like small things that have turned out to be important. Like having consistent color schemes for all our various kinds of graphs, so when one sees what someone has done, one immediately knows “that’s a causal graph”, “that’s a branchial graph” and so on, without even having to read any explanation. But in addition to opening up the functions and ongoing notebooks we produce, we’ve also done something more radical: we’ve opened up our process of work, routinely livestreaming our working meetings. (There’ve been 168 hours of them this year; we’ve now also posted 331 hours from the 6 months before the launch of the project.) I’ve personally even gone one step further: I’ve posted “video work logs” of my personal ongoing work (so far, 343 hours of them)—right down to, for example, the writing of this very sentence. We started doing all this partly as an experiment, and partly following the success we’ve had over the past few years in livestreaming our internal meetings designing the Wolfram Language. But it’s turned out that capturing our Physics Project being done has all sorts of benefits that we never anticipated. You see something in a piece I’ve written. You wonder “Where did that come from?”. Well, now you can drill all the way down, to see just what went into making it, missteps and all. It’s been great to share our experience of figuring things out. And it’s been great to get all those questions, feedback and suggestions in our livestreams. I don’t think there’s any other place where you can see science being done in real time like this. Of course it helps that it’s so uniquely easy to do serious research livecoding in the Wolfram Language. But, yes, it takes some boldness (or perhaps foolhardiness) to expose one’s ongoing steps—forward or backward—in real time to the world. But I hope it helps people see more about what’s involved in figuring things out, both in general and specifically for our project. When we launched the project, we put online nearly a thousand pages of material, intended to help people get up to speed with what we’d done so far. And within a couple of months after the launch, we had a 4-week track of our Wolfram Summer School devoted to the Wolfram Physics Project. We had 30 students there (as well as another 4 from our High School Summer Camp)—all of whom did projects based on the Wolfram Physics Project. And after the Summer School, responding to tremendous demand, we organized two week-long study sessions (with 30 more students), followed in January by a 2-week Winter School (with another 17 students). It’s been great to see so many people coming up to speed on the project. And so far there’ve been a total of 79 publications, “bulletins” and posts that have come out of this—containing far more than, for example, I could possibly have summarized here. There’s an expanding community of people involved with the Wolfram Physics Project. And to help organize this, we created our Research Affiliate and Junior Research Affiliate programs, now altogether with 49 people from around the world involved. Something else that’s very important is happening too: steadily increasing engagement from a wide range of areas of physics, mathematics and computer science.  In fact, with every passing month it seems like there’s some new research community that’s engaging with the project. Causal set theory. Categorical quantum mechanics. Term rewriting. Numerical relativity.  Topos theory. Higher category theory. Graph rewriting.  And a host of other communities too. We can view the achievement of our project as being in a sense to provide a “machine code” for physics. And one of the wonderful things about it is how well it seems to connect with a tremendous range of work that’s been done in mathematical physics—even when it wasn’t yet clear how that work on its own might relate to physical reality. Our project, it seems, provides a kind of Rosetta stone for mathematical physics—a common foundation that can connect, inform and be informed by all sorts of different approaches. Over the past year there’s been a repeated, rather remarkable experience. For some reason or another we’ll get exposed to some approach or idea. Constructor theory. Causal dynamical triangulation. Ontological bases. Synthetic differential geometry. ER=EPR. And we’ll use our models as a framework for thinking about it. And we’ll realize: “Gosh, now we can understand that!” And we’ll see how it fits in with our models, how we can learn more about our models from it—and how we can use our models and our formalism to bring in new ideas to advance the thing itself. In some ways our project represents a radical shift from the past century or so of physics. And more often than not, when such intellectual shifts are made in the history of science, they’ve been accompanied by all kinds of difficulties in connecting with existing communities. But I’m very happy to report that over the past year our project has been doing quite excellently in connecting with existing communities—no doubt helped by its “Rosetta stone” character.  And as we progress, we’re looking forward to an increasing network of collaborations, both within the community that’s already formed and with other communities. And over the coming year, as we start to more seriously explore the implications of our models and formalism even beyond physics, I’m anticipating still more connections and collaborations.
The Wolfram Physics Project: A One-Year Update 15.8 The Personal Side : It’s hard to believe it’s only been a little over 18 months since we started working in earnest on the Wolfram Physics Project. So much has happened, and we’ve gotten so much further than I ever thought possible. And it feels like a whole new world has opened up. So many new ideas, so many new ways of looking at things. I’ve been fortunate enough to have already had a long and satisfying career, and it’s a surprising and remarkable thing at this stage to have what seems like a fresh, new start. Of course, in some respects I’ve spent much of my life preparing for what is now the Wolfram Physics Project. But the actuality of it has been so much more exciting and invigorating than anything I imagined. There’ve been so many questions—about all sorts of different things—that I’ve been accumulating and mulling over for decades. And suddenly it seems as if a door I never knew existed has opened, and now it’s possible to go forward on a dizzying array of fronts. I’ve spent most of my life building a whole tower of things—alternating between science and technology. And in this tower it’s remarkable the extent to which each level has built on what’s come before: tools from technology have made it possible to explore science, and ideas from science have made it possible to create technology. But a year ago I thought the Wolfram Physics Project might finally be the end of the line: a piece of basic science that was really just science, and nothing but science, with no foreseeable implications for technology. But it turns out I was completely wrong. And in fact of all the pieces of basic science I’ve ever done, the Wolfram Physics Project may be the one which has the greatest short-term implications for technology. We’re not talking about building starships using physics. We’re talking about taking the formalism we’ve developed for physics—and applying it, now informed by physics, in all sorts of very practical settings in distributed computing, modeling, chemistry, economics and beyond. In the end, one may look back at many of these applications and say “that didn’t really need the Physics Project; we could have just got there directly”.  But in my experience, that’s not how intellectual progress works. It’s only by building a tower of tools and ideas that one can see far enough to understand what’s possible. And without that decades or centuries may go by, with the path forward hiding in what will later seem like plain sight. A year ago I imagined that in working on the Wolfram Physics Project I’d mostly be doing things that were “obviously physics”. But in actuality the project has led me to pursue all sorts of “distractions”. I’ve studied things like multiway Turing machines, which, yes, are fairly obviously related to questions about quantum mechanics. But I’ve also studied combinators and tag systems (OK, these were induced by the arrival of centenaries). And I spent a while looking at the empirical mathematics of Euclid and beyond. And, yes, the way I approached all these things was strongly informed by our Physics Project. But what’s surprising is that I feel like doing each of these projects advanced the Physics Project too. The “Euclid” project has started to build a bridge that lets us import the intuition and formalism of metamathematics—informed by the concrete example of Euclid’s Elements.  The combinator project deepened my understanding of causal invariance and of the possible structures of things like space. And even the historical scholarship I did on combinators taught me a lot about issues in the foundations of mathematics that have languished for a century but I now realize are important. In all the pieces I’ve written over the past year add up to about 750 pages of material (and, yes, that number makes me feel fairly productive). But there’s so much more to do and to write. A few times in my life I’ve had the great pleasure of discovering a new paradigm and being able to start exploring what’s possible within it. And in many ways the Wolfram Physics Project has—yes, after three decades of gestation—been the most sudden of these experiences. It’s been an exciting year.  And I’m looking forward to what comes next, and to seeing the new paradigm that’s been created develop both in physics and beyond.
Why Does the Universe Exist? Some Perspectives from Our Physics Project 16.1 : What Is Formal, and What Is Actualized? Why does the universe exist? Why is there something rather than nothing? These are old and fundamental questions that one might think would be firmly outside the realm of science. But to my surprise I’ve recently realized that our Physics Project may shed light on them, and perhaps even show us the way to answers. We can view the ultimate goal of our Physics Project as being to find an abstract representation of what our universe does. But even if we find such a representation, the question still remains of why that representation is actualized: why what it represents is “actually happening”, with the actual stuff our universe is “made of”. It’s one thing to say that we have a rule or program that can reproduce a representation of what our universe is doing. But it seems very different to say that the rule or program is “actually being run” and is “actually generating” the “physical reality” of our universe. As soon as one starts talking about “running programs” some people will immediately ask “On what computer?” But a key intellectual point is that computational processes can ultimately be defined completely abstractly, without reference to anything like a physical computer. Consider for example one of my favorite cellular automata. Here’s its rule: And here’s what it does in a particular case: And, yes, I made this picture by running the rule on my actual, physical computer. But we can think of the rule as just giving an abstract definition of a computation to do. It’s like the abstract computation 2 + 2 → 4. It’s something that necessarily works the way it does, as a consequence of the abstract definitions that specify it. It doesn’t depend on having actual colored squares or an actual computer CPU any more than the mathematical computation 2 + 2 → 4 depends on having actual counters or a person arranging them. OK, but so we can potentially imagine a purely abstract computational rule that can abstractly reproduce a representation of what the universe does. And since we are part of the universe, the rule must also reproduce whatever processes are involved in our perception of what goes on in the universe. But the question still remains: why is anything “actually happening”? We can consider a rule. But based on what we’ve said so far, it’s just an abstract possibility—something we can choose to define. There is nothing that says there’s anything “real” or “actual” about it, any more than there is about the abstract mathematical statement 2 + 2 = 4.
Why Does the Universe Exist? Some Perspectives from Our Physics Project 16.2 Is the Universe Inevitable for Us? : There are an infinite number of possible programs that one can abstractly define. But we might assume that when it comes to representing our universe there’s just a particular one that gets picked out. In other words, that in the computational universe of all possible programs, there’s a specifically selected program that our physical universe follows.  After all, we might argue, we perceive definite physical laws in our universe. And surely, we might assume, there’s nothing necessary about these laws; there could perfectly well be a universe with different laws. So there must be something specific about the rule that’s being used by our universe, to give its particular laws. Of course, there’s something rather unsatisfying about this. Because it says that in the end there’s something about our universe that is arbitrary, and that in a sense has to be “explained from outside”. I’ve long suspected that this isn’t quite right, and that once one thinks about observers within a universe observing that universe, the arbitrariness will somehow “cancel out”. And, as we’ll see, in our new models of fundamental physics something a bit like this happens, though the detailed story is considerably more subtle. It all has to do with the interplay between computationally bounded observers and the whole computationally irreducible evolution of the universe, that effectively defines the passage of time. The basic point is that while there are all sorts of details of the underlying evolution that depend on the particular rule used, a computationally bounded observer can only sample certain overall features—which inevitably turn out to be essentially independent of the rule. There’s a familiar (and closely related) analog of this associated with the Second Law of Thermodynamics. A gas, for example, involves lots of molecules bouncing around. The details of their trajectories will depend on the particulars of their interactions—and will inevitably show computational irreducibility (in the sense that the trajectories cannot be computed much more efficiently than effectively just simulating the whole sequence of interactions). But an observer who uses only bounded computation will inevitably see just a “coarse-grained” version of what’s going on—and will “summarize” the behavior of the gas, say in terms of the laws of fluid motion. And the point is that these laws have the same form regardless of the particular characteristics of the underlying molecules. They’re in effect generic—and we can now understand this as an inevitable consequence of the interplay of computational boundedness of observers and computational irreducibility of underlying dynamics. And in our new models of fundamental physics, fundamentally the same thing happens. Except that now the emerging “generic laws” turn out to be general relativity and quantum mechanics. And instead of “what’s underneath” being a gas made of molecules we’re now talking about what’s underneath being a large number of abstract elements (that we can somewhat whimsically call “atoms of space”) with certain abstract relations between them. We can think of these elements and relations as defining a hypergraph: Then we can say that there’s a rule such as which specifies how updates should be done to this hypergraph. The effect of these updates will typically be a computationally irreducible process whose details depend on the particular rule used. But imagine an observer embedded in the hypergraph—as all observers in our actual universe must be. Assuming the observer uses only bounded amounts of computation, they can “read” only certain features of the hypergraph, and in so doing they (essentially) inevitably see the universe as having relativistic invariance, and ultimately as following the Einstein equations for the structure of spacetime. There are many details to this whole picture. When we specify a rule it’s effectively saying “here’s how to update any piece of the hypergraph where the (left-hand side of the) rule matches”. But in general there will be many different ways to do that. And the result is a whole bundle of “possible histories” for the system—which it turns out inevitably work according to the laws of quantum mechanics. What are we assuming to come to these conclusions? Most fundamentally, there’s what I call the Principle of Computational Equivalence. This principle says that rules whose behavior isn’t obviously simple will all show the same level of computational sophistication—and they’ll all exhibit computational irreducibility. And that’s what in effect gives us a notion of time. In essence, the passage of time is just the inexorable and irreducible process of computation. But there’s another piece. Across our hypergraph there’s a huge amount of irreducible computation that goes on—with all the various individual “atoms of space” in effect continually getting regenerated. But we humans have a particular way of experiencing all this—that involves sampling just those features that allow us to construct a coherent “thread of consciousness”. It’s not obvious that such sampling could maintain its consistency over time, or could make different “consciousnesses” come to the same conclusions about the universe. But the phenomenon of causal invariance (which itself can be an emergent consequence of constructing a thread of consciousness) makes such things inevitable. And in the end we can conclude that with our particular “sequentialized” way of “reading the universe” as computationally bounded observers, it’s inevitable that we’ll end up concluding that the universe follows the particular laws of general relativity and quantum mechanics that physics has established. Underneath, there’s all sorts of computational irreducibility that we’re not directly sensitive to. But as computationally bounded observers we’re picking out a certain slice of computational reducibility—and with our particular sequentialization of experience in time, the slice we pick out corresponds to our known laws of physics. But, OK, what this says is that given that we’re “reading” the universe in a certain way, it’s basically inevitable that we’ll conclude that the universe follows certain laws. There could be almost any rule operating underneath; we’ll always come to the same conclusion. But in and of itself this doesn’t explain why there’s a rule operating at all, or in effect why the universe exists, or why there’s something rather than nothing.
Why Does the Universe Exist? Some Perspectives from Our Physics Project 16.3 The Laws of the Rulial Universe : As we work towards the question of why the universe exists, we need to talk about another formal, scientific result. We said above that our underlying rule is used wherever it applies in the hypergraph. And that doing this gives us a structure that we read as following relativity and quantum mechanics. But so far we’re still assuming that there’s some particular rule for the universe—that’s just getting applied in lots of different places. But what if instead there are lots of different rules that can be applied? In fact, what about the limiting case where any possible rule can be applied? What if each piece of our hypergraph is updated according to all possible rules—generating many different possible histories? Or, in other words, what if the universe in some sense “simultaneously runs” all possible rules—generating all possible resulting histories? Our first instinct might be that if all these possibilities are allowed then there could never be anything definite said about what would happen in the universe. But it turns out that this is far from correct. And it all has to do with the entangling of different possibilities associated with the repeated application of rules. Given a particular state of the universe, applying different rules can lead to different states. But applying rules to different states can also potentially lead to the same state. Or, in other words, the “rulial multiway graph” that represents how one state leads to another can exhibit both branching and merging. Let’s look at a very simple example. Imagine that our rules just take a number x and add a constant n. Starting with 0, and allowing rules with n up to 5, we get after 1 step mostly just a bunch of separate branches: But after 2 steps, some of these branches merge, and we actually get a fairly complicated structure—in effect reflecting equivalences between different sequences of rule applications, associated with facts like 1 + 1 = 2: If we use, say, all possible addition rules mod 10 we get a more symmetrical structure: But the important point is that even though all possible rules (at least of a certain type) are being used, there’s still a very definite structure that emerges. So why does this happen? If we just enumerated the results of independently updating some state according to all possible rules, then, yes, in some sense all possible states could be produced. But the crucial point is that we’re thinking about repeatedly applying all possible rules—and in doing this, there’s inevitably a certain entanglement associated with the fact that different rules (or sequences of rules) can lead to the same states. So in some sense we generate structure from the interplay between the structural relations of states and the computational process of applying rules (or, in effect, in our models, the “passage of time”). OK, but let’s come back to our broader questions about the universe. The set of all possible rules is something purely formal. Yes, we might choose some particular language or basis for representing these rules (say in terms of Turing machines, or hypergraph rewriting). But in the end the concept of computation universality (or, more tightly, the Principle of Computational Equivalence) tells us that all these different languages or bases will be equivalent. So when we talk about all possible rules, we’re not relying on anything about our particular universe. We’re just describing something abstract, that is the way it is merely as a result of the definition of terms. But now what we’ve seen is that just starting with this “formal concept” of all possible rules, we’re getting a definite structure, which, yes, so far we’re still also thinking of as something “purely formal”. Remember that we originally started from our models of physics, where we’re talking about rules whose application determines the progress of time, the structure of space, and so on. But then we took the “formal limit” of considering all possible rules. And we found that the resulting “rulial multiway graph” has a definite structure. But what is the relation of this to our universe, and to the established laws of physics? To answer that, we have to think about how this rulial multiway graph would be perceived by observers embedded within it. Looked at “from the outside” the rulial multiway graph involves many different threads of history, sometimes branching, sometimes merging. But an observer within the rulial multiway graph also involves many different threads of history, with the same kinds of branching and merging. It’s very much like the situation for quantum mechanics in our models: we have to work out how a “branching brain” will “read” a “branching universe”—except that now it’s a “rulially branching brain” reading a “rulially branching universe”. But it turns out that the same basic arguments apply. There are many different possible “reference frames” that the observer can use to “organize” what they experience in the rulial multiway graph. But if the observer is computationally bounded—and adopts a description of the universe based on a “sequentialized thread of experience”—then it turns out that any reference frame they pick will give as perceived laws of physics general relativity and quantum mechanics. Let’s review what we’ve discussed. In the previous section we talked about how different underlying rules lead to the same perceived laws of physics. But now we’re saying that this doesn’t just happen for different specific rules; it happens even when the universe is in effect “running all possible rules”. So in a sense, in the end all that matters about the “underlying stuff” of the universe is its formal structure. The laws of physics are then just “formal consequences” of this structure, that we perceive to be the way they are because of the way we choose to “read” the universe, and in particular the fact that we construct a sequentialized thread of experience.
Why Does the Universe Exist? Some Perspectives from Our Physics Project 16.4 From Formal Inevitability to Actuality : The set of all possible rules is something purely formal—and something that in some sense has no structure. But what we’ve discovered is that from the formal computational process of applying these rules we inevitably get structure. And if we think about how an observer like us embedded within this system perceives what’s going on, we conclude that they inevitably describe the system as following known laws of physics. So how does this help us understand why the universe exists? We’re starting from all possible rules. And basically we’re saying that having a universe that operates in the way we perceive ours to operate is an inevitable consequence of there being all these possible rules. Or, in other words, if these rules “exist” then it follows that so will our universe. But what does it mean for rules to “exist”, and in particular for all possible rules to exist? The key point, I believe, is that it’s in a sense an abstract necessity. The set of all possible rules is something purely formal. It can be represented in an infinite number of ways. But it’s always there, existing as an abstract thing, completely independent of any particular instantiation. It’s crucial that we’re talking about all possible rules. If we were talking about particular rules, then we’d need to specify which rules those are, and we’d need a whole language and structure for doing that. But that’s not our situation. We’re talking about all possible rules. We can construct some explicit symbolic representation for these rules, but the deductions we make ultimately won’t depend on this; they would work the same whatever representation we chose to construct. We might have assumed that to get our universe we’d need some definite input, some specific information. But what we’re discovering is that our universe is in some sense like a tautology; it’s something that has to be the way it is just because of the definition of terms. In effect, it exists because it has to, or in a sense because everything about it is a “logical inevitability”, with no choice about anything. As I mentioned in the previous section, we might have assumed that with all possible rules there’d never be anything definite to say. But the crucial scientific fact is that that isn’t the case. Just from the formal idea of all possible rules, lots of specific detail follows. Is it obvious that this would be the case? Well, no. It’s taken the whole stack of ideas and development associated with our Physics Project to figure it out. Although it also actually turns out to be closely related to mathematical concepts, like the so-called infinity groupoid, that arise at what might be thought of as an outer limit of abstraction in modern pure mathematics.
Why Does the Universe Exist? Some Perspectives from Our Physics Project 16.5 Our Experience of the Universe : I’ve made an argument for why it’s inevitable that the whole rulial universe exists. But what about our particular perception of the universe? Well, that perception is in some sense constructed by us. We are for example picking particular reference frames in which to organize our view of what’s happening in the universe. In the end, the result of our perception is again something abstract—our particular “symbolic description” of what’s “actually happening” in the universe. And from this, we might then think that perhaps we’re not really saying anything about things “existing”. After all, we’re starting from abstract rules, and in the end all we have is abstract perception. But the big question is: why is there consistency in our perception of the universe? It could for example be that each different possible rule would make us get an utterly different impression of the universe. But again, the crucial scientific fact is that the rulial universe has certain inevitable consistent features, that will be perceived in essentially the same way by any “method of perception” that’s at all like ours. And what this means is that it makes sense to talk about there being an “objective reality” independent of the details of our methods of perception—that for example we know includes the basic laws of physics. There are certainly different ways an observer like us embedded in the rulial universe could define their experience of it. In fact, we can in effect imagine a whole rulial space of possible forms of description. It’s a bit like saying we experience the familiar universe based on the particular place in ordinary space at which we find ourselves. Well, similarly we experience the whole rulial universe based on the place in rulial space that corresponds to the description of the universe that we use. The overall structure of the whole rulial universe is inevitable. But the particular place in rulial space at which we find ourselves is a matter of how we choose things. And yes, given our particular history—both biological and intellectual—there are certain constraints on where in rulial space we can readily be. And as I recently discussed in connection with understanding the nature of consciousness those constraints (essentially our computational boundedness, and our notion of a definite thread of experience through time) inevitably lead us to the pillars of known physics: general relativity and quantum mechanics. OK, so why does the universe exist? What we’re saying is that its whole rulial structure is a logical inevitability. The way we choose to describe it, however, has a certain arbitrariness. But across different descriptions there is a fundamental consistency that lets us reasonably think that the universe does indeed exist. Is there a shorter way to say what’s going on? The main scientific result that we’re using is that arbitrary abstraction, if played out completely enough, in a sense inevitably reads like physics to observers like us. So the universe—with the physics we perceive in it—exists because it’s formally inevitable that it does.
Why Does the Universe Exist? Some Perspectives from Our Physics Project 16.6 Where Does Everything Come From? : Closely related to the question of why the universe exists is why there is something rather than nothing. And what we can now say is that there is something because—essentially as a matter of definition—all possible formal rules inevitably in some abstract sense exist. The science says that if one applies these rules then eventually they build up the rulial universe. But why should they be applied? Why aren’t they just static, abstract constructs? In a sense it’s confusing to talk about the rules “being applied”. The rules just abstractly define what gives what—or how to “build out” the consequences of the rules. It’s a bit like with an axiom system in mathematics. Once one has the axioms there’s immediately a collection of abstract consequences, namely all the possible theorems that can be proved from the axioms. But in a sense what we’re talking about in terms of arbitrary rules is lower level than the usual treatment of axiom systems in mathematics. The rules define their own “way of being applied”. And we are not just looking at “final consequences” of rules, but at the structure generated by the whole “step-by-step” process of every single possible rule application, with all the complex entanglements between them. At some level, we can just think of rules abstractly being applied. But in our models of physics, there’s an interpretation of that: sequences of rules being applied define the passage of time. There are “logically inevitable” chains of rule applications. But for us as observers embedded in the system—particularly with our way of setting up our thread of consciousness—we perceive this as time passing, or in effect, “things happening in the universe”. Talking about things this way might lead one to ask “What got the universe started?” “What makes rules actually get applied?” Well, nothing. Because the rules are just defining how abstract sequences can be constructed. And if you follow a sequence, it can be interpreted as reflecting the passage of time. But there’s no “driver” that’s saying anything like “now this rule gets applied”. The sequences generated by the successive application of rules are somehow just abstract “logical possibilities”. But now remember that we are observers embedded within this whole setup, with everything about our operation also defined by these same abstract rules. So, yes, we can interpret rule applications as being associated with the passage of time. But it’s not like time has to pass at a certain rate. Time for us as observers, and for the universe that we observe, in a sense “passes when it passes”. There are rule applications that can occur in the universe as a whole, and there are rule applications that can occur in us. And it’s just the abstract interplay of those that makes us perceive the universe the way we do. Nothing had to “get the rule applications started”. They are abstractly defined, and abstract sequences of them correspond to the evolution of the universe and us within it. Like the rules themselves, they are inevitable abstract constructions. And these abstract constructions when played out completely enough yield what we can interpret as the whole evolution of the universe through time. But nothing needed to get it started. The whole structure just in some sense “abstractly exists”. So does this mean that the whole history of the universe can be thought of as an “abstract thing” that is somehow “immediately defined”? Well, no, not in any useful sense. Because the phenomenon of computational irreducibility implies that longer chains of rule applications in a sense “do irreducibly more” or “get irreducibly further”. They define a structure that irreducibly needs those rule applications. There’s no general way to “jump ahead” and get the outcome without going through all the rule applications. Computational irreducibility is what in a sense builds up structure in the universe. All possible rules are being applied. But there’s an inevitable and irreducible pattern to it, which we interpret as being built up by the passage of time. And while everything about it is in a sense “abstractly defined” from the outset, there’s something “fundamentally heavier” about these “consequences” as they are played out. By the way, in addition to asking how the rules “got started” one might try to ask “what did they start with”, or in other words, what was the “initial data structure” of the universe? But once again, we don’t need to ask this. Because among “all possible rules” will be ones that effectively create any possible starting condition. But then, one might ask, why doesn’t this mean that absolutely anything could just be inserted into the universe from the beginning? It’s all rather subtle, and not as formally worked out as I hope it will be. But the interplay of computational irreducibility, computation universality and the embedding of us as observers using the same rules as the universe does seem to imply that the ultimate perceived structure of the rulial universe can be thought of as being built from “basic rules” that have limited complexity, even though in a sense all possible rules can contribute.
Why Does the Universe Exist? Some Perspectives from Our Physics Project 16.7 Is This the Only Universe? : If the universe were based on a particular underlying rule there would seem to be no reason why there shouldn’t be other universes based on other rules. But an initially quite surprising implication of our approach is that actually the universe is in effect based on all formally possible rules. And a consequence of this is that there can only be one universe—which, as we’ve argued, in some sense inevitably exists. But this whole “rulial universe” based on all possible rules is something very big—and we experience only a small part of it. In physical space we’re used to the fact that we live on a particular planet at the edge of a particular galaxy. But what we now realize is that we are also sampling only a tiny part of the rulial space of all possible descriptions of the universe. Were our sensory apparatus or our intellectual development different, we might describe the universe in very different ways. As I’ve argued elsewhere, however, as soon as we imagine that we’re operating with something like consciousness, there are constraints, in particular that we must be computationally bounded observers who describe things in terms of a single thread of experience through time. And it’s then a crucial fact that these constraints alone make it inevitable that we’ll attribute to the universe a physics with familiar core features like general relativity and quantum mechanics. Even though in the full rulial universe there are lots of other possibilities, the features that we sample, with our consciousness, follow our familiar laws of physics. Or in other words, we have a particular perception of the universe, that operates according to particular laws. But the crucial point is that those particular laws are not a fundamental feature of the universe; they are merely a feature of our description and our sampling of the universe. And just as we can imagine using spacecraft or telescopes to study distant regions in physical space, we can imagine using different forms of description and analysis to study different parts of rulial space. I won’t discuss this in detail here, but one can for example imagine motion in rulial space: progressively changing with time one’s description of the universe. And just like the speed of light limits motion in physical space, there’s a constant we call ρ (but whose value we don’t yet know) that limits motion in rulial space. When we first asked the question “Why does the universe exist?” it seemed difficult to understand how one could possibly make an abstract argument that would conclude anything about the existence of the particular universe in which we seem to find ourselves. But the key point is that the full rulial universe involves no particular choices; it is something formally inevitable. But what we perceive as our universe is just a part of that full rulial universe, and a part determined by the particulars of how we—with our consciousness—choose to describe the world. An “alien intelligence” might not use the same description. Inevitably it must exist in the same rulial universe. But it might be quite distant in rulial space. And so its perception of the universe and its laws might be quite different from ours. It might, for example, have none of our familiar notions of space or time, but instead describe the universe in terms of features of the configurations of atoms of space and their causal connections which we are not set up to access or reason about. Could we in principle translate from their description to ours? In a sense this involves motion in rulial space—so there are limitations imposed by ρ. But as a practical matter it is something immensely challenging, as our inability to translate even from descriptions presumably close in rulial space, like animals or AIs, suggests. If we restrict ourselves to descriptions associated with something like consciousness then we’ve argued that we inevitably attribute to our universe our familiar laws of physics. But without that restriction, is there anything we can say about our universe, and what laws we’ll attribute to it? In other words, even beyond the particular slice that we sample, are there general laws of the rulial universe? The answer is that there is definitely one such law, namely that the universe is fundamentally computational. In other words, whatever description language we use, we can always ultimately represent what the universe is doing in terms of the operation of a standard universal computer, like a universal Turing machine. We’ve argued that the rulial universe is in a sense just a representation of the (entangled and computationally irreducible) inevitable consequences of following all possible formal rules. But when we talk about all possible formal rules we’re implicitly making the assumption that these rules can be stated in some kind of explicit symbolic form. And this is why we conclude that everything in the rulial universe is computational—in the sense that it can be represented by a standard universal computer—rather than “hypercomputational” (e.g. requiring an oracle that can immediately answer questions that would require infinite computation in a standard universal computer, or involving “hyperrules” that directly represent such operations). So could there actually be a “hyperrulial universe” that also contains formal rules that involve hypercomputation? The answer is that, yes, there certainly could be. But in some sense it doesn’t matter. Because if the way we sample the rulial universe involves at most ordinary universal computation, we will never probe these hypercomputational elements. Or, put another way, any hypercomputational part of the rulial universe will always be shielded from us by a rulial event horizon. So, yes, a “hyperuniverse” could exist. But it’d never be relevant to us, or to our goal of understanding why the universe—as we perceive it—exists. To recap then: we start from the idea that all possible formal rules “exist” as abstract constructs that follow directly from abstract definitions. Then we argue that the rulial universe is an inevitable structure derived from these rules. We as observers choose a certain way (consistent for example with our notion of consciousness) to describe this structure. And that form of description is what gives us our perceived laws of physics. The existence and structure of the rulial universe is in a sense a necessary abstract fact (like 1 + 1 = 2). Our particular sampling of it is a “point of view” we choose. But whatever that “point of view” is—or in other words wherever in rulial space we are—we will still conclude that there is a universe, or that the universe exists. The laws we perceive will depend on our “point of view”—though we know that just adding the constraint of having a “consciousness-based point of view” will already give us core familiar laws of physics. When it comes to a hyperuniverse, we can say that, yes, if our point of view lay in hyperrulial space then we might want to address why the hyperuniverse exists. But as it is, our point of view is just somewhere in ordinary rulial space, where we know that there’s a “rulial universe that exists underneath”.
Why Does the Universe Exist? Some Perspectives from Our Physics Project 16.8 Why This Universe? : Why did we get this universe, with its detailed features, and not another? In the past we might have assumed that this involved arbitrariness in the creation or setup of the universe itself. And indeed endless mythologies describe mechanisms by which choices about the universe might have been made. In recent times it’s also been common in some circles to talk about the possibility that the universe might be a “simulation”, operating like a videogame that’s being run on some kind of “outer computer”. But what I’ve argued here is that in some sense there’s no choice about the fundamental structure of the universe. At the level of the full rulial universe everything is inevitable and necessary. There is no choice being made about the rule—or program—for the universe.  All rules are there, and it’s a matter of pure abstract “logical” consequence that the full rulial universe has the structure it does. What is “contingent”, however, is how we sample this full rulial universe. We have a certain point of view on the universe, and that is what gives us the particulars we perceive. We could have a different point of view. And maybe at some point in the development of our species and our civilization we will have. And at that point we might describe the universe differently than we do now, invoking different laws of physics to explain what we see in the context of the description we’re then using. But the crucial point is that there is no choice at the level of the fundamental structure of the universe. The choice is about our way of describing the universe, and in effect where we happen to be in rulial space. If we think about the universe as a simulation, what’s being said is that there’s no choice of “which videogame is being run”. In a sense it’s all possible videogames—with all possible underlying rules. And the point is that the resulting rulial universe is just an inevitable formal consequence of the effects of those underlying rules. As a matter of whimsy one could choose to talk about the rules as “running on an outer computer”. But it would be like talking about 1 + 1 = 2 in terms of counting physical stones. None of that “instantiation story” makes any difference to the structure of what comes out. It’s perhaps useful to talk more practically about actual “existence inside a videogame”. Imagine at some point in the future a digital representation of our brain functions can be uploaded to a computer. And imagine the computer is running some kind of “universe operating system” that implements the videogame—with our “digital minds” inside it. The question is then what perception our digital minds will have about the environment they’re in, and for example what its effective laws of physics are. We might at first assume that we’ll just perceive whatever laws of physics were “programmed into the videogame”. But it’s more complicated than that. Because if we’re “part of the videogame” our minds must operate according to the same programming as everything else—which is much closer to the setup for our models of fundamental physics. And—assuming that the “universe operating system” implements universal computation (which presumably it must in order to include our digital minds)—we’re basically thrust back into essentially the same formal situation we’ve been talking about. Of course, if there are resource constraints the “outside world” will poke into our experience. But without those, we’re again just dealing with a formal, computational system. What about the idea of “running all possible rules”? Well, if we have a universal computational system we can in principle do that. And build up the same formal structure of the rulial universe. But then the issue is: where in this rulial universe will our digital minds “naturally find themselves”? And the point is that there is no reason to think it will have any correspondence to the “rulial location” for our non-uploaded minds in the ordinary universe. In other words, the description we give of the ordinary universe may be completely different from the description our uploaded minds will give in the “simulation universe”. In both cases (ignoring “resource constraints”) there’s the same full rulial universe. But we’re potentially sampling different parts of it—with views of the universe as incoherent as those that we and putative aliens might have.
Why Does the Universe Exist? Some Perspectives from Our Physics Project 16.9 Prime Movers and the Path from Abstraction to Abstraction : In a sense the question of why the universe exists is all about why anything is “actualized”. It’s one thing to say that the universe can be represented by formal rules; it’s another to say that those rules are “actualized”. And many past analyses have concluded that there has to be some kind of “prime mover” that “breathes reality” into the formal rules. But what I’ve argued here is that nothing like that is needed. Instead, the whole elaborate structure of the rulial universe inevitably arises as a necessary consequence of the very definition of all possible formal rules. And the point is that we—as observers embedded in this rulial universe—have a certain perception of what is happening that in a sense “constitutes our reality”. But just where in all of this did the merely formal turn into something “real” or “actual”? Well, at some level, it ultimately didn’t. Because in the end what we’re talking about is our perception—or in effect our abstract description of what’s happening in the universe. So in some sense, everything we’re doing is just going from abstraction to abstraction. So where’s the “actualization” or the “reality”? Well, the whole point is that our science implies that there’s a certain robustness or invariance to the final abstract description that we form of the universe.  If we started from all formally possible rules and in a sense everyone’s perception of what was going on in the universe did something like “picked a different rule” and gave a completely different abstract description of the universe, then it wouldn’t be useful to think of anything as being “actualized”. But instead we know that—at least so long as we follow the constraints associated with consciousness—there’ll be great alignment between the different abstract descriptions we end up with, and for example they’ll all agree about the basic laws of physics. I suppose we could say that “reality is an illusion” in the sense that in the end we’re going from pure abstraction to pure abstraction. But the whole point is that as a matter of science there’s necessarily agreement about what “comes in the middle”.  And that’s why we can usefully talk about “reality” as an objective kind of thing. And the formal inevitability of this is what we can say is “why the universe exists”.
Why Does the Universe Exist? Some Perspectives from Our Physics Project 16.10 The Relation to Mathematics : Most people would view it as self-evident that the physical universe exists. But the question of whether mathematics “intrinsically exists as a definite thing” has been debated since at least Plato. One view of mathematics is that it involves just writing down whatever axiom system one wants, and then working out its consequences. And at least at first this seems very different from physics, where one might imagine that one starts from our particular world as it is, and then tries to find a formal representation (or “model”) of it. And, yes, our effort to find a complete and precise fundamental theory of physics can be thought of as trying to “reduce physics to mathematics”—in the sense that it’s trying to give us a particular formal (“axiomatic”) system that reproduces what the physical universe does. At first, we might imagine that this formal system for physics must be something very special, and not something like the “arbitrary axiom systems” that we could choose to write down as foundations for mathematics. But from what we’ve seen here, the situation is more subtle than this. Because “underneath physics” there are in a sense all possible formal systems. At the outset we might have imagined that mathematics is somehow much more general than physics, because it can operate with whatever formal system we write down. But now it’s seeming like the opposite: physics is based on all possible formal systems, but mathematics is based on particular formal systems (geometry, algebra, etc.) that we happen to have written down in the history of human mathematics. We’ve argued that the existence of the universe is ultimately a consequence of the fact that all possible formal systems exist as a matter of abstract necessity. So can we use a similar argument for the “existence of mathematics”? Most likely we can—at least if we subtly change our description of what mathematics is. In the “axiomatic tradition” it’s been common to imagine that mathematics could in principle be based on whatever formal axioms we want, although in practice we pick particular ones. But an alternative view is that ultimately mathematics, like physics, is actually based on all possible formal (axiom) systems. In physics, we describe the construct created from all possible formal systems as the rulial universe. So what is the analog in mathematics? Well, it’s the same thing! In the actual pure mathematics of the past century it’s become increasingly common to study spaces of all possible things of certain types. But the analog of the rulial universe is a kind of ultimate limiting example, probably best captured through higher category theory as the infinity groupoid. And, yes, the remarkable fact is that constructs like this—formed “from all possibilities”—actually end up having definite, rich structure. But, OK, so in the interpretation that mathematics is what’s formed from all possible formal systems there’s a definite thing underlying it—and it’s the very same rulial structure that underlies physics. At the lowest level the structure is full of computational irreducibility. But in the case of physics we know that we as computationally bounded “single-thread-of-experience” observers sample particular slices of the structure that show computational reducibility—and give us known physical laws. So how do we sample the rulial structure when we do mathematics?  I’m not quite sure what the metamathematical analog of the constraints of consciousness in physics are—though the concept of combining things into reference frames may be related to the “equivalence is equivalent to equality” concept captured by the univalence axiom in homotopy type theory. Each potential rulial reference frame can be thought of as using a different description language for mathematics (say, in a simple case, algebraic vs. geometrical, etc.). But then it’s a fundamental feature of the rulial structure (essentially a consequence of causal invariance) that different reference frames will give equivalent results. In physics this leads to general relativity and quantum mechanics—and a meaningful notion of “objective reality” for spacetime and for quantum phenomena. And—assuming we can construct something like reference frames—there should be similar “global laws of metamathematics”, that in effect describe the “objective reality of mathematics” independent of the particular mode of description we use. I don’t yet know quite how to formulate these global laws, though perhaps approaches like category theory give suggestions. In explaining why the physical universe exists we get part of the way there by talking about the abstract necessity of the rulial universe. And with the idea that the same rulial structure exists “underneath mathematics” we again get part of the way to explaining that mathematics “exists as a definite thing”. But to get further I think we have to consider “perception by mathematical observers” just as we consider “perception by physical observers”. And what’s crucial about physical observers like us is that they manage to sample computationally reducible slices of the rulial universe—which they perceive as containing “definite things” consistent for example between different observers. One of the mysteries of mathematics is that it’s possible to make progress without continually getting mired in computational irreducibility and phenomena like undecidability.  But in some sense this is just the mathematical analog of the fact that we as observers in the physical universe can identify slices of computational reducibility—and not be mired in computational irreducibility and utter unpredictability. I must say that in the past I wasn’t at all sure that mathematics could be thought of as “fundamentally existing as a definite thing”. I thought of it more as a human-created artifact, built on specific axiomatic foundations that were in some sense historical accidents. But now that we understand more about why the physical universe exists, it’s looking to me more likely that we should think of mathematics also as a “thing that exists”. And that all our historically created towers of mathematical results are just views—based on particular reference frames—into the ultimate structure that “is mathematics”.
Why Does the Universe Exist? Some Perspectives from Our Physics Project 16.11 How Should We Feel about All This? Can the universe really at some level just be a formal thing? : As conceptually elegant—or even beautiful—as this conclusion seems, as I sit here typing these words, some part of me still resists it. After all, the keyboard under my fingers feels like a real thing. The screen in front of me I can reach out and touch. But of course the history of science keeps on showing us how wrong our feelings and intuition can be. Four centuries ago, for example, we learned that—despite the immediate evidence of our senses—the Earth goes around the Sun. And now what I’m arguing is that again—despite the immediate evidence of our senses— everything we experience is actually just a reflection of an abstract formal structure. And that there is no “special spark of reality” associated with anything. Our increasingly virtualized modern world might make it seem more plausible that this could be correct. But there is still something shocking about the notion that in a sense everything is fundamentally virtual, or formal. And that there is no ultimate underlying “real substrate”; everything is “formal all the way down”. At first, that might seem disappointing. We might like to think that the things that happen—or that we do—in the universe are somehow “real”, not just virtual or formal. After all, we might think, if everything is virtual, nothing can ever fundamentally be “achieved” by, for example, the passage of time. But there’s an important sense in which this is wrong. Because even though an outcome may be “virtual”, there may still be something definite and irreducible that’s needed to produce it. And in particular the phenomenon of computational irreducibility—together with the Principle of Computational Equivalence—implies that there are many processes defined by formal rules where the only way to find their outcome is effectively to run the rules an irreducible number of times. If one could always “jump ahead” and immediately determine the outcome one could reasonably say that “nothing is achieved” by “living through” the whole evolution of the system. But computational irreducibility implies that the “journey is necessary”—and in effect that it achieves something that couldn’t have been achieved without it. Ever since Copernicus science has been showing us ways in which we are not special.  The Earth is not at the center of everything. Life is not made from different stuff than everything else. And indeed my own previous work has argued that the Principle of Computational Equivalence implies that there’s nothing fundamentally special about intelligence, beyond “mere computation”. And what I’m arguing here is that there’s nothing special even about our whole universe; it’s all just determined by abstract necessity. At some level we might see this as the ultimate “Copernican put-down”, and the ultimate statement of how “unspecial” and unimportant we are. But there’s a different view one can take—that in a sense instead puts us firmly in the center of everything. Yes, the whole rulial universe is an abstract necessity. But the particular description language—or “rulial position”—that we occupy is something completely special to us. In the space of all possible rulial positions, the one we occupy can be viewed as arbitrary and “unspecial”. But for everything about the universe as we perceive it, it is crucial and central—and in a sense it is what creates for us the universe as we know it. We might have thought that the ultimate story of science would be about what the universe “gives to us”. But what the arguments here imply instead is that it’s actually about what we “take from the universe”—and how we are set up to sample the whole rulial universe. It’s notable that our way of describing—or sampling—the rulial universe is not just determined by the “pure physicality” of where we are in physical space, or how we’re configured as biological organisms. It also depends on how we “mentally model” the world, and how our formal thinking describes what we perceive is going on. We might have hoped that intelligence would be something that science would show is fundamentally special about us. But the Principle of Computational Equivalence implies that “abstract intelligence” is actually a common and general phenomenon. So does this mean that there is nothing special about us? Well, no. It just highlights the importance of all those particular details that make human intelligence and the human condition what it is. In other words, from exploring “general science” we come to realize that it’s our human details that are actually what’s important. And so it is, I think, when it comes to looking at our whole universe. What is special and significant isn’t some general aspect of what underlies the structure of the universe. Instead, it’s the details of how we—as humans—describe the universe.  (Though, as we’ve discussed, the general “consciousness features” of that description robustly give us laws of physics such as relativity and quantum mechanics.) So, in the end, as we reach what we might see as the ultimate foundations of science, we are in some sense back to us humans being at the center of everything. We might have thought that somehow science would long ago have “abstracted us humans” away. But instead, the only way to make anything meaningful is in a sense to put us right in the middle of everything, with features of our perception defining how we describe the universe, and what we consider the reality of the universe. As I look around at my physical environment it’s still a little shocking to think that it’s all in a sense just “created by me” by “knitting together” some kind of generic “formal abstraction”. Yes, the computational boundedness and sequentialized thread of experience that seem to form the basis for what we view as consciousness force certain properties of the physical environment that correspond to laws of physics we know. But some putative alien—or even future human or human-like entity— could still perceive this physical environment quite differently, even though it’s always ultimately based on the same formal structure. But while it might seem like a different universe, it’ll still seem like some universe (albeit perhaps one incoherently different from ours).  And however we perceive it, the conclusion will be the same: from pure abstract necessity it will follow that the universe exists, and that there is something rather than nothing. I’ve tried here to sketch my recent thinking about the question defined in the title. I’ve suspected for some time that the discoveries of our Physics Project (and its precursors in my work from the 1990s) might perhaps have something to say about the question. But in the recent thinking I describe here I’ve been able to make what at least seems like much more headway than I’d expected. I’m well aware that the question I address has been considered by philosophers and others for centuries. And it would certainly be very interesting to know how what I say here—with its foundations in our recent science—might relate to what has been said before. But I expect this will be a challenging project—that will inevitably need to build intellectual bridges between very different kinds of thinking, and I haven’t personally tried to do it. It’s worth emphasizing that what I say in this piece is just a sketch. And among other things, I’ve glossed over many technical details, especially about the structure of rulial space. And while both Jonathan Gorard and I have made studies of at least some aspects of rulial space, there’s considerably more to figure out.
The Problem of Distributed Consensus 17.1 : In preparation for a conference entitled “Distributed Consensus with Cellular Automata & Related Systems” that we’re organizing with NKN (= “New Kind of Network”) I decided to explore the problem of distributed consensus using methods from A New Kind of Science (yes, NKN “rhymes” with NKS) as well as from the Wolfram Physics Project. A Simple Example Consider a collection of “nodes”, each one of two possible colors. We want to determine the majority or “consensus” color of the nodes, i.e. which color is the more common among the nodes. One obvious method to find this “majority” color is just sequentially to visit each node, and tally up all the colors. But it’s potentially much more efficient if we can use a distributed algorithm, where we’re running computations in parallel across the various nodes. One possible algorithm works as follows. First connect each node to some number of neighbors. For now, we’ll just pick the neighbors according to the spatial layout of the nodes: The algorithm works in a sequence of steps, at each step updating the color of each node to be whatever the “majority color” of its neighbors is. In the case shown, this procedure converges after a few steps to make all nodes have the “majority color” (which here is yellow)—or in effect “agree” on what the majority color is: This is a simple example of a distributed consensus algorithm in action. The challenge we’ll discuss here is to find the most efficient and robust such algorithms.
The Problem of Distributed Consensus 17.2 The Background : In any decentralized system with computers, people, databases, measuring devices or anything else one can end up with different values or results at different “nodes”. But for all sorts of reasons one often wants to agree on a single “consensus” value, that one can for example use to “make a decision and go on to the next step”. Blockchains are one example of systems that need this kind of consensus to “finish each block”. Traditional blockchains achieve consensus through what amounts to a centralized mechanism (even though there are multiple “decentralized” copies of the blockchain that is produced). But there are now starting to be distributed analogs of blockchains that need distributed consensus algorithms. And the main inspiration for the algorithms being developed are cellular automata (and to a lesser extent spin systems in statistical mechanics). One issue is to make the algorithm as efficient as possible. Another is to make it as robust as possible, for example with respect to random noise—or malicious errors—introduced at or between nodes. The amount of random noise can be thought of as something like a temperature. And at least in certain cases there can be a “phase transition” so that below a certain “temperature” there can be zero effect on the consensus output—implying robustness to a certain level of noise. Some of what happens can be studied using methods from standard equilibrium statistical physics. But in most cases one has to take account of the time dependence or evolution of the system, leading to something like a probabilistic cellular automaton (closely related to directed percolation, dynamic spin systems, etc.). As I’ll discuss below, in the early days of computing, there was great interest in synthesizing reliable systems out of unreliable components. And by the 1960s there was study first of neural nets and then of cellular automata with probabilistic elements. And some surprising results were obtained that showed that cellular automata could be set up that would be robust with respect to a certain nonzero level of noise. One feature of cellular automata is that their elements are all assumed to be arranged in a definite array, and to be updated in parallel “at the same time” in a sequence of steps. For many practical applications, however, one instead wants elements that are connected in some kind of graph (that may even be dynamic), and that are in general updated asynchronously, in no particular order. The simple example we gave above is a graph cellular automaton: the connections between elements are defined by a graph, but the updates are all done synchronously at each step. In the past, it’s been difficult to analyze the more general setup where there is no rigid notion of either space or time. But this is exactly the setup in our new Physics Project, and so there’s now the potential to use its formalism and results (as well as intuition imported from physics) to make further progress.
The Problem of Distributed Consensus 17.3 Deterministic Cellular Automata : To start getting some intuition for the problem of distributed consensus, let’s consider the following very simple setup. We have a line of cells, each with one of two possible colors. Then we update the colors of these cells in a sequence of steps, based on a local rule which depends on neighboring cells. This system is a one-dimensional cellular automaton—of the kind that I started studying more than 40 years ago. We imagine that the initial condition involves a fraction p of red cells. We want all the cells to turn red if p > , and all of them to turn yellow if p < . The most obvious rule that might achieve this would just replace each cell by the majority color in its neighborhood (rule 232 in my numbering scheme): Here’s what rule 232 does starting with 70% red cells in a “random configuration”: As we can see, it manages to achieve a little “local consensus”, but ultimately it’s not successful at reaching a “global consensus” in which all cells are the same color. And we might imagine that there’d be no rule for a 1D deterministic cellular automaton that would lead to global consensus (or be able to solve the “density classification problem” of deciding whether the density of initial red cells is above or below 50%). But it turns out that this isn’t true. And for example in 1978 the following “radius 3” rule (operating on size-7 neighborhoods) was constructed (and we’ll call it the “GKL rule”): Here’s what this rule does with 60% red cells: And here’s what it does with 40% red cells: In both these cases, the rule successfully achieves “global consensus”. And in fact one can prove that this rule will always do this, at least after sufficiently many steps. Here’s a plot of how the density evolves as a function of time for different initial densities: And what we see is that there’s what looks like a phase transition: for initial density p < 0.5, the final density is exactly 0, while for initial density p > 0.5, it’s instead exactly 1. What happens precisely at p = 0.5? In a sense the cellular automaton “can’t make up its mind” and on an infinite line it generates an infinite nested sequence of domains that alternate between 0 and 1: This nested structure is typical of what’s seen in critical phenomena in statistical physics, and in fact cellular automata like this are the very simplest examples of “true” phase transitions that I know. (Like other phase transitions, these don’t become “sharp” except in infinite systems. In typical statistical mechanics one doesn’t get phase transitions in 1D, but that’s a consequence of the assumption of microscopic reversibility, which doesn’t apply to cellular automata like this.) So what other cellular automaton rules achieve consensus like this? There are no radius-1 rules that work. And if one searches all 232 radius-2 rules (as I did for A New Kind of Science), the best one finds are a handful of examples that achieve “approximate consensus” in the sense that most, though not all, of the cells go to the “majority value” (this is the r = 2 rule 4196304428, for p = 0.6): By the way, among radius-1 rules, there is rule 184 (often used as a basic model of road traffic flow), which doesn’t achieve consensus on “overall density”, but does do so with respect to left- and right-moving stripes, here with the nested pattern generated when p = 0.5: What about “achieving consensus faster”? Here’s a comparison of our original GKL rule with another radius-3 rule (discovered by genetic programming methods) whose average consensus time is shorter: It’s not known in general what the “fastest” radius-3 rule is. The two rules above have the feature that they “do their job” in a fairly “simple-looking” way. But there are also rules like the following that do their job in a “more ornate” way: Human-engineered rules (like the first one above) almost inevitably work in simpler and more “understandable” ways. But experience elsewhere (such as with optimal sorting networks) suggests that optimal rules will often be ones that don’t look simple in their behavior, and that can’t realistically be constructed by standard engineering methods, and essentially just have to be found “experimentally” by searching the computational universe of possible rules. A notable feature of particularly the earlier rules we looked at is that they show a small number of types of very distinct “domains” with definite walls or boundaries between them. And in many ways such walls can be thought of as being like localized structures, “defects” or “particles”. But for our purposes here what tends to be important is whether these particles move around, and whether they annihilate each other to leave a uniform “consensus” final state. In the simple majority rule it’s inevitable that there are static domain walls: The reason is that as soon as a domain is larger than the cellular automaton neighborhood, a cell at the boundary of the domain will inevitably see a balanced number of cells of each color on the two sides of the boundary. So the cell itself will act as a “tie breaker”, and will always decide to stay its own color—thereby making the domain boundary stay as it is. So what if we have a longer-range rule, that samples more distant cells? With range-2 (i.e. a 5-cell neighborhood) “wrong domains” with widths below 4 disappear: Things work a bit better if the cells being sampled aren’t adjacent, but are for example in the pattern : But no finite-size sampling with the pure majority rule will remove all domains. What about the GKL rule? This rule actually only samples 5 cells, but its “extremities” are at distance 3. So can we “improve” it by having it sample more distant cells? Here’s a comparison of a few cases (the first one is the original): Here we’ve only discussed cellular automata with two possible colors for each cell. We could also consider rules that involve other “helper” colors that either disappear before the final state is reached, or define additional consensus states.
The Problem of Distributed Consensus 17.4 But Does It Always Work? : We’ve seen that there are 1D cellular automata that—at least in the examples we’ve looked at—achieve “majority consensus”. But given a particular rule, will it always reach consensus, or are there exceptions? As a first way to get a well-defined version of that question, we can consider finite cellular automata, say with a total of n cells, and cyclic boundary conditions. There are a total of 2n possible configurations in this case, and we can represent all possible paths of evolution of the cellular automaton using a state transition graph. Here’s the graph for the GKL rule we discussed above, for the case n = 5. Each node in the graph is colored according to whether its “red-cell fraction” is above or below . And what we see in this case is “perfect density classification” or “perfect consensus”, with all states correctly leading to all-red or all-yellow states: But as soon as we look, for example, at n = 7, we immediately see a problem: The states and their cyclic variations “get stuck” and do not achieve consensus. At size 11 there’s another issue: now a few states that should have achieved “consensus 1” actually go to “consensus 0”: The states that “get to the wrong consensus” here all turn out to be cyclic variations of the following where in the first case there are 6 cells and 5 , yet the final state is all , and in the second case it’s the other way around. And it turns out that there’s actually a general problem: one can prove that there’s no rule that can perfectly achieve “majority consensus” on a finite array with cyclic boundary conditions. What about on an infinite array? Here it’s possible to achieve “perfect majority consensus” for all but a set of “special initial conditions” with measure 0. An example of such a “special initial condition” is an infinite repetition of either of the two blocks shown above. These initial conditions—instead of going to consensus—will just remain fixed with time. If initial conditions are generated “at random”, with the value of each cell being chosen according to certain fixed probabilities, then there’s effectively zero probability of getting one of the “exception” initial conditions. And even though the “tapers” might be arbitrarily long, there’s no chance of not eventually reaching a consensus state. But this conclusion depends on the idea that initial conditions are really generated “at random”. If, for example, they were generated by a definite program, then though the initial conditions might seem “statistically random” with respect to certain tests, it doesn’t mean that they won’t give special weight to the “exceptional” initial conditions.
The Problem of Distributed Consensus 17.5 Beyond One Dimension : In one dimension one can explain the fact that certain configurations “get stuck” and don’t achieve consensus by saying that in 1D effects can’t “get around each other”. But in 2D there is no such constraint. So then what about the “pure 2D majority rule” (totalistic code 56): Starting say from 30% 1s we again see that things get stuck: Here is the corresponding evolution shown in 3D, with time going down: But here is another rule (9-neighbor totalistic code 976): And now what we see is that in this case blob-like domains of the “minority color” get left over, but gradually get smaller. We can see the phenomenon in 3D: Looking at a spacetime slice in the center, and letting more distant cells “recede into the fog”, we see what looks like “diffusive” behavior, with domain walls in effect executing random walks that eventually annihilate: The rule we just saw is close to the majority rule on a 9-cell 3×3 region, except for totals 4 and 5, which are taken to give 1 and 0 rather than 0 and 1. If we use the pure majority rule on the 3×3 region it gets stuck: But it turns out to be straightforward to find 2D majority rules that do not get stuck. In fact, basically any majority rule that samples cells in an asymmetric way will work. As an example, consider a rule that samples the following cells in each 3×3 neighborhood: Here is the 3D evolution of this rule starting from 45% 1s: And here is what a spacetime slice looks like: The behavior as a function of the initial density shows a clear transition at 50%: Here are results for different samplings of cells in the 3×3 neighborhood; all successfully achieve consensus: With our original 5-cell “symmetrical” neighborhood we can get very similar behavior by setting things up like the 1D GKL rule.
The Problem of Distributed Consensus 17.6 Cellular Automata with Noise : So far we’ve assumed that once it’s started, the evolution of the cellular automaton is entirely deterministic. But what if there’s some “noise” in the evolution—say if the values of cells are randomly flipped with some probability? Here’s what happens with the simple majority rule in 1D in this case: What about the GKL rule? At low levels of noise the rule will typically “fight it off” and still achieve consensus: But eventually the level of noise becomes too great, and consensus is typically lost: In general, the presence of “noise” turns our system from an ordinary cellular automaton into a probabilistic cellular automaton. (And this, in turn, is equivalent to what’s sometimes called directed percolation, or to a spin system that’s taken to evolve in time with random updates according to rules with certain weightings. It’s also related to what’s sometimes been called an “interacting particle system”—in which for example boundaries of regions follow something like an array of random walks, that annihilate when they meet. ) Let’s talk in a bit more detail about the overall behavior of the GKL rule. When there’s no noise, it shows a sharp transition from final state 0 to final state 1 when the initial density goes from below 0.5 to above 0.5. But what happens when we add noise? We can summarize the result by a classic physics-style phase diagram: This diagram shows the final density produced by the rule as a function of the initial density and the noise level. At zero noise level, there’s a fairly sharp transition as a function of initial density. (It’s not perfectly sharp because this diagram was generated by sampling a finite number of initial conditions in a finite region.) And as the noise level increases, the sharp transition seems to survive for a while—until eventually a critical noise level is reached at which it disappears. Is there a rigorous way to analyze what’s going on? Well, not yet. And in fact for a long time it was thought that in the presence of noise any 1D system like this would necessarily be ergodic, in the sense that it would eventually visit all possible states, and certainly not evolve from different initial densities to different final states. But in the 1980s a complicated cellular automaton was constructed that it was possible to prove would not show such behavior. The system was put together for the purpose of “doing reliable computation even in the presence of noise” and was set up using rather elaborate software-engineering-like methodology. But ultimately it was just a 1D cellular automaton, albeit with an astronomically complicated rule. And the crucial point was that up to some nonzero level of noise, the system could reliably perform a computation—such as achieving majority consensus. But does one really need a system with such complicated underlying rules to do this? Undoubtedly not. And the situation reminds me of what happened with the problem of ordinary computation universality in cellular automata. Back in the 1950s it seemed one could achieve this with a very complicated setup, constructed in an engineering-like way. But now we know that actually even one of the very simplest conceivable 1D cellular automata—rule 110—is already computation universal. And in fact the Principle of Computational Equivalence implies that whenever we see behavior that is not obviously simple, we can expect computation universality. Of course, it doesn’t seem like we should need to have computation universality to get distributed consensus—though the Principle of Computational Equivalence suggests that computation universality is “cheap” so might in effect “come along for free” with rules that have other necessary properties. (And by the way, this isn’t a trivial issue, because when systems are capable of universal computation there’s the potential for them to “do something one couldn’t predict”, including, for example, break out of some computer security constraint one thinks one’s defined.) But knowing that there’s a very complicated cellular automaton that achieves distributed consensus even in the presence of noise makes one wonder what the simplest cellular automaton which can do this might be. And based on my previous experience, I would expect it’ll be very simple—like the GKL rule—though it may be very difficult to prove this. It might be useful to make a few remarks about the whole issue of “noise”. In a sense when one says there’s “noise” in a system one’s saying that the system is “open”, and there’s something coming from “outside” it that one can’t predict. But as an “approximation” one can imagine just having some pseudorandom generator of noise—like the rule 30 cellular automaton. And then one once again has a “closed” system, to which one can immediately apply thinking based for example on the Principle of Computational Equivalence. But what about “truly unpredictable noise”? To say that this is present is to say that there are different paths of history that the system could follow, and one doesn’t know which one will be followed in any particular case. Informed by our Physics Project, we can represent these possibilities by defining a multiway graph, in which there’s a branch whenever two different states are generated, depending on the noise. But in addition to branches, there can also be merges in the multiway graph. And in the rather trivial case of a cellular automaton with the identity rule, allowing every possible individual cell to be flipped by noise, we get the multiway graph: Here’s what happens if we apply the majority cellular automata rule (rule 232) after each “noise flip” (and go a total of just 2 steps): Going more steps, with thicker edges representing more updating events connecting the same states, we get: There are several subtle limits to be taken. The size of the cellular automaton is being taken to infinity. The number of steps is also being taken to infinity (though slower). And by saying that there is only a certain “density of noise” we’re effectively taking limits on the relative weightings of edges. To have a system which achieves consensus even in the presence of noise, only particular attractors must survive in these limits. But quite what kind of underlying rule is necessary for this we don’t know—though my guess is that it will ultimately be surprisingly simple. Will computation universality “come along for the ride”? I don’t know, but I wouldn’t be surprised if it did. Though it’s worth understanding that the definition of computation universality in a multiway system like this is somewhat subtle. (I recently discussed it in the context of multiway Turing machines, but there are still more issues when one’s interested in probabilities and “probabilistic weightings” of different paths..
The Problem of Distributed Consensus 17.7 “Purposeful Attacks” on a Cellular Automaton : We’ve just talked about the effects of “random noise” on consensus in a cellular automaton. But what about “noise” (or “errors”) that are “purposefully introduced”? Is there a pattern of some potentially small number of errors that will, for example, flip the consensus result? One version of this question—reminiscent of adversarial examples in neural networks—is just to ask what changes need to be made to an initial condition to “flip its result”. Or, put another way: let’s say one has a system (like the GKL rule) that basically achieves correct consensus for almost all randomly chosen initial conditions. Now we ask the question of whether there is a systematic way to tweak a given randomly chosen initial condition to make it “lead to the wrong answer”. (One can think of this as a bit like asking whether one can find a nonce that will make a cryptographic hash come out in a particular way.) Needless to say, there are many subtleties to this question. What do we mean by “random initial conditions”? Presumably a periodic state wouldn’t qualify. What kinds of “tweaks” can we make? Something that might conceivably happen is that there’s a certain behavior with “ordinary” initial conditions, but there’s some special “seed” that—if it occurs—will produce unbounded (“tumor-style”) growth that eventually takes over the system, as in this simple example from rule 122: If instead of just “attacking” the initial conditions one allows the possibility of, say, changing the value of a particular cell on every step, it’s easy to end up, for example, with “immovable lumps” that in effect prevent “full consensus”: But what if one considers making changes at a small number of places in the ongoing evolution—say in effect adding a little “intelligent noise” at locations carefully computed from the actual pattern of evolution? Will the system always be able to “heal itself” from such “Byzantine tampering”, in effect “correcting few-bit errors”? Or is there some particular “vulnerability” that can be exploited to “corrupt” the final results with just a few carefully chosen changes? One can think of the two consensus final states as being attractors, whose basins of attraction include all initial conditions above or below density . Alternatively, one can think of the cellular automaton as “solving the classification problem” of “recognizing the initial density”. And perhaps there is some way to extend the cellular automaton to a neural net with continuous weights, and then use machine learning methods to iteratively find minimal places where weights can be changed.
The Problem of Distributed Consensus 17.8 Graph Cellular Automata : In an ordinary cellular automaton, values are assigned to cells laid out in a definite grid. But as a generalization one can allow the cells to lie at the nodes of a graph—and then to take the neighbors on the graph to define the neighbors to be used in the rule: There is one immediate issue here. In the basic definition of a standard cellular automaton, the rule “takes its arguments” in a definite order. But if one’s dealing with an ordinary graph (as opposed to, for example, an ordered hypergraph), all one knows is what nodes are connected to a given node—with no immediate ordering defined. And this implies constraints on the type of cellular automaton rule we can use. One can think of setting up a “geodesic ball” around each node in the graph. Successive “shells” contain nodes that are successive graph distances away from a given node. But the cellular automaton rule can’t distinguish which “position” a given cell is at within a particular shell; all it can do is count the total number of cells in each shell that have a given value. If the graph is vertex transitive, so that the graph structure around every node in the graph is the same (as for a Cayley graph), then the cellular automaton rule can basically contain a fixed table of results that depend only on the number of cells of each value in each shell. But for a general graph the rule for the cellular automaton must allow for arbitrary numbers of cells in each shell. So what globally happens with this rule? For the graph No doubt there are general results that can be proved about the “success rate” for the majority cellular automaton rule on graphs. But experiments tend to suggest that the rule does much better on graphs than it does on regular arrays. Presumably there are graph-theoretic features of the underlying graph that affect the performance. Higher connectivity presumably helps, not least because it tends to avoid “bridges” where colors can be balanced on “all sides” of a particular node. Lack of symmetry also probably tends to inhibit the appearance of cycles. And in general one can think of the “spreading of consensus” as being at least somewhat like a percolation process. For ordinary cellular automata, it’s clear what it means to ask about the “infinite-size limit”. But for graphs it’s only immediately clear when one’s dealing with some readily extensible family of graphs (like grids or torus graphs or various Cayley graphs). And for arbitrary “random graphs” the results will probably depend significantly on the graph distribution used. In our Physics Project we have been concerned with large graphs that can be “grown” according to local rules. We expect such graphs often to show certain “statistical regularities” in the “continuum limit”. In our project, we characterize the structure of these graphs by looking for example at the growth rates of volumes of geodesic balls, and identifying things like dimension and curvature from them. So what will happen if we run a majority rule cellular automaton on a large graph that has certain “geometrical” properties? Essentially we need to ask what the “continuum limit” of the majority rule cellular automaton is. The grids used in ordinary cellular automata are too special for them to achieve any kind of generic such limit. But on “geometrizable” graphs, it’s more reasonable to expect such a continuum limit. We can try considering a 1D example. The initial values are then just given by a continuous function of position: The “consensus result” in this case should be a constant function whose value is effectively the sign of the integral of this function. But what kind of integro-differential-algebraic equation can reproduce the time evolution isn’t clear. Going back to majority cellular automata on graphs, it’s worth noting that if the edges of the graph can be assigned both positive and negative weights, then the system is effectively like a synchronous version of a neural net. The analog of this on a regular grid (which is structurally like a spin glass) is then known to show various features of computational irreducibility. Instead of thinking about underlying graphs with weighted edges, we can consider cellular automaton rules that don’t just involve pure nearest-neighbor majority. For example, we could consider rules that have different weights for geodesic shells of different radii (much like the activation-inhibition cellular automata used to model things like biological pigmentation patterns). But is it really true that only totalistic rules based on geodesic shells can be used for graph cellular automata? To do more than this requires in effect defining “directions” in the graph. But our Physics Project has provided a variety of mechanisms for doing just this—and this in principle for setting up non-totalistic graph cellular automata.
The Problem of Distributed Consensus 17.9 Asynchronous Updating : An important feature of cellular automata is the assumption that all cell values are updated “simultaneously” or “synchronously” in a definite series of steps. But in practical examples of distributed consensus one’s often dealing with values that are instead updated asynchronously. In effect, what one wants is to “break down” the synchronous updating of an ordinary cellular automaton into a sequence of updates of individual cells, with the order of these updates not being specified by any particular rule.So an obvious first question is: “Does it actually matter in what order these individual updates are done?” And sometimes it doesn’t. Here’s an example. Instead of an ordinary cellular automaton, consider a block cellular automaton in which at each step pairs of values adjacent cells are replaced by new values: For synchronous updating, we might apply these rules in a systematic “brick-like” pattern. But to study asynchronous updating, let’s just apply these rules in random positions at each step. Here are a few examples of what can happen: And the notable feature is that even though the specific evolution in each case is different, the final result is always the same—in this case just corresponding to having all sorted before . It doesn’t work this way for all rules, but for this rule, regardless of the intermediate states that are produced, there is always eventual consistency in the final result. As it turns out, this kind of phenomenon is crucial in our Physics Project. And indeed the generalization that we call “causal invariance” is what leads, for example, to relativistic invariance. But from the formalism of the Physics Project we also get a general approach to asynchronous evolution: trace all possible “update histories” using a multiway graph. Here is the multiway graph for the simple sorting rule above: As expected, all possible update histories eventually converge to the same final state. So what about the majority rule cellular automaton? It doesn’t always show eventual consistency, as this example shows: So this means that in general it matters in what order asynchronous updates are done, or, in effect, what “path of history” is taken. But to get a sense of typical behavior, we can consider random sequences of updates. Here’s an example of what one gets, doing one update per step: Should be compiled for all machine targets: And here’s the corresponding result if we do 20 updates per step: Should be compiled for all machine targets: We might have thought that asynchronous updating would add enough randomness to “break ties” and prevent things getting stuck. But in fact it’s not hard to see that the results are in this case in the end no different from synchronous updating. What about for something like the GKL rule? Here are asynchronous results for it, now with 50 updates per step (with initially 60% ). Should be compiled for all machine targets: So what happens if we search for rules that achieve consensus asynchronously? In the nearest-neighbor case, the simple majority rule does best, although it’s basically no good. Here are results for a few rules found by searching a million range-2 rules: Should be compiled for all machine targets: These rules were the top performers in terms of having “closest-to-majority-consensus” average behavior. In the pictures here, an average of 2 updates per cell is being done between successive rows. If we plot the final density as shown in these pictures against initial density, here are the results for the first 3 rules (with rule numbers 4272826020, 4242057736, and 4265795970): Should be compiled for all machine targets: In a perfect consensus rule, these would be step functions at —and one can expect that these results may get closer to that with larger numbers of cells and steps. In an ordinary, synchronous cellular automaton, every cell is in effect updated at every step, and the graph of “causal relationships” between “updating events” forms a trivial grid. But in an asynchronous cellular automaton the graph is sparser—with a particular updating event being causally connected to the previous event that happened to update that cell. But with the setup we have so far, this causal graph depends only on which cells are updated, not on what their colors might be. And with random updates, the causal graph will basically be like a “random meshing” of the spacetime structure of a system—so that for example for a cellular automaton with cyclic boundaries it becomes an approximation to a tube: Note that this is just a causal graph for a “single thread of history”, associated with a particular sequence of updating events. We can also imagine constructing a multiway causal graph that records the causal relationships both within and between different possible threads of history.
The Problem of Distributed Consensus 17.10 Dynamic Connectivity : Just as we can consider asynchronous updates in ordinary cellular automata, we can also consider them for graph cellular automata. But once we’re considering asynchronous updates on graphs, we can go still further, and consider not just updating “values at nodes” of a graph, but also the graph itself. And in this case, we’re basically dealing with the so-called Wolfram models of our Physics Project. As a kind of bridge to such models, let’s consider using them to represent a majority graph cellular automaton. We imagine setting up a hypergraph where all that exists is connectivity of the hypergraph, so “values” in the cellular automata have to be represented by connectivity structures—say with a 0 corresponding to a unary hyperedge, and a 1 corresponding to a ternary one (binary hyperedges are used to make “spatial” connections in the hypergraph). With this setup, the majority rule becomes a hypergraph transformation rule: Running this from a particular initial hypergraph, we see consensus achieved in a few steps: Here is a slightly larger example, that again succeeds in achieving consensus: The particular rules we’re using here move around the unary and ternary self-loop hyperedges, but do not affect the “backbone” of the hypergraph. And just as for our earlier examples with ordinary graphs, the simple majority rule doesn’t always succeed in achieving consensus. But now that we have formulated everything in terms of hypergraphs, it’s straightforward to have rules that not only change “colors” but also change the underlying structure. As a very simple example, consider adding a “structural rearrangement” case to our rule: Now in addition to moving around “colors”, the rule continually restructures the whole hypergraph: Ultimately this is something very close to our Physics Project. We can imagine encoding values in certain localized structures in our hypergraph—just as we imagine that particles (like photons or quarks) in physics correspond to something like “topological obstructions” in the hypergraph that represents physical space. And in these terms one can imagine formulating questions about consensus in terms of some kind of generalization of conservation laws for particles.
The Problem of Distributed Consensus 17.11 What’s Left to Figure Out : The problem of distributed consensus is in many ways a tantalizing one. The most obvious approach to it—with the simple majority rule—gets a fair distance, but has definite limitations. And as we’ve seen here, in specific, well-controlled situations there are much better rules—and setups—that can be used. But we don’t yet know robust, general, efficient solutions. One might imagine that to find one would just take “inventing the right algorithm” or “writing the right program”. But I think it’s unlikely that this kind of traditional “engineering” approach will bear fruit. Instead, I think the most promising path forward is to try to “mine the computational universe” for appropriate rules, in the style suggested by A New Kind of Science. And I expect that the best rules will be ones that don’t have “readily human understandable” behavior, but instead “do their job” in surprising and perhaps elaborate ways that we would never anticipate. How can we search for these rules? The most important challenge is to have a good definition of our objective with them. There’ll always be tradeoffs. How important is an occasional failure of consensus? How important are different features of the distribution of times to reach consensus? How much do we care about the complexity of the rules? And so on. So given an objective, what’s the best way to actually conduct the search? My consistent experience in mining the computational universe has been that the best results come from the most straightforward strategies. More elaborate strategies tend to make implicit assumptions, that prevent the discovery of truly surprising or unexpected results. A good start is just to do an exhaustive search. It’s important to be very careful in pruning it, lest one miss the “unexpected way” that a system can achieve some particular objective. Is it likely to be possible to “incrementally improve” rules, say with genetic algorithms? I’m not especially hopeful. Because to make serious use of what the computational universe has to offer, our rules are likely to need to show computational irreducibility—and this makes it essentially inevitable that the “landscape” of “nearby” rules will be irreducibly “rough”, making any computationally bounded incremental improvement unlikely to be successful. Could we perhaps train a machine learning system to suggest useful rules? It may be possible to do some pruning of candidate rules this way, although inevitably there is some risk of missing the “unexpected rule”. And in general the presence of computational irreducibility makes it implausible that an incrementally trained machine learning system will be extremely successful. One might have thought that something like exhaustive search could never find useful results, because the space of rules is in some sense just too big. But a key discovery from my explorations of the computational universe is that in fact there are surprisingly simple rules that can show rich and sophisticated behavior. And this makes it plausible that one could discover a good solution to the problem of distributed consensus just by appropriately searching the computational universe—and “mining” some rule that can then be used quite generally as a basis for all sorts of practical distributed consensus.
The Problem of Distributed Consensus 17.12 Some Historical Background : Investigations of what amounts to distributed consensus have a fairly long, if seemingly scattered history. As soon as even somewhat complex electromechanical and electronic systems were being built, the question arose of how to make the whole system behave in a reliable way even if some of its components were unreliable. The simplest answer was to have redundancy, and somehow to “take a vote”, and go with the “majority” decision. In the earliest computers (and later particularly in aerospace systems) such a vote was typically between copies of more-or-less complete systems. But by the beginning of the 1950s there was increasing interest in moving the voting down to the level of smaller components. And in 1952 John von Neumann, in his “Probabilistic Logics and the Synthesis of Reliable Organisms from Unreliable Components”, began to give a mathematical structure for analyzing this. Central to his discussion was what he called the “majority organ”, which is essentially a component for computing the Boolean majority function: von Neumann imagined building up everything (including the majority organ) from what he called “Sheffer organs”—or what we would now call Nand gates. And as an example of the need for redundancy he says “Consider a computing machine with 2500 vacuum tubes, each actuated on average every 5 microseconds. Assume a mean free path of 8 hours between errors is desired.” Somewhat mysteriously he then assumes the very high (even for the time) error rate ϵ = 0.005 and concludes that to operate reliably “the system should be multiplexed 14,000 times”. (von Neumann goes on to talk about errors in brains, as modeled by neural nets. Rather implausibly he states that in brains “errors are not ordinarily observed”, and concludes from this that the multiplexing factor must be about 20,000—which he viewed as being consistent with what was then known about actual brains.) Fortunately for the history of computing von Neumann’s example error rate turned out to be very wide of the mark—and once vacuum tubes were replaced by solid-state devices the problem of component failures in computers more or less disappeared (though it reappears in modern thinking about molecular-scale computing). In communications systems (and, to a lesser extent, storage devices) errors were always still important, and this led by the 1960s to increasing work on error-correcting codes. But perhaps because the transition to solid-state electronics happened more slowly in the Soviet Union interest in the problem of getting reliable results from unreliable components lasted much longer there. And while in the West, such issues tended to be thought of as matters of applied engineering, in the Soviet Union they were much more considered a matter of pure mathematics. (In the West, there was also in the 1950s the rather amorphous idea of “cybernetics”, which was initially considered ideologically inappropriate in the Soviet Union, but was later adopted there, and turned in a much more mathematical direction.) But in addition to questions coming essentially from the construction of machines (or brains), there was an initially quite separate strand of questions coming from physics. A very basic observation in physics is that materials undergo so-called phase transitions. For example, as one heats up water, there is a definite temperature at which all the molecules “together decide” to make the transition from liquid to gas. A somewhat more subtle version of the same kind of thing occurs in magnetic materials like iron. Below a certain temperature, electron spins associated with all the atoms in the material tend to line up. But in what direction? Somehow a “consensus direction” is selected—that defines the macroscopic direction of the magnetic field produced by the material. And in the 1920s the Ising model was suggested as a simple model for this. But before getting to phase transitions, there was a more basic physics question of how microscopic discrete elements like atoms could in general lead to macroscopic phenomena. And a key part of this question had to do with understanding the motion of molecules and how this could lead to “thermodynamic equilibrium”. The whole story of the foundations of “statistical mechanics” got quite muddled (and I think it’s only quite recently, with computation-based ideas, that we’ve finally been able to properly sort it out). But particularly in the first few decades of the 1900s the key idea was thought to be ergodicity: essentially the notion that the equations of motion of molecules will lead them eventually to visit all possible states of a system, thereby, it was argued, making their behavior seem random. It was difficult to establish ergodicity mathematically. But beginning around the 1930s this was a major emphasis of the field of dynamical systems theory. Meanwhile, there were also difficulties in understanding mathematically how phase transitions could occur. And one point of contact was that when there’s a phase transition, ergodicity effectively has to be broken: the spins in a magnet end up in a particular direction, and don’t visit all directions. At the beginning of the 1960s there was a convergence in Moscow of a considerable number of top Soviet mathematicians (notably including Andrei Kolmogorov) who were variously working on statistical mechanics, ergodic theory, dynamical systems—and some of the mathematical sequelae of cybernetics. And one of the pieces of work that emerged was a paper in 1968 by a then-young math-competition-winning mathematician named Andrei Toom. The (translated) title of the paper is “A Family of Uniform Nets of Formal Neurons”. The paper is cast in terms of formal probability theory and the study of Markov chains. But basically it’s a construction of what amounts to a probabilistic cellular automaton, and a proof that even though it’s probabilistic, certain aspects of its behavior are “non-ergodic” and effectively deterministic. (It’s notable that in 1963 Toom had done another construction: of what’s now called the Toom–Cook algorithm for fast multiplication of integers with many digits.) In traditional statistical mechanics (which was somewhat distinct from ergodic theory) the original focus was on studying “equilibrium” systems, in which different possible configurations (say of the Ising model) occurred with particular weightings. But by the 1950s—especially in work at Los Alamos—the idea arose of sequentially “Monte Carlo” sampling these configurations on a computer. And in 1963 Roy Glauber suggested thinking of the actual dynamics of the Ising model in terms of sequential probabilistic updating of spins. Meanwhile, somewhat separately, there was increasing study—particularly by American mathematicians such as Frank Spitzer—of the probability theory of collections of random walks, often referred to as “interacting particle systems”. And one of the main results was that as soon as nonzero probabilities were involved, ergodicity was typically found. Apparently independent of these developments the Moscow group in 1969 produced a paper entitled “Modeling of Voting with Random Error”. It featured a calculation done on an “electronic computing machine” (“ЭВМ” in Russian) of the probabilistic evolution of a majority model (and quite likely the machine used was a base-3 Setun computer developed by the mathematician Sergei Sobolev): The paper concluded that in 1D, the model was probably always ergodic, but in 2D, for sufficiently small noise level, it might not be. In 1971 Roland Dobrushin (a student of Kolmogorov’s) connected the investigation of ergodicity in these networks with phase transitions in Ising models—which helped define the program of research at his “Laboratory of Multi-component Random Systems” at the “Institute for Problems of Information Transmission” that brought together the Soviet cybernetics tradition (with its work on things like neural nets, Markov chains and formal computability theory) with international work on mathematical physics and ergodic theory. A typical product of this was the 1976 conference organized by Dobrushin (along with Toom and others) nominally entitled “Locally Interacting Systems and Their Application in Biology”—but actually with very little biology in sight, and steeped in sophisticated mathematics, about things like Markov fields, Gibbs measures and algorithmic unsolvability. A key question that had emerged was whether a homogeneous array of probabilistic elements (i.e. a probabilistic cellular automaton) could consistently and deterministically store information, or whether inevitably there would be ergodicity that would destroy it. In 1974 Toom showed that a multidimensional probabilistic cellular automaton could do this—essentially just using a majority rule on a non-symmetric neighborhood to generate a “global consensus state”, as we showed above. But the question still remained of whether anything similar was possible in 1D. Phase transitions in traditional statistical physics don’t happen in 1D if microscopic reversibility is assumed—making it seem like it might be impossible to maintain multiple distinct global states. But in 1976 Boris Tsirelson pointed out that at least with a hierarchical arrangement of interactions one could in fact achieve long-range order in a probabilistic 1D system: Soon thereafter Georgii Kurdyumov—having at first discussed the undecidability of ergodicity in the 1D case—then argued that there should be a pure cellular automaton that would work. And in 1978, Peter Gacs, Georgii Kurdyumov and Leonid Levin (all of whom had been in the Kolmogorov orbit) wrote a short paper entitled “One-Dimensional Uniform Arrays That Wash Out Finite Islands” that introduced the “GKL rule” we discussed above. They didn’t show any actual pictures of the behavior of the system, but they gave a proof that in the deterministic case the rule leads to two distinct phases, corresponding to the two distinct consensus states. And then they showed the result of a simulation that suggested that even when a certain amount of noise was added the two consensus states would still be reached: However, what had become known as the “positive probability conjecture” implied that there couldn’t in the end actually be non-ergodicity in the 1D case. But in 1983 Peter Gacs came up with what he claimed was a counterexample based on an elaborate construction described in many pages of pseudocode: It took many years for the proof of this to clarify, with Gacs publishing a final version only in 2001. Meanwhile, there’d been several other developments. Starting in 1982 my own discoveries about deterministic cellular automata had made 1D cellular automata much more prominent—and had made physicists aware of them. (As it happens, Gacs announced his 1983 result at a conference at Los Alamos I had organized, that I believe was the first ever to be devoted to cellular automata.) Around the end of the 1980s there was then a burst of activity by several leading mathematical physicists devoted to applying methods from statistical mechanics (and especially from areas like directed percolation theory) to the analysis of probabilistic cellular automata. There was awareness of Toom’s work, and for example connections were made between PDEs (like the KPZ equation) and things like the average behavior of “domain walls” in the probabilistic Toom rule. One can view the process of coming to consensus in a 1D cellular automaton as being like a “density classification” problem: if the initial density of 1s is above , classify as 1, otherwise classify as 0. And starting in the 1990s density classification in 1D (deterministic) cellular automata was used as a prime example of a place where algorithms might be discovered by genetic or other search techniques. In a quite different direction, work on cryptographic protocols in the 1980s had highlighted various models for achieving consensus between agents, even in the presence of adversarial efforts. Meanwhile, there was increasing interest in formal models of parallel computation, their computational complexity, and their fault tolerance. And by the early 2000s there was work being done (notably by Nick Pippenger) on connections between these things and what was known about probabilistic cellular automata, and the possibility of deterministic computation in them. And this pretty much takes us to the current time—and the new applications of distributed consensus in blockchain-like systems. And here it’s interesting to see the rather different intellectual lineages of two different efforts: Yilun Zhang at NKN coming from a statistical physics/computational neuroscience/information theory tradition, and Serguei Popov at Iota coming from probability theory and stochastic processes—as a great-grand-student of Kolmogorov.
The Problem of Distributed Consensus 17.13 Some Personal Notes : Of all the work I’ve done on cellular automata and related systems over the past more than forty years rather little has been devoted to the topics I’ve been discussing here. There are a couple of reasons for this. The most important is that my main interest has been in studying the remarkable richness and complexity that cellular automata and other very simple programs can generate—and in building a paradigm for thinking about this. Yet something like distributed consensus is at some level about getting rid of complexity rather than generating it. It’s about taking whatever complicated initial state there may be, and somehow reducing it to a “simple consensus”, where there’s none of that complexity. Another point is that at least some of what we’ve discussed here has concerned probabilistic systems, which I’ve tended to ignore on the grounds that they obscure the fundamental phenomena of the computational universe. If one didn’t know that simple, deterministic rules could do complex things, one might imagine that would have to inject randomness from the outside to make this happen. But the fact is that even very simple, deterministic rules can produce highly complex behavior, that in fact often makes its own apparent randomness. So that means there’s no need to “go outside the system”—and to introduce external randomness or probabilities. And in fact such probabilities tend to have the effect of hiding whatever complexity is intrinsically produced—even if they do “smooth out average behavior” to make things more accessible to traditional mathematical methods. There are actually some new perspectives on this from our Physics Project. First, the project makes clear the crucial interplay between underlying computational irreducibility, and effectively probabilistic large-scale behavior that can be treated in computationally reducible ways. And second, the project suggests that instead of thinking about probabilities for different behavior, one should think about the whole multiway system of possible behaviors, and its overall properties. It so happens that when I first became interested in the origins of complexity the first two kinds of models I thought about were spin systems (like the Ising model) and neural nets. But as I tried to simplify things I ended up inventing for myself what I soon found out were one-dimensional cellular automata. Much of my effort was then concentrated in doing experiments on these systems, and in developing theories and principles around the results I found. But I also tried to do my homework on earlier work. Cellular automata had gone by many names.  But leafing through the (then on paper) Science Citation Index I slowly began to piece together some of their history, and soon found things like the paper introducing the GKL rule. In my first long paper on cellular automata (entitled “Statistical Mechanics of Cellular Automata” and published in 1983) I have just a few paragraphs about “probabilistic rules”, discussing ergodicity and phase transitions, and referencing the GKL paper. Over the years I accumulated five thick folders of copies of papers that I labeled as being about “Stochastic Cellular Automata”. And I also purchased books. And in writing this piece I was able to just pull off my shelf things like Dobrushin’s 1976 book. And in one of those manifestations of the smallness of the scientific world, when I looked in the front of my (apparently used) copy of this book yesterday, what should I see there but the signature of Frank Spitzer—who I had just been writing about! When I was writing A New Kind of Science, both probabilistic cellular automata and what amounts to the problem of consensus did come up, and there are several mentions of such things in the book, notably in connection with my discussion of the “Origins of Discreteness”: But these things were never a big emphasis of my work, and so it’s been interesting here to trace just how the methods I’ve developed can be applied to them, and to realize that—despite its slightly different presentation—the problem of distributed consensus is in many ways actually a quintessential question that can be addressed by the kind of science that’s derived from studying the computational universe.
How Inevitable Is the Concept of Numbers? 18.1 Based on a talk at Numerous Numerosity : Everyone Has to Have Numbers… Don’t They? The aliens arrive in a starship. Surely, one might think, to have all that technology they must have the idea of numbers. Or maybe one finds an uncontacted tribe deep in the jungle. Surely they too must have the idea of numbers. To us numbers seem so natural—and “obvious”—that it’s hard to imagine everyone wouldn’t have them. But if one digs a little deeper, it’s not so clear. It’s said that there are human languages that have words for “one”, “a pair” and “many”, but no words for specific larger numbers. In our modern technological world that seems unthinkable. But imagine you’re out in the jungle, with your dogs. Each dog has particular characteristics, and most likely a particular name. Why should you ever think about them collectively, as all “just dogs”, amenable to being counted? Imagine you have some sophisticated AI. Maybe it’s part of the starship. And in it this computation is going on: Where are the numbers here? What is there to count? Let’s change the rule for the computation a bit. Now here’s what we get: And now we’re beginning to have something where numbers seem more relevant. We can identify a bunch of structures. They’re not all the same, but they have certain characteristics in common. And we can imagine describing what we’re seeing by just saying for example “There are 11 objects…”.
How Inevitable Is the Concept of Numbers? 18.2 What Underlies the Idea of Numbers? : Dogs. Sheep. Trees. Stars. It doesn’t matter what kinds of things they are. Once you have a collection that you view as all somehow being “of the same kind”, you can imagine producing a count of them. Just consider each of them in turn, at every step applying some specific operation to the latest result from your count—so that computationally you build up something like: For our ordinary integers, we can interpret s as being the “successor function”, or “add 1”. But at a fundamental level all that really matters is that we’ve reduced considering each of our original things separately to just repeatedly applying one operation, that gives a chain of results. To get to this point, however, there’s a crucial earlier step: we have to have some definite concept of “things”—or essentially a notion of distinct objects. Our everyday world is of course full of these. There are distinct people. Distinct giraffes. Distinct chairs. But it gets a lot less clear if we think about clouds, for example. Or gusts of wind. Or abstract ideas. So what is it that makes us able to identify some definite “countable thing”? Somehow the “thing” has to have some distinct existence—some degree of permanence or universality, and some ability to be independent and separated from other things. There are many different specific criteria we could imagine. But there’s one general approach that’s very familiar to us humans: the way we talk about “things” in human language. We take in some visual scene. But when we describe it in human language we’re always in effect coming up with a symbolic description of the scene. There’s a cluster of orange pixels over there. Brown ones over there. But in human language we try to reduce all that detail to a much simpler symbolic description. There’s a chair over there. A table over there. It’s not obvious that we would be able to do this kind of “symbolicization” in any meaningful way. But what makes it possible is that pieces of what we see are repeatable enough that we can consider them “the same kind of thing”, and, for example, give them definite names in human language. “That’s a table; that’s a chair; etc.”. There’s a complicated feedback loop, that I’ve written about elsewhere. If we see something often enough, it makes sense to give it a name (“that’s a shrub”; “that’s a headset”). But once we’ve given it a name, it’s much easier for us to talk and think about it. And so we tend to find or produce more of it—which makes it more common in our environment, and more familiar to us. In the abstract, it’s not obvious that “symbolicization” will be possible. It could be that the fundamental behavior of the world will always just generate more and more diversity and complexity, and never produce any kind of “repeated objects” that could, for example, reasonably be given consistent names. One might imagine that as soon as one believes that the world follows definite laws, then it’d be inevitable that there’d be enough regularity to guarantee that “symbolicization” is possible. But that ignores the phenomenon of computational irreducibility. Consider the rule: We might imagine that with such a simple rule we’d inevitably be able to describe the behavior it produces in a simple way. And, yes, we can always run the rule to find out what behavior it produces. But it’s a fundamental fact of the computational universe that the result doesn’t have to be simple: And in general we can expect that the behavior will be computationally irreducible, in the sense that there’s no way to reproduce it without effectively tracing through each step in the application of the rule. With behaviors like these it’s perfectly possible to imagine giving a complete symbolic description of what’s going on. But as soon as there’s computational irreducibility, this won’t be possible. There’ll be no way to have a “compressed” symbolicized description of the whole behavior. So how come we manage to describe so much with language, in a “symbolic” way? It turns out that even when a system—such as our universe—is fundamentally computationally irreducible, it’s inevitable that it will have “pockets” of computational reducibility. And these pockets of computational reducibility are crucially important to how we operate in the universe. Because they’re what let us have a coherent experience of the world, with things happening predictably according to identifiable laws, and so on. And they also mean that—even though we can’t expect to describe everything symbolically—there’ll always be some things we can. And some places where we can expect the concept of numbers to be useful.
How Inevitable Is the Concept of Numbers? 18.3 What the Universe Is Like : The history of physics might make one think that numbers would be a necessary part of the structure of any fundamental theory of our physical universe. But the models of physics suggested by our Physics Project have no intrinsic reference to numbers. Instead, they just involve a giant network of elements that’s continually getting rewritten according to certain rules. There aren’t intrinsically coordinates, or quantities, or anything that would normally be associated with numbers. And even though the underlying rules may be simple, the detailed overall behavior of the system is highly complex, and full of computational irreducibility. But the key point is that as observers with particular characteristics embedded in this system we’re only sampling certain features of it. And the features we sample in effect tap into pockets of reducibility. Which is where “simplifying concepts” like numbers can enter. Let’s talk first about time. We’re used to the experience that time progresses in some kind of linear fashion, perhaps marked off by something like counting rotations of our planet (i.e. days). But at the lowest level in our models, time doesn’t work that way. Instead, what happens is that the universe evolves by virtue of lots of elementary updating events happening throughout the network. These updating events have certain causal relationships. (A particular updating event, for example, might “causally depend” on another event because it uses as “input” something that’s the “output” of the other event.) In the end, there’s a whole “causal graph” of causal relationships between updating events: The full causal graph is immensely complex, and suffused with computational irreducibility. But we—as the observers we are—sample only certain features of this graph. And—as I’ve recently discussed elsewhere—it seems that the essence of our concept of consciousness is to define certain aspects of that sampling. In particular, despite all the updating events in the universe, and the complex causal relationships between them, we end up “parsing” the samples we take by imagining that we have a definite “sequentialized” thread of experience, or in effect that time progresses in a purely linear fashion. How do we achieve this? One convenient idealization—developed for thinking about spacetime and relativity—is to set up a “reference frame” in which we imagine dividing the causal graph into a sequence of slices (as in the picture above) that we consider to correspond to “instantaneous complete states of the universe” at successive “moments in time”. It’s not obvious that it’ll be consistent to do this. But between causal invariance and assumptions about the computational boundedness of the observer it turns out that it is—and that the “experience” of the universe for such an observer must follow the laws of physics that we know from general relativity. So what does this tell us about the emergence of numbers? At the lowest level, the universe is full of computational irreducibility in which there’s no obvious sign of anything like numbers. But in experiencing the universe through the basic features of our consciousness we essentially force some kind of “number-like” sequentiality in time, reflected in the validity of general relativity, with its “essentially numericalized” notion of time. Or, in other words, “time” (or the “progress of the universe”) isn’t intrinsically “numerical”. But the way we—as “conscious observers”—sample it, it’s necessarily sequentialized, with one moment of time being succeeded by another, in a fundamentally “numerical” sequence. It’s one thing, though, to sample the behavior of the universe in “time slices” in which all of space has been elided together. But for one to be able to “count” the moments in the passage of time (say aggregated into days), there has to be a certain “sameness” to those moments. The universe can’t do wildly different things at each successive moment; it has to have a certain coherence and uniformity that let us consider different moments as somehow “equivalent enough” to be able simply to be “counted”. And in fact the emergence of general relativity as the large-scale limit of our models (as viewed by observers like us) pretty much guarantees this result, except in certain pathological or extreme cases. OK, so for observers like us, time in our universe is in some sense “inevitably numerical”. But what about space? At the lowest level in our models, space just consists of a giant and continually updating network of “atoms of space”. And to talk about something like “distance in space” we first have to get some kind of “time-consistent” version of the network. It’s very much the same situation as with time. To get a simple definition of how time works, we have to elide space. Now, to have any chance of getting a simple definition of how space works, we have to somehow “elide time”. Or, put another way, we have to think about dividing up the causal graph into “spatial regions” (the vertical “timelike” analog of the horizontal “spacelike slices” we used above) where we can in effect combine all events that occur at any time, in that “region of space”. (Needless to say, in practice we don’t want it to be “any time”—just some span of time that is long compared to what elapses between individual updating events.) What is the analog for space of the “consciousness assumption” that time progresses in a single, sequential thread? Presumably it’s that we can sample space without having to think about time, or in other words, that we can consistently construct a stable notion of space. Let’s say we’re trying to find the shortest “travel path” between two “points in space”. At the outset, the definition is quite subtle—not least because there are no “statically defined” “points in space”. Every part of the network is being continually rewritten, so in a sense by the time you “get to the other point”, it certainly won’t be the same “atom of space” as when you started out. And to avoid this, you essentially have to elide time. And just like for the case of spacelike slices for sequentialization in time, there are certain consistent choices of timelike slices that can be made. And assuming such a choice is made, there will then be “time-elided” (or, roughly, time-independent) paths between points in space, analogous to our previous “space-elided” “path through time”. So then how might we measure the length of a path in space, or, effectively the distance between two points? In direct analogy to the case of time, if there is sufficient uniformity in the spatial structure then we can expect to just “count things” to get a numerical version of distance. Sequentialization in time is what allows us to have the sense that we maintain a coherent existence—and a coherent thread of experience—through time. The ability to do something similar in space is what gives us the sense that we have a coherent existence through space, or, in other words, that we can maintain our identity when we move around in space. In principle, there might be nothing like “pure motion”: it might be that any “movement in space” would necessarily change the structure and character of things. But the point is that one can consistently label positions in space so that this doesn’t happen, and “pure motion” is possible. And once we’ve done that, we’re again essentially forcing there to be a notion of distance, that can be measured with numbers. OK, but so if we sample the universe in the way we expect a conscious observer who maintains their identity as they move to do, then there’s a certain inevitable “numerical character” to the way we measure time and space. But what “stuff in the universe”? Can we expect that also to be characterized by numbers? We talked above about “things”. Can the universe contain “things” that can for example readily be counted? Remember that in our models the whole universe—and everything in it—is just a giant network. And at the lowest level this network is just atoms of space and connections between them—and nothing that we can immediately consider a “thing”. But we expect that within the structure of the network there are essentially topological features that are more like “things”. A good example is black holes. When we look at the network—and particularly the causal graph—we can potentially identify the signature of event horizons and a black hole. And we can imagine “counting black holes”. What makes this possible? First, that black holes have a certain degree of permanence. And second, that they can be to a large extent treated as independent. And third, that they can all readily be identified as “the same kind of thing”. Needless to say, none of these features is absolute. Black holes form, merge, evaporate—and so aren’t completely permanent. Black holes can have gravitational—and also presumably quantum—effects on each other, and so aren’t completely independent. But they’re permanent and independent enough that it’s a useful approximation to treat them as “definite things” that can readily be counted. Beyond black holes, there’s another clear example of “countable” things in the universe: particles, like electrons, photons, quarks and so on. (And, yes, it won’t be a big surprise if there’s a deep connection between particles and black holes in our models.) Particles—like black holes—are somewhat permanent, somewhat independent and have a high degree of “sameness”. A defining feature of particles is that they’re somewhat localized (for us, presumably in both physical and branchial space), and maintain their identity with time. They can be emitted and absorbed, so aren’t completely permanent, but somehow they exist for long enough to be identified. It’s then a fundamental observation in physics that particles come only in certain discrete species—and within these species every particle (say, every electron) is identical, save for its position and momentum (and spin direction). We don’t yet know within our models exactly how such particles work, but the assumption is that they correspond to certain discrete possible “topological obstructions” in the behavior of the network. And much like a vortex in a fluid, their topological character endows them with a certain permanence. It’s worth understanding that in our models, not everything that “goes on in the universe” can necessarily be best characterized in terms of particles. In principle one might be able to think of every piece of activity in the network as somehow related to a sufficiently small or short-lived “particle”. But mostly there won’t be “room for” the characteristics of something we can identify as a particular “countable” particle to emerge. An extreme case is what would be considered zero-point fluctuations in traditional quantum field theory: an ever-present infinite collection of short-lived virtual particle pairs. In our models this is not something one immediately thinks of in terms of particles: rather, it is continual activity in the network that in effect “knits space together”. But in answering the question of whether physics inevitably leads to a notion of numbers, one can certainly point to situations where definite “countable” particles can be identified. But is this like the case of time and space that we discussed above: the numbers are somehow “not intrinsic” but just appear for “observers like us”? Once again I suspect the answer is “yes”. But now the special feature of us as observers is that we think about the universe in terms of multiple, independent processes or experiments. We set things up so that we can concentrate, say, on the scattering of two particles that are initially sufficiently separated from everything else to be independent of it. But without this separation, we’d have no real way to reliably “count the particles”, and characterize what’s happening in terms of specific particles. There’s actually a direct analog of this in a simple cellular automaton. On the left is a process involving “separated countable particles”; on the right—using exactly the same rule—is one where there are no similar particle-based “asymptotic states”.
How Inevitable Is the Concept of Numbers? 18.4 Is All Computational Reducibility Numerical? : As we’ve discussed, even with simple underlying rules, many systems behave in computationally irreducible ways. But when there’s computational reducibility—and when, in a sense, we can successfully “jump ahead” in the computation—are numbers always involved in doing that? In cases like these where there’s clear repetition in the behavior, numbers are an obvious path to figuring out what’s going to happen. Want to know what the system will do at step number t? Just take the number t and do some “numerical computation” on it (typically here involving modulo arithmetic) and immediately get the result. But very often you end up treating t as a “number in name only”. Consider nested patterns like these: It’s possible to work out the behavior at step t in a computationally reduced way, but it involves treating t not so much as a number (that one might, say, do arithmetic on) but instead more just a sequence of bits that one computes bitwise functions like BitXor on. There are definitely other cases where the ability to jump ahead in a computation relies specifically on the properties of numbers. A somewhat special example is a cellular automaton whose rows can be thought of as digits of a number in base 6, that at each step gets multiplied by 3 (it’s not obvious that this procedure will be local to digits, “cellular-automaton-style”, but it is): In this case, repeated squaring of the rows thought of as numbers quickly gets the result—though actually t is again used more for its digits than its “numerical value”. When one explores the computational universe, by far the most common sources of computational reducibility are repetition and nesting. But other examples do show up. A few are obviously “numerical”. But most are not. And typically what happens is just that there is an alternative, very much more efficient program that exists to compute the same results as the original program. But the more efficient program is still “just a program” with no particular connection to anything involving numbers. Fast numbers-based ways to do particular computations are often viewed as representing “exact solutions” to corresponding mathematical problems. Such exact solutions tend to be highly prized. But they also tend to be few and far between—and rather specific. Could there be other “generic” forms of computational reducibility beyond repetition and nesting? In general we don’t know—though it’d be an important thing to find out. Still, there is in a sense one other kind of computational reducibility that we do know about, and that’s been very widely used in mathematical science: the phenomenon of continuity. So far, we’ve mostly been talking about numbers that are integers, and that can at some level be used to “count distinct things”. But in mathematics and mathematical science it’s very common to think not about discrete integers, but about the continuum of real numbers. And even when there’s some discrete process going on underneath—that might even show computational irreducibility—it can still be the case that in the continuum limit there’s a “numerical description”, say in terms of a differential equation. If one looks, say, at cellular automata, it’s fairly rare to find examples that have such continuum limits. But in the models from our Physics Project—that have much less built-in structure—it seems to be almost a generic feature that there’s a continuum limit that can be described by continuous equations of just the kind that have shown up in traditional mathematical physics. But beyond taking limits to derive continuum behavior, one can also just symbolically specify equations whose variables are from the start, say, real numbers. And in such cases one might think that everything would always “work out in terms of numbers”. But actually, even in cases like this, things can be more complicated. Yes, for the equations that are typically discussed in textbooks, it’s common to get solutions that can be represented just as evaluating certain functions of numbers. But if one looks at other equations and other situations, there’s often no known way to get these kinds of “exact solutions”. And instead one basically has to try to find an explicit computation that can approximate the behavior of the equation. And it seems likely that in many cases such computations will end up being computationally irreducible. Yes, they’re in principle being done in terms of numbers. But the dominant force in determining what happens is a general computational process, not something that depends on the specific structure of numbers. And, by the way, it’s no coincidence that in the past couple of decades, as more and more modeling of systems with complex behavior is done, there’s been an overwhelming shift away from models that are based on equations (and numbers) to ones that are based directly on computation and computational rules.
How Inevitable Is the Concept of Numbers? 18.5 But Do We Have to Use Numbers? The Computational Future : Why do we use numbers so much? Is it something about the world? Or is it more something about us? We discussed above the example of fundamental physics. And we argued that even though at the most fundamental level numbers really aren’t involved, our sampling of what happens in the universe leads us to a description that does involve numbers. And in this case, the origin of the way we sample the universe has deep roots in the nature of our consciousness, and our fundamental way of experiencing the universe, with our particular sensory apparatus, place in the universe, etc. What about the appearance of numbers in the history of science and engineering? Why are they so prevalent there? In a sense, like the situation with the universe, I don’t think it’s that the underlying systems we’re dealing with have any fundamental connection to numbers. Rather, I think it’s that we’ve chosen to “sample” aspects of these systems that we can somehow understand or control, and these often involve numbers. In science—and particularly physical science—we have tended to concentrate on setting up situations and experiments where there’s computational reducibility and where it’s plausible that we can make predictions about what’s going to happen. And similarly in engineering, we tend to set up systems that are sufficiently computationally reducible that we can foresee what they’re going to do. As I discussed above, working with numbers isn’t the only way to tap into computational reducibility, but it’s the most familiar way, and it’s got an immense weight of historical experience behind it. But do we even expect that computational reducibility will be a continuing feature of science and engineering? If we want to make the fullest use of computation, it’s inevitable that we’ll have to bring in computational irreducibility. It’s a new kind of science, and it’s a new kind of engineering. And in both cases we can expect that the role of numbers will be at least much reduced. If we look at human history, numbers have played a quite crucial role in the organization of human society. They’re used to keep records, specify value in commerce, define how resources should be allocated, determine how governance should happen, and countless other things. But does it have to be that way, or is it merely that numbers provide a convenient way to set things up so that we humans can understand what’s going on? Let’s say that we’re trying to achieve the objective of having an efficient transportation system for carrying people around. The traditional “numbers-based” way of doing that would be to have, say, trains that run at specific “numerical” times (“every 15 minutes”, or whatever). In a sense, this is a simple, “computationally reducible” solution—that for example we can easily understand. But there’s potentially a much better solution, at least if we’re able to make use of sophisticated computation. Given the complete pattern of who wants to go where, we can dispatch specific vehicles to drive in whatever complicated arrangement is needed to optimally deliver people to their destinations. It won’t be like the trains, with their regular times. Instead, it’ll be something that looks more complex, and computationally irreducible. And it won’t be easy to characterize in terms of numbers. And I think it’s a pretty general phenomenon: numbers provide a good “computationally reducible” way to set something up. But there are other—perhaps much more efficient—ways, that make more serious use of computation, and involve computational irreducibility, but don’t rely on numbers. None of these computational approaches are possible until we have sophisticated computation everywhere. And even today we’re just in the early stages of broadly deploying the level of computational sophistication that’s needed. But as another example of how this can play out, consider economic systems. One of the first and historically strongest uses of numbers has been in characterizing amounts of money and prices of things. But are “numerical prices” the only possible setup for an economic system? We already have plenty of examples of dynamic pricing, where there’s no “list price”, but instead AIs or bots are effectively bidding in real time to determine what transaction will happen. Ultimately an economic system is based on a large network of transactions. One person wants to get a cookie. The person they’re getting it from wants to rent a movie. Somewhat in analogy to the transportation example above, with enough computation available, we could imagine a situation where at every node in the network there are bots dynamically arranging transactions and deciding what can happen and what cannot, ultimately based on certain goals or preferences expressed by people. This setup is slightly reminiscent of our model of fundamental physics—with causal graphs from physics now being something like supply chains. And as in the physics case, there’s no necessity to have numbers involved at the lowest level. But if we want to “sample the system in a human way” we’ll end up describing it in collective terms, and potentially end up with an emergent notion of price a bit like the way there’s an emergent notion of gravitational field in the case of physics. So in other words, if it’s just the bots running our economic system, they’ll “just be doing computation” without any particular need for numbers. But if we try to understand what’s going on, that’s when numbers will appear. And so it is, I suspect, with other examples of the appearance of numbers in the organization of human society. If things have to be implemented—and understood—by humans, there’s no choice but to leverage computational reducibility, which is most familiarly done through numbers. But when things are instead done by AIs or bots, there’s no such need for computational reducibility. Will there still be “human-level descriptions” that involve numbers? No doubt there’ll at least be some “natural-science-like” characterizations of what’s going on. But perhaps they’ll most conveniently be stated in terms of computational reducibility that’s set up using concepts other than numbers—that humans in the future will learn about. Or perhaps numbers will be such a convenient “implementation layer” that they’ll end up being used for essentially all human-level descriptions. But at a fundamental level my guess is that ultimately numbers will fall away in importance in the organization of human society, giving way to more detailed computation-based decision making. And maybe in the end numbers will come to seem a little like the way logic as used in the Middle Ages might seem to us today: a framework for determining things that’s much less complete and powerful than what we now have.
How Inevitable Is the Concept of Numbers? 18.6 Are Numbers Even Inevitable in Mathematics? : Whatever their role in science, technology and society, one place where numbers seem fundamentally central is mathematics. But is this really something that is necessary, or is it instead somehow an artifact of the particular history or presentation of human mathematics? A common view is that at the most fundamental level mathematics should be thought of as an exploration of the consequences of certain abstract underlying axioms. But which axioms should these be? Historically a fairly small set has been used. And a first question is whether these implicitly or explicitly lead to the appearance of numbers. The axioms for ordinary logic (which are usually assumed in all areas of mathematics) don’t have what’s needed to support the usual concept of numbers. The same is true of axioms for areas of abstract algebra like group theory—as well as basic Euclidean geometry (at least for integers). But the Peano axioms for arithmetic are specifically set up to support integers. But there is a subtlety here. What the Peano axioms actually do is effectively define certain constraints on abstract constructs. Ordinary integers are one “solution” to those constraints. But Gödel’s theorem shows that there are also an infinite number of other solutions: non-standard “numbers” with weird properties that also happen to follow the same overall axioms. So in a sense mathematics based on the Peano axioms can be interpreted as being “about” ordinary numbers—but it can also be interpreted as being about other, exotic things. And it’s pretty much the same story with the standard axioms of set theory: the mathematics they generate can be interpreted as covering ordinary numbers, but it can also be interpreted as covering other things. But what happens if we ignore the historical development of human mathematics, and just start picking axiom systems “at random”? Most likely they won’t have any immediately recognizable interpretation, but we can still go ahead and build up a whole network of theorems and results from them. So will such axiom systems end up leading to constructs that can be interpreted as numbers? This is again a somewhat tricky question. The Principle of Computational Equivalence suggests that axiom systems with nontrivial behavior will typically show computation universality. And that means that (at least in some metamathematical sense) it’s possible to set up an encoding of any other axiom system within them. So in particular it should be possible to reproduce what’s needed to support numbers. (Again, there are subtleties here to do with axiom schemas, and their use in supporting the concept of induction, which seems quite central to the idea of numbers.) But if we just look at the raw theorems from a particular axiom system—say as generated by an automated theorem-proving system—it’ll be very hard to tell what can be interpreted as being “related to numbers”. But what if we restrict ourselves to mathematical results that have been proved by humans—of which there are a few million? There are a number of recent efforts to formalize at least a few tens of thousands of these, and show how they can be formally derived from specific axioms. But now we can ask what the dependencies of these results are. How many of them need to “go through the idea of numbers”? We can get a sense of this by doing “empirical metamathematics” on a particular math formalization system (here Metamath): And what we see is that at least in a human formalization of mathematics, numbers do indeed seem to play a very central role. Of course, this doesn’t tell us whether in principle results, say in topology, could be proved “without numbers”; it just tells us that in this particular formalization numbers are used to do that. We also can’t tell whether numbers were just “convenient for proofs” or whether in fact the actual mathematical results picked to formalize were somehow based on their “accessibility” through numbers. Given any (universal) axiom system there are an infinite number of theorems that can be proved from it. But the question is: which of these theorems will be considered “interesting”? And one should expect that theorems that can be interpreted in terms of concepts—like numbers—that have historically become well known in human mathematics will be preferred.
How Inevitable Is the Concept of Numbers? 18.7 But is this just a story of accidents of the history of mathematics, or is there more to it? : The traditional view of the foundations of mathematics has involved imagining that some particular axiom system is picked, and then mathematics is some kind of exploration of the implications of this axiom system. It’s the analog of saying: pick some particular rule for a potential model of the universe, then see what consequences it has. But what we’ve realized is that at least when it comes to studying the universe, we don’t fundamentally have to pick a particular rule: instead, we can construct a rulial multiway system in which, in effect, all possible rules are simultaneously used. And we can imagine doing something similar for mathematics. Instead of picking a particular underlying axiom system, just consider the structure made from simultaneously working out the consequences of all possible axiom systems. The resulting object seems to be closely related to things like the infinity groupoid that arises in higher category theory. But the important point here is that in a sense this object is a representation of all possible results in all possible forms of mathematics. But now the question is: how should we humans sample this? If we’re in a sense computationally bounded, we basically have to pick a certain “reference frame”. There seems to be a close analogy here to physics. In the case of physics, basic features of our consciousness seem to constrain us to certain kinds of reference frames, from which we inevitably “parse” the whole rulial multiway system as following known laws of physics. So perhaps something similar is going on in mathematics. Perhaps here too something very much like the basic features of consciousness constrain our sampling of the limiting rulial object. But what then are the analogs of the laws of physics? Presumably they will be some kind of as-yet-undiscovered general “laws of bulk metamathematics”. Maybe they correspond to overall structural principles of “mathematics as we sample it” (conceivably related to category theory). Or maybe—as in the case of space and time in physics—they actually inevitably lead to something akin to numbers. In other words, maybe—just as in physics the appearance of numbers can be thought of as reflecting aspects of our characteristics as observers—so too this may be happening in mathematics. Maybe given even the barest outline of our human characteristics, it’s inevitable that we’ll perceive numbers to be central to mathematics. But what about our aliens in their starship? In physics we’ve realized that our view of the universe—and the laws of physics we consider it to follow—is not the only possible one, and there are others completely incoherent with ours that other kinds of observers could have. And so it will be with mathematics. We have a particular view—that’s perhaps ultimately based on things like features of our consciousness—but it’s not the only possible one. There can be other ones that still describe the same limiting rulial object, but are completely incoherent with what we’re used to. Needless to say, by the time we can even talk about “aliens arriving in a starship”, we’ve got to assume that their “view of the universe” (or, in effect, their location in rulial space) is not too far from our own. And perhaps this also implies a certain alignment in the “view of mathematics”, perhaps even making numbers inevitable. But in the abstract, I think we can expect that there are “views of mathematics” that are incoherently different from our own, and that while in a sense they are “still mathematics”, they don’t have any of the familiar features of our typical view of mathematics, like numbers.
How Inevitable Is the Concept of Numbers? 18.8 So, Are Numbers Inevitable? : Numbers have been part of human civilization throughout recorded history. But here we’ve asked the fundamental question of why that’s been the case. And what we’ve seen is that there doesn’t appear to be anything ultimately fundamental about the universe—or, for example, about mathematics—that inevitably leads to numbers. Instead, numbers seem to arise through our human efforts to “parse” what’s going on. But it’s not just that numbers were invented at some point in human history, and then used. There’s something more fundamental and essential about us that makes numbers inevitable for us. Our general capability for sophisticated computation—which the Principle of Computational Equivalence implies is shared by many systems—isn’t what does it. And in fact when there’s lots of sophisticated computation—and computational irreducibility—going on, numbers aren’t a particularly useful description. Instead, it’s when there’s computational reducibility that numbers can appear. And the point is that there are fundamental things about us that lead us to pick out pockets of computational reducibility. In particular, what we view as consciousness seems to be fundamentally related to the fact that we sample things in a particular way that leverages computational reducibility. Not all computational reducibility need be related to numbers, but some examples of it are. And it’s these that seem to lead to the widespread appearance of numbers in our experience of the universe. Could things be different? If we were different, definitely. And, for example, there’s no reason to think that a distributed AI system would have to intrinsically make use of anything like numbers. Yes, in our attempts to understand or explain it, we might use numbers. But nothing in the system itself would “know about” numbers. And indeed by operating like this, the system would be able to make richer use of the computational resources available in the computational universe of possible programs. Numbers have been widely used in science, engineering and many aspects of the organization of society. But as things become more computationally sophisticated, I think we can expect that the intrinsic use of numbers will progressively taper off. But it’ll still be true that as long as we preserve core aspects of our experience as what we consider conscious observers some version of numbers will in the end be inevitable for us. We can aspire to generalize from numbers, and, for example, sample other representations of computational reducibility. But for now, numbers seem to be inextricably connected to core aspects of our existence. Thanks to the organizers of Numerous Numerosity for the “essay prompt” that led to this piece, and to Jonathan Gorard for some very helpful input.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.1 The Path to a New Paradigm : One might have thought it was already exciting enough for our Physics Project to be showing a path to a fundamental theory of physics and a fundamental description of how our physical universe works. But what I’ve increasingly been realizing is that actually it’s showing us something even bigger and deeper: a whole fundamentally new paradigm for making models and in general for doing theoretical science. And I fully expect that this new paradigm will give us ways to address a remarkable range of longstanding central problems in all sorts of areas of science—as well as suggesting whole new areas and new directions to pursue. If one looks at the history of theoretical science, I think one can identify just three major modeling paradigms that have been developed over the course of scientific history—each of them leading to dramatic progress. The first, originating in antiquity, one might call the “structural paradigm”. Its key idea is to think of things in the world as being constructed from some kind of simple-to-describe elements—say geometrical objects—and then to use something like logical reasoning to work out what will happen with them. Typically this paradigm has no explicit notion of time or dynamical change, though in its modern forms it often involves making descriptions of structures of relationships, usually built from logical or “flowchart-like” elements. Many would say that modern exact science was launched in the 1600s with the introduction of what we can call the “mathematical paradigm”: the idea that things in the world can be described by mathematical equations—and that their behavior can be determined by finding solutions to these equations. It’s common in this paradigm to discuss time—but normally it’s just treated as a variable in the equations, and one hopes that to find out what will happen at some arbitrary time one can just substitute the appropriate value for that variable into some formula derived by solving the equations. For three hundred years the mathematical paradigm was the state of the art in theoretical science—and immense progress was made using it. But there remained plenty of phenomena—particularly associated with complexity—that this paradigm seemed to have little to say about. But then—basically starting in the early 1980s—there was a burst of progress based on a new idea (of which, yes, I seem to have ultimately been the primary initiator): the idea of using simple programs, rather than mathematical equations, as the basis for models of things in nature and elsewhere. Part of what this achieves is to generalize beyond traditional mathematics the kind of constructs that can appear in models. But there is something else too—and it’s from this that the full computational paradigm emerges. In the mathematical paradigm one imagines having a mathematical equation and then separately somehow solving it. But if one has a program one can imagine just directly taking it and running it to find out what it does. And this is the essence of the computational paradigm: to define a model using computational rules (say, for a cellular automaton) and then explicitly be able to run these to work out their consequences. And one feature of this setup is that time becomes something much more fundamental and intrinsic. In the mathematical paradigm it’s in effect just the arbitrary value of a variable. But in the computational paradigm it’s a direct reflection of the actual process of applying computational rules in a model—or in other words in this paradigm the passage of time corresponds to the actual progress of computation. A major discovery is that in the computational universe of possible programs even ones with very simple rules can show immensely complex behavior. And this points the way—through the Principle of Computational Equivalence—to computational irreducibility: the phenomenon that there may be no faster way to find out what a system will do than just to trace each of its computational steps. Or, in other words, that the passage of time can be an irreducible process, and it can take an irreducible amount of computational work to predict what a system will do at some particular time in the future. (Yes, this is closely related not only to things like undecidability, but also to things like the Second Law of Thermodynamics.) In the full arc of scientific history, the computational paradigm is very new. But in the past couple of decades, it’s seen rapid and dramatic success—and by now it’s significantly overtaken the mathematical paradigm as the most common source for new models of things. Despite this, however, fundamental physics always seemed to resist its advance. And now, from our Physics Project, we can see why. Because at the core of our Physics Project is actually a new paradigm that goes beyond the computational one: a fourth paradigm for theoretical science that I’m calling the multicomputational paradigm. There’ve been hints of this paradigm before—some even going back a century. But it’s only as a result of our Physics Project that we’ve been able to start to see its full depth and structure. And to understand that it really is a fundamentally new paradigm—that transcends physics and applies quite generally as the foundation for a new and broadly applicable methodology for making models in theoretical science.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.2 Multiway Systems and the Concept of the Multicomputation : In the ordinary computational paradigm the typical setup is to have a system that evolves in a series of steps by repeatedly applying some particular rule. Cellular automata are a quintessential example. Given a rule like one can think of the evolution implied by the rule as corresponding to a sequence of states of the cellular automaton: The essence of the multicomputational paradigm is to generalize beyond just having simple linear sequences of states, and in effect to allow multiple interwoven threads of history. Consider as an example a system defined by the string rewrite rules: Starting from A, the next state has to be BBB. But now there are two possible ways to apply the rules, one generating AB and the other BA. And if we trace both possibilities we get what I call a multiway system—whose behavior we can represent using a multiway graph: A typical way to think about what’s going on is to consider each possible underlying rule application as an “updating event”. And then the point is that even within a single string multiple updating events (shown here in yellow) may be possible—leading to multiple branches in the multiway graph: At first, one might want to say that while many branches are in principle possible, the system must somehow in any particular case always choose (even if perhaps “non-deterministically”) a single branch, and therefore a particular history. But a key to the multicomputational paradigm is not to do this, and instead to say that “what the system does” is defined by the whole multiway graph, with all its branches. In the ordinary computational paradigm, time in effect progresses in a linear way, corresponding to the successive computation of the next state of the system from the previous one. But in the multicomputational paradigm there is no longer just a single thread of time; instead one can think of every possible path through the multiway system as defining a different interwoven thread of time. If we look at the four paradigms for theoretical science that we’ve identified we can now see that they involve successively more complicated views of time. The structural paradigm doesn’t directly talk about time at all. The mathematical paradigm does consider time, but treats it as a mathematical variable whose value can in a sense be arbitrarily chosen. The computational paradigm treats time as reflecting the progression of a computation. And now the multicomputational paradigm treats time as something multithreaded, reflecting the interwoven progression of multiple threads of computation. It’s not difficult to construct multiway system models. There are multiway Turing machines. There are multiway systems based on rewriting not only strings, but also trees, graphs or hypergraphs. There are also multiway systems that work just with numbers. It’s even possible (though not especially natural) to define multiway cellular automata. And in fact, whenever there’s a system where a single state can be updated in multiple ways, one’s led to a multiway system. (Examples include games where multiple moves are possible at each turn, and computer systems with asynchronous or distributed elements that operate independently.) And once one has the idea of multiway systems it’s amazing how often they end up being the most natural models for things. And indeed one can see them as minimal models pretty much whenever there’s no rigid built-in notion of time, and no predefined specification of “when things happen” in a system. But right now the “killer app” for multiway systems is our Physics Project. Because what we seem to be learning is that in fact our whole universe is operating as a giant multiway system. And it’s the limiting properties of that multiway system that give us space and time and relativity and quantum mechanics.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.3 Observers, Reference Frames and Emergent Laws : In the mathematical paradigm one expects to immediately “read off” from a model what happens at a particular time. In the computational paradigm one might have to run an irreducible computation, but then one can still “read off” what happens after a certain time. But in the multicomputational paradigm, it’s more complicated—because now there are multiple threads of time, with no intrinsic way to line up “what happens when” across different threads. But imagine you’re trying to see what’s going on in a multicomputational system. In principle you could keep track of the behaviors on all the threads as well as the complicated interweavings between them. But a crucial fact about us as observers is that we don’t normally do that. Instead, we normally combine things so that we can describe the system as somehow just progressively “evolving through time”. There might in principle be some alien intelligence that routinely keeps track of all the different threads. But we humans—and the descriptions we use of the world—always tend to sequentialize things. In other words, in order to understand “what’s happening in the world” we try to approximate what might underneath be multicomputational by something that is “merely computational”. Instead of following lots of different “local times” on different threads, we try to think about things in terms of a single “global time”. And this isn’t just something we do “for convenience”; the tendency to “sequentialize” like this is directly related to our perception that we have a single thread of experience, which seems to be a key defining feature of our notion of consciousness and our general way of relating to the world. But how should we line up different threads of time in a multicomputational system? A crucial point is that there typically isn’t just “one natural way” to do it. Instead, there are many possible choices. And it’s “up to the observer” which one to use—and therefore “how to parse” the behavior of the multicomputational system. The underlying structure of the multiway system puts constraints on what’s possible, but typically there are many ways of choosing a sequence of “time slices” that successively sample the behavior of the system. Here are two choices of how to do this for the multiway system above: In both cases the underlying multicomputational behavior is the same. But the “experience” of the observer is different. And—taking a term used in relativity theory that we’ll later see captures exactly the same idea—we can consider the different choices of time slices as different “reference frames” from which to view what’s going on. The reference frame isn’t something intrinsic to the underlying multicomputational system (though the system does put constraints on what reference frames are possible). Instead, the reference frame is just something the observer “uses to understand the system”. But as soon as an observer sequentializes time—as I believe we characteristically do—then essentially by definition they must be using some reference frame. In the ordinary computational paradigm there are fundamental limits on our prediction or understanding of the behavior of systems, associated with the phenomenon of computational irreducibility. And things get even more difficult when it comes to multicomputational systems—where not only can individual threads of history show computational irreducibility, but also these threads can interweave in computationally irreducible ways. But what will an observer with a certain reference frame perceive about the multicomputational system? Well, it depends on the reference frame. And for example one might imagine that one could have a very elaborate reference frame that somehow “untangles” the computational irreducibility associated with the weaving of different threads and delivers some arbitrarily different “perception” of what’s going on. But now there’s another crucial point: actual observers such as us don’t use arbitrary reference frames; they only use computationally bounded ones. In other words, there’s a limit to how complicated the reference frame can be, and how much computation it can effectively serve to “decode”. If the observer is somehow embedded inside the multicomputational system (as must be the case if, for example, the system corresponds to the fundamental physics of our whole universe), then it’s necessary and inevitable that the observer (being a subpart of the whole system)—and the reference frames they use—must be computationally bounded. But the notion of a computationally bounded observer is actually something much more general—and as we’ll see in a series of examples later—it’s a central part of multicomputational models for all sorts of systems. By the way, we’ve discussed sequentialization in time separately from computational boundedness. But in some sense sequentialization in time is actually just a particular example of computational boundedness that happens to be very obvious and significant for us humans. And potentially some alien intelligence could act as a computationally bounded observer with some other way of “simplifying time”. But, OK, so we have a multicomputational system that’s behaving in some computationally irreducible way. And we have a computationally bounded observer who’s “parsing” the multicomputational system using particular reference frames. What will that observer perceive about the behavior of the system? Well, here’s the crucial and surprising thing that’s emerged from our Physics Project: with the setup for multicomputational systems that we’ve described, the observer will almost inevitably perceive the system to follow laws that are simple enough to be captured by mathematical equations. And in the case of physics these laws basically correspond in different situations to general relativity and to quantum mechanics. In other words, despite the complexity of the underlying behavior of the multicomputational system, the comparative simplicity of the observer makes them inevitably sample only certain “simple aspects” of the whole behavior of the multicomputational system. In computational terms, the observer is perceiving a computationally reducible slice of the whole computationally irreducible behavior of the system. But what exactly will they perceive? And how much does it depend on the details of the underlying computationally irreducible behavior? Well, here’s something very crucial—and surprising—about multicomputational systems: there’s a lot that can be said quite generically about what observers will perceive, largely independent of the details of underlying computationally irreducible behavior. It’s deeply related to (but more general than) the result in thermodynamics and statistical physics that there are generic laws for, say, the perceived behavior of gases. At an underlying level, gases consist of large numbers of molecules with complicated and computationally irreducible patterns of motion. But a computationally bounded observer perceives only certain “coarse-grained” features—which don’t depend on the underlying properties of the molecules, and instead correspond to the familiar generic laws for gases. And so it is in general with multicomputational systems: that quite independent of the details of underlying computationally irreducible behavior there are generic (“computationally reducible”) laws that computationally bounded observers will perceive. The specifics of those laws will depend on aspects of the observer (like their sequentialization of time). But the fact that there will be such laws seems to be an essentially inevitable consequence of the core structure of multicomputational systems. As soon as one imagines that events can occur “whenever and wherever” rules allow, this inevitably leads to a kind of inexorable combinatorial structure of interwoven “threads of time” that necessarily leads to certain “generic perceptions” by computationally bounded observers. There can be great complexity in the underlying behavior of multicomputational systems. But there’s a certain inevitable overall structure that gets revealed when observers sample the systems. And that inevitable structure can manifest itself in fairly simple laws for certain aspects of the system. A characteristic feature of systems based on the ordinary computational paradigm is the appearance of computational irreducibility and complex behavior. And with such systems it’s perfectly possible to have computationally bounded observers who sample this complex behavior and reduce it to rather simple features. But what tends to happen is that rather little is left; the observer has in a sense crushed everything out. (Imagine, say, an observer averaging the colors of a complex-enough-to-seem-random sequence of black and white cells to a simple uniform gray.) But with a multicomputational system, things work differently. Because there’s enough inevitable structure in the fundamental multicomputational setup of the system that even when it’s sampled by a somewhat arbitrary observer there are still nontrivial effective laws that remain. And in the case of fundamental physics we can identify these laws as general relativity and quantum mechanics. But the point is that because these laws depend only on the fundamental setup of the system, and on certain basic properties of the observer, we can expect that they will apply quite generally to multicomputational systems. Or, in other words, that we can expect to identify overall laws in basically any multicomputational system—and those laws will in effect be direct analogs of general relativity and quantum mechanics. In ordinary computational systems there’s a very powerful general result: the Principle of Computational Equivalence, which leads to computational irreducibility. And this result also carries over to multicomputational systems. But in multicomputational systems—which basically inevitably have to be sampled by an observer—there’s an additional result. that from the fundamental structure of the system (and the observer) there’s a certain amount of computational reducibility, which leads to certain specific overall laws of behavior. We might have thought that as we made the underlying structure of models more complicated—going from the ordinary computational paradigm to the multicomputational one—we’d inevitably have less to say about how systems generally behave. But actually—basically because of the interplay of the observer with the fundamental structure of the system—it’s the exact opposite. And that’s very important when it comes to theoretical science. Because it means that systems that seemed like they would show only unreachably complex behavior can actually have features that are described by definite overall laws that are potentially within reach even of the mathematical paradigm. Or, in other words, if one analyzes things correctly through the multicomputational paradigm, it’s potentially possible to find overall laws even in situations and fields where this seemed hopeless before.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.4 Leveraging Ideas from Physics : The multicomputational paradigm is something that’s emerging from our Physics Project, and from thinking about fundamental physics. But one of the most powerful things about having a general paradigm for theoretical science is that it implies a certain unity across different areas of science—and by providing a common framework it allows results and intuitions developed in one area to be transferred to others. So with its roots in fundamental physics the multicomputational paradigm immediately gets to leverage the ideas and successes of physics—and in effect use them to illuminate other areas. But just how does the multicomputational paradigm work in physics? And how did it even arise there? Well, it’s not something that the traditional mathematical approach to physics would readily lead one to. And instead what basically happened is that having seen how successful the computational paradigm was in studying lots of kinds of systems I started wondering whether something like it might apply to fundamental physics. It was fairly clear, though, that the ordinary computational paradigm—especially with its “global” view of time—wasn’t a great match for what we already knew about things like relativity in physics. But the pivotal idea that eventually led inexorably to the multicomputational paradigm was a hint from the computational paradigm about the nature of space. The traditional view in physics had been that space is something continuous, that serves just as a kind of “mathematical source of coordinates”. But in the computational paradigm one tends to imagine that everything is ultimately made of discrete computational elements. So in particular I began to think that this might be true of space. But how would the elements of space behave? The computational approach would suggest that there must be “finitely specifiable” rules—that effectively define “update events” involving limited numbers of elements of space. But right here is where the multicomputational idea comes in. Because inevitably—across all the elements of space in our universe—there must be a huge number of different ways these updating events could be applied. And the result is that there is no just one unique “computational history”—but instead a whole multiway system with different threads of history for different sequences of updating events. As we’ll discuss later, it’s the updating events—and the relations between them—that are in a sense really the most fundamental things in the multicomputational paradigm. But in understanding the multicomputational paradigm, and its way of representing fundamental physics, it’s helpful to start instead by thinking about what the updating events act on, or, in effect, the “data structure of the universe”. A convenient way to set this up is to imagine that the universe—or, in particular, space and everything in it—is defined by a large number of relations between the elements of space. Representing each element of space by an integer, one might have a collection of (in this case, binary) relations like which can in turn be thought of as a defining a graph (or, in general a hypergraph): But now imagine a rule like: or, stated pictorially, that specifies what updating events should occur. There are in general many different places where a rule like this can be applied to a given hypergraph: So—in a multicomputational fashion—we can define a multiway graph to represent all the possibilities (here starting from {{0,0},{0,0}}): In our model of fundamental physics, the presence of many different branching and merging paths is a reflection of quantum mechanics—with every path in effect representing a history for the universe. But to get at least some idea of “what the universe does” we can imagine following a particular path, and seeing what hypergraphs are generated: And the concept is that after a large number of steps of such a process we’ll get a recognizable representation of an “instantaneous state of space” in the universe. But what about time? Ultimately it’s the individual updating events that define the progress of time. Representing updating events by nodes, we can now draw a causal graph that shows the “causal relationships” between these updating events—with each edge representing the fact that the “output” from one event is being “consumed” as “input” by another event: And as is characteristic of the multicomputational paradigm this causal graph reflects the fact that there are many possible sequences in which updating events can occur. But how does this jibe with our everyday impression that a definite sequence of things happen in the universe? The basic point is that we don’t perceive the whole causal graph in all its detail. Instead, as computationally bounded observers, we just pick some particular reference frame from which to perceive what’s going on. And this reference frame defines a sequence of global “time slices” such as: Each “time slice” contains a collection of events that—with our reference frame—we take to be “happening simultaneously”. And we can then trace the “steps in the evolution of the universe” by seeing the results of all updating events in successive time slices: But how do we determine what reference frame to use? The underlying rule determines the structure of the causal graph, and what event can follow what other one. But it still allows huge freedom in the choice of reference frame—in effect imposing only the constraint that if one event follows another, then these events must appear in that order in the time slices defined by the reference frame: In general each of these different choices of reference frame will lead to a different sequence of “instantaneous states of space”. And in principle one could imagine that some elaborately chosen reference frame could lead to arbitrarily pathological perceived behavior. But in practice there is an important constraint on possible reference frames: as computationally bounded observers we are limited in the amount of computational effort that we can put into the construction of the reference frame. And in general to achieve a “pathological result” we’ll typically have to “reverse engineer” the underlying computational irreducibility of the system—which we won’t be able to do with a reference frame constructed by a computationally bounded observer. (This is directly analogous to the result in the ordinary computational paradigm that computationally bounded observers effectively can’t avoid perceiving the validity of the Second Law of Thermodynamics.) So, OK, what then will an observer perceive in a system like the one we’ve defined? With a variety of caveats the basic answer is that in the limit of a “sufficiently large universe” they’ll perceive average behavior that’s simple enough to describe mathematically, and specifically to describe as following Einstein’s equations from general relativity. And the key point is that this is in a sense a generic result (a bit like the gas laws in thermodynamics) that’s independent of the details of the underlying rule. But there’s more to this story. We’ll talk about it a bit more formally in the next section. But the basic point is that so far we’ve just talked about picking reference frames in a “spacetime causal graph”. But ultimately we have to consider the whole multiway graph of all possible sequences of update events. And then we have to figure out how an observer can set up some kind of reference frame to give them a perception of what’s going on. At the core of the concept of a reference frame is the idea of being able to treat certain things (typically events) as somehow “equivalent”. In the case of the causal graphs we’ve discussed so far, what we’re doing is to treat certain events as equivalent in the sense that they can be viewed as happening “in the same time slice” or effectively “simultaneously”. But if we just pick two events at random, there’s no guarantee that it’ll be consistent to consider them to be in the same time slice. In particular, if one event causally depends on another (in the sense that its input requires output from the other), then it can only occur in a later time slice. And in this situation (which corresponds to one event being reachable from the other by following directed edges in the causal graph) we can say that these events are “timelike separated”. Similarly, if two events can occur in the same time slice, we can say that they are “spacelike separated”. And in the language of relativity, this means that our “time slices” are spacelike hypersurfaces in spacetime—or at least discrete analogs of them. So what about with the full multiway graph? We can look at every event that occurs in every state in the multiway graph. And there are then basically three kinds of separation between events. There can be timelike separation, in the sense that one event causally depends on another. There can be spacelike separation, in the sense that different events occur in different parts of space that are not causally connected. And then there’s a third case, which is that different events can occur on different branches of the multiway graph—in which case we say that they’re branchlike separated. And in general when we pick a reference frame in the full multiway system, we can have time slices that contain both spacelike- and branchlike-separated events. What’s the significance of this? Basically, just as spacelike separation is associated with the concept of ordinary space, branchlike separation is associated with a different kind of space, that we call branchial space. With a multiway graph of the kind we’ve drawn above (in which every node represents a possible “complete state of the universe”), we can investigate “pure branchial space” by looking at time slices in the graph: For example, we can construct “branchial graphs” by looking at which states are connected by having immediate common ancestors. And in effect these branchial graphs are the branchial-space analogs of the hypergraphs we’ve constructed to represent the instantaneous state of ordinary space. But now, instead of representing ordinary space—with features like general relativity and gravity—they represent something different: they represent a space of quantum states, with the branchial graph effectively being a map of quantum entanglements. But to define a branchial graph, we have to pick the analog of a reference frame: we have to say what branchlike-separated events we consider to happen “at the same time”. In the case of spacelike-separated events it’s fairly easy to interpret that what we get from a reference frame is a view of what’s happening throughout space at a particular time. But what’s the analog for branchlike-separated events? In effect what we’re doing when we make a reference frame is to treat as equivalent events that are happening “on different branches of history”. At first, that may seem like a very odd thing to do. But the thing to understand is that as entities embedded in the same universe that’s generating all these different branches of history, we too are branching. So it’s really a question of how a “branching brain” will perceive a “branching universe”. And that depends on what reference frame (or “quantum observation frame”) we pick. But as soon as we insist that we maintain a single thread of experience, or, equivalently, that we sequentialize time, then—together with computational boundedness—this puts all sorts of constraints on the reference frames we pick. And just as in the case of ordinary space, the result is that it ultimately seems to be possible to give a fairly simple—and essentially mathematical—description of what the observer will perceive. And the answer is that it basically appears to correspond to quantum mechanics. But there’s actually more to it. What we get is a kind of generic multicomputational result—that doesn’t depend on the details of underlying rules or particular choices of reference frames. Structurally it’s basically the same result as for ordinary space. But now it’s interpreted in terms of branchial space, quantum states, and so on. And what was interpreted as the geodesic equation of general relativity now essentially gets interpreted as the path integral of quantum mechanics. In a sense it’s then a basic consequence of the multicomputational nature of fundamental physics that quantum mechanics is the same theory as general relativity—though operating in branchial space rather than ordinary space. There are important implications here for physics. But there are also general implications for all multicomputational systems. Because the sophisticated definitions and phenomena of both general relativity and quantum mechanics we can now expect will have analogs in any system that can be modeled in a multicomputational way, whatever field of science it may come from. So, later, when we talk about the application of the multicomputational paradigm to other fields, we can expect to talk and reason in terms of things we know from physics. So we’ll be able to bring in light cones, inertial frames, time dilation, black holes, the uncertainty principle, and much more. In effect, the common use of the multicomputational paradigm will allow us to leverage the development of physics—and its status as the most advanced current area of theoretical science—to “physicalize” all sorts of other areas, and shed new light on them. As well, of course, as taking ideas and intuition from other areas (including ones much closer to everyday experience) and “applying them back” to physics.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.5 The Formal Structure of Multicomputation : In the previous section, we discussed how the multicomputational paradigm plays out in the particular case of fundamental physics. And in many ways, physics is probably a fairly typical application of the paradigm. But there are some special features it has that add complication, though also add concreteness. So what are the ultimate foundations of the multicomputational paradigm? At its core, I think it’s fair to say that the paradigm is about events and their relationships—where the events are defined by some kind of rules. What happens in an event? It’s a bit like the application of a function. It takes some set of “expressions” or “tokens” as input, and returns some other set as output. But what starts making a nontrivial multiway graph is when some states that are generated in different ways end up being the same, so that they get merged in the graph. As an example, consider the rule The multiway graph produced in this case is: where now we see merging even at the top of the graph, associated, for example, with the equivalence of 1 + 1 + 1 and 2 × 1 + 1. And as we can see, even with this very simple rule, the multiway graph is not so simple. But there’s still an important simplifying feature in the system we’re considering—that affects causal dependence: in all its events complete states (here integers) are used as input and output. But in a string-based system (say with a rule like A → BBB, BB → A) the situation is different. Because now the events can act on just part of the string: And it’s the same when we use hypergraphs—as in our models of fundamental physics. The events don’t typically apply to complete hypergraphs, but instead to subhypergraphs within them. But let’s look a bit more carefully at the string case above. When we see different update events for a given string, we can identify two different cases. The first is a case like where the updates don’t overlap, and the second is a case like where they do. And what we’ll find is that in the first case we can consider the events to be purely spacelike separated, while in the second they are also branchlike separated. The full multiway graph above effectively shows all possible histories for our system—obtained by running all possible update events on each state (i.e. string) that is generated. But what if we choose for example just to use the first update event found in a left-to-right scan of each state (so we’ve got a “sequential substitution system”)? Then we’d get a “single-way” graph with no branching: As another “evaluation strategy” we could scan the string at each step, applying all updates that don’t overlap: Both the results we’ve just got are subgraphs of the full multiway graph. But they both have the feature that they effectively just yield a single sequence of states for the system. In the first case this is obvious. In the second case there are little “temporary branchings” but they always merge back into a single state. And the reason for this is that with the evaluation strategy we’ve used, we only ever get spacelike-separated events, so there are no “true multiway branches”, as would be generated by branchlike-separated events. But even though ultimately there’s just a “single branch of history” there’s a “shadow” of the presence of other branches visible in the nontrivial causal graph that shows causal relationships between updating events: So what about our models of physics? In exploring them it’s often convenient not to track the whole multiway system but instead just to look at the results from a particular “evaluation strategy”. In a hypergraph there’s no obvious “scan order”, but we still often use a strategy that—like our second string strategy above—attempts to “do whatever can be done in parallel”. Inevitably, though, there’s a certain arbitrariness to this. But it turns out there’s a more “principled” way to set things up. The basic idea is not to think about complete states (like strings or hypergraphs) but instead just to think separately about the “tokens” that will be used as inputs and outputs in events. And—giving yet more evidence that even though hypergraphs may be hard for us humans to handle, there’s great naturalness to them—it’s considerably easier to do this for hypergraphs than for strings. And the basic reason is that in a hypergraph each token automatically comes “appropriately labeled”. The relevant tokens for hypergraphs are hyperedges. And in a rule like we see that two “hyperedge tokens” are used as input, and four “hyperedge tokens” are generated as output. And the point is that when we have a hypergraph like that corresponds for example to: we can just think of this as an unordered “sea” (or multiset) of hyperedges, where each event will just pick up some pair of hyperedges that match the pattern {{x, y}, {y, z}}. But what does it mean to “match the pattern”? x, y, z can each correspond to any node of the hypergraph (i.e. for our model of physics, any atom of space). But the key constraint is that the two instances of y must refer to the same node. If we tried to do the same thing for strings, it’d be considerably more difficult. Because then the relevant tokens would be individual characters in the string. But whereas in a hypergraph every token is a hyperedge that can be identified from the uniquely named nodes it contains, every A or every B in a string is usually thought of as just being the same—giving us no immediate way to identify distinct tokens in our system. But assuming we have a way to identify distinct tokens, we can consider representing the evolution of our system just in terms of events applied to tokens (or what we can call a “token-event graph”). This is going to get a bit complicated. But here’s an example of how it works for the hypergraph system we’ve just shown. Each blue node is a token (i.e. a hyperedge) and each yellow node is an event: What’s going on here? At each step shown, there’s a token (i.e. blue node) for every hyperedge generated at that step. Let’s compare with the overall sequence of hypergraphs: The initial state contains two hyperedges, so there are two tokens at the top of the token-event graph. Both these hyperedges are “consumed” by the event associated with applying the rule—and out of that event come four new hyperedges, represented by the four tokens on the next step. Let’s look in a little more detail at what’s happening. Here is the beginning of the token-event graph, annotated with the actual hyperedges represented by each token (the numbers in the hyperedges are the “IDs” assigned to the “atoms of space” they involve): At the first step our rule {{x,y},{y,z}} → {{w,y},{y,z},{z,w},{x,w}} consumes the hyperedges {0,0} and {0,0} and generates four new hyperedges. (Note that the {0,0} on step 2 is considered a “new hyperedge”, even though it happens to have the same content as both hyperedges on step 1; more about this later.) At the second step, the rule consumes {1,0} and {0,0}. And at the third step, there are two invocations of the rule (i.e. two events), each consuming two hyperedges, and generating four new ones. And looking at this one might ask “Why did the second event consume {1,0} and {0,0} rather than, say, {1,0} and one of the {0,1} tokens?” Well, the answer is that the token-event graph we’re showing is just for a specific possible history, obtained with a particular “evaluation strategy”—and this is what that strategy picked to do. But it’s possible to extend our token-event graph to show not just what can happen for a particular history, but for all possible histories. In effect what we’re getting is a finer-grained version of our multiway graph, where now the (blue) nodes are not whole states (i.e. hypergraphs) but instead just individual tokens (i.e. hyperedges) from inside those states. Here’s the result for a single step: There are two possible events because the two initial hyperedges given here can in effect be consumed in two different orders. Continuing even one more step things rapidly get significantly more complicated: Let’s compare this with our ordinary multiway graph (including events) for the same system: Why is this so much simpler? First, it’s because we’ve collected the individual tokens (i.e. hyperedges) into complete states (i.e. hypergraphs)—“knitting them together” by seeing which atoms they have in common. But we’re doing something else as well: even if hypergraphs are generated in different ways, we’re conflating them whenever they’re “the same”. And for hypergraphs our definition of “being the same” is that they’re isomorphic, in the sense that we can transform them into each other just by permuting node labels. Note that if we don’t conflate isomorphic hypergraphs the multiway graph we get is: which corresponds much more obviously to the token-event graph above. When we think about multicomputational systems in general, the conflation of “identical” (say by isomorphism) states is in a sense the “lowest-level act” of an observer. The “true underlying system” might in some sense “actually” be generating lots of separate, identical states. But if the observer can’t tell them apart then we might as well say they’re all “the same state”. (Of course, when there are different numbers of paths that lead to different states, this can affect the weightings of these different states—and indeed in our model of physics this is where the different magnitudes of quantum amplitudes for different states come from.) It seems natural and obvious to conflate hypergraphs if they’re isomorphic. But actual observers (say humans observing the physical universe) typically conflate much, much more than that. And indeed when we say that we’re operating in some particular reference frame we’re basically defining potentially huge collections of states to conflate. But there’s actually also a much lower level at which we can do conflation. In the token-event graphs that we’ve looked at so far, every token generated by every event is shown as a separate node. But—as the labeled versions of these graphs make clear—many of these tokens are actually identically the same, in the sense that they’re just direct copies created by our way of computing (and rendering) the token-event graph. So what about conflating all of these—or in effect “deduplicating” tokens so that we have just one unique shared representation of every token, regardless of how many times or where it appears in the original graph? Here’s the result after doing this for the 2-step version of the token-event graph above: This deduplicated-token-event graph in essence records every “combinatorially possible” “distinct event” that yields a “transition” between distinct tokens. But while sharing the representation of identical tokens makes the graph much simpler, the graph now no longer has a definite notion of “progress in time”: there are edges “going both up and down”, sometimes even forming loops (i.e. “closed timelike curves”). So how can this graph represent the actual evolution of the original system with a particular evaluation strategy (or, equivalently, as “viewed in a particular reference frame”)? Basically what we need are some kind of “markers” that move around the graph from “step to step” to indicate which tokens are “reached” at each step. And doing this, this is what we get for the “standard evaluation strategy” above: In a sense a deduplicated token-event graph is the ultimate minimal invariant representation of a multicomputational system. (Note that in physics and elsewhere the graph is typically infinite.) But any particular observer will effectively make some kind of sampling of this graph. And in working out this sampling we’re going to encounter issues about knitting together states and reference frames—that are ultimately equivalent to what we saw before from our earlier perspective on multiway systems. (Note that if we had a deduplicated token-event graph with markers that is finite then this would basically be a Petri net, with decidable reachability results, etc. But in most relevant cases, our graphs won’t be finite.) So although token-event graphs—even in deduplicated form—don’t ultimately avoid the complexities of other representations of multicomputational systems, they do make some things easier to discuss. For example, in a token-event graph there’s a straightforward way to read off whether two events are branchlike or spacelike separated. Consider the “all histories” token-event graph we had before: To figure out the type of separation between events, we just need to look at their first common ancestor. If it’s a token, that means the events are branchlike separated (because there must have been some “ambiguity” in how the token was transformed). But if the common ancestor is an event, that means the events we’re looking at are spacelike separated. So, for example, events 1 and 2 here are branchlike separated, as are 4 and 5. But events 4 and 9 are spacelike separated. Note that if instead of looking at an “all histories” token-event graph, we restrict to a single history then there will only be spacelike- (and timelike-) separated events: A token-event graph is in a sense a lowest-level representation of a multicomputational system. But when an observer tries to “see what’s going on” in the system, they inevitably conflate things together, effectively perceiving only certain equivalence classes of the lowest-level elements. Thus, for example, an observer might tend to “knit together” tokens into states, and pick out particular histories or particular sequences of time slices—corresponding to using what we can call a certain “reference frame”. (In mathematical terms, we can think of particular histories as like fibrations, and sequences of time slices as like foliations. ) And in studying multicomputational systems of different types a key question is what kinds of reference frames are “reasonable” based on some general model for the observer. And one almost inevitable constraint is that it should only require bounded computational effort to construct the reference frame. Our Physics Project then suggests that in appropriate large-scale limits specific structures like general relativity and quantum mechanics should then emerge. And it seems likely that this is a general and very powerful result that’s essentially inexorably true about the limiting behavior of any multicomputational system. But if so, the result represents an elaborate—and unprecedented—interweaving of fundamentally computational and fundamentally mathematical concepts. Maybe it’ll be possible to use a generalization of category theory as a bridge. But it’s going to involve not only discussing ways in which operations can be applied and composed, but also what the computational costs and constraints are. And in the end computational concepts like computational reducibility are going to have to be related to mathematical concepts like continuity—I suspect shedding important new light all around. Before closing our discussion of the formalism of multicomputation there’s something perhaps still more abstract to discuss—that we can call “rulial multicomputation”. In ordinary multicomputation we’re interested in seeing what happens when we follow certain rules in all possible ways. But in rulial multicomputation we go a step further, and also ask about following all possible rules. One might think that following all possible rules would just “make every possible thing happen”—so there wouldn’t be much to say. But the crucial point is that in a (rulial) multiway system, different rules can lead to equivalent results, yielding a whole elaborately entangled structure of possible states and events. But at the level of the token-event formalism we’ve discussed above, rulial multicomputation in some sense just “makes events diverse”, and more like tokens. For in a rulial multiway system, there are lots of different kinds of events (representing different rules)—much as there are different kinds of tokens (containing, for example, different underlying elements or atoms). And if we look at different events in a rulial multiway system, there is now another possible form of separation between them: in addition to being timelike, spacelike or branchlike separated, the events can also be rulelike separated (i.e. be based on different rules). And once again we can ask about an observer “parsing” the (rulial) multiway system, and defining a reference frame that can, for example, treat events in a single “rulelike hypersurface” as equivalent. I’ve discussed elsewhere the rather remarkable implications of rulial multiway systems for our understanding of the physical (and mathematical) universe, and its fundamental formal necessity. But here the main point to make is that the presence of many possible rules doesn’t fundamentally affect the formalism for multicomputational systems; it just requires the observer to define yet more equivalences to reduce the “raw multiway behavior” to something computationally simple enough to parse. And although one might have thought that adding the concept of multiple rules would just make everything more complicated, I won’t be surprised if in the end the greater “separation” between “raw behavior” and what the observer can perceive will actually make it easier to derive robust general conclusions about overall behavior at the level of the observer.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.6 Physicalized Concepts in Multicomputation : From the basic definitions of multicomputation it’s hard to have much intuition about how multicomputational systems will work. But knowing how multicomputation works in our model of fundamental physics immediately gives us not only powerful intuition, but also all sorts of metaphors and language for describing multicomputation in pretty much any possible setting. As we saw in the previous section, at the lowest level in any multicomputational system there are what we can call (in correspondence with physics) “events”—that represent individual applications of rules, “progressing through time”. We can think of these rules as operating on “tokens” of some kind (and, yes, that’s a term from computation, not physics). And what these tokens represent will depend on what the multicomputational system is supposed to be modeling. Often (as in our Physics Project) the tokens will consist of combinations of elements—where the same elements can be shared across different tokens. (In the case of our Physics Project, we view the elements as “atoms of space”, with the tokens representing connections among them.) The sharing of elements between tokens is one way in which the tokens can be “knitted together” to define something like space. But there is another, more robust way as well: whenever a single event produces multiple tokens it effectively defines a relation between those tokens. And the map of all such relations—which is essentially the token-event graph—defines the way that different tokens are knitted together into some kind of generalized spacetime structure. At the level of individual events, ideas from the theory and practice of computation are useful. Events are like functions, whose “arguments” are incoming tokens, and whose output is one or more outgoing tokens. The tokens that exist in a certain “time slice” of the token-event graph together effectively represent the “data structure” on which the functions are acting. (Unlike in basic sequential programming, however, the functions can act in parallel on different parts of the data structure.) The whole token-event graph then gives the complete “execution history” of how the functions act on the data structure. (In an ordinary computational system, this “execution history” would essentially form a single chain; a defining feature of a true multicomputational system is that this history instead forms a nontrivial graph.) In understanding the analogy with our everyday experience with physics, we’re immediately led to ask what aspect of the token-event graph corresponds to ordinary, physical space. But as we’ve discussed, the answer is slightly complicated. As soon as we set up a foliation of the token-event graph, effectively dividing it into a sequence of time slices, we can say that the tokens on each slice correspond to a certain kind of space, “knitted together” by the entanglements of the tokens defined by their common ancestry in events. But the kind of space we get is in general something beyond ordinary physical space—effectively something we can call “multispace”. In the specific setup of our Physics Project, however, it’s possible to define at least in certain limits a decomposition of this space into two components: one that corresponds to ordinary physical space, and one that corresponds to what we call branchial space, that is effectively a map of entangled possible quantum states. In multicomputational systems set up in different ways, this kind of decomposition may work differently. But given our everyday intuition—and mathematical physics knowledge—about ordinary physical space it’s convenient first to focus on this in describing the general “physicalization” of multicomputational systems. In our Physics Project ordinary “geometrical” physical space emerges as a very large-scale limit of slices of the token-event graph that can be represented as “spatial hypergraphs”. In the Physics Project we imagine that at least in the current universe, the effective dimension of the spatial hypergraph (measured, for example, through growth rates of geodesic balls) corresponds to the observed 3 dimensions of physical space. But it’s important to realize that the underlying structure of multicomputation doesn’t in any way require such a “tame” limiting form for space—and in other settings (even branchial space in physics) things may be much wilder, and much less amenable to present-day mathematical characterization. But the picture in our Physics Project is that even though there is all sorts of computationally irreducible—and seemingly random—underlying behavior, physical space still has an identifiable large-scale limiting structure. Needless to say, as soon as we talk about “identifiable structure” we’re implicitly assuming something about the observer who’s perceiving it. And in seeing how to leverage intuition from physics, it’s useful to discuss what we can view as the simpler case of thermodynamics and statistical mechanics. At the lowest level something like a gas consists of large numbers of discrete molecules interacting according to certain rules. And it’s almost inevitable that the detailed behavior of these molecules will show computational irreducibility—and great complexity. But to an observer who just looks at things like average densities of molecules the story will be different—and the observer will just perceive simple laws like diffusion. And in fact it’s the very complexity of the underlying behavior that leads to this apparent simplicity. Because a computationally bounded observer (like one who just looks at average densities) won’t be able to do more than just read the underlying computational irreducibility as being like “simple randomness”. And this means that for such an observer it’s going to be reasonable to model the overall behavior by using mathematical concepts like statistical averaging, and—at least at the level of that observer—to describe the system as showing computationally reducible behavior represented, say, by the diffusion equation. It’s interesting to note that the emergence of something like diffusion depends on the presence of certain (identifiable) underlying constraints in the system—like conservation of the number of molecules. Without such constraints, the underlying computational irreducibility would lead to “pure randomness”—and no recognizable larger-scale structure. And in the end it’s the interplay of identifiable underlying constraints with identifiable features of the observer that leads to identifiable emergent computational reducibility. And it’s very much the same kind of thing with multicomputational systems—except that the “identifiable constraints” are much more abstract ones having to do with the fundamental structure of multicomputation. But much as we can say that the detailed computationally irreducible behavior of underlying molecules leads to things like large-scale fluid mechanics at the level of practical (“coarse-grained”) observers, so also we can say that the detailed computationally irreducible behavior of the hypergraph that represents space leads to the large-scale structure of space, and things like Einstein’s equations. And the important point is that because the “constraints” in multicomputational systems are generic features of the basic abstract structure of multicomputation, the emergent laws like Einstein’s equations can also be expected to be generic, and to apply with appropriate translation to all multicomputational systems perceived by observers that operate at least somewhat like the way we operate in perceiving the physical universe. Any system in which the same rules get applied many times must have a certain ultimate uniformity to its behavior, manifest, for example, in the “same laws” applying “all over the system”. And that’s why, for example, we’re not surprised that physical space seems to work the same throughout the physical universe. But given this uniformity, how do there come to be any identifiable features or “places” in the universe, or, for that matter, in other kinds of systems that are constructed in similar ways? One possibility is just that the observer can choose to name things: “I’ll call this token ‘Tokie’ and then I’ll trace what happens, and describe the behavior of the universe in terms of the ‘adventures of Tokie’”. But as such, this approach will inevitably be quite limited. Because a feature of multicomputational systems is events are continually happening, consuming existing tokens and creating new ones. In physics terms, there is nothing fundamentally constant in the universe: everything in it (including space itself) is being continually recreated. So how come we have any perception of permanence in physics? The answer is that even though individual tokens are continually being created and destroyed, there are overall patterns that are persistent. Much like vortices in a fluid, there can for example be essentially topological phenomena whose overall structure is preserved even though their specific component parts are continually changed. In physics, those “topological phenomena” presumably correspond to things like elementary particles, with all their various elaborate symmetries. And it’s not clear how much of this structure will carry over to other multicomputational systems, but we can expect that there will be some kinds of persistent “objects”—corresponding to certain pockets of local computational reducibility. An important idea in physics is the concept of “pure motion”: that “objects” can “move around in space” and somehow maintain their character. And once again the possibility of this depends on the observer, and on what it means that their “character is maintained”. But we can expect that as soon as there is a concept of space in a multicomputational system there will also be a concept of motion. What can we say about motion? In physics, we can discuss how it will be perceived in different reference frames—and for example we define inertial frames that explore space and time differently precisely so as to “cancel out motion”. This leads to phenomena like time dilation, which we can view as a reflection of the fact that if an object is “using its computational resources to move in space” then it has less to devote to its evolution in time—so it will “evolve less in a certain time” than if it wasn’t moving. So if we can identify things like motion (and, to make it as simple as possible, things like inertial frames) in any multicomputational system, we can expect to see phenomena like time dilation—though potentially translated into quite different terms. What about phenomena like gravity? In physics, energy (and mass) act as a “source of gravity”. But in our models of physics, energy has a rather simple (and generic) interpretation: it is effectively just the “density of activity” in the multicomputational system—or the number of events in a certain “region of space”. Imagine that we pick a token in a multicomputational system. One question we can ask is: what is the shortest path through the token-event graph to get to some specific other token? There’ll be a “light cone” that defines “how far in space” we can get in a certain time. But in general in physics terms we can view the shortest path as defining a spacetime geodesic. And now there’s a crucial—but essentially structural—fact: the presence of activity in the token-event graph inevitably effectively “deflects” the geodesic. And at least with the particular setup of our Physics Project it appears that that deflection can be described (in some appropriate limit) by Einstein’s equations, or in other words, that our system shows the phenomenon of gravity. And once again, we can expect that—assuming there is any similar kind of notion of space, or similar character of the observer—a phenomenon like gravity will also show up in other multicomputational systems. Once we have gravity, what about phenomena like black holes? The concept of an event horizon is immediately something quite generic: it is just associated with disconnection in the causal graph, which can potentially occur in basically any multicomputational system. What about a spacetime singularity? In the most familiar kind of singularity in physics (a “spacelike singularity” of the kind that appears at the center of a non-rotating black hole spacetime), what fundamentally happens is that there is a piece of the token-event graph to which no rules apply—so that in essence “time ends” there. And once again, we can expect that this will be a generic phenomenon in multicomputational systems. But there’s more to say about this. In general relativity, the singularity theorems say that when there’s “enough energy or mass” it’s inevitable that a singularity will be formed. And we can expect that the same kind of thing will happen in any multicomputational system, though potentially it’ll be interpreted in very different terms. (By the way, the singularity theorems implicitly depend on assumptions about the observer and about what “states of the universe” they can prepare, and these may be different for other kinds of multicomputational systems.) It’s worth mentioning that when it comes to singularities, there’s a computational characterization that may be more familiar than the physics one (not least since, after all, we don’t have direct experience of black holes). We can think of the progress of a multicomputational system through time as being like a process of evaluation in which rules are repeatedly applied to transform whatever “input” was given. In the most familiar case in physics, this process will just keep going forever. But in the more familiar case in practical computing, it will eventually reach a fixed point representing the “result of the computation”. And this fixed point is the direct analog of a “time ends” singularity in physics. When we have a large multicomputational system we can expect that—like in physics—it will seem (at least to appropriate observers, etc.) like an approximate continuum of some kind. And then it’s essentially inevitable that there will be a whole collection of rather general “local” statements about the behavior that can be made. But what if we look at the multicomputational system as a whole? This is the analog of studying cosmology in physics. And many of the same concepts can be expected to apply, with, for example, the initial conditions for the multicomputational system playing the role of the Big Bang in physics. In the history of physics over the past century or so three great theoretical frameworks have emerged: statistical mechanics, general relativity and quantum mechanics. And when we look at multicomputational systems we can expect to get intuition—and results—from all three of these. So what about quantum mechanics? As I mentioned above, quantum mechanics is—in our model of physics—basically just like general relativity, except played out not in ordinary physical space, but instead in branchial space. And in many ways, branchial space is a more immediate kind of space to appear in multicomputational systems than physical space. But unlike physical space, it’s not something about which we have everyday experience, and instead to think about it we tend to have to rely on the somewhat elaborate traditional formalism of quantum mechanics. A key question about branchial space both in physics and in other multicomputational systems is how it can be coordinatized (and, yes, that’s inevitably a question about observers). In general the issue of how to put meaningful “numerical” coordinates on a very “non-numerical space” (where the “points of the space” are for example tokens corresponding to strings or hyperedges or whatever) is a difficult one. But the formalism of quantum mechanics makes for example the suggestion of thinking in terms of complex numbers and phases. The spaces that arise in multicomputational systems can be very complicated, but it’s rather typical that they can be thought of as somehow “curved”, so that, for example, “parallel” lines (i.e. geodesics) don’t stay a fixed distance apart, and that “squares” drawn out of geodesics won’t close. And in our model of physics, this kind of phenomenon not only yields gravity in physical space, but also yields things like the uncertainty principle when applied to branchial space. We might at first have imagined that a theory of physics would be specific to physics. But as soon as we imagine that physics is multicomputational then that fact alone leads to a robust and inexorable structure that should appear in any other multicomputational system. It may be complicated to know quite what the detailed translations and interpretations are for other multicomputational systems. But we can expect that the core phenomena we’ve identified in physics will somehow be reflected there. So that through the common thread of multicomputation we can leverage the tower of successes in physics to shed light on all sorts of systems in all sorts of fields.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.7 Potential Application Areas : We’ve talked about the nature of the multicomputational paradigm, and about its application in physics. But where else can it be applied? Already in the short time I’ve been thinking directly about this, I’ve identified a remarkable range of fields that seem to have great potential for the multicomputational paradigm, and where in fact it seems quite conceivable that by using the paradigm one might be able to successfully unlock what have been long-unresolved foundational problems. Beginning a few decades ago, the computational paradigm also helped shed new light on the foundations of all sorts of fields. But typically its most important message has been: “Great and irreducible complexity arises here—and limits what we can expect to predict or describe”. But what’s particularly exciting about the multicomputational paradigm is that it potentially delivers a quite different message. It says that even though the underlying behavior of a system may be mired in irreducible complexity, it’s still the case that those aspects of the system perceived by an observer can show predictable and reducible behavior. Or, in other words, that at the level of what the observer perceives, the system will seem to follow definite and understandable laws. But that’s not all. As soon as one assumes that the observer in a multicomputational system is enough “like us” to be computationally bounded and to “sequentialize time” then it follows that the laws they will perceive will inevitably be some kind of translated version of the laws we’ve already identified in fundamental physics. Physics has always been a standout field in its ability to deliver laws that have a rich (typically mathematical) structure that we can readily work with. But with the multicomputational paradigm there’s now the remarkable possibility that this feature of physics could be transported to many other fields—and could deliver there what’s in many cases been seen as a “holy grail” of finding “physics-like” laws. One might have thought that what would be required most would be to do a successful “reduction” to an accurate model of the primitive parts of the system. But actually what the multicomputational paradigm indicates is that there’s a certain inexorability to what happens, independent of those details. The challenge, though, is to figure out what an “observer” of a certain kind of system will actually perceive. In other words, successfully finding overall laws isn’t so much about applying reductionism to the system; it’s more about understanding how observers fit together the details of the system to synthesize their perception of it. So what kinds of systems can we expect to describe in multicomputational terms? Basically any kind of system where there are many component parts that somehow “operate independently in parallel”—interacting only through certain “events”. And the key idea is that there are many possible detailed histories for the system—but in the multicomputational paradigm we look at all of them together, thereby building up a structure with inexorable properties, at least as perceived by certain general kinds of observers. In areas like statistical physics it’s been common for a century to think about “ensembles of possible states” for a system. But what’s different about the multicomputational paradigm is that it’s not just looking “statically” at “possible states”; instead it’s “taking a bigger gulp”, and looking at all possible whole histories for the system, essentially developing through time. And, yes, a slice at a particular time will show some ensemble of possible states—but they’re states generated by the entangled possible histories of the system, not just states “statically” and combinatorially generated from the possible configurations of the system. OK, so what are some areas to which the multicomputational paradigm can potentially be applied? There are many. But among the examples I’ve at least begun to investigate are metamathematics, molecular biology, evolutionary biology, molecular computing, neuroscience, machine learning, immunology, linguistics, economics and distributed computing. So how can one start developing a multicomputational model in a particular area? Ultimately one wants to see how the structure and behavior of the system can be broken down into elementary “tokens” and “events”. The network of events will define some way in which the histories of tokens are entangled, and in which the tokens are effectively “knitted together” to define something that in some limiting sense can be interpreted as some kind of space. Often it’ll at first seem quite unclear that anything significant can be built up from the things one identifies as tokens and events—and the emergent space may seem more familiar, as it does in the case of physical space in our model of physics. OK, so what might the tokens and events be in particular areas? I’m not yet sure about most of these. But here are a few preliminary thoughts: It’s important to emphasize that the multicomputational paradigm is at its core not about particular histories (say particular interactions between organisms or particular words spoken) but about the evolution of all possible histories. And in most cases it won’t have things to say about particular histories. But instead what it will describe is what an observer sampling the whole multicomputational process will perceive. And in a sense the nub of the effort of using the multicomputational paradigm to find new laws in new fields is to identify just what it is that one should be looking at, or in effect what one would think an observer should do. Imagine one’s looking at the behavior of a gas. Underneath there’s all sorts of irreducible complexity in the particular motions of the molecules. But if we consider the “correct” kind of observer, we’ll just sample the gas at a level where they’ll perceive overall laws like the diffusion equation or the gas laws. And in the case of a gas we’re immediately led to that “correct” kind of observer, because it’s what we get with our usual human sensory perception. But the question is what the appropriate “observer” for the analog of molecules in metamathematics or linguistics might be. And if we can figure that out, we’ll potentially have overall laws—like diffusion or fluid dynamics—that apply in these quite different fields.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.8 Metamathematics : Let’s start by talking about perhaps the most abstract potential application area: metamathematics. The individual “tokens of mathematics” can be mathematical statements, written in some symbolic form (as they would be in the Wolfram Language). In a sense these mathematical statements are like the hyperedges of our spatial hypergraph in physics: they define relations between elements, which in the case of physics are “atoms of space” but in the case of mathematics are “literal mathematical objects”, like the number 1 or the operation Plus (or at least single instances of them). Now we can imagine that the “state of mathematics” at some particular time in its development consists of a large number of mathematical statements. Like the hyperedges in the spatial hypergraph for physics, these mathematical statements are knitted together through their common elements (two mathematical statements might both refer to Plus, just as two hyperedges might both refer to a particular atom of space). What is the “evolution of mathematics” like? Basically we imagine that there are laws of inference that take, say, two mathematical statements and deduce another one from them, either using something like structural substitution, or using some (symbolically defined) logical principle like modus ponens. The result of repeatedly applying a law of inference in all possible ways is to build up a multiway graph of—essentially—what statements imply what other ones, or in other words, what can be proved from what. But what does a human mathematician perceive of all this? Most mathematicians don’t operate at the level of the raw proof graph and individual raw formalized mathematical statements. Instead, they aggregate together the statements and their relationships to form more “human-level” mathematical concepts. In effect that aggregation can be thought of as choosing some “mathematical reference frame”—a slice of metamathematical space that can successfully be “parsed” by a human “mathematical observer”. No doubt there will be certain typical features of that reference frame; for example it might be set up so that things are “sufficiently organized” that “category theory works”, in the sense that there’s enough uniformity to be able to “move between categories” while preserving structure. There are both familiar and unfamiliar features of this emerging picture. There are the analog of light cones in “proof space” that define dependencies between mathematical results. There are geodesics that correspond to shortest derivations. There are regions of “metamathematical space” (the slices of proof space) that might have higher “densities of proofs” corresponding to more interconnected fields of mathematics—or more “metamathematical energy”. And as part of the generic behavior of multicomputational systems we can expect an analog of Einstein’s equations, and we can expect that “proof geodesics” will be “gravitationally attracted” to regions of higher “metamathematical energy”. In most areas of metamathematical space there will be “proof paths” that go on forever, reflecting the fact that there may be no path of bounded length that will reach a given statement, so that the question of whether that statement is present at all can be considered undecidable. But in the presence of large amounts of “metamathematical energy” there’ll effectively be a metamathematical black hole formed. And where there’s a “singularity in metamathematical space” there’ll be a whole collection of proof paths that just end—effectively corresponding to a decidable area of mathematics. Mathematics is normally done at the level of “specific mathematical concepts” (like, say, algebraic equations or hyperbolic geometry)—that are effectively the “populated places” (or “populated reference frames”) of metamathematical space. But by having a multicomputational model of the low-level “machine code of metamathematics” there’s the potential to make more general statements—and to identify what amount to general “bulk laws of metamathematics” that apply at least to the “metamathematical reference frames” used by human “mathematical observers”. What might these laws tell us? Perhaps they’ll say something about the homogeneity of metamathematical space and explain why the same structures seem to show up so often in different areas of mathematics. Perhaps they’ll say something about why the “aggregated” mathematical concepts we humans usually talk about can be connected without infinite paths—and thus why undecidability is so comparatively uncommon in mathematics as it is normally done. But beyond these questions about the “insides of mathematics”, perhaps we’ll also understand more about the ultimate foundations of mathematics, and what mathematics “really is”. It might seem a bit arbitrary to have mathematics be constructed according to some particular law of inference. But in direct analogy to our Physics Project, we can also consider the “rulial multiway system” that allows all possible laws of inference. And as I’ve argued elsewhere, the limiting object that we get for mathematics will be the same as for physics, connecting the question of why the universe exists to the “Platonic” question of whether mathematics “exists”.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.9 Chemistry / Molecular Biology : In mathematics, the tokens of our multicomputational model are abstract mathematical statements, and the “events between them” represent the application of laws of inference. In thinking about chemistry we can make a much more concrete multicomputational model: the tokens are actual individual molecules (represented say in terms of bonds) and the events are reactions between them. The token-event graph is then a “molecular-dynamics-level” representation of a chemical process. But what would a more macroscopic observer make of this? One “chemical reference frame” might combine all molecules of a particular chemical species at a given time. The result would be a fairly traditional “chemical reaction network”. (The choice of time slices might reflect external conditions; the number of microscopic paths might reflect chemical concentrations.) Within the chemical reaction network a “synthesis pathway” is much like a proof in mathematics: a path that leads from certain “inputs” in the network to a particular output. (And, yes, one can imagine “chemical undecidability” where it’s not clear if there’s any path of any length to make “this from that”.) A chemical reaction network is much like a multiway graph of the kind we’ve shown for string substitutions. And just as in that case we can define a branchial graph that describes relationships between chemical species associated with their “entanglement” through participation in reactions—and from which a kind of “chemical space” emerges in which different chemical species appear at different positions. There’s plenty to study at this “species level”. (As a simple example, small loops represent equilibria but larger ones can reflect the effect of protecting groups, or give signatures of autocatalysis.) But I suspect there’s even more to learn by looking at something closer to the underlying token-event graph. In standard chemistry, one typically characterizes the state of a chemical system at a particular time in terms of the concentrations of different chemical species. But ultimately there’s much more information in the whole token-event graph—for example about the entangled histories of individual molecules and the causal relationships between events that produced them (which at a physical level might be manifest in things like correlations in orientations or momenta of molecules). Does this matter, though? Perhaps not for chemistry as it’s done today. But in thinking about molecular computing it may be crucial—and perhaps it’s also necessary for understanding molecular biology. Processes in molecular biology today tend to be described—like chemical reactions—in terms of networks and concentrations of chemical species. (There are additional pieces having to do with the spatial structure of molecules and the possibility of “events at different places on a molecule”.) But maybe the whole “entanglement network” at the “token-event level” is also important in successfully capturing what amounts to the molecular-scale “chemical information processing” going on in molecular biology. Just as in genetics in the 1950s there was a crucial realization that information could be stored not just, say, in concentrations of molecules, but in the structure of a single molecule, so perhaps now we need to consider that information can be stored—and processed—in a dynamic network of molecular interactions. And that in addition to seeing how things behave in “chemical species space”, one also needs to consider how they behave in branchial space. And in the end, maybe it just takes a different kind of “chemical observer” (and maybe one more embedded in the system and operating at a molecular scale) to be able to understand the “overall architecture” of many of the molecular computations that go on in biology. (By the way, it’s worth emphasizing that even though branchial space is what’s associated with quantum mechanics in our model of fundamental physics we’re not thinking about the “physical quantum mechanics” of molecules here. It’s just that through the general structure of multicomputational models “quantum formalism” may end up being central to molecular computing and molecular biology even though—ironically enough—there doesn’t have to be anything “physically quantum” about them..
Multicomputation: A Fourth Paradigm for Theoretical Science 19.10 Evolutionary Biology : What would it take to make a global theory of evolutionary biology? At a “local level” there’s natural selection. And there are plenty of “chemical-reaction-equation-like” (and even “reaction-diffusion-equation-like”) models for relations between the “concentrations” of small numbers of species. And, yes, there are global “developmental constraints”, that I for example have studied quite extensively with the computational paradigm. But somehow the multicomputational paradigm seems to have the potential to make global “structural” statements about things like the whole “space of species” (and even why there are lots of species at all) just on the basis of the pure “combinatorial structure” of biological processes. For example, one can imagine making a multicomputational model of “generalized evolutionary biology” in which the tokens are possible specific individual organisms, and the events are all their conceivable behaviors and interactions (e.g. two organisms mating in all possible ways to produce another). (An alternative approach would take the tokens to be genes.) The particular history of all life on Earth would correspond to sampling a particular path through this giant token-event graph of all possibilities. And in a sense the “fitness environment” would be encoded in the “reference frame” being used. The “biologist observer” might “coarse grain” the token-event graph by combining tokens that are considered to be the “same species”—potentially reducing the graph to some kind of phylogenetic tree. But the overall question is whether—much like in fundamental physics—the underlying multicomputational structure (as sampled by some class of observers) might inexorably imply certain “general emergent laws of biological evolution”. One might imagine that the layout of organisms in “evolutionary space” at a particular time could be defined from a slice of a causal graph. And perhaps there is an analog of general relativity that exists when the “fitness environment reference frames” are “computationally tame enough” relative to the computational process of evolution. And maybe there are even analogs of the singularity theorems of general relativity, that would generically lead to the formation of event horizons—so that in some sense the distribution of species is like the distribution of black holes in a late-stage universe. (There is a certain analogy with metamathematics here too: different organisms are like different mathematical statements, and finding a “paleontological connection” between them is like finding proofs in mathematics. Sometimes a particular evolutionary path might end in an “extinction singularity” but often—like in mathematics—the path can be infinite, representing an infinite future of “evolutionary innovations”..
Multicomputation: A Fourth Paradigm for Theoretical Science 19.11 Neuroscience : How do brains work? And how for example are “thoughts formed” out of the firings of lots of individual neurons? Maybe there’s an analog to how the coherent physical world we perceive is formed from the interactions of lots of individual atoms of space. And to explore this we might consider a multicomputational model of brains in which the tokens are individual neurons in particular states and events are possible interactions between them. There’s a strange bit of circularity though. As I’ve argued elsewhere, what’s key to deriving the perceived laws of physics is our particular way of parsing the world (which we may view as core to our notion of consciousness)—in particular our concept that we have a single thread of experience, and thus “sequentialize time”. When applied to a multicomputational model of brains our core “brain-related” way of parsing the world suggests reference frames that again sequentialize time and turn all those parallel neuron firings into a sequence of coherent “thoughts”. Just like in physics one can expect that there are many possible reference frames—and one might imagine that ultimate equivalence between them (which leads to relativity in physics) might lead to the ability of different brains to “think comparable thoughts”. Are there analogs of other physical phenomena? One might imagine that in addition to a main “thread of conscious thought” there might be alternate multiway paths whose presence would lead to “quantum-like effects” that would manifest as “influence of the unconscious” (making an analog of Planck’s constant a “measure of the importance of the unconscious”).
Multicomputation: A Fourth Paradigm for Theoretical Science 19.12 Immunology : The immune system, like the brain, involves a large number of elements “doing different things”. In the brain, there are neurons in a definite physical arrangement that interact electrically. In the (adaptive) immune system there are things like white blood cells and antibodies that basically just “float around”, occasionally interacting though molecular-scale “shape-based” physical binding. It seems pretty natural to make a multicomputational model of this, in which individual immune system elements interact through all possible binding events. One can pick an “assay” reference frame in which one “coarse grains together”, say, all antibodies or all T-cell receptors that have a particular sequence. And by aggregating the underlying token-event graph one will be able to get (at least approximately) a “summary graph” of interactions between types of antibodies, etc. Then much like we imagine physical space to be knitted together from atoms of space by their interactions, so also we can expect that the “shape space” of antibodies, etc. will also be defined by their interactions. Maybe “interactionally near” shapes will also be near in some simple sequence-based metric, but not necessarily. And for example there’ll be some analog of a light cone that governs any kind of “spreading of immunity” associated with an antigen “at a particular position in shape space”—and it’ll be defined by the causal graph of interactions between immune elements. When it comes to understanding the “state of the immune system” we can expect—in a typical multicomputational way—that the whole dynamic network will be important. Indeed, perhaps for example “immune memory” is maintained as a “property of the network” even though individual immune elements are continually being created and destroyed—much as particles and objects in physics persist even though their constituent atoms of space are continually changing.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.13 Linguistics : Languages, like all the other kinds of things we’ve discussed, are fundamentally dynamic constructs. And to make a multicomputational model of them we can for example imagine every instance of every word (or even every conceivable word) being a token—with events being utterances that involve multiple words. The resulting token-event graph then defines relationships between instances of words (essentially by the “context of their usage”). And within any given time slice these relationships will imply a certain layout of word instances in what we can interpret as “meaning space”. There’s absolutely no guarantee that “meaning space” will be anything like a manifold, and I expect that—like the emergent spaces from most token-event graphs we’ve generated—it’ll be considerably more complicated to “coordinatize”. Still, the expectation is that instances of a word with a given sense will appear nearby, as will synonymous words—while different senses of a given word will appear as separate clusters. In this setup, the time evolution of everything would be based on there being a sequence of utterances that are effectively strung together by someone somehow hearing a given word in a certain utterance, then at some point later using that word in another utterance. What utterances are possible? Essentially it’s all “meaningful” ones. And, yes, this is really the nub of “defining the language”. As a rough approximation one could for example use some simple grammatical rule—in which case the possible events might themselves be determined by a multiway system. But the key point is that—like in physics—we may expect that there’ll be global laws quite independent of the “microscopic details” of what precise utterances are possible, just as a consequence of the whole multicomputational structure. What might these “global laws” of language be like? Maybe they’ll tell us things about how languages evolve and “speciate”, with event horizons forming in the “meaning space of words”. Maybe they’ll tell us slightly smaller-scale things about the splitting and merging of different meanings for a single word. Maybe there’ll be an analog of gravity, in which the “geodesic” associated with the “etymological path” for a word will be “attracted” to some region of meaning space with large amounts of activity (or “energy”)—or in effect, if a concept is being “talked about a lot” then the meanings of words will tend to get closer to that. By the way, picking a “reference frame for a language” is presumably about picking which utterances one’s effectively chosen to have heard by any given time, and thus which utterances one can use to build up one’s “sense of the meanings of words” at that time. And if the selection of utterances for the reference frame is sufficiently wild, then one won’t get a “coherent sense of meaning” for the language as a whole—making the “emergence of meaning” something that’s ultimately about what amounts to human choices.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.14 Economics : A basic way to imagine “microscopically” modeling an economic system is to have a token-event graph in which the tokens are something like “configurations of agents”, and the events are possible transactions between them. Much like the relations between atoms of space that are the tokens (represented by hyperedges) in our models of fundamental physics, the tokens we’re describing here as “configurations of agents” can more accurately be thought of as relations between elements that, say, represent economic agents, objects, goods, services, currency, etc. In a transaction we’re imagining that an “interaction” between such “relations” leads to new “relations”—say representing the act of exchanging something, making something, doing something, buying something, etc. At the outset, we’re not saying which transactions happen, and which don’t. And in fact we can imagine a setup (essentially a rulial token-event graph) where every conceivable transaction can in principle happen. The result will be a very complicated structure—though with certain inexorable features. But now consider how we would “observe” this system. Maybe there’d be a “from-the-outside” way to do this, but we could also just “be in the system” getting data through transactions that we’re involved in. But then we’re in a situation that’s pretty closely analogous to fundamental physics. And to make sense of what we observe, we’ll basically inevitably end up sampling the system by setting up some kind of reference frame. But if this reference frame has typical “generalized human” characteristics such as computational boundedness it’ll end up weaving through all possible transactions to pick out slices that are “computationally simple to describe”. And this seems likely to be related to the origin of “value” in economics (or perhaps more so to the notion of a numéraire). Much like in physics, a reference frame can allow coordinates to be assigned. But the question is what reference frames will lead to coordinates that are somehow stable under the time evolution of the system. And in effect this is what general relativity tells us. And quite possibly there’s an analog of this in economic systems. Why isn’t there just an immediate value for everything? In the model we’re discussing, all that’s defined is the network of transactions. But just seeing particular local transactions only tells us about things like “local value equivalences”. To say something more global requires the whole knitting together of “economic space” achieved by all the local transactions in the network. It’s very much like in the emergence of physical space. Underneath, there’s all sorts of complicated and computationally irreducible behavior. But if we look at the right things, we see computational reducibility, and something we can describe in the limit as continuum space. In economic systems, low-level transactions may show complicated and computationally irreducible behavior. But the point is that if we look at the right things, we again see something like continuum behavior, but now it corresponds to money and value. (And, yes, it is ironic that computational irreducibility is the basic phenomenon that seems to lead to a robust notion of value—even as it’s also what proof-of-work cryptocurrencies use to “mine” value.) Like a changing metric etc. in spacetime, “value” can vary with position and time. And we can expect that there will be some general-relativity-like principles about how this works (perhaps with “curvature in economic space” allowing arbitrage etc.). There also might be analogs of quantum effects—in which a value depends on a bundle of alternate paths in the multiway graph. (In “quant finance”—which, yes, coincidentally sounds a bit like “quantum”—it’s for example common to estimate prices from looking at the effects of all possible paths, say approximated by Monte Carlo.) At the outset, it’s not obvious that one can make any “economics-level” conclusion just based on thinking about what amount to arbitrary token-event graphs. But the remarkable thing about multicomputational models is that just from their general structure there are often inexorable quantitative laws that can be derived. And it’s conceivable that at least in the limit of a large economic system, it may finally be possible to do this.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.15 Machine Learning : One can think of machine learning as being about deducing models of what can happen in the world from collections of “training examples”. Often one imagines a collection of conceivable inputs (say, possible arrays of pixels corresponding to images), for which one wants to “learn a structure for the space”—in such a way that one can for example find a “manifold-style” “coordinatization” in terms of a feature vector. How can one make a multicomputational model for this process? Imagine for example that one has a neural net with a certain architecture. The “state” of the net is defined by the values of a large number of weights. Then one can imagine a multiway graph in which each state can be updated according to a large number of different events. Each possible event might correspond to the incremental update of weights associated with back-propagating the effect of adding in a single new training example. In present-day neural net training one normally follows a single path in which one applies a particular (perhaps randomly chosen) sequence of weight updates. But in principle there’s a whole multiway graph of possible training sequences. The branchial space associated with this graph in effect defines a space of possible models obtained after a certain amount of training—complete with a measure on models (derived from path weights), distances between models, etc. But what about a token-event graph? Present-day neural nets—with their usual back-propagation methods—tend to show very little factorizability in the updating of weights. But if one could treat certain collections of weights as “independently updatable” then one could use these to define tokens—and eventually expect to identify some kind of “localized-effects-in-the-net” space. But if training is associated with the multiway (or token-event) graph, what is evaluation? One possible answer is that it’s basically associated with the reference frame that we pick for the net. Running the net might generate some collection of output numbers—but then we have to choose some way to organize these numbers to determine whether they mean, say, that an image is of a cat or a dog. And it is this choice that in effect corresponds to our “reference frame” for sampling the net. What does this mean, for example, about what is learnable? Perhaps this is where the analog of Einstein’s equations comes in—defining the possible large-scale structure of the underlying space, and telling us what reference frames can be set up with computationally bounded effort?.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.16 Distributed Computing : In the applications we’ve discussed so far, the multicomputational paradigm enters basically in a descriptive way, providing raw material from which models can be made. In distributed computing, the paradigm also plays a very powerful prescriptive role, suggesting new ways to do computations, and new kinds of computations to do. One can think of traditional sequential computation as being based on a simple chain of “evaluation events”, with each event being essentially the evaluation of a function that transforms its input to produce output. The input and output that the functions deal with can involve many “parallel” elements (as, for example, in a cellular automaton) but there’s always just “one output” produced by a given function. The multicomputational paradigm, however, suggests computations that involve not just chains of evaluation events, but more complicated graphs of them. And one way this can arise is through having “functions” (or “evaluation events”) that produce not just one but several “results” that can then independently be “consumed” by future evaluation events. A feature of traditional sequential computations is that they’re immediately suitable for execution on a single processor that successively performs a chain of evaluations. But the multicomputational paradigm involves in a sense lots of “separate” evaluation events, that can potentially occur on a whole distributed collection of processors. There are definite causal relations that have to exist between evaluation events, but there need be no single, total ordering. Some events require as input the output from other events, and so have a definite relative ordering, making them—in physics terminology—“timelike separated”. Some events can be executed in parallel, essentially independently or “asynchronously” of each other—and so can be considered “spacelike separated”. And some events can be executed in different orders, but doing so can lead to different results—making these events (in our physics terminology) “branchlike separated”. In practical distributed computing, there are usually great efforts made to avoid branchlike-separated events (or “race conditions”, as they’re commonly called). And if one can do this then one has a computation that—despite its distributed character—can still be interpreted in a fundamentally sequential way in time, with a succession of “definite results” being generated. And, yes, this is certainly a natural thing for us humans to try to do, because it’s what allows us to map the computation into our typical sequentialized “single thread of experience” that seems to be a fundamental feature of our basic notion of consciousness. But what the multicomputational paradigm suggests is that this isn’t the only way to set up distributed computing. Instead, we just want to think about the progress of a computation in terms of the “bulk perception” of some observer. The observer may be able to pick many different reference frames, but each will represent some computation. Sometimes this computation will correspond to a distributed version of something we were already familiar with. But sometimes it’ll effectively be a new kind of computation. It’s common to talk about nondeterministic computation in which many paths can be followed—but ultimately one picks out one particular path (say, one that successfully satisfies some condition one’s looking for). The multicomputational paradigm is about the rather different idea of actually treating the “answer” as corresponding to a whole bundle of paths that are combined or conflated through a choice of reference frame. And, yes, this type of thing is rather alien to our traditional “single-thread-of-time” experience of computing. But the point is that particularly through its use and interpretation in physics and so many other areas the multicomputational paradigm gives us a general way to think about—and harness—such things. And potentially gives us a very different—and powerful—new approach to distributed computing, perhaps complete with very general physics-like “bulk” laws. OK, so what about other areas? There are quite a few more that I’ve thought at least a little about. Among them are ones like history, psychology and the general development of knowledge. And, yes, it might seem quite surprising that there could be anything scientific or systematic to say about such areas. But the remarkable thing is that by having a new paradigm for theoretical science—the multicomputational paradigm—it becomes conceivable to start bringing science to areas it’s never been able to touch before. But even in what I’ve said above, I’ve only just begun to sketch how the multicomputational paradigm might apply in different areas. In each case there’s years of work to do in developing and refining things. But I think there’s amazing promise in expanding the domain of theoretical science, and potentially bringing physics-like laws to fields that have long sought such things, but never been able to find them.
Multicomputation: A Fourth Paradigm for Theoretical Science 19.17 Some Backstory : What’s the backstory of what I’m calling the multicomputational paradigm? For me, the realization that one can formulate such a broad and general new paradigm is something that’s emerged only over the past year or so. But given what we now know it’s possible to go back and see a quite tangled web of indications and precursors of it stretching back decades and perhaps more than a century. A key technical step in the development of the multicomputational paradigm was the idea of what I named “multiway systems”. I first used the term “multiway systems” in 1992 when I included a placeholder for a future section about them in an early draft of what would become my 2002 book A New Kind of Science. A major theme of my work during the development of A New Kind of Science was exploring as broadly as possible the computational universe of simple programs. I had already in the 1980s extensively studied cellular automata—and discovered all sorts of interesting phenomena like computational irreducibility in them. But now I wanted to see what happened in other parts of the computational universe. So I started investigating—and inventing—different kinds of systems with different underlying structures. And there, in Chapter 5, sandwiched between a section on Network Systems (one of the precursors of the discrete model of space in our Physics Project) and Systems Based on Constraints, is a section entitled Multiway Systems. The basic thrust is already the core multicomputational one: to break away from the idea of (as I put it) a “simple one-dimensional arrangement of states in time”: I studied multiway systems as abstract systems. Later in the book I studied them as idealizations of mathematical proofs. And I mentioned them as a possible (but as I thought then, rather unsatisfactory) underpinning for quantum mechanics. I came back to multiway systems quite a few times over the years. But it was only when we started the Wolfram Physics Project in the latter half of 2019 that it began to become clear (particularly through an insight of Jonathan Gorard’s) just how central multiway systems would end up being to fundamental physics. The basic idea of multiway systems is in a sense so straightforward and seemingly obvious that one might assume it had arisen many times. And in a sense it has, at least in special cases and particular forms—in quite a range of fields, under a wide variety of different names—though realistically in many cases we can only see the “multiway system character” when we look back from what we now know. The rather trivial barely-a-real-multiway-system case of pure trees no doubt arose for example with the construction of family trees, presumably already in antiquity. Another special and rather trivial case arose in the construction of Pascal’s triangle, which one can think of as working just like the “very simple multiway system” shown above. (In Pascal’s triangle, of course, the emphasis is not on the pattern of states, but on the path weights, which are binomial coefficients.) Another very early—if implicit—use of multiway systems was in recreational mathematics puzzles and games. Perhaps in antiquity, and certainly by 800 AD, there was for example the wolf-goat-cabbage river crossing problem, whose possible histories form a multiway system. Mechanical puzzles like Chinese rings are perhaps even older. And games like Mancala and Three Men’s Morris might date from early antiquity—even though the explicit “mathematical” concept of “game trees” (which are typically multiway graphs that include merging) seems to have only arisen (stimulated by discussions of chess strategies) as an “application of set theory” by Ernst Zermelo in 1912. In mathematics, multiway systems seem to have first appeared—again somewhat implicitly—in connection with groups. Given words in a group, written out as sequences of generators, successive application of relations in the group essentially yield multiway string substitution systems—and in 1878 Arthur Cayley drew a kind of dual of this to give what’s now called a Cayley graph. But the first more-readily-recognizable examples of multiway systems seem to have appeared in the early 1900s in connection with efforts to find minimal representations of axiomatic mathematics. The basic idea—which arose several times—was to think of the process of mathematical deduction as consisting of the progressive transformation of something like sequences of symbols according to certain rules. (And, yes, this basic idea is also what the Wolfram Language now uses for representing general computational processes.) The notion was that any given deduction (or proof) would correspond to a particular sequence of transformations. But if one looks at all possible sequences what one has is a multiway system. And in what seems to be the first known explicit example Axel Thue in 1914 considered (in a paper entitled “Problems Concerning the Transformation of Symbol Sequences According to Given Rules”) what are essentially string equivalences (two-way string transformations) and discussed what paths could exist between strings—with the result that string substitution systems are now sometimes called “semi-Thue systems”. In 1921 Emil Post then considered (one-way) string transformations (or, as he called them, “productions”). But, like Thue, he focused on the “decision problem” of whether one string was reachable from another—and never seems to have considered the overall multiway structure. Thue and Post (and later Markov with his so-called “normal algorithms”) considered strings. In 1920 Moses Schönfinkel introduced his S, K combinators and defined transformations between what amount to symbolic expressions. And here again it turns out there can be ambiguity in how the transformations are applied, leading to what we’d now consider a multiway system. And the same issue arose for Alonzo Church’s lambda calculus (introduced around 1930). But in 1936 Church and Rosser showed that at least for lambda calculus and combinators the multiway structure basically doesn’t matter so long as the transformations terminate: the final result is always the same. (Our “causal invariance” is a more general version of this kind of property.) Even in antiquity the idea seems to have existed that sentences in languages were constructed from words according to certain grammatical rules, that could (in modern terms) perhaps be recursively applied. For a long time this wasn’t particularly formalized (except conceivably for Sanskrit). But finally in 1956—following work on string substitution systems—there emerged the concept of generative grammars. And while the focus was on things like what sentences could be generated, the underlying representation of the process of generating all possible sentences from grammatical rules can again be thought of as a multiway system. In a related but somewhat different direction, the development of various kinds of (often switch-like) devices with discrete states had led by the 1940s to the fairly formal notion of a finite automaton. In many engineering setups one wants just one “deterministic” path to be followed between the states of the automaton. But by 1959 there was explicit discussion of nondeterministic finite automata, in which many paths could be followed. But while in principle tracing all these paths would have yielded a multiway system it was quickly discovered that as far as the set of possible strings generated or recognized was concerned, there was always an equivalent deterministic finite automaton. One way to think about multiway systems is that they’re the result of “repeated nondeterminism”—in which there are multiple outcomes possible at a sequence of steps. And over the course of the past century or so quite a few different kinds of systems based on repeated nondeterminism have arisen—a simple example being a random walk, investigated since the late 1800s. Usually in studying systems like this, however, one’s either interested only in single “random instances”, or in some kind of “overall probability distribution”—and not the more detailed “map of possible histories” defined by a multiway system. Multiway systems are in a sense specifically about the structure of “progressive evolution” (normally in time). But given what amount to rules for a multiway system one can also ask essentially combinatorial questions about what the distribution of all possible states is. And one place where this has been done for over a century is in estimating equilibrium properties of systems in statistical physics—as summarized by the so-called partition function. Even after all these years there is, however, much less development of any general formalism for non-equilibrium statistical mechanics—though some diagrammatic techniques are perhaps reminiscent of multiway systems (and our full multicomputational approach might now make great progress). Yet another place where multiway systems have implicitly appeared is in studying systems where asynchronous events occur. The systems can be based on Boolean algebra, database updating or other kinds of ultimately computational rules. And in making proofs about whether systems are for example “correct” or “safe” one needs to consider all possible sequences of asynchronous updates. Normally this is done using various optimized implicit methods, but ultimately there’s always effectively a multiway system underneath. One class of models that’s been rediscovered multiple times since 1939—particularly in various systems engineering contexts—are so-called Petri nets. Basically these generalize finite automata by defining rules for multiple “markers” to move around a graph—and once again if one were to make the trace of all possible histories it would be a multiway system, so that for example “reachability” questions amount to path finding. (Note that—as we saw earlier—a token-event graph can be “rolled up” into what amounts to a Petri net by completely deduplicating all instances of equivalent tokens.. In the development of computer science, particularly beginning in the 1970s, there were various investigations of parallel and distributed computer systems where different operations can occur concurrently, but at flexible or essentially asynchronous times. Concepts like channels, message-passing and coroutines were developed—and formal models like Communicating Sequential Processes were constructed. Once again the set of possible histories can be thought of as a multiway system, and methods like process algebra in effect provide a formalism for describing certain aspects of what can happen. I know of a few perhaps-closer approaches to our conception of multiway systems. One is so-called Böhm trees. In studying “term rewriting” systems like combinators, lambda calculus and their generalization the initial focus was on sequences of transformations that terminate in some kind of “answer” (or “normal form”). But starting in the late 1970s a small amount of work was done on the non-terminating case and on what we would now call multiway graphs describing it. We usually think of multiway graphs as being generated progressively by following certain rules. But if we just look at the final graphs, they (often) have the mathematical structure of Hasse diagrams for partial orderings. Most posets that get constructed in mathematics don’t have particularly convenient interpretations in terms of interesting multiway systems (especially when the posets are finite). But for example the “partially ordered set of finite causets” (or “poscau”) generated by all possible sequential growth paths for causal sets (and studied since about 2000) can be thought of as a multiway system. Beginning around the 1960s, the general idea of studying systems based on repeated rewriting—of strings or other “terms”—developed essentially into its own field, though with connections to formal language theory, automated theorem proving, combinatorics on words and other areas. For the most part what was studied were questions of termination, decidability and various kinds of path equivalence—but apparently not what we would now consider “full multiway structure”. Already in the late 1960s there started to be discussion of rewriting systems (and grammars) based on graphs. But unlike for strings and trees, defining how rewriting might structurally work was already complicated for graphs (leading to concepts like double-pushout rewriting). And in systematically organizing this structure (as well as those for other diagrammatic calculi), connections were made with category theory—and this in turn led to connections with formalizations of distributed computing such as process algebra and π calculus. The concept that rewritings can occur “in parallel” led to use of monoidal categories, and consideration of higher categories provided yet another (though rather abstract) perspective on what we now call multiway systems. (It might be worth mentioning that I studied graph rewriting in the 1990s in connection with possible models of fundamental physics, but was somewhat unhappy with its structural complexity—which is what led me finally in 2018 to start studying hypergraph rewriting, and to develop the foundations of our Physics Project. Back in the 1990s I did consider the possibility of multiway graph rewriting—but it took the whole development of our Physics Project for me to understand its potential significance.) An important feature of the multicomputational paradigm is the role of the observer and the concept of sampling multiway systems, for example in “slices” corresponding to certain “reference frames”. And here again there are historical precursors. The term “reference frame” was introduced in the 1800s to organize ideas about characterizing motion—and was generalized by the introduction of special relativity in 1905 and further by general relativity in 1915. And when we talk about slices of multiway systems they’re structurally very much like discrete analogs of sequences of spacelike hypersurfaces in continuum spacetime in relativity. There’s a difference of interpretation, though: in relativity one’s dealing specifically with physical space, whereas in our multiway systems we’re dealing first and foremost with branchial space. But beyond the specifics of relativity and spacetime, starting in the 1940s there emerged in mathematics—especially in dynamical systems theory—the general notion of foliations, which are basically the continuous analogs of our “slices” in multiway systems. We can think of slices of multiway systems as defining a certain ordering in which we (or an observer) “scans” the multiway system. In mathematics starting about a century ago there were various situations in which different scanning orders for discrete sets were considered—notably in connection with diagonalization arguments, space-filling curves, etc. But the notion of scanning orders became more prominent through the development of practical algorithms for computers. The basic point is that any nontrivial recursive algorithm has multiple possible branches for recursion (i.e. it defines a multiway system), and in running the algorithm one has to decide in what order to follow these. A typical example involves scanning a tree, and deciding in what order to visit the nodes. One possibility is to go “depth first”, visiting nodes all the way down the bottom of one branch first—and this approach was used even by hand for solving mazes before 1900. But by the end of the 1950s, in the course of actual computer implementations, it was noticed that one could also go “breadth first”. Algorithms for things like searching are an important use case for different scanning orders (or in our way of describing things, different reference frames). But another use case intimately tied into things like term rewriting is for evaluation orders. And indeed—though I didn’t recognize it at the time—my own work on evaluation orders in symbolic computation around 1980 is quite related to what we’re now doing with multicomputation. (Evaluation orders are also related to lazy evaluation and to recent ideas like CRDTs.) The most typical “workflow” for a computation is a direct one (corresponding to what I call here the computational paradigm): start with an “input” and progressively operate on it to generate “output”. But another “workflow” is effectively to define some goal, and then to try to find a path that achieves it. Early examples around automated theorem proving were already implemented in the 1950s. By the 1970s the approach was used in practice in logic programming, and was also at a theoretical level formalized in nondeterministic Turing machines and NP problems. At an underlying level, the setup was “very multiway”. But the focus was almost always just on finding particular “winning” paths—and not on looking at the whole multiway structure. Slight exceptions from the 1980s were various studies of “distributions of difficulty” in NP problems, and early considerations of “quantum Turing machines” in which superpositions of possible paths were considered. But as so often happens in the history of theoretical science, it was only through the development of a new conceptual framework—around our Physics Project—that the whole structure of multiway systems was able to emerge, complete with ideas like causal graphs, branchial space, etc. Beyond the formal structure of multiway systems, etc., another important aspect of the multicomputational paradigm is the central role of the observer. And in a sense it might seem antithetical to “objective” theoretical science to even have to discuss the observer. But the development of relativity and quantum mechanics (as well as statistical mechanics and the concept of entropy) in the early 1900s was predicated on being “more realistic” about observers. And indeed what we now see is that the role of observers in these theories is deeply connected to their fundamentally multicomputational character. The whole question of how one should think about observers in science has been discussed, arguably for centuries, particularly in the philosophy of physics. But for the most part there hasn’t been much intersection with computational ideas—with exceptions including Heinz von Foerster’s “Second-Order Cybernetics” from the late 1960s, John Wheeler’s “It from Bit” ideas, and to some extent my own investigations about the origins of perceived complexity. In some respects it might seem surprising that there’s such a long and tangled backstory of “almost sightings” of multiway systems and the multicomputational paradigm. But in a sense this is just a sign of the fact that the multicomputational paradigm really is a new paradigm: it’s something that requires a new conceptual framework, without which it really can’t be grasped. And it’s a tribute to how fundamental the multicomputational paradigm is that there have been so many “shadows” of it over the years. But now that the paradigm is “out” I’m excited to see what it leads to—and I fully expect this new fourth paradigm for theoretical science to be at least as important and productive as the three we already have.
Charting a Course for “Complexity”: Metamodeling, Ruliology and More 20.1 : This is the first of a series of pieces I’m planning in connection with the upcoming 20th anniversary of the publication of A New Kind of Science. “There’s a Whole New Field to Build…” For me the story began nearly 50 years ago—with what I saw as a great and fundamental mystery of science. We see all sorts of complexity in nature and elsewhere. But where does it come from? How is it made? There are so many examples. Snowflakes. Galaxies. Lifeforms. Turbulence. Do they all work differently? Or is there some common underlying cause? Some essential “phenomenon of complexity”? It was 1980 when I began to seriously work on these questions. And at first I did so in the main scientific paradigm I knew: models based on mathematics and mathematical equations. I studied the approaches people had tried to use. Nonequilibrium thermodynamics. Synergetics. Nonlinear dynamics. Cybernetics. General systems theory. I imagined that the key question was: “Starting from disorder and randomness, how could spontaneous self-organization occur, to produce the complexity we see?” For somehow I assumed that complexity must be created as a kind of filtering of ubiquitous thermodynamic-like randomness in the world. At first I didn’t get very far. I could write down equations and do math. But there wasn’t any real complexity in sight. But in a quirk of history that I now realize had tremendous significance, I had just spent a couple of years creating a big computer system that was ultimately a direct forerunner of our modern Wolfram Language. So for me it was obvious: if I couldn’t figure out things myself with math, I should use a computer. And there was something else: the computer system I’d built was a language that I’d realized (in a nod to my experience with reductionist physical science) would be the most powerful if it could be based on principles and primitives that were as minimal as possible. It had worked out very well for the language. And so when it came to complexity, it was natural to try to do the same thing. And to try to find the most minimal, most “meta” kind of model to use. I didn’t know just what magic ingredient I’d need in order to get complexity. But I thought I might as well start absolutely as simple as possible. And so it was that I set about running programs that I later learned were a simplified version of what had been called “cellular automata” before. I don’t think it was even an hour before I realized that something very interesting was going on. I’d start from randomness, and “spontaneously” the programs would generate all sorts of complex patterns. At first, it was experimental work. I’d make observations, cataloging and classifying what I saw. But soon I brought in analysis tools—from statistical mechanics, dynamical systems theory, statistics, wherever. And I figured out all sorts of things. But at the center of everything, there was still a crucial question: what was the essence of what I was seeing? And how did it connect to existing science? I wanted to simplify still further. What if I didn’t start from randomness, but instead started from the simplest possible “seed”? There were immediately patterns like fractals. But somehow I just assumed that a simple program, with simple rules, starting from a simple seed just didn’t have what it took to make “true complexity”. I had printouts (yes, that was still how it worked back then) that showed this wasn’t true. But for a couple of years I somehow ignored them. Then in 1984 I made my first high-resolution picture of rule 30. And I now couldn’t get away from it: a simple rule and simple seed were making something that seemed extremely complex. But was it really that complex? Or was there some magic method of analysis that would immediately “crack” it? For months I looked for one. From mathematics. Mathematical physics. Computation theory. Cryptography. But I found nothing. And slowly it began to dawn on me that I’d been fundamentally wrong in my basic intuition. And that in the world of simple programs—or at least cellular automata—complexity was actually easy to make. Could it really be that this was the secret that nature had been using all along to make complexity? I began to think it was at least a big part of it. I started to make connections to specific examples in crystal growth, fluid flow, biological forms and other places. But I also wanted to understand the fundamental principles of what was going on. Simple programs could produce complex behavior. But why? It wasn’t long before I realized something fundamental: that this was at its core a computational phenomenon. It wasn’t something one could readily see with math. It required a different way of thinking about things. A fundamentally computational way. At first I had imagined that having a program as a model of something was essentially just a convenience. But I realized that it wasn’t. I realized that computational models were something fundamentally new, with their own conceptual framework, character and intuition. And as an example of that, I realized that they showed a new central phenomenon that I called computational irreducibility. For several centuries, the tradition and aspiration of exact science had been to predict numbers that would say what a system would do. But what I realized is that in most of the computational universe of simple programs, you can’t do that. Even if you know the rules for a system, you may still have to do an irreducible amount of computational work to figure out what it will do. And that’s why its behavior will seem complex. By 1985 I knew these things. And I was tremendously excited about their implications. I had got to this point by trying to solve the “problem of complexity”. And it seemed only natural to label what could now be done as “complex systems theory”: a theory of systems that show complexity, even from simple rules. And so it was that in 1985 I began to promote the idea of a new field of “complex systems research”, or, for short “complexity”—fueled by the discoveries I’d made about things like cellular automata. Now that I know more about history I realize that the thrust of what I wanted to do had definite precursors, especially from the 1950s. For that was a time when the concepts of computing were first being worked out—and through approaches like cybernetics and the nascent area of artificial intelligence, people started exploring the broader scientific implications of computational ideas. But with no inkling of the phenomena I discovered decades later, this didn’t seem terribly promising, and the effort was largely abandoned. By the late 1970s, though, there were other initiatives emerging, particularly coming from mathematics and mathematical physics. Among them were fractals, catastrophe theory and chaos theory. Each in a different way explored some form of complexity. But all of them in a sense operated largely in the “comfort” of traditional mathematical ideas. And while they used computers as practical tools, they never made the jump to seeing computation as a core paradigm for thinking about science. So what became of the “complex systems research” I championed in 1985? It’s been 36 years now. Careers have come and gone. Several academic generations have passed by. Some things have developed well. Some things have not developed so well. But I, for one, know much more than I did then. For me, my work in the early 1980s was a foundation for the whole tower of science and technology that I’ve spent my life since then building, most recently culminating in our Wolfram Physics Project and what in just the past few weeks I’ve called the multicomputational paradigm. Nothing I’ve learned in these 36 years has dulled the strength and beauty of rule 30 and those early discoveries about complexity. But now I have so much more context, and a so-much-bigger conceptual framework—from which it’s possible to see so much more about complexity and about its place and potential in science. Back in 1985 I was pretty much a lone voice expressing the potential for studying complexity in science. Now there are perhaps a thousand scientific institutes around the world nominally focused on complexity. And my goal here is to share what I’ve learned and figured out about what’s now possible to do under the banner of complexity. There are exciting—and surprising—things. Some I was already beginning to think about in the 1980s. But others have only come into focus—or even become conceivable—as a result of very recent progress around our Physics Project and the formalism it has developed.
Charting a Course for “Complexity”: Metamodeling, Ruliology and More 20.2 The Emergence of a New Kind of Science : Back in 1985 I was tremendously excited about the potential for developing the field of complex systems research. It seemed as if there was a vast new domain that had suddenly been made accessible to scientific exploration. And in it I could see so much great science that could be done, and so many wonderful opportunities for so many people. I myself was still only 25 years old. But I’d had some organizational experience, both leading a research group, and starting my first company. And I set about applying what I knew to complex systems research. By the following year, I’d founded the first research center and the first journal in the field (Complex Systems, still going strong after 35 years). (And I’d also done things like suggesting “complexity” as the theme for what became the Santa Fe Institute.) But somehow everything moved very slowly. Despite my efforts, complex systems research wasn’t a thing yet. It wasn’t something universities were teaching; it wasn’t something that was a category for funding. There were some applications for the field emerging. And there was tremendous pressure—particularly in the context of those applications—to shoehorn it into some existing area. Yes, it might have to take on the methodology of its “host” area. But at least it would have a home. But it really wasn’t physics, or computer science, or math, or biology, or economics, or any known field. At least as I envisioned it, it was its own thing, with its own, new, emerging methodology. And that was what I really thought should be developed. I was impatient to have it happen. And by late 1986 I’d decided the best path was just to try to do it myself—and to set up the best tools and the best environment for that. The result was Mathematica (and now the Wolfram Language), as well as Wolfram Research. For a few years the task of creating these entirely consumed me. But in 1991 I returned to basic science and set about continuing where I had left off five years earlier. It was an exciting time. I quickly found that the phenomena I had discovered in cellular automata were quite general. I explored all sorts of different kinds of rules and programs, always trying to understand the essence of what they were doing. But every time, the core phenomena I found were the same. Computational irreducibility—as unexpected as it had been when I first saw it in cellular automata—was everywhere. And I soon realized that beneath what I was seeing, there was a deep and general principle—that I called the Principle of Computational Equivalence—that I now consider to be the most fundamental thing we know about the computational universe. But what did these discoveries about simple programs and the computational universe apply to? My initial target had been immediately observable phenomena in the natural world. And I had somehow assumed that ideas like evolutionary adaptation or mathematical proof would be outside the domain. But as the years went by, I realized that the force of the Principle of Computational Equivalence was much greater than I’d ever imagined, and that it encompassed these things too. I spent the 1990s exploring the computational universe and its applications, and steadily writing a book about what I was discovering. At first, in recognition of my original objective, I called the book A Science of Complexity. But by the mid-1990s I had realized that what I was doing far transcended the specific goal of understanding the phenomenon of complexity. Instead, the core of what I was doing was to introduce a whole new kind of science, based on a new paradigm—essentially what I would now call the paradigm of computation. For three centuries, theoretical science had been dominated by the idea of using mathematical equations to describe the world. But now there was a new idea. The idea not of solving equations, but instead of setting up computational rules that could be explicitly run to represent and reproduce things in the world. For three centuries theoretical models had been based on the fairly narrow set of constructs provided by mathematical equations, and particularly calculus. But now the whole computational universe of possible programs and possible rules was opened up as a source of raw material for making models. But with this new power came a sobering realization. Out in the unrestricted computational universe, computational irreducibility is everywhere. So, yes, there was now a way to create models for many things. But to figure out the consequences of those models might take irreducible computational work. Without the computational paradigm, systems that showed significant complexity had seemed quite inaccessible to science. But now there was an underlying way to model them, and to successfully reproduce the complexity of their behavior. But computational irreducibility was all over them, fundamentally limiting what could be predicted or understood about how they behave. For more than a decade, I worked through the implications of these ideas, continually surprised at how many foundational questions across all sorts of fields they seemed to address. And particularly given the tools and technology I’d developed, I think I became pretty efficient at the research I did. And finally in 2002 I decided I’d pretty much “picked all the low-hanging fruit”, and it was time to publish my magnum opus, titled—after what I considered to be its main intellectual thrust—A New Kind of Science. The book was a mixture of pure, basic science about simple programs and what they do, together with a discussion of principles deduced from studying these programs, as well as applications to specific fields. If the original question had been “Where does complexity come from?” I felt I’d basically nailed that—and the book was now an exploration of what one could do in a science where the emergence of complexity from simplicity was just a feature of the deeper idea of introducing the computational paradigm as a foundation for a new kind of science. I put tremendous effort into making the exposition in the book (in both words and pictures) as clear as possible—and contextualizing it with extensive historical research. And in general all this effort paid off excellently, allowing the message of the book to reach a very wide audience. What did people take away from the book? Some were confused by its new paradigm (“Where are all the equations?”). Some saw it as a somewhat mysterious wellspring of new forms and structures (“Those are great pictures!”). But what many people saw in it was a thousand pages of evidence that simple programs—and computational rules—could be a rich and successful source of models and ideas for science. It’s hard to trace the exact chains of influence. But in the past two decades there’s been a remarkable—if somewhat silent—transformation. For three hundred years, serious models in science had essentially always been based on mathematical equations. But in the short space of just twenty years that’s all changed—and now the vast majority of new models are based not on equations but on programs. It’s a dramatic and important paradigmatic change, whose implications are just beginning to be felt. But what of complexity? In the past it was always a challenge to “get complexity” out of a model. Now—with computational models—it tends to be very easy. Complexity has gone from something mysterious and out of reach to something ubiquitous and commonplace. But what has that meant for the “study of complexity”? Well, that’s a story with quite some complexity to it….
Charting a Course for “Complexity”: Metamodeling, Ruliology and More 20.3 The Growth of “Complexity” : From about 1984 to 1986 I put great effort into presenting and promoting the idea of “complex systems research”. But by the time I basically left the field in 1986 to concentrate on technology for a few years, I hadn’t seen much traction for the idea. A decade later, however, the story was quite different. I myself was quietly working away on what became A New Kind of Science. But elsewhere it seemed like my “marketing message” for complexity had firmly taken root, and there were complexity institutes starting to pop up all over the place. What did people even mean by “complexity”? It often seemed to mean different things to different people. Sometimes it just meant “stuff in our field we haven’t figured out yet”. More often it meant “stuff that seems fundamental but we haven’t been able to figure out”. Pretty often there was some visualization component: “Look at how complex this plot seems!” But whatever exactly it might mean to different people, “complexity” was unquestionably becoming a popular science “brand”, and there were plenty of people eager to affiliate with it—at the very least to give work they’d been doing for years a new air of modernity. And while it was easy to be cynical about some of this, it had one very important positive consequence: “complexity” became a kind of banner for interdisciplinary work. As science had gotten bigger and more institutionalized, it inevitably become more siloed, with people in different departments at the same university routinely never having even met. But now people from all kinds of fields could say, “Yes, we run into complexity in our field”, and with complexity as a banner they now had a reason to connect, and maybe even to form an institute together. So what actually got done? Some of it I might summarize as “Yes, it’s complex, but we can find something mathematical in it”—with a typical notion being the pursuit of some kind of power law formula. But the more important strand has been one that starts to actually take the computational paradigm on board—with the thrust typically being “We can write a program to reproduce what we’re looking at”. And one of the great feelings of power has been that even in fields—like the social sciences—where there haven’t really been much more than “verbal” models before, it’s now appeared possible to get models that at least seem much more “scientific”. Sometimes the models have been purely empirical (“Look, there’s a power law!”). Sometimes they have been based on constructing programs to reproduce behavior. The definition of success has often been a bit questionable, however. Yes, there’s a program that shows some features of whatever system one’s looking at. But how complicated is the program? How much of what’s coming out is basically just being put right into the program? For mathematical models, people have long had familiarity with questions like “How many parameters does that model have?”. But when it comes to programs, there’s been a tendency just to put more and more into them without doing much accounting of it. And then there’s the matter of complexity. Let’s say whatever one’s trying to model shows complexity. Then often the thinking seems to be that to get that complexity out, there’s a need to somehow have enough complexity in the model. And when complexity does manage to come out, there’s a feeling that this is some kind of triumph, and evidence that the model is on the right track. But actually—as I discovered in studying the computational universe of simple programs—this really isn’t the right intuition at all. Because it fundamentally doesn’t take into account computational irreducibility. And knowing that computational irreducibility is ubiquitous, we know that complexity is too. It’s not something special and “on the right track” that’s making a model produce complexity; instead producing complexity is just something a very wide range of computational models naturally do. Still, the general field and brand of complexity continued to gain traction. Back in 1986 my Complex Systems had been the only journal devoted to complex systems research. By the late 2010s there were dozens of journals in the field. And my original efforts from the 1980s to promote the study of complexity had been thoroughly dwarfed by a whole “complexity industry” that had grown up. But looking at what’s been done, I feel like there’s something important that’s missing. Yes, it’s wonderful that there’s been so much “complexity activity”. But it feels scattered and incoherent—and without a strong common thread.
Charting a Course for “Complexity”: Metamodeling, Ruliology and More 20.4 Returning to the Foundations of Complexity : There’s a vast amount that’s now been done under the banner of complexity. But how does it fit together? And what are its intellectual underpinnings? The dynamics of academia has led most of the ongoing activity of complexity research to be about specific applications in specific fields—and not really to concern itself with what basic science might lie underneath, and what the “foundations of complexity” might be. But the great power of basic science is the economy of scale it brings. Find one principle of basic science and it can inform a vast range of different specific applications, that would otherwise each have to be explored on their own. Learn one principle of basic science and you immediately know something that subsumes all sorts of particular things you would otherwise separately have to learn. So what about complexity? Is there something underneath all those specifics that one can view as a coherent “basic science of complexity”—and for example the raw material for something like a course on the “Foundations of Complexity”? At first it might not be obvious where to look for this. But there’s immediately a big clue. And it’s what is in a sense the biggest “meta discovery” of the study of complexity over the past few decades: that across all kinds of systems, computational models work. So then one’s led to the question of what the basic science of computational models—or computational systems in general—might be. But that’s precisely what my work on the computational universe of simple programs—and my book A New Kind of Science—are about. They’re about the core basic science of the computational universe, and the principles it involves—in a sense the foundational science of computation. It’s important, by the way, to distinguish this from computer science. Computer science is about programs and computations that we humans construct for certain purposes. But the foundational science we need is instead about programs and computations “in the wild”—and about what’s out there in general in the computational universe, independent of whether we humans would have a reason to construct or use it. It’s a very abstract kind of thing. That—like pure mathematics—can be studied completely on its own, without reference to any particular application. And in fact the analogy to pure mathematics is an apt one. Because just as pure mathematics is in a sense the abstract underpinning for the mathematical sciences and the whole mathematical paradigm for representing the world, so now our foundational science of computation is the abstract underpinning for the computational paradigm for representing the world—and for all the “computational X” fields that flow from it. So, yes, there is a core basic science of complexity. And it’s also essentially the foundational science of computation. And by studying this, we can bring together all sorts of seemingly disparate issues that arise in the study of complexity in different systems. Everywhere we’ll see computational irreducibility. Everywhere we’ll see intrinsic randomness generation. Everywhere we’ll see the effects of the Principle of Computational Equivalence. These are general, abstract things from pure basic science. They’re the intellectual underpinnings of the study of complexity—the “foundations of complexity”.
Charting a Course for “Complexity”: Metamodeling, Ruliology and More 20.5 Metamodeling and the Metatheory of Models : I was at a complexity conference once, talking to someone who was modeling fish and their behavior. Proudly the person showed me his simulated fish tank. “How many parameters does this involve?”, I asked. “About 90”, he said. “My gosh”, I said, “with that many parameters, you could put an elephant in your fish tank too!” If one wanted to make a simulated fish tank display just for people to watch, then having all those parameters might be just fine. But it’s not so helpful if one wants to understand the science of fish. The fish have different shapes. The fish swim around in different configurations. What are the core things that lead to what we see? To answer that, we have to drill down: we have to find the essence of fish shape, or fish behavior. At first, if confronted with complexity, we might say “It’s hopeless, we’ll never find the essence of what’s going on—it’s all too complicated”. But the whole point is that we know that in the computational universe of possible programs, there can in fact be simple programs with simple rules that lead to immense complexity. So even though there’s immense complexity in behavior we see, underneath it all there can still be something simple and understandable. In a sense, the concept of taking phenomena and drilling down to find their underlying essential causes is at the heart of reductionist science. But as this has traditionally been practiced, it’s relied on being able to see one’s way through this “drilling down” process, or in effect, to explicitly do reverse engineering. But a big lesson of the computational paradigm is the phenomenon of computational irreducibility—and the “irreducible distance” that can exist between rules and the behavior they produce. It’s a double-edged thing, however. Yes, it’s hard to drill down through computational irreducibility. But in the end the details of what’s underneath may not matter so much; the main features one sees may just be generic reflections of the phenomenon of computational irreducibility. Still, there are normally structural features of the underlying models (or their interpretations) that matter for particular applications. Is one dealing with something on a 2D grid? Are there nonlocal effects in the system? Is there directionality to the states of the system? And so on. If one looks at the literature of complexity, one finds all sorts of models for all sorts of systems. And often—like the fish example—the models are very complicated. But the question is: are there simpler models lurking underneath? Models simple enough that one can readily understand at least their basic rules and structure. Models simple enough that it’s plausible that they could be useful for other systems as well. To find such things is in a sense an exercise in what one can call “metamodeling”: trying to make a model of a model, doing reductionist science not on observations of the world, but on the structure of models. When I first worked on the problem of complexity, one of the main things I did was a piece of metamodeling. I was looking at models for a whole variety of phenomena, from snowflake growth to self-gravitating gases to neural nets. But what I did was to try to identify an underlying “metamodel” that would cover all of them. And what I came up with was simple cellular automata (which, by the way, don’t cover everything I had been looking at, but turn out to be very interesting anyway). As I think about it now, I realize that the activity of metamodeling is not a common one in science. (In mathematics, one could argue that something like categorification is somewhat analogous.) But to me personally, metamodeling has seemed very natural—because it’s very much like something I’ve done for a very long time, which is language design. What’s involved in language design? You start off from a whole collection of computations, and descriptions of how to do them. And then you try to drill down to identify a small set of primitives that let you conveniently build up those computations. Just like metamodeling is about removing all the “hairy” parts of models to get to their minimal, primitive forms, so also language design is about doing that for computations and computational structures. In both cases there’s a certain art to it. Because in both cases the consumers of those minimal forms are humans. And it’s to humans that they need to seem “simple” and understandable. Some of the practical definition of simplicity has to do with history. What, for example, has become familiar, or are there words for? Some is more about human perception. What can be represented by a diagram that our visual processing system can readily absorb? But once one’s found something minimal, the great value of it is that it tends to be very general. Whereas a detailed “hairy” model tends to have all sorts of features specific to a particular system, a simple model tends to be applicable to all sorts of systems. So by doing the metamodeling, and finding the simplest “common” model, one is effectively deriving something that will have the greatest leverage. I’ve seen this quite dramatically with cellular automata over the past forty years. Cellular automata are in a sense minimal models in which there’s a definite (discrete) structure for space and time and a finite number of states associated with each discrete cell. And it’s been remarkable how many different kinds of systems can successfully be modeled by cellular automata. So that for example of the 256 very simplest 2-color nearest-neighbor 1D rules, a significant fraction have found application somewhere, and many have found several (often completely different) applications. I have to say that I haven’t explicitly thought of myself as pursuing “metamodeling” in the past (and I only just invented the term!). But I believe it’s an important technique and idea. And it’s one that can “mine” the specific modeling achievements of work on complexity and bring them to a broader and more foundational level. In A New Kind of Science I cataloged and studied minimal kinds of models of many types. And in the twenty years since A New Kind of Science was finished, I have only seen a modest number of new minimal models (though I haven’t been looking for them with the focus that metamodeling now brings). But recently, I have another major example of what I now call metamodeling. For our Physics Project we’ve developed a particular class of models based on multiway hypergraph rewriting. But I’ve recently realized that there’s metamodeling to do here, and the result has been the general concept of multicomputation and multicomputational models. Returning to complexity, one can imagine taking all the academic papers in the field and identifying the models they use—and then trying to do metamodeling to classify and boil down these models. Often, I suspect, the resulting minimal classes of models will be ones we’ve already seen (and that, for example, appear in A New Kind of Science). But occasionally they will be new: in a sense new primitives for the language of modeling, and new “metascientific” output from the study of complexity.
Charting a Course for “Complexity”: Metamodeling, Ruliology and More 20.6 The Pure Basic Science of Ruliology : If one sets up a system to follow a particular set of simple rules, what will the system do? Or, put another way, how do all those simple programs out there in the computational universe of possible programs behave? These are pure, abstract questions of basic science. They’re questions one’s led to ask when one’s operating in the computational paradigm that I describe in A New Kind of Science. But at some level they’re questions about the specific science of what abstract rules (that we can describe as programs) do. What is that science? It’s not computer science, because that would be about programs we construct for particular purposes, rather than ones that are just “out there in the wilds of the computational universe”. It’s not (as such) mathematics, because it’s all about “seeing what rules do” rather than finding frameworks in which things can be proved. And in the end, it’s clear it’s actually a new science—that’s rich and broad, and that I, at least, have had the pleasure of practicing for forty years. But what should this science be called? I’ve wondered about this for decades. I’ve filled so many pages with possible names. Could it be based on Greek or Latin words associated with rules? Those are arch- and reg-: very well-trafficked roots. What about words associated with computation? That’d be logis- or calc-. None of these seem to work. But—in something akin to the process of metamodeling—we can ask: What is the essence of what we want to communicate in the word? It’s all about studying rules, and what their consequences are. So why not the simple and obvious “ruliology”? Yes, it’s a new and slightly unusual-sounding word. But I think it does well at communicating what this science that I’ve enjoyed for so long is about. And I, for one, will be pleased to call myself a “ruliologist”. But what is ruliology really about? It’s a pure, basic science—and a very clean and precise one. It’s about setting up abstract rules, and then seeing what they do. There’s no “wiggle room”. No issue with “reproducibility”. You run a rule, and it does what it does. The same every time. What does the rule 73 cellular automaton starting from a single black cell do? What does some particular Turing machine do? What about some particular multiway string substitution system? These are specific questions of ruliology. At first you might just do the computation, and visualize the result. But maybe you notice some particular feature. And then you can use whatever methods it takes to get a specific ruliological result—and to establish, for example, that in the rule 73 pattern, black cells appear only in odd-length blocks. Ruliology tends to start with specific cases of specific rules. But then it generalizes, looking at broader ranges of cases for a particular rule, or whole classes of rules. And it always has concrete things to do—visualizing behavior, measuring specific features, and so on. But ruliology quickly comes face to face with computational irreducibility. What does some particular case of some particular rule eventually do? That may require an irreducible amount of computational effort to find out—and if one insists on knowing what amounts to a general truly infinite-time result, it may be formally undecidable. It’s the same story with looking at different cases of a rule, or different rules. Is there any case that does this? Or any rule that does it? What’s remarkable to me—even after 40 years of ruliology—is how many surprises there end up being. You have some particular kind of rule. And it looks as if it’s only going to behave in some particular way. But no, eventually you find a case where it does something completely different, and unexpected. And, yes, this is in effect computational irreducibility reaching into what one’s seeing. Sometimes I’ve thought of ruliology as being at first a bit like natural history. You’re exploring the world of simple programs, finding what strange creatures exist in it—and capturing them for study. (And, yes, in actual biological natural history, the diversity of what one sees is presumably at its core exactly the same computational phenomenon we see in abstract ruliology.) So how does ruliology relate to complexity? It’s a core part—and in fact the most fundamental part—of studying the foundations of complexity. Ruliology is like studying complexity at its ultimate source. And about seeing just how complexity is generated from its simplest origins. Ruliology is what builds raw material—and intuition—for making models. It’s what shows us what’s possible in the computational universe, and what we can use to model—and understand—the systems we study. In metamodeling we’re going from models that have been constructed, and drilling down to see what’s underneath them. In ruliology we’re in a sense going the other way, building up from the minimal foundations to see what can happen. In some ways, ruliology is like natural science. It’s taking the computational universe as an abstracted analog of nature, and studying how things work in it. But in other ways, ruliology is something more generative than natural science: because within the science itself, it’s thinking not only about what is, but also about what can abstractly be generated. Ruliology in some ways starts as an experimental science, and in some ways is abstract and theoretical from the beginning. It’s experimental because it’s often concerned with just running simple programs and seeing what they do (and in general, computational irreducibility suggests you often can’t do better). But it’s abstract and theoretical in the sense that what’s being run is not some actual thing in the natural world, with all its details and approximations, but something completely precise, defined and computational. Like natural science, ruliology starts from observations—but then builds up to theories and principles. Long ago I found a simple classification of cellular automata (starting from random initial conditions)—somehow reminiscent of identifying solids, liquids and gases, or different kingdoms of organisms. But beyond such classifications, there are also much broader principles—with the most important, I believe, being the Principle of Computational Equivalence. The everyday course of doing ruliology doesn’t require engaging directly with the whole Principle of Computational Equivalence. But throughout ruliology, the principle is crucial in guiding intuition, and having an idea of what to expect. And, by the way, it’s from ruliology that we can get evidence (like the universality of rule 110, and of the 2,3 Turing machine) for the broad validity of the principle. I’ve been doing ruliology (though not by that name) for forty years. And I’ve done a lot of it. In fact, it’s probably been my top methodology in everything I’ve done in science. It’s what led me to understand the origins of complexity, first in cellular automata. It’s what led me to formulate the general ideas in A New Kind of Science. And it’s what gave me the intuition and impetus to launch our new Physics Project. I find ruliology deeply elegant, and satisfying. There’s something very aesthetic—at least to me—about the purity of just seeing what simple rules do. (And it doesn’t hurt that they often make very pleasing images.) It’s also satisfying when one can go from so little and get so much—and do so automatically, just by running something on a computer. And as well I like the fundamental permanence of ruliology. If one’s dealing with the simplest rules of some type, they’re going to be foundational not only now, but forever. It’s like simple mathematical constructs—like the icosahedron. There were icosahedral dice in ancient Egypt. But when we find them today, their shapes still seem completely modern—because the icosahedron is something fundamental and timeless. Just like the rule 30 pattern or countless other discoveries in ruliology. In a sense perhaps one of the biggest surprises is that ruliology is such a comparatively new activity. But as I cataloged in A New Kind of Science, it has precursors going back hundreds and perhaps thousands of years. But without the whole paradigm of A New Kind of Science, there wasn’t a context to understand why ruliology is so significant. So what constitutes a good piece of ruliology? I think it’s all about simplicity and minimality. The best ruliology happens after metamodeling is finished—and one’s really dealing with the simplest, most minimal class of rules of some particular type. In my efforts to do ruliology, for example in A New Kind of Science, I like to be able to “explain” the rules I’m using just by an explicit diagram, if possible with no words needed. Then it’s important to show what the rules do—as explicitly as possible. Sometimes—as in cellular automata—there’s a very obvious visual representation that can be used. But in other cases it’s important to do the work to find some scheme for visualization that’s as explicit as possible, and that both shows the whole of what’s going on and doesn’t introduce distracting or arbitrary additional elements. It’s amazing how often in doing ruliology I’ll end up making an array of thumbnail images of how certain rules behave. And, again, the explicitness of this is important. Yes, one often wants to do various kinds of filtering, say of rules. But in the end I’ve found that one needs to just look at what happens. Because that’s the only way to successfully notice the unexpected, and to get a sense of the irreducible complexity of what’s out there in the computational universe of possible rules. When I see papers that report what amounts to ruliology, I always like it when there are explicit pictures. I’m disappointed if all I see are formal definitions, or plots with curves on them. It’s an inevitable consequence of computational irreducibility that in doing good ruliology, one has to look at things more explicitly. One of the great things about ruliology as a field of study is how easy it is to explore new territory. The computational universe contains an infinite number of possible rules. And even among ones that one might consider “simple”, there are inevitably astronomically many on any human scale. But, OK, if one explores some particular ruliological system, what of it? It’s a bit like chemistry where one explores properties of some particular molecule. Exploring some particular class of rules, you may be lucky enough to come upon some new phenomenon, or understand some new general principle. But what you know you’ll be doing is systematically adding to the body of knowledge in ruliology. Why is that important? For a start, ruliology is what provides the raw material for making models, so you’re in effect creating a template for some potential future model. And in addition, when it comes to technology, an important approach that I’ve discussed (and used) quite extensively involves “mining” the computational universe for “technologically useful” programs. And good ruliology is crucial in helping to make that feasible. It’s a bit like creating technology in the physical universe. It was crucial, for example, that good physics and chemistry had been done on liquid crystals. Because that’s what allowed them to be identified—and used—in making displays. Beyond its “pragmatic” value for models and for technology, another thing ruliology does is to provide “empirical raw material” for making broader theories about the computational universe. When I discovered the Principle of Computational Equivalence, it was as a result of several years of detailed ruliology on particular types of rules. And good ruliology is what prepares and catalogs examples from which theoretical advances can be made. It’s worth mentioning that there’s a certain tendency to want to “nail down ruliology” using, for example, mathematics. And sometimes it’s possible to derive a nice summary of ruliological results using, say, some piece of discrete mathematics. But it’s remarkable how quickly the mathematics tends to get out of hand, with even a very simple rule having behavior that can only be captured by large amounts of obscure mathematics. But of course that’s in a sense just computational irreducibility rearing its head. And showing that mathematics is not the methodology to use—and that instead something new is needed. Which is precisely where ruliology comes in. I’ve spent many years defining the character and subject matter of what I’m now calling ruliology. But there’s something else I’ve done too, which is to build a large tower of practical technology for actually doing ruliology. It’s taken more than forty years to build up to what’s now the full-scale computational language that is the Wolfram Language. But all that time, I was using what we were building to do ruliology. The Wolfram Language is great and important for many things. But when it comes to ruliology, it’s simply a perfect fit. Of course it’s got lots of relevant built-in features. Like visualization, graph manipulation, etc., as well as immediate support for systems like cellular automata, substitution systems and Turing machines. But what’s even more important is that its fundamental symbolic structure gives it an explicit way to represent—and run—essentially any computational rule. In doing practical ruliological explorations—and for example searching the computational universe—it’s also useful to have immediate support for things like parallel computation. But another crucial aspect of the Wolfram Language for doing practical ruliology is the concept of notebooks and computable documents. Notebooks let one organize both the process of research and the presentation of its results. I’ve been accumulating research notebooks about ruliology for more than 30 years now—with textual notes, images of behavior, and code. And it’s a great thing. Because the stability of the Wolfram Language (and its notebook format) means that I can immediately go back to something I did 30 years ago, run the code, and build on it. And when it comes to presenting results, I can do it as a computational essay, created in a notebook—in which the task of exposition is shared between text, pictures and computational language code. In a traditional technical paper based on the mathematical paradigm, the formal part of the presentation will normally use mathematical notation. But for ruliology (as for “computational X” fields) what one needs instead is computational notation, or rather computational language—which is exactly what the Wolfram Language provides. And in a good piece of ruliology—and ruliology presentation—the notation should be simple, clear and elegant. And because it’s in computational language, it’s not just something people read; it’s also something that can immediately be executed or integrated somewhere else. What should the future of ruliology be? It’s a huge, wide-open field. In which there are many careers to be made, and immense numbers of papers and theses and books that can be written—that will build up a body of knowledge that advances not just the pure, basic science of the computational universe but also all the science and technology that flows from it.
Charting a Course for “Complexity”: Metamodeling, Ruliology and More 20.7 Philosophy and the Foundations of Complexity : How should the phenomenon of complexity affect one’s worldview, and one’s general way of thinking about things? It’s a bit of a roller-coaster-like ride. When first confronted with complexity in a system, one might think “There doesn’t seem to be any science to that”. But then with great effort it may turn out to be possible to “drill down” and find the underlying rules for the system, and perhaps they’ll even be quite simple. And at that point we might think “OK, science has got this one”—we’ve solved it. But that ignores computational irreducibility. And computational irreducibility implies that even though we may know the underlying rules, that doesn’t mean we can necessarily “scientifically predict” what the system will do; instead, it may take an irreducible amount of computational work to figure it out. Yes, you may have a model that correctly captures the underlying rules for a system—and even explains the overall complexity in the behavior of the system. But that absolutely does not mean that you can successfully make specific predictions about what the system will do. Because computational irreducibility gets in the way, essentially “eating away the power of science from the inside”—as an inevitable formal fact about how systems based on the computational paradigm typically behave. But in a sense even the very phenomenon of computational irreducibility—and even more so, the Principle of Computational Equivalence—give us ways to reason and think about things. It’s a bit like in evolutionary biology, or in economics, where there are principles that don’t specifically define predictions, but do give us ways to reason and think about things. So what are some conceptual and philosophical consequences of computational irreducibility? One thing it does is to explain ubiquitous apparent randomness in the world, and say why it must happen—or at least must be perceived to happen by computationally bounded observers like us. And another thing it does is to tell us something about the perception of free will. Even if the underlying rules for a system (such as us humans) are deterministic, there can be an inevitable layer of computational irreducibility which makes the system still seem to a computationally bounded observer to be “free”. Metamodeling and ruliology are in effect the extensions of traditional science needed to handle the phenomenon of complexity. But what about extensions to philosophy? For that one must think not just about the phenomenology of complexity, but really about its foundations. And that’s where I think one inevitably runs into the whole computational paradigm, with all its intellectual implications. So, yes, there’s a “philosophy of complexity”, but it’s really the “philosophy of the computational paradigm”. I started to explore this towards the end of A New Kind of Science. But there’s much more to be done, and it’s yet something else that can be reached by serious study of the foundations of complexity.
Charting a Course for “Complexity”: Metamodeling, Ruliology and More 20.8 Multicomputation and the (Surprise) Return of Reducibility : Computational irreducibility is a very strong phenomenon, that in a sense pervades the computational universe. But within computational irreducibility, there must always be pockets—or slices—of computational reducibility: aspects of a system that are amenable to a reduced description. And for example in doing ruliology, part of the effort is to catalog the computational reducibility one finds. But in typical ruliology—or, for example, a random sampling of the computational universe of possible programs—computational reducibility is at best a scattered phenomenon. It’s not something one can count on seeing. But there’s something confusing about this when it comes to thinking about our universe, and our experience of it. Because perhaps the most striking fact about our universe—and indeed the one that leads to the possibility of what we normally call science—is that there’s order in what happens in it. Yet even if the universe ultimately operates at the lowest level according to simple rules, we might expect that at our level, all we would see is rampant computational irreducibility. But in our recent Physics Project there has been a big surprise. Because with the structure of the models we used, it seemed that within all that computational irreducibility, we were always seeing certain slices of reducibility—that turn out to correspond to the major known laws of physics: general relativity and quantum mechanics. A more careful examination showed that what was picking out this computational reducibility was really the combination of two things. First, a certain general structure to the underlying model. And second, certain rather general features of us as observers of the system. In the usual computational paradigm, one imagines rules that are successively applied to determine how the state of a system should evolve in time. But our Physics Project needed a new paradigm—that I’ve recently called the multicomputational paradigm—in which there can be many possible states of a system evolving in effect on many possible interwoven threads of time. In the computational paradigm, one can always identify the particular state reached after a certain amount of evolution. But in the multicomputational paradigm, it takes an observer to define how a “perceived state” should be extracted from all the possible threads of time. In the multicomputational paradigm, the actual evolution on all the threads of time will show all sorts of computational irreducibility. But somehow what an observer like us perceives has “smoothed” all of that out. And what’s left is something that’s a reflection of the core structure of the underlying multicomputational rules. And that turns out to show a certain set of emergent “physics-like laws”. It’s all an important piece of metamodeling. We started from a model intended to capture fundamental physics. But we’ve been able to “drill down” to find the essential “primitive structure” underneath—which turns out to be the idea of multicomputation. And wherever multicomputation occurs, we can expect that there will be computational reducibility and emergent physics-like laws, at least for certain kinds of observers. So how does this relate to complexity? Well, when systems fundamentally follow the computational paradigm—with standard computational models—they’ll tend to show computational irreducibility and complexity. But if instead they follow the multicomputational paradigm, then there’ll be emergent laws to discover in them. There are all sorts of fields—like economics, linguistics, molecular biology, immunology, etc.—where I have recently come to suspect that there may be good multicomputational models to be made. And in these fields, yes, there will be complexity to be seen. But the multicomputational paradigm suggests that there will also be definite regularities and emergent laws. So in a sense from “within complexity” there will inexorably emerge a certain simplicity. So that if one “observes the right things” one can potentially find what amount to “ordinary scientific laws”. It’s a curious twist in the story of complexity, and one that I, for one, did not see coming. Back in the early 1980s when I was first working on complexity, I used to talk about finding “scientific laws of complexity”. And at some level computational irreducibility and the Principle of Computational Equivalence are very general such laws—that were at first very surprising to see. But what we’ve discovered is that in the multicomputational paradigm, there’s another surprise: complexity can produce simplicity. But not just any simplicity. Simplicity that specifically follows physics-like laws. And that for a variety of fields might indeed give us something we could consider to be “scientific laws of complexity”.
Charting a Course for “Complexity”: Metamodeling, Ruliology and More 20.9 What Should Happen Now : It’s a wonderful thing to see something go from “just an idea” to a whole, developed ecosystem in the world. But that’s what’s happened over the past forty years with the concept of doing science around the phenomenon of complexity. And over that time countless “workflows” associated with particular applications have been developed—and there’s been all sorts of activity in all sorts of areas. But now I think it’s time to take stock of what’s been achieved—and see what might be possible going forward. I myself have not been much involved in the “daily work of the complexity field” since my early efforts in the 1980s. And perhaps that distance makes it easier to see what lies ahead. For, yes, by now there’s plenty of understanding of how to apply “complexity-inspired methodology” (and computational models) in particular areas. But the great opportunity is to turbocharge all this by focusing again on the “foundations of complexity”—and bringing the basic science that arises from that to bear on all the various applications whose “workflows” have now been defined. But what is that basic science? Its great “symptom” is complexity. But there’s much more to it than that. It’s heavily based on the computational paradigm. And it’s full of deep and powerful ideas and methods. And I’ve been thinking about it for more than forty years. But it’s only very recently—particularly based on what I’ve learned from our Physics Project—that I think I see with true clarity just how that science should be defined and pursued. First, there’s what I’m calling here metamodeling: going from specific models constructed for particular applications, and working out what the underlying more minimal and more general models are. And second, there’s what I’m calling ruliology: the study of what possible rules (or possible programs) in the computational universe do. Metamodeling is a kind of “meta” analog of science, probably most directly related to activities like computational language design. Ruliology is a pure, basic science, a bit like pure mathematics, but based on a very different methodology. In both metamodeling and ruliology there is much of great value to do. And even after more than forty years of pursuing what I’m now calling ruliology, I feel as if I’ve only just scratched the surface of what’s possible. Applications under the banner of complexity will come and go as different fields and their objectives ebb and flow. But both metamodeling and ruliology have a certain purity, and clear anchoring to intellectual bedrock. And so we can expect that whatever is discovered there will—like the discoveries of pure mathematics—be part of the permanent corpus of theoretical knowledge. Hovering over all of what we might study around complexity is the phenomenon of computational irreducibility. But within that irreducibility are pockets and slices of reducibility. And informed by our Physics Project, we now know that multicomputational systems can be expected to expose to observers like us what amount to physics-like laws—in effect leveraging the phenomenon of complexity to deliver accessible scientific laws. Complexity is a field that fundamentally rests on the computational paradigm—and in a sense when we see complexity what is really happening is that some lump of irreducible computation is being exposed. So at its core, the study of complexity is a study of irreducible computation. It’s computation whose details are irreducibly hard to figure out. But which we can reason about, and which, for example, we can also use for technology. Even forty years ago, the fundamental origin of complexity still seemed like a complete mystery—a great secret of nature. But now through the computational paradigm, I think we have a clear notion of where complexity fundamentally comes from. And by leveraging the basic science of the computational universe—and what I’m now calling metamodeling and ruliology—there’s a tremendous opportunity that now exists to dramatically advance everything that’s been done under the banner of complexity. The first phase of “complexity” is complete. The ecosystem is built. The applications are identified. The workflows are defined. And now it’s time to return to the foundations of complexity. And to take the powerful basic science that lies there to define “complexity 2.0”. And to deliver on the amazing potential that the concept of studying complexity has for science.
Multicomputation with Numbers: The Case of Simple Multiway Systems 21.1 A Minimal Example of Multicomputation : Multicomputation is an important new paradigm, but one that can be quite difficult to understand. Here my goal is to discuss a minimal example: multiway systems based on numbers. Many general multicomputational phenomena will show up here in simple forms (though others will not). And the involvement of numbers will often allow us to make immediate use of traditional mathematical methods. A multiway system can be described as taking each of its states and repeatedly replacing it according to some rule or rules with a collection of states, merging any states produced that are identical. In our Physics Project, the states are combinations of relations between elements, represented by hypergraphs. We’ve also often considered string substitution systems, in which the states are strings of characters. But here I’ll consider the case in which the states are numbers, and for now just single integers. And in this case multiway systems can be represented in a particularly simple way, with each state s just being repeatedly replaced according to: For a “binary branching” case the update rule is: and one can represent the evolution of the system by the multiway graph which begins: and continues (indicating by red and by blue): With arbitrary “symbolic” this (“free multiway system”) tree is the only structure one can get. But things can get much less trivial when there are forms for , that “evaluate” in some way, because then there can be identities that make branches merge. And indeed most of what we’ll be discussing here is associated with this phenomenon and with the “entanglements” between states to which it leads. It’s worth noting that the specific setup we’re using here avoids quite a lot of the structural complexity that can exist in multicomputational systems. In the general case, states can contain multiple “tokens”, and updates can also “consume” multiple tokens. In our case here, each state just contains one token—which is a single number—and this is what is “consumed” at each step. (In our Physics Project, a state corresponds to a hyperedge which contains many hyperedge tokens, and the update rule typically consumes multiple hyperedges. In a string substitution system, a state is a character string which contains many character tokens, and the update typically consumes multiple—in this case, adjacent—character tokens.) With the setup we’re using here there’s one input but multiple outputs (2 in the example above) each time the update rule is applied (with the inputs and outputs each being individual numbers). It’s also perfectly possible to consider cases in which there are multiple inputs as well as multiple outputs. But here we’ll restrict ourselves to the “one-to-many” (“traditional multiway”) case. And it’s notable that this case is exceptionally easy to describe in the Wolfram Language: Multiway Systems Based on Addition As our first example, let’s consider multiway systems whose rules just involve addition. The trivial (“one-input, one-output”) rule: gives a multiway graph corresponding to a “one-way number line”: The rule: gives a “two-way number line”: But even: gives a slightly more complicated multiway graph: What’s going on here? Basically each triangle represents an identity. For example, starting from 1, applying twice gives 3, which is the same result as applying once. Or, writing the rule in the form the triangles are all the result of the fact that in this case For the “number line” rule, it’s obvious that we’ll eventually visit every integer—and the +1, +2 rule also visits every integer. Consider now instead of +1 and +2 the case of +2 and +3: After a few steps this gives: Continuing a little longer gives: It’s a little difficult to see what’s going on here. It helps to show which edges correspond to +2 and +3: We’ll return to this a little later, but once again we can see that there are cycles in this graph, corresponding to simple “commutativity identities”, such as as well as “LCM identities” such as (Note that in this case, all integers above 1 are eventually generated.) Let’s look now at a case with slightly larger integers: After 6 steps one gets a simple grid essentially made up of “commutativity identities”. But continuing a little longer one sees that it begins to “wrap around” eventually forming a kind of “tube” with a spiral grid on the outside: The “grid” is defined by “commutativity identities”. But the reason it’s a “closed tube” is that there are also “LCM identities”. To understand this, unravel everything into a grid with +4 and +7 directions—then draw lines between the duplicated numbers: The “tube” is formed by rolling the grid up in such a way as to merge these numbers. But now if we assume that the multiway graph is laid out (in 3D) so that each graph edge has unit length, application of Pythagoras’s theorem in the picture above shows that the effective circumference of the tube is . In another representation, we can unravel the tube by plotting numbers at {x, y} according to their decomposition in the form : (From this representation we can see that every value of n can be reached so long as .) For the rule the multiway graph forms a tube of circumference which can be visualized in 3D as: And what’s notable here is that even though we’re just following a simple discrete arithmetic process, we’re somehow “inevitably getting geometry” out of it. It’s a tiny, toy example of a much more general and powerful phenomenon that seems to be ubiquitous in multicomputational systems—and that in our models of physics is basically what leads to the emergence of things like the limiting continuum structure of space. We’ve seen a few specific example of “multiway addition systems”. What about the more general case? a “tube” is generated with circumference where = {a, b}/GCD[a, b] After enough steps, all integers of the form k GCD[a, b] will eventually be produced—which means that all integers are produced if a and b are relatively prime. There’s always a threshold, however, given by FrobeniusNumber[{a, b}]—which for a and b relatively prime is just a b – a – b. By the way, a particular number n—if it’s going to be generated at all—will first be generated at step (Note that the fact that the multiway graph approximates a finite-radius tube is a consequence of the commensurability of any integers a and b. If we had a rule like , we’d get an infinite 2D grid.) a tube is again formed, with a circumference effectively determined by the smaller pair (after GCD reduction) of a, b and c. And if GCD[a, b, c] = 1, all numbers above FrobeniusNumber[{a, b, c}] will eventually be generated.
Multicomputation with Numbers: The Case of Simple Multiway Systems 21.2 Pure Multiplication : One of the simplest cases of multiway systems are those based on pure multiplication. An example is (now starting from 1 rather than 0): In general, for we’ll get a simple 2D grid whenever a and b aren’t both powers of the same number. With d elements in the rule we’ll get a d-dimensional grid. For example, gives a 3D grid: If the multipliers in the rule are all powers of the same number, the multiway graph degenerates to some kind of ladder. In the case this is just: while for and in general for it is a “width-m” ladder graph. Multiplication and Addition: n ⟼ {a n, n + b} Let’s look now at combining multiplication and addition—to form what we might call affine multiway systems. As a first example, consider the case (which I actually already mentioned in A New Kind of Science): Considering the simplicity of the rule by which it was generated, this result looks surprisingly complex. One immediate result is that after t steps, the total number of distinct numbers reached is Fibonacci[t – 1], which increases exponentially like . Eventually the ensures that every integer is generated. But the often “jumps ahead”, and since the maximum number generated at step t is the “average density” of numbers falls exponentially like . Continuing the evolution further and using a different rendering we get the very “geometrical” (planar) structure What can we say about this structure? Apart from the first few steps (rendered at the center), it consists of a spiral of pentagons. Each pentagon (except the one at the center) has the form reflecting the relation Going out from the center, each successive layer in the spiral has twice the number of pentagons, with each pentagon at a given layer “spawning” two new pentagons at the next layer. Removing “incomplete pentagons” this can be rendered as: What about other rules of the general form: Here are the corresponding (“complete polygon”) results for through 5: The multiway graphs in these cases correspond to spirals of ()-gons defined by the identity or equivalently At successive layers in the spiral, the number of ()-gons increases like . Eventually the evolution of the system generates all possible integers, but at step t the number of distinct integers obtained so far is given by the generalized Fibonacci series obtained from which for large t is where is the k-nacci generalized golden ratio, which approaches for large k. If we consider it turns out that one gets the same basic structure (with ()-gons) for as for . For example, with one gets: The Rule n ⟼ {2n + 1, 3n + 1} For the rule: there are at first no equivalences that cause merging in the multiway graph: But after 5 steps we get: where now we see that 15 and 31 are connected “across branches”. After 10 steps this becomes: At a visual level this seems to consist of two basic components. First, a collection of loops, and second a collection of tree-like “loose ends”. Keeping only complete loops and going a few more steps we get: Unlike in previous cases, the “loops” (AKA “polygons”) are not of constant size. Here are the first few that occur (note these loops “overlap” in the sense that several “start the same way”): As before, each of these loops in effect corresponds to an identity about compositions of functions—though now it matters what these compositions are applied to. So, for example, the 4th loop above corresponds to (where k stands for the function ): In explicit form this becomes: where both sides evaluate to the same number, in this case 26815. Much as in the Physics Project, we can think of each “loop” as beginning with the creation of a “branch pair”, and ending with the merger of the different paths from each member of the pair. In a later section we’ll discuss the question of whether every branch pair always in the end re-merges. But for now we can just enumerate mergers—and we find that the first few occur at: (Note that a merger can never involve more than two branches, since any given number has at most one “pre-image” under and one under .) Here is a plot of the positions of the mergers—together with a quadratic fit (indicated by the dotted line): (As we’ll discuss later, the numbers at which these mergers occur are for example always of the form .) Taking second differences indicates a certain apparent randomness: What can we say about the overall structure of the multiway graph? One basic question is what numbers ever even occur in the evolution of the system. Here are the first few, for evolution starting from 0: And here are successive differences: Dividing successive m by the number gives a progressive estimate of the density of numbers: On a log-log scale this becomes showing a rough fit to —and suggesting an asymptotic density of 0. Note, by the way, that while the maximum gap grows on average linearly (roughly like 0.17 m) the distance between gaps of size 1 shows evidence of remaining bounded: (A related result from the 1970s states that the original sequence contains infinite-length arithmetic progressions—implying the presence of infinite runs of numbers whose differences are constant.) The More General “Affine” Case: n ⟼ {a n + b, c n + d} Not every rule of the form: leads to a complex multiway graph. For example: just gives a pure binary tree since 2n just adds a 1 at the beginning of the binary digit sequence of n, while adds one at the end: Meanwhile: gives a simple grid: where at level t the numbers that appear are simply: and the pattern of use of the two cases in the rule makes it clear why the grid structure occurs. Here are the behaviors of all inequivalent nontrivial rules of the form: with constants up to 3: “Ribbons” are seen only when . “Simple webs” are seen when . “Simple grids” are seen whenever the two cases in the rule commute, i.e. which occurs whenever: “Simple trees” are seen whenever: In other cases there seems to be irregular merging, as in the case above. And keeping only nontrivial inequivalent cases these are the results after removing loose ends: Note that adding another element in the rule can make things significantly more complicated. An example is: After 8 steps this gives or in another rendering: After a few more steps, with “loose ends” removed, one gets the still-rather-unilluminating result (though one that we will discuss further in the next section).
Multicomputation with Numbers: The Case of Simple Multiway Systems 21.3 The Phenomenon of Confluence : Will every branching of paths in the multiway graph eventually merge again? If they do, then the system is confluent (which in this case is equivalent to saying that it’s causal invariant—an important property in our Physics Project). It turns out that all rules of the following forms are confluent: But among rules of the form: confluence depends on the values of a, b, c and d. When multiway graphs are “simple webs” or “simple grids” there is obvious confluence. And when the graphs are simple trees, there is obviously not confluence. But what about a case like the rule we discussed above: We plotted above the “positions” of mergers that occur. But are there “enough” mergers to “rejoin” all branchings? Here are the first few branchings that occur: For the pair 3, 4 one can reach a “merged” end state on the following paths: which are embedded in the whole multiway graph (without loose ends) as: For the pair 9, 13 both eventually reach 177151, but 9 takes 13 steps to do so: Here’s a summary of what we know about what happens with the first few branchings: So what about the total number of branchings and mergings? This is what happens for the first several steps: The number of branchings at step t approximates: while the number of mergings seems to grow systematically more slowly, perhaps like 1.: And based on this it seems plausible that the system is not in the end confluent. But how might we show this? And what is the best way to figure out if any particular branch pair (say 21, 31) will ever merge? One way to look for mergings is just to evolve the multiway graph from each member of the pair, and check if they overlap. But as we can see even for the pair {3, 4} this effectively involves “treeing out” an exponential number of cases: Is there a way to do this more efficiently, or in effect to prune the trees? A notable feature of the original rule is that the numbers it generates always increase at each step. So one thing to do is just to discard all elements at a particular step in one graph that cannot reach the “minimum frontier” in the other graph. But on its own, this leads to only very minor reduction in the size of graph that has to be considered. To find what is potentially a much more effective “optimization” let’s look at some examples of mergings: It’s clear that the final step has to consist of one application of and one of (i.e. one red edge and one blue edge). But these examples suggest that there are also further regularities. At the merging point it must be true that for some integers u and v. But for this to be true, the merged value (i.e. or ) must for example be equal to 1 mod 2, 3 and 6. Using the structure one level back we also have: implying that the merged value must be 3 mod 4, 7 mod 12, 13 mod 18 and 36 mod 31. Additional constraints from going even further back imply in the end that the merged value must have the following pattern of residues: But now let’s consider the whole system modulo k. Then there are just k possible values, and the multiway graph must be finite. For example, for we get: Dropping the “transient parts” leaves just: These graphs can be thought of as reductions of the multiway graph (and, conversely, the multiway graph is a covering of them). The graphs can also be thought of as finite automata that define regular languages whose elements are the “2” and “3” transformations that appear on the edges. Any sequence of “2” and “3” transformations that can occur in the multiway graph must then correspond to a valid word in this regular language. But what we have seen is that for certain values of k, mergers in the multiway graph always occur at particular (“acceptor”) states in the finite automata. In the case , every merger occurs at the 7 state. But by tracing possible paths in the finite automaton we now can read off what sequences of transformations can lead to a merger: And what’s notable is that only a certain fraction of all possible sequences of length m can occur; asymptotically, about 28%. The most stringent analogous constraints come from the graph: And we see that even for sequences of length 3 fewer are allowed than from the graph: Asymptotically the number of allowed sequences is about 3% of the possible. And so the conclusion is that if one wants to find mergings in the multiway graph it’s not necessary to tree out all possible sequences of transformations; one only needs at most the 30× smaller number of sequences “accepted by the mod-144 finite automaton”. It’s possible to do a little better than this, by looking not just at sequences allowed by the finite automaton for a particular k, but at finite automata for a collection of values of k (say as in the table above). But while these techniques deliver significant practical speedups they do not seem to significantly alter the asymptotic resources needed. So what will it take to determine whether the pair {21, 31} ever merges? I don’t know. And for example I don’t know any way to find an upper bound on the number of steps after which we’d be able to say “if it hasn’t merged yet, it never will”. I’m sure that if we look at different branch pairs, there will be tricks for particular cases. But I suspect that the general problem of determining merging will show computational irreducibility, and that for example there will be no fundamentally better way to determine whether a particular branch pair has merged after t steps than by essentially enumerating every possible evolution for that number of steps. But if this is the case, it means that the general infinite-time question of whether a branch pair will merge is undecidable—and can never be guaranteed to be answerable with a bounded amount of computational effort. It’s a lower bar to ask whether the question can be answered using a finite proof in, say, Peano arithmetic. And I think it’s very likely that the overall question of whether all branch pairs merge—so that the system is confluent—is a statement that can never, for example, be established purely within Peano arithmetic. There are quite a few other candidates for the “simplest ‘numerical’ statement independent of Peano arithmetic”. But it seems at least conceivable that this one might be more accessible to proof than most. It’s worth mentioning, by the way, that (as we have seen extensively in the Physics Project) the presence of confluence does not imply that a multiway system must show simple overall behavior. Consider for example the rule (also discussed at the end of the previous section): Running for a few more steps, removing loose ends and rendering in 3D gives: But despite this complexity, this is a confluent rule. It’s already an indication of this that mergings pretty much “keep up” with branchings in this multiway system: The first few branchings (now all 3-way) are: All the pairs here merge (often somewhat degenerately) in just a few steps. Here are examples of how they work: Branchial Space and Numerical Value Space Consider the first few steps of the rule At each “layer” we can form a branchial graph by joining nodes that have common ancestors on the step before: Continuing for a few more steps we get: We can imagine (as we do in our Physics Project) that in an appropriate (if rather subtle) limit such branchial graphs can be thought of as defining a “branchial space” in which each node has a definite position. (One of many subtleties is that the particular branchial graphs we show here are specific to the particular “layering” of the multiway graph that we’ve used; different foliations would give different results.) But whereas in our Physics Project and many other applications of the multicomputational paradigm the only real way to define “positions” for nodes in the multiway graph is through something like branchial space, there is a much more direct approach that can be taken in multiway systems based on numbers—because every node is labeled by a number which one can imagine directly using as a coordinate. As an example, let’s take the multiway graph above, and make the horizontal position of each node be determined by its value: Or, better, by the log of its value: Continuing for more steps, we get: Now, for example, we can ask—given the particular choice of layers we have made here—what the distribution of (logarithmic) values reached on successive layers will be, and one finds that the results converge quite quickly: (By the way, in these results we’ve not included “path weights”, which determine how many different paths lead from the initial number to a particular result. In the example shown, including path weights doesn’t make a difference to the form of the final result.) So what is the correspondence between the layout of nodes in “branchial space” and in “numerical value space”? Here’s what happens if we lay out a branchial graph using (logarithmic) numerical value as x coordinate: Perhaps more useful is to plot branchial distance versus (logarithmic) numerical distance for every pair of connected nodes at a particular layer: And at least in this case, there is perhaps a slight correlation to be seen.
Multicomputation with Numbers: The Case of Simple Multiway Systems 21.4 Negative Numbers : The rules we’ve considered so far all involve only non-negative numbers. What happens if we include negative numbers? Generally the results are very similar to those with non-negative numbers. For example: just gives in which there is effectively both a “positive” and “negative” “web”. A rule like turns out to yield essentially only positive numbers, yielding after removing loose ends gives a more balanced collection of positive and negative numbers (with positive numbers indicated by dark nodes), but the final graph is still quite similar: “Floor” and Related Rules So far we’ve considered only rules based on ordinary arithmetic functions. As a first example of going beyond that, consider the rule: Running this for 50 steps we get: A notable feature here is that only one “fresh” node is added at each step—and the whole thing grows like a Fermat spiral. After 250 steps the multiway graph has the form which we can readily see is essentially a “binary tree superimposed on a spiral”. Dividing by 3 instead of 2 makes it a ternary tree: Using Round instead of Floor gives a mixed binary and ternary tree: What about rules of the form: Here are the results for a few values of a: Continuing for more steps we get: has far fewer “loose ends”: What are the “grid patches”? Picking out some of the patches we can see they’re places where a number that can be “halved a lot” appears—and just like in our pure multiplication rules above, and 3n represent commuting operations that form a grid: Conditional Division, Inverse Iterations and the 3n+1 Problem Including Floor[] is a bit like having different functions for even and odd n. What happens if we do this more explicitly? Consider for example The result is essentially identical to the Floor case: Here are a couple of other cases, at least qualitatively similar to what we’ve seen before: But now consider as we did at the beginning: What is the inverse of this? One can think of it as being which gives for example: or continuing for longer: How about: Now the “inverse” is: But in this case since most numbers are not reached in the original iteration, most “don’t have inverses”. However, picking an initial number like 4495, which happens to be a merge point, yields: Note that this “inverse iteration” always monotonically decreases towards 0—reaching it in at most steps. But now we can compare with the well-known 3n+1 problem, defined by the “singleway” iteration: And while in this case the intermediate numbers sometimes increase, all known initial conditions eventually evolve to a simple cycle: But now we can “invert” the problem, by considering the rule: equivalent to: which gives after 10 steps: Continuing this to 25 steps one gets: Removing loose ends this then becomes: or after more steps, and rendered in 3D: The 3n+1 problem now asks whether as the multiway graph is built, it will eventually include every number. But from a multicomputational point of view there are new questions to ask—like whether the “inverse-3n+1-problem” multiway system is confluent. The first few branchings in the multiway graph in this case are and all of these re-merge after at most 13 steps. The total number of branchings and mergings on successive steps is given by: Including more steps one gets: which suggests that there is indeed confluence in this case—though, like for the problem of termination in the original 3n+1 problem, it may be extremely difficult to determine this for sure.
Multicomputation with Numbers: The Case of Simple Multiway Systems 21.5 Other Kinds of Rules : All the rules we’ve used so far are—up to conditionals—fundamentally “linear”. But we can also consider “polynomial” rules. With pure powers, as in the multiway graph is just the one associated with the addition of exponents: In a case like: the graph is a pure tree while in a case like: there is “early merging”, followed by a pure tree: There are also cases like: which lead to “continued merging” but when loose ends are removed, they are revealed to behave in rather simple ways: In a case like: however, there is at least slightly more complicated merging (shown here after removing loose ends): If we include negative numbers we find cases like: But in other “polynomial” cases one tends to get only trees; a merging corresponds to a solution to a high-degree Diophantine equation, and things like the ABC conjecture tend to suggest that very few of these exist. Returning to the “linear” case, we can consider—as we did above—multiway graphs mod k. Such graphs always have just k nodes. And in a case like: with graph: Not too surprisingly, there is a definite structure to such remainder graphs. Here is the sequence of “binary remainder graphs” generated from the rule for successive values of k: Continuing a number-theoretical theme, we may note that the familiar “divisor graph” for a number can be considered as a multiway graph generated by the rule: Here’s an example for 100: Transitive reduction gives a graph which in this case is essentially a grid: Other initial numbers can give more complicated graphs but in general the transitive reduction is essentially a grid graph of dimension PrimeNu: As an alternative to looking at divisors, we can look, for example, at a rule which transforms any number to the list of numbers relatively prime to it: The transitive reduction of this is always trivial, however: One general way to “probe” any function is to look at a multiway graph generated by the rule: Here, for example, is the result for: starting with: Once again, the transitive reduction is very simple: As another example, we can look at: where each “efflorescence” corresponds to a prime gap: As a final example we can consider the digit-reversal function.
Multicomputation with Numbers: The Case of Simple Multiway Systems 21.6 Non-Integer Values : In almost everything we’ve discussed so far, we’ve been considering only integer values, both in our rules and our initial conditions. So what happens if we start a rule like with a non-integer value? Rather than taking a specific initial value, we can just use a symbolic value x—and it then turns out that the multiway graph is the same regardless of the value of x, integer or non-integer: What if the rule contains non-integer values? In a case like: the basic properties of addition ensure that the multiway graph will always have the same grid structure, regardless of a, b and the initial value x: But in a case like: things are more complicated. For arbitrary symbolic a, b and initial x, there are no relations that apply, and so the multiway graph is a pure tree: For a specific value of b, however, there are already relations, and a more complicated structure develops: Continuing for more steps and removing loose ends we get: which is to be compared to the result from above for , : What happens if we choose a non-integer value of b, say: We immediately see that there are “special relations” associated with and its powers: Continuing for longer we get the somewhat complex structure: or in a different rendering with loose ends removed: This structure is very dependent on the algebraic properties of . For a transcendental number like π there are no “special relations”, and the multiway graph will be a tree. For we get and for .
Multicomputation with Numbers: The Case of Simple Multiway Systems 21.7 Complex Numbers : There are many possible generalizations to consider. An immediate one is to complex integers. For real numbers always generates a grid. But for example: instead generates: Continuing for longer, the graph becomes: One feature of having values that are complex numbers is that these values themselves can be used to define coordinates to lay out the nodes of the multiway graph in the plane—giving in this case: or after more steps: Similarly: The non-branching rule: yields: If we combine multiplication with addition, we get different forms—and we can make some interesting mathematical connections. Consider rules of the form where c is some complex number. I considered such rules in A New Kind of Science as a practical model of plant growth (though already then I recognized their connection to multiway systems). If we look at the case the multiway graph is structurally just a tree: But if we plot nodes at the positions in the complex plane corresponding to their values we get: Continuing this, and deemphasizing the “multiway edges” we see a characteristic “fractal-like” pattern: Note that this is in some sense dual to the typical “line segment iteration” nested construction: Adding a third “real” branch: we get: And with: the result builds up to a typical Sierpinski pattern: These pictures suggest that at least in the limit of an infinite number of steps there will be all sorts of merging between branches. And indeed it is fairly straightforward to prove this. But what about after, say, t steps? The result from each branch for the rule: is a polynomial such as: So now the question of merging becomes a question of finding solutions to equations which equate the polynomials associated with different possible branches. The simplest nontrivial case equates branch {1, 1} with branch {2, 2}, yielding the equation: with solution: We can see this merging in action with the rule: The core of what it generates is the repetitive structure: A few additional results are (where the decimals are algebraic numbers of degree 6, and a is a real number): In a case like: there is an “early merger” but then the system just generates a tree: The family of rules of the form: shows more elaborate behavior. For we get: Continuing for more steps this becomes: For we get instead: If we look at the actual distribution of values obtained by such rules we find for example: If we go beyond multiway systems with pure “1 + c n” rules we soon get results very similar to ones we’ve seen in previous sections. For example gives multiway graph (after removing loose ends) Placing nodes according to their numerical values this then has the form.
Multicomputation with Numbers: The Case of Simple Multiway Systems 21.8 Collections of Numbers, and Causal Graphs : In studying multiway systems based on complex numbers we’re effectively considering a special case of multiway systems based on collections of numbers. If the complex-number rules are linear, then what we have are iterated affine maps—that form the basis for what I’ve called geometric substitution systems. As a slightly more general case we can consider multiway systems in which we take pairs of numbers v and apply the rule: where now a and b are matrices. If both matrices are the form then this is equivalent to the case of complex numbers. But we can also for example consider a rule like: which yields: or after more steps and in a different rendering: Laying this out in 2D using the actual pairs of numbers as coordinates, this becomes: Here are samples of typical behavior with 0, 1 matrices: Beyond pure matrix multiplication, we can also consider a rule that adds constant vectors, as in: We can also think in a more “elementwise” way, constructing for example simple rules such as This generates the multiway graph: Continuing for longer and removing loose ends yields: Using values as coordinates then gives: In our Physics Project and other applications of multicomputation, we often discuss causal graphs, that track the causal relationships between updating events. So why is it that these haven’t come up in our discussion of multiway systems based on numbers? The basic reason is that when our states are individual numbers, there’s no reason to separately track updating events and transformations of states because these are exactly the same—because every time a state (i.e. a number) is transformed the number as a whole is “consumed” and new numbers are produced. Or, in other words, the flow of “data” is the same as the flow of “causal information”—so that if we did record events, there’d just be one on each edge of the multiway graph. But the story is different as soon as our states don’t just contain individual “atomic” things, like single numbers. Because then an updating event can affect just part of a state—and asking what causal relationships there may be between events becomes something separate from asking about the transformation of whole states. With a rule of the form, say: things are still fairly trivial. Yes, there are separate “x” and “y” events. But they don’t mix, so we’ll just get two independent causal graphs. Things can be less trivial in a case like the one above, of the form: But now there is a different problem. Let’s say that the rule transforms {x, y} to {y + 1, x + 1}. How should we decompose that into “elementary events”? We could say there’s one event that swaps x and y, and others that add 1. Or something different. It’s hard to know. So why haven’t we encountered this kind of problem in other multicomputational systems, say in hypergraph rewriting systems or string substitution systems? The point is that in these systems the underlying elements always have a certain unique identity, which allows their “flow” to be traced. In our Physics Project, for example, each hypergraph updating event that occurs affects certain particular “atoms of space” (that we can think of as being labeled by unique identifiers)—and so we can readily trace how the effects of different events are related. Similarly, in a string substitution system, we can trace which characters at which positions in the string were affected by a given event, and we can then trace which new characters at which new positions these affect. But in a system based on numbers this tracing of “unique elements” doesn’t really apply. We might think of 3 as being . But there’s nothing that uniquely tags these 1s, and allows us to trace how they affect 1s that might make up other numbers. In a sense, the whole point of numbers is to abstract away from the labeling of individual objects—and just ask the aggregate question of “how many” there are. So in effect the “packaging” of information into numbers can be thought of as “washing out” causal relationships. When we give a rule based on numbers what it primarily does is to specify transformations for values. But it’s perfectly possible to add an ancillary “causal rule”, that, for example, can define which elements in an “input” list of numbers should be thought of as being “used as the inputs” to produce particular numbers in an output list of numbers. There’s another subtlety here, though. The point of a multiway graph is to represent all possible different histories for a system, corresponding to all possible sequences of transformations for states. A particular history corresponds to a particular path in the multiway graph. And if—as in a multiway system based on single numbers—each step in this path is associated with a single, specific event, then the causal graph associated with a particular history will always be trivial. But in something like a hypergraph- or string-based system there’s usually a nontrivial causal graph even for a single path of history. And the reason is that each transformation between states can involve multiple events—acting on different parts of the state—and there can be nontrivial causal relationships between these events “mediated” by shared elements in the state. One can think of the resulting causal graph as representing causal relationships in “spacetime”. Successive events define the passage of time. And the layout of different elements in each state can be thought of as defining something like space. But in a multiway system based on single numbers, there isn’t a natural notion of space associated with each state, because the states are just single numbers which “don’t have enough structure” to correspond to something like space. If we’re dealing with collections of numbers, there’s more possibility of “having something like space”. But it’s easiest to imagine this when one’s dealing with very large collections of numbers, and when the “locations” of the numbers are more important than their values—at which point the fact that they’re numbers (rather than, say, characters in a string) doesn’t make much difference. But in a multiway system one’s dealing with multiple paths of history, not just one. And one can then start asking about causal relationships not just within a single path of history, but across different paths: a multiway causal graph. And that’s the kind of causal graph we’ll readily construct for a multiway system based on numbers. For a system based on strings or hypergraphs there’s a certain wastefulness to starting with a standard multiway graph of transformations between states. Because if one looks at all possible states, there’s typically a lot of repetition between the “context” of different updating events. And so an alternative approach is to look just as the “tokens” that are involved in each event: hyperedges in a hypergraph, or runs of characters in a string. So how does it work for a multiway system based on numbers? For this we have to again think about how our states are decomposed for purposes of events, or, in other words, what the “tokens” in them are. And for multiway systems based on single numbers, the natural thing is just to consider each number as a token. For collections of numbers, it’s less obvious how things should work. And one possibility is to treat each number in the collection as a separate token, and perhaps to ignore any ordering or placement in the collection. We could then end up with a “multi-token” rule like whose behavior we can represent with a token-event graph: But given this, there is then the issue of deciding how collections of tokens should be thought of as aggregated into states. And in general multi-token numerical multiway systems represent a whole separate domain of exploration from what we have considered here. A basic point, however, is that while our investigations of things like hypergraph and string systems have usually had a substantial “spatial component”, our investigation of multiway systems based on numbers tends to be “more branchial”, and very much centered around the relationships between different branches of history. This does not mean that there is nothing “geometrical” about what is going on. And in fact we fully expect that in an appropriate limit branchial space will indeed have a geometrical structure—and we have even seen examples of this here. It is just that that geometrical structure is—in the language of physics—about the space of quantum states, not about physical space. So this means that our intuition about ordinary physical space won’t necessarily apply. But the important point is that by studying multiway systems based on numbers we can now hope to sharpen our understanding and intuition about things like quantum mechanics.
Multicomputation with Numbers: The Case of Simple Multiway Systems 21.9 Much More to Explore… : The basic setup for multiway systems based on numbers is very simple. But what we’ve seen here is that—just like for so many other kinds of systems in the computational universe—the behavior of multiway systems based on numbers can be far from simple. In many ways, what’s here just scratches the surface of multiway systems based on numbers. There is much more to explore, in many different directions. There are many additional connections to traditional mathematics (and notably number theory) to be made. There are also questions about the geometrical structures that can be generated, and their mathematical characterization. In the general study of multicomputational systems, branchial—and causal—graphs are important. But here we have barely begun to consider them. A particularly important issue that we haven’t addressed at all is that of alternative possible foliations. In general it has been difficult to characterize these. But it seems possible that in multiway systems based on numbers these may be amenable to investigation with some kind of mathematical techniques. In addition, for things like our Physics Project questions about the coordinatization of branchial space are of great significance—and the “natural coordinatizability” of numbers makes multiway systems based on numbers potentially an attractive place to study these kinds of questions. Here we’ve considered only ordinary multiway systems, in which the rules always transform one object into several. It’s also perfectly possible to study more general multicomputational systems in which the rules can “consume” multiple objects—and this is particularly straightforward to set up in the case of numbers. Here we’ve mostly looked at multiway systems whose states are individual integers. But we can consider other kinds of numbers and collections of numbers. We can also imagine generalizing to other kinds of mathematical objects. These could be algebraic constructs (such a polynomials) based on ordinary real or complex numbers. But they could also, for example, be objects from universal algebra. The basic setup for multiway systems—involving repeatedly applying functions—can be thought of as equivalent to repeatedly multiplying by elements (say, generators) of a semigroup. Without any relations between these elements, the multiway graphs we’ll get will always be trees. But if we add relations things can be more complicated. Multiway systems based on semigroups are in a sense “lower level” than ones based on numbers. In something like arithmetic, one already has immediate knowledge of operations and equivalences between objects. But in a semigroup, these all have to be built up. Of course, if one goes beyond integers, equivalences can be difficult to determine even between numbers (say different representations of radicals or, worse, transcendental numbers). In their basic construction, multiway systems are fundamentally discrete—involving as they do discrete states, discrete branches, and discrete notions like merging. But in our Physics Project and other applications of the multicomputational paradigm it’s often of interest to think about “continuum limits” of multiway systems. And given that real numbers provide the quintessential example of a continuum one might suppose that by somehow looking at multiway systems based on real numbers one could understand their continuum limit. But it’s not so simple. Yes, one can imagine allowing a whole “real parameter’s worth” of outputs from the multiway rule. But the issue is how to “knit these together” from one step to the next. The situation is somewhat similar to what happens when one looks at ensembles of random walks, or stochastic partial differential equations. But with multiway systems things are both cleaner and more general. The closest analogy is probably to path integrals of the kind considered in quantum mechanics. And in a sense this is not surprising, because it is precisely the appearance of multiway systems in our Physics Project that seems to lead to quantum mechanics—and in a “continuum limit” to the path integral there. It’s not clear just how multiway systems are best generalized to the continuum case. But multiway systems based on numbers seem to provide a potentially promising bridge to existing mathematical investigations of the continuum—and I think have a good chance of revealing some elegant and powerful mathematics. I first looked at multiway systems based on numbers back in the early 1990s, and I always meant to come back and look at them further. But what we’ve found here is that they’re richer and more interesting than I ever imagined. And particularly from what we’ve now seen I expect them to have a very bright future, and for all sorts of important science and mathematics to connect to them, and flow from them.
The Concept of the Ruliad 22.1 The Entangled Limit of Everything : I call it the ruliad. Think of it as the entangled limit of everything that is computationally possible: the result of following all possible computational rules in all possible ways. It’s yet another surprising construct that’s arisen from our Physics Project. And it’s one that I think has extremely deep implications—both in science and beyond. In many ways, the ruliad is a strange and profoundly abstract thing. But it’s something very universal—a kind of ultimate limit of all abstraction and generalization. And it encapsulates not only all formal possibilities but also everything about our physical universe—and everything we experience can be thought of as sampling that part of the ruliad that corresponds to our particular way of perceiving and interpreting the universe. We’re going to be able to say many things about the ruliad without engaging in all its technical details. (And—it should be said at the outset—we’re still only at the very beginning of nailing down those technical details and setting up the difficult mathematics and formalism they involve.) But to ground things here, let’s start with a slightly technical discussion of what the ruliad is. In the language of our Physics Project, it’s the ultimate limit of all rulial multiway systems. And as such, it traces out the entangled consequences of progressively applying all possible computational rules. Here is an example of an ordinary multiway system based on the string replacement rules {A → AB, BB → A} (indicated respectively by blueish and reddish edges): At each step, the rules are applied in all possible ways to each state. Often this generates multiple new states, leading to branching in the graph. But, importantly, there can also be merging—from multiple states being transformed to the same state. The idea of a rulial multiway system is not just to apply particular rules in all possible ways, but to apply all possible rules of a given form. For example, if we consider “1 → 2, 2 → 1 A, B string rules”, the possible rules are and the resulting multiway graph is (where now we’re using purple to indicate that there are edges for every possible rule): Continuing a little longer, and with a different layout, we get: This may already look a little complicated. But the ruliad is something in a sense infinitely more complicated. Its concept is to use not just all rules of a given form, but all possible rules. And to apply these rules to all possible initial conditions. And to run the rules for an infinite number of steps. The pictures above can be thought of as coarse finite approximations to the ruliad. The full ruliad involves taking the infinite limits of all possible rules, all possible initial conditions and all possible steps. Needless to say, this is a complicated thing to do, and there are many subtleties yet to work out about how to do it. Perhaps the most obviously difficult issue is how conceivably to enumerate “all possible rules”. But here we can use the Principle of Computational Equivalence to tell us that whatever “basis” we use, what comes out will eventually be effectively equivalent. Above we used string substitution systems. But here, for example, is a rulial multiway system made with 2-state 2-color Turing machines: And here is a rulial multiway system made from hypergraph rewriting of the kind used in our Physics Project, using all rules with signature : As another example, consider a multiway system based on numbers, in which the rules multiply by each possible integer: Here’s what happens starting with 1 (and truncating the graph whenever the value exceeds 100): Even with this simple setup, the results are surprisingly complicated (though it’s possible to give quite a bit of analysis in this particular case, as described in the Appendix at the end of this piece). The beginning of the multiway graph is nevertheless simple: from 1 we connect to each successive integer. But then things get more complicated. To see what’s going on, let’s look at a fragment of the graph: In a sense, everything would be simple if every path in the graph were separate: But the basic concept of multiway systems is that equivalent states should be merged—so here the “two ways to get 6” (i.e. 1 × 2 × 3 and 1 × 3 × 2) are combined, and what appears in the multiway graph is: For integers, the obvious notion of equivalence is numerical equality. For hypergraphs, it’s isomorphism. But the important point is that equivalence is what makes the multiway graph nontrivial. We can think about what it does as being to entangle paths. Without equivalence, different paths in the multiway system—corresponding to different possible histories—would all be separate. But equivalence entangles them. The full ruliad is in effect a representation of all possible computations. And what gives it structure is the equivalences that exist between states generated by different computations. In a sense, there are two forces at work: the “forward” effect of the progress of computation, and the “sideways” effect of equivalences that entangle different computations. (Mathematically this can be thought of as being like decomposing the ruliad structure in terms of fibrations and foliations..
The Concept of the Ruliad 22.2 Experiencing the Ruliad : In thinking about finding a fundamental theory of physics, one thing always bothered me. Imagine we successfully identify a rule that describes everything about our universe. Then the obvious next question will be: “Why this rule, and not another?” Well, how about if actually the universe in effect just runs every possible rule? What would this mean? It means that in a sense the “full story” of the universe is just the ruliad. But the ruliad contains everything that is computationally possible. So why then do we have the perception that the universe has specific laws, and that definite things happen in it? It all has to do with the fact that we are bounded observers, embedded within the ruliad. We never get to see the full ruliad; we just sample tiny parts of it, parsing them according to our particular methods of perception and analysis. And the crucial point is that for coherent observers like us, there are certain robust features that we will inevitably see in the ruliad. And these features turn out to include fundamental laws of our physics, in particular general relativity and quantum mechanics. One can imagine an observer very different from us (say some kind of alien intelligence) who would sample different aspects of the ruliad, and deduce different laws. But one of the surprising core discoveries of our Physics Project is that even an observer with quite basic features like us will experience laws of physics that precisely correspond to ones we know. An analogy (that’s actually ultimately the result of the same underlying phenomenon) may help to illustrate what’s going on. Consider molecules in a gas. The molecules bounce around in a complicated pattern that depends on their detailed properties. But an observer like us doesn’t trace this whole pattern. Instead we only observe certain “coarse-grained” features. And the point is that these features are largely independent of the detailed properties of the molecules—and robustly correspond to our standard laws of physics, like the Second Law of thermodynamics. But a different kind of observer, sampling and “parsing” the system differently, could in principle identify different features, corresponding to different laws of physics. One of the conceptual difficulties in thinking about how we perceive the ruliad is that it’s a story of “self-observation”. Essentially by the very definition of the ruliad, we ourselves are part of it. We never get to “see the whole ruliad from the outside”. We only get to “experience it from the inside”. In some ways it’s a bit like our efforts to construct the ruliad. In the end, the ruliad involves infinite rules, infinite initial conditions, and infinite time. But any way of assembling the ruliad from pieces effectively involves making particular choices about how we take those infinite limits. And that’s pretty much like the fact that as entities embedded within the ruliad, we have to make particular choices about how to sample it. One of the remarkable aspects of the ruliad is that it’s in some sense the unique ultimately inevitable and necessary formal object. If one sets up some particular computational system or mathematical theory, there are choices to be made. But in the ruliad there are no choices. Because everything is there. And in a sense every aspect of the structure of the ruliad is just something formally necessary. It requires no outside input; it is just a formal consequence of the meaning of terms, like the abstract fact . But while the ruliad is unique, the description of it is not. In constructing it, one can imagine using Turing machines or hypergraph rewriting systems or indeed any other kind of computational system. Each will ultimately lead to the same limiting object that is the ruliad, but each of them can be thought of as defining a different coordinate system for describing the ruliad. The very generality of the ruliad makes it unsurprising that there is vast diversity in how it can be described. And in a sense each possible description is like a possible way of experiencing the ruliad. In analogy to the (deeply related) situation with spacetime in general relativity, we might say that there are many reference frames in which to experience the ruliad—but it’s always the same ruliad underneath. It’s important to understand that the “ruliad from the outside” could seem very different from any “internal” experience of it by an observer like us. As an example, consider a simple finite approximation to the ruliad, built from string substitution systems. In what we did above, we always started from a specific initial condition. But the full ruliad involves starting from all possible initial conditions. (Of course, one could always just say one starts from a “null” initial condition, then have rules of the form null → everything.) So now let’s consider starting from all possible strings, say of length 4. If we use all possible 2-element-to-2-element rules, the finite approximation to the ruliad that we’ll get will be: At some level this is a simple structure, and—as is inevitable for any finite approximation to the ruliad—its transitive closure is just the complete graph: So why doesn’t this mean that the ruliad is somehow trivial? A key part of the story is that we never get to “see the ruliad from the outside” like this. We are always part of it, sampling it according to some procedure, or, somewhat equivalently, thinking about constructing it according to some procedure. As an analogy, consider the real numbers. The whole continuum of all real numbers is “from the outside” in many ways a simple construct. But if we imagine actually trying to construct real numbers, say digit by digit, according to some definite procedure, then we’re dealing precisely with what Turing machines were originally invented to model, and the whole structure of computation is involved. (As we’ll see, our way of thinking about “observers like us” is ultimately quite related to “Turing machines with bounded descriptions”.) In a sense, at an outside “holistic” level, the ruliad has a certain simple perfection. But as soon as you try to look at “what’s in the ruliad”, you have to parametrize or coordinatize it, and then you’re inevitably exposed to its intricate internal structure.
The Concept of the Ruliad 22.3 Observers Like Us : One could imagine very different ways in which entities embedded within the ruliad could “experience” it. But what’s most relevant for us is how “observers like us” do it—and how we manage to synthesize from what’s going on in the ruliad our perception of reality, and our view of how our physical universe works. Let’s start by talking not about the full ruliad but rather about models in our Physics Project based on specific underlying rules. At the lowest level, we have a “machine-code” description of the universe is which everything just consists of a network of “atoms of space” that is continually being updated—and which we can think of as carrying out a giant, if incoherent, computation, full of computational irreducibility. But the remarkable fact is that somehow we, as observers of this, manage to pick out of it a certain slice that ends up showing coherent, computationally reducible features—that for example seem to reproduce our known laws of physics. How does this work? Partly it has to do with features of us as observers, partly with features of how the universe fundamentally works, and partly with an interplay between these. The first crucial feature of us as observers is that we’re computationally bounded: the way we “parse” the universe involves doing an amount of computation that’s absolutely tiny compared to all the computation going on in the universe. We sample only a tiny part of what’s “really going on underneath”, and we aggregate many details to get the summary that represents our perception of the universe. But why should that summary have any coherence? Basically it’s because we impose coherence through our definition of how observers like us work. One part of the universe will be affected by others. But to consider part of the universe as an “observer”, there has to be a certain coherence to it. The behavior of the universe somehow has to imprint itself on a “medium” that has a certain coherence and consistency. Down at the level of atoms of space, everything is always changing. But we can still identify emergent features that have a certain persistence. And it’s out of those features that what we call observers are built. Given only the atoms of space with all their computationally irreducible behavior, it’s not at the outset obvious that any real persistence could exist or be identified. But in our models we expect that there will, for example, be essentially topological features that correspond to particles that persistently maintain their identity. And the point is that we can expect to “aggregate up” much further and be able to identify something like a human observer—that we can consider to persistently maintain its identity to the point where phenomena from the universe can be “systematically imprinted” on it. Down at the level of atoms of space, there’s a whole multiway graph of possible sequences of updates that can occur—with each path in effect corresponding to a different “thread of time” for the universe. But it’s a crucial fact about us as observers of the universe that we don’t perceive all those branching and merging threads of time. Instead, we imagine that we have a single, definite thread of experience—in which everything is sequentialized in time. I’ve argued elsewhere that this sequentialization in time is a defining characteristic of “human-like consciousness”. And it turns out that one of its consequences is that it implies that the particular perception we will have of the universe must be one in which there are laws of physics that correspond to ones we know. It’s not obvious, by the way, that if we sequentialize time we can form any consistent view of the universe. But the phenomenon of causal invariance—which seems ultimately to be guaranteed by the fundamental structure of the ruliad—turns out to imply that we can expect a certain generalized relativistic invariance that will inevitably lead to eventual consistency. The notion of sequentialization in time is closely related to the idea that—even though our individual atoms of space are continually changing—we can view ourselves as having a coherent existence through time. And there’s a similar phenomenon for space. At the outset, it’s not obvious that there can be “pure motion”, in which something can move in space without “fundamentally changing”. But it turns out again to be consistent to view this as how things work for us: that even though we’re “made of different atoms of space” when we’re in different places, we can still imagine that in some sense we maintain the “same identity”. Down at the level of individual atoms of space, there really isn’t any coherent notion of space. And the fact that we form such a notion seems to be intimately connected to what we might think of as details of us. Most important is that we’re in a sense “intermediate in size” in the universe. We’re large relative to the effective distance between atoms of space (which might be m), yet we’re small compared to the size of the whole universe ( m). And the result is that we tend to aggregate the effects of many atoms of space, but still perceive different features of space (say, different gravitational fields) in different parts of the universe. The fact that we “naturally form a notion of space” also seems to depend on another issue of scale—that for us the speed of light “seems fast”. It takes our brains perhaps milliseconds to process anything we see. But the point is that this is very long compared to the time it takes light to get to us from objects in our typical local environment. And the result is that we tend to perceive there as being an instantaneous configuration of the world laid out in space, that “separately” changes in time. But if, for example, our brains ran much faster, or we were much bigger than we are, then the speed of light would “seem slower” to us, and we wouldn’t tend to form the notion of an “instantaneous state of space”. OK, so what about quantum mechanics? The most fundamental feature of quantum mechanics is that it implies that things in the universe follow not just one but many possible paths of history—which we only get to make certain kinds of measurements on. And in our Physics Project this is something natural, and in fact inevitable. Given any particular configuration of the universe, there are many possible updates that can occur. And when we trace out all the possibilities, we get a multiway system, in which different threads of history continually branch and merge. So how do observers like us fit into this? Being part of the universe, we inevitably branch and merge, just like the rest of the universe. So to understand our experience, what we need to ask is how a “branching brain” will perceive a “branching universe”. And the story is remarkably similar to what we discussed above for our experience of space and time: it all has to do with imagining ourselves to have a certain definite persistence. In other words, even if when “viewed from the outside” our brain might be following many different paths of history, “from the inside” we can still potentially assume that everything is conflated into a single thread of history. But will this ultimately be a consistent thing to do? Once again, causal invariance implies that it will. There are specific “quantum effects” where we can tell that there are multiple branches of history being followed, but in the end it’ll be consistent to imagine an “objective reality” about “what happened”. In our Physics Project we imagine that there are abstract relations between atoms of space, and in the end the pattern of these relations defines the structure of physical space. But what about different branches of history in the multiway graph? Can we think of these as related? The answer is yes. For example, we can say that at a particular time, states on two branches are “adjacent” if they share an immediate ancestor in the multiway graph. And tracing through such connections we can develop a notion of “branchial space”—a kind of space in which states on different branches of history are laid out: One can think of branchial space as being defined by the pattern of entanglements between different branches of history. And in our Physics Project it turns out that the fundamental laws of quantum mechanics seem to just be a direct translation of the fundamental laws of spacetime into branchial space. And just like the speed of light governs the maximum rate at which effects can propagate in physical space, so similarly in our models there’s a “maximum entanglement speed” at which effects can propagate in branchial space. So what are we like as observers in branchial space? Just like in physical space, we can presumably be thought of as having a certain size in branchial space. We don’t yet know quite how to measure this size, but it’s surely related to the effective number of quantum degrees of freedom we involve. In our everyday experience of things like gases, we’re sufficiently large compared to individual molecules that we normally just perceive the gas as some kind of continuum fluid—and in normal circumstances we can’t even tell that it’s made of molecules. Well, it’s presumably the same kind of thing for physical space—where we’re even much larger compared to the atoms of space, and it’s a major challenge to figure out how to detect their presence. What about for branchial space? As the underlying rules for the system get applied, different branches of history will in effect “move around” in branchial space in complex and computationally irreducible ways. And just like when we observe molecules in a gas, we’ll mostly just observe overall aggregate effects analogous to fluid mechanics—and only in special circumstances will we notice “quantum effects” that reveal the presence of multiple independent threads of history.
The Concept of the Ruliad 22.4 Living in Rulial Space : We’ve discussed how “observers like us” perceive models of physics of the type that arise in our Physics Project. But how will we perceive the whole ruliad? It begins with a generalization of the story for branchial space. Because now as well as having different branches associated with different updatings according to a particular rule, we have different branches associated with updatings according to different rules. And just as we can slice an ordinary multiway system at a particular time to get an instantaneous version of branchial space, so now we can slice a rulial multiway system to get an instantaneous version of what we can call rulial space—a space in which different branches can correspond not just to different histories, but to different rules for history. It’s a fairly complicated setup, with “pure branchial space” inevitably being deeply interwoven with rulial space. But as a first approximation, we can think of rulial space as being somewhat separate, and laid out so that different places in it correspond to the results of applying different rules—with nearby places effectively being associated with “nearby” rules. And just as we can think of effects propagating in branchial space, so also we can think of them propagating in rulial space. In branchial space we can talk about entanglement cones as the analog of light cones, and a maximum entanglement speed as the analog of the speed of light. In rulial space we can instead talk about “emulation cones”—and a “maximum emulation speed”. In our rough approximation of rulial space, each point is in effect associated with a particular rule. So how do we “move” from one point to another? Effectively we have to be emulating the behavior of one rule by another. But why should it even be possible to do this? The answer is the Principle of Computational Equivalence, which states that, in effect, most rules will be equivalent in their computational capabilities—and in particular they will be capable of universal computation, so that any given rule can always “run a program” that will make it emulate any other rule. One can think of the program as an interpreter or translator that goes from one rule to another. The Principle of Computational Equivalence tells one that such a translator must essentially always exist. But how fast will the translator run? Effectively that’s what distance in rulial space measures. Because to “do a certain translation”, branches in the rulial multiway system have to reach from one rule to another. But they can only do that at the maximum emulation speed. What does the maximum emulation speed measure? Effectively it corresponds to the raw computational processing speed of the universe. We can think of representing computations in some language—say the Wolfram Language. Then the processing speed will be measured in “Wolfram Language tokens processed per second” (“WLT/s”). In some sense, of course, giving a value for this speed is just a way of relating our human units of time (say, seconds) to the “intrinsic unit of time” associated with the computational processing that’s going on in the universe. Or, in other words, it’s a kind of ultimate definition of a second relative to purely formal constructs. OK, but how does this relate to us as observers embedded within the ruliad? Well, just as we imagine that—along with the rest of the universe—we’re continually branching and merging in branchial space, so also this will be what happens in rulial space. In other words—like the rest of the universe—our brains aren’t following a particular rule; they’re following branching and merging paths that represent all possible rules. But “from inside” we can still potentially imagine that we have a single thread of experience—effectively conflating what happens on all those different branches. And once again we can ask whether doing this will be consistent. And the answer seems to be that, yes, it can be. And what guarantees this is again a kind of “rulial relativity” that’s a consequence of causal invariance. There are many details here, which we’ll address to some extent later. But the broad outline is that causal invariance can be thought of as being associated with paths of history that diverge, eventually converging again. But since the ruliad contains paths corresponding to all possible rules, it’s basically inevitable that it will contain what’s needed to “undo” whatever divergence occurs. So what does this mean? Basically it’s saying that even though the universe is in some sense intrinsically “following all possible rules”—as represented by paths in the ruliad—we as observers of the universe can still “take the point of view” that the universe follows a particular rule. Well, actually, it’s not quite a particular rule. Because just as we’re in some sense “quite big” in physical and presumably branchial space, so also we’re potentially “quite big” in rulial space. And being extended in rulial space is basically saying that we consider not just one, but a range of possible rules to be what describe the universe. How can it work this way? Well, as observers of the universe, we can try to deduce what the “true rule for the universe” is. But inevitably we have to do this by performing physical experiments, and then using inductive inference to try to figure out what the “rule for the universe is”. But the issue is that as entities embedded within the universe, we can only ever do a finite number of experiments—and with these we’ll never be able to precisely nail down the “true rule”; there’ll always be some uncertainty. When we think of ourselves as observers of the universe, there’s in a sense lots of “arbitrariness” in the way we’re set up. For example, we exist at a particular location in physical space—in our particular solar system and so on. Presumably we also exist at a particular location in branchial space, though it’s less clear how to “name” that. And in addition we exist at a particular location in rulial space. What determines that location? Essentially it’s determined by how we operate as observers: the particular sensory system we have, and the particular means of description that we’ve developed in our language and in the history of knowledge in our civilization. In principle we could imagine sensing or describing our universe differently. But the way we do it defines the particular place in rulial space at which we find ourselves. But what does all this mean in terms of the ruliad? The ruliad is the unique limiting structure formed by following all possible rules in all possible ways. But when we “observe the ruliad” we’re effectively “paying attention to” only particular aspects of it. Some of that “paying attention” we can conveniently describe in terms of our particular “location in the ruliad”. But some is more naturally described by thinking about equivalence classes in the ruliad. Given two states that exist in the ruliad, we have to ask whether as observers we want to consider them distinct, or whether we want to conflate them, and consider them “the same”. When we discussed the construction of the ruliad, we already had many versions of this issue. Indeed, whenever we said that two paths in the ruliad “merge”, that’s really just saying that we treat the outcomes as equivalent. “Viewed from the outside”, one could imagine that absolutely nothing is equivalent. Two hypergraphs produced in two different ways (and thus, perhaps, with differently labeled nodes) are “from the outside” in some sense different. But “viewed from the inside”, they pretty much have to be viewed as “the same”, in essence because all their effects will be the same. But at some level, even such conflation of differently labeled hypergraphs can be thought of as an “act of the observer”; something that one can only see works that way if one’s “observing it from inside the system”. But all the way through our description of the observer, it’s very much the same story: it’s a question of what should be considered equivalent to what. In sequentializing time, we’re effectively saying that “all of space” (or “all of branchial space”, or rulial space) should be considered “equivalent”. There are many subtle issues of equivalence that also arise in the construction of states in the ruliad from underlying tokens, in defining what rules and initial conditions should be considered the same, and in many other places. The ruliad is in some sense the most complicated constructible object. But if we as computationally bounded observers are going to perceive things about it, we have to find some way to “cut it down to size”. And we do that by defining equivalence classes, and then paying attention only to those whole classes, not all the details of what’s going on inside them. But a key point is that because we are computationally bounded observers who imagine a certain coherence in their experience, there are strong constraints on what kinds of equivalence classes we can use. If we return again to the situation of molecules in a gas, we can say that we form equivalence classes in which we look only coarsely at the positions of molecules, in “buckets” defined by simple, bounded computations—and we don’t look at their finer details, with all the computational irreducibility they involve. And it’s because of this way of looking at the system that we conclude that it follows the Second Law of thermodynamics, exhibits fluid behavior, etc. And it’s very much the same story with the ruliad—and with the laws of physics. If we constrain the kind of way that we observe—or “parse”—the ruliad, then it becomes inevitable that the effective laws we’ll see will have certain features, which turns out apparently to be exactly what’s needed to reproduce known laws of physics. The full ruliad is in a sense very wild; but as observers with certain characteristics, we see a much tamer version of it, and in fact what we see is capable of being described in terms of laws that we can largely write just in terms of existing mathematical constructs. At the outset, we might have imagined that the ruliad would basically just serve as a kind of dictionary of possible universes—a “universe of all possible universes” in which each possible universe has different laws. But the ruliad is in a sense a much more complicated object. Rather than being a “dictionary” of possible separate universes, it is something that entangles together all possible universes. The Principle of Computational Equivalence implies a certain homogeneity to this entangled structure. But the crucial point is that we don’t “look at this structure from the outside”: we are instead observers embedded within the structure. And what we observe then depends on our characteristics. And it turns out that even very basic features of our consciousness and sensory apparatus in a sense inevitably lead to known laws of physics—and in a sense do so generically, independent of details of just where in rulial space we are, or exactly what slice of the ruliad we take. So far we’ve primarily talked about the ruliad in terms of physics and the fundamental structure of our physical universe. But the ruliad is actually something still more general than that. Because ultimately it is just created from the abstract concept of following all possible computational rules. And, yes, we can interpret these rules as representing things going on in our universe. But we can also interpret them as representing things going on in some other, less immediately physically realizable system. Or, for that matter, representing something purely formal, and, say, mathematical. This way of talking about the ruliad might make one think that it should be “considered a possible model” for our universe, or for other things. But the bizarre and surprising point is that it is more than that. It’s not just a possible model that might be one of many. Rather, it is the unique ultimate representation of all possible models, entangled together. As we’ve discussed, there are many subtle choices about how we observe the ruliad. But the ultimate ruliad itself is a unique thing, with no choice about what it is. As I’ve discussed at more length elsewhere, the ruliad is in a sense a representation of all possible necessary truths—a formal object whose structure is an inevitable consequence of the very notion of formalization. So how does this relate to the idea that the ruliad also at an ultimate level represents our physical universe? What I’ve argued elsewhere is that it means that the ultimate structure of our universe is a formal necessity. In other words, it’s a matter of formal necessity that the universe must exist, and have an ultimate ruliad structure. The fact that we perceive the universe to operate in a certain way—with our standard laws of physics, for example—is then a consequence of the particular way observers like us perceive it, which in turn depends on things like where in rulial space we happen to find ourselves. But beyond physics, what else might the ruliad represent? The ruliad is an ultimate example of multicomputation, and of what I’ve characterized as the fourth major paradigm for theoretical science. Often in multicomputation, what’s of interest is multiway systems with specific underlying rules. And already at this level, much of the apparatus that we’ve described in connection with the ruliad also applies—and in a sense “trickles down” to give various universal results. But there are also definitely cases of multicomputation (other than physics) where the full notion of applying all possible rules is relevant. The global structures of metamathematics, economics, linguistics and evolutionary biology seem likely to provide examples—and in each case we can expect that at the core is the ruliad, with its unique structure. Of course, this doesn’t mean that what we observe must always be the same, because what we observe depends on our characteristics as an observer—and the characteristics of “being an observer” in metamathematics, for example, are surely different from those for economics or evolutionary biology, or, for that matter, physics. For sure, the “sensory apparatus” that we effectively use is different in different cases. But there are certain similar human-based features that still seem to apply. Whatever the domain, we always act as computationally bounded observers. And it seems that we also always have a certain coherence, consistently maintaining our “observerhood” through time or across some form of space. And it seems likely that these “human-induced” characteristics alone are sufficient to yield some very global implications for observed behavior.
The Concept of the Ruliad 22.5 The View from Mathematics : How should we think about the ruliad mathematically? In many ways, the ruliad is more an object of metamathematics than of mathematics itself. For in talking about the effects of all possible rules, it in a sense transcends individual mathematical theories—to describe a kind of metatheory of all possible theories. Given a particular mathematical axiom system, it’s rather easy to see correspondence with a multiway system. There are a variety of ways to set it up, but one approach is to think of states in the multiway system as being expressions in the language used for the axiom system, and then to think of rules in the multiway system as applying transformations on these expressions that implement axioms in the axiom system. For example, with the (Abelian semigroup) axioms: here’s a multiway system generated from the expression by applying the (two-way) transformations defined by the axioms in all possible ways to each expression: But now from this graph we can read off the “theorem”: A proof of this theorem: is just a path in the multiway graph: A significantly less direct but still perfectly valid proof would correspond to the 13-step path: It’s a slightly technical point, but perhaps worth mentioning that there are alternative ways to set up the correspondence between axiomatic mathematical systems and multiway systems. One such way is to take the states in the multiway system to be not expressions (like ) but whole propositions (like ). Axioms then show up as states, and the rules for the multiway system are “rules of inference” or “rules of entailment” that define how, say, pairs of propositions “entail” (or “imply”) other ones. (And, yes, this requires a generalized multiway system with not just 1 state → many states, but for example 2 states → 1 state.) Typical automated theorem provers (like FindEquationalProof) operate in this kind of setup, attempting to find paths that lead from initial propositions and axioms to some final state that corresponds to an “obviously true” proposition, like . But whatever the detailed setup, the basic picture is that an axiomatic mathematical system has an associated multiway graph, in which paths correspond to proofs. Given the rules for the multiway system, there is in general no way to guarantee that the path (if it exists) corresponding to the proof of some particular result will be of bounded length, leading to the possibility of undecidability. But even when a path exists, it may require an irreducibly large amount of computation to find it. Still, finding such paths is what automated theorem provers do. For example, we know (as I discovered in 2000) that ((b · c) · a) · (b · ((b · a) · b)) = a is the minimal axiom system for Boolean algebra, because FindEquationalProof finds a path that proves it. But this path—and the corresponding proof—is a very “non-human” construct (and, for example, in 21 years essentially no progress has been made in finding a “human-understandable narrative” for it). And we can make an analogy here to the situation in physics. The individual rule applications in the multiway graph (or the proof) are like individual updating events applied to the atoms of space—and they show all kinds of complexity and computational irreducibility. But in physics, human observers work at a higher level. And the same, one suspects, is true in mathematics. Rather than looking at every detail of the multiway graph, human “mathematical observers” (i.e. pure mathematicians) in effect define all sorts of equivalences that conflate together different parts of the graph. If the individual updates in the multiway graph are like molecular dynamics, human pure mathematics seems to operate much more at the “fluid dynamics level”, concentrating on “broad mathematical constructs”, not the “machine code” of specific low-level axiomatic representations. (Of course, there are some situations, for example related to undecidability, where the “molecular dynamics” effectively “breaks through”.) We’ve outlined above (and discussed at length elsewhere) how physical observers like us “parse” the low-level structure of the physical universe (and the ruliad). How might mathematical observers do it? A large part has to do with the identification of equivalences. And the key idea is that things which are considered equivalent should be assumed to be “the same”, and therefore “conflated for mathematical purposes”. The most elementary example of something like this is the statement (already present in Euclid) that if and , then . The extensionality axiom of set theory is a more sophisticated example. And the univalence axiom of homotopy type theory is perhaps the most sophisticated current version. There’s a very operational version of this that appears in automated theorem proving. Imagine that you’ve proved that and . Then (by the assumed properties of equality) it follows that . One way we could use this result is just to merge the nodes for and . But a “bigger” thing we can do is to add the “completion” as a general rule for generating the multiway system. Consider, for example, the string substitution multiway system A ↔ AB: But notice here that both ABA ↔ ABBA and ABA ↔ ABAB. So now add the “completion” ABBA ↔ ABAB. Here’s the resulting multiway graph: Adding the completion has generated lots of new “direct equivalences”. But we can also think of it as having done something else: not only has it defined equivalences between states; it’s also defined equivalences between paths—or in effect between proofs. (Or, put another way, it’s implementing homotopic equivalence between proofs. By the way, it’s an important feature of human mathematics that progress is typically measured in theorems proved; different proofs of the same theorem are normally implicitly considered equivalent in terms of the progress of mathematics.) In a category theory interpretation, transformations between states in the original multiway graph are like ordinary morphisms (1-morphisms). But when we’re making transformations between “proof paths”, this is like 2-morphisms. And just as we can add transformations between proofs, we can also add transformations between proofs-between-proofs, and so on. The result is that we can build up a whole hierarchy of higher and higher categories, eventually ending with an ∞-category. But now we can begin to see the connection with the ruliad. The ruliad is in a sense the result of taking the limit of adding more and more possible rules. Above we did this quite explicitly in terms of the original underlying rules, for example by enumerating possible strings or possible integer multipliers. But we can view successive completions as doing something very similar. Yes, the rules are enumerated in a different order. But in the end there’ll still be an infinite number of distinct rules being used. Of course there are many mathematical details missing here. But in broad outline, it seems that one can think of the approach to the ruliad as some kind of limit of successively higher categories. But this limit is something that’s been studied (albeit in the upper reaches of pure mathematics), and it’s an object called the ∞-groupoid. (It’s a groupoid because when all the rules are included things inevitably “go both ways”). So, OK, is the ruliad “just” the ∞-groupoid? Not quite. Because there are more rules and more initial conditions in the ruliad, even beyond those added by completions. And in the end the ruliad actually seems to be the ∞-category of ∞-groupoids, or what’s called the (∞,1)-category. But knowing that the ruliad can be thought of as composed of ∞-groupoids means that we can apply mathematical ideas about the ∞-groupoid to the ruliad. Probably the most important is Grothendieck’s hypothesis, which asserts that the ∞-groupoid inevitably has a topological and (with a few other conditions) ultimately geometric structure. In other words, even though one might have imagined that one constructed the ∞-groupoid from “pure logic” (or from pure formal axiomatic structures), the assertion is that the limiting object one obtains inevitably exhibits some kind of geometrical or “spatial” structure. Viewed in terms of the ruliad—and our explicit finite examples of it—this might not seem surprising. And indeed in our Physics Project, the whole concept of the emergence of space from large-scale hypergraphs is closely related. But here from Grothendieck’s hypothesis we’re basically seeing a general claim that the ruliad must have “inevitable geometry”—and we can then view things like the emergence of space in our Physics Project as a kind of “trickle down” from results about the ruliad. (And in general, a big “application” of geometrical structure is the possibility of “pure motion”.) What does all this mean about the ruliad and mathematics? In a sense the ruliad represents all possible mathematics—the application of all possible rules, corresponding to all possible axiom systems. And from this “ultimate metamathematics”, human “mathematical observers” are sampling pieces that correspond to the pure mathematics they consider of interest. Perhaps these will align with particular axiom systems of the kind automated theorem provers (or proof assistants) use. But things may be “sloppier” than that, with human mathematical observers effectively being extended in rulial space—and capable of making “fluid-dynamics-level” conclusions, even if not “molecular-dynamics-level” ones. But a key (and in some ways very surprising) point is that the ruliad can be viewed as the basis of both physics and mathematics. In some sense, physics and mathematics are at their core the same thing. They only “appear different” to us because the way we “observe” them is different. I plan to discuss the implications for mathematics at greater length elsewhere. But suffice it to say here that the existence of a common underlying core—namely the ruliad—for both physics and mathematics immediately allows one to start importing powerful results from physics into mathematics, and vice versa. It also allows one, as I have done elsewhere, to start comparing the existence of the universe with the (Platonic-style) concept of the fundamental existence of mathematics.
The Concept of the Ruliad 22.6 The View from Computation Theory : The ruliad can be thought of as an encapsulation of doing all possible computations in all possible ways. What we might think of as a “single computation” might consist of repeatedly applying the rules for a Turing machine to “deterministically” generate a sequence of computational steps: But one can also consider a “multicomputational” system, in which rules can generate multiple states, and the whole evolution of the system can be represented by a multiway graph: In traditional computation theory, one talks about rules like these as “nondeterministic”, because they can have multiple outcomes—though one usually imagines that the final answer one wants from a computation can be found as the result of some particular path. (In what I now call the multicomputational paradigm—that I believe is important for modeling in physics and other places—one instead considers the complete multiway graph of entangled possible histories.) In constructing the ruliad, one is in a sense going to a more extreme version of multicomputation, in which one uses not just a particular rule with multiple outcomes, but all possible rules. In effect, the concept is to use “maximal nondeterminism”, and at each step to independently “pick whatever rule one wants”, tracing out a rulial multiway system that includes all the different possible paths this generates. For the kind of Turing machines illustrated above, the rulial multiway graph one gets after one step is: After 2 steps the result is: The full ruliad is then some kind of infinite limit of this process. But as before, there’s lots of subtlety in how this limit is taken. But we can at least characterize some ways of approaching it using ideas from computational complexity theory. Increasing the number of steps of evolution is like increasing the time complexity one allows. Increasing the “size of states” (e.g. the width of nonzero Turing machine tape) that one includes is like increasing the space complexity one allows. And increasing the complexity of the rule (as measured in the number of bits needed to specify it) is like increasing the algorithmic complexity one allows. The ruliad is what is obtained by taking all these computational resource measures to infinity. And a critical claim is that regardless of how this is done, the final ruliad construct one gets will always—at least in some sense—be the same. There will be many ways to coordinatize it, or to sample it, but the claim is that it’s always the same object that one’s dealing with. And ultimately the reason for this is the Principle of Computational Equivalence. Because it implies that whatever “computational parametrization” or “computational description language” one uses for the ruliad, one will almost always get something that can be viewed as “computationally equivalent”. We’ve talked about building up the ruliad using Turing machines. But what about other models of computation—like cellular automata or register machines or lambda calculus? As soon as there’s computation universality we know that we’ll get results that are at least in principle equivalent, because in a sense there’s only a “finite translation cost” associated with setting up an interpreter from one model of computation to another. Or, put another way, we can always emulate the application of the rule for one system by just a finite number of rule applications for the other system. But from computation universality alone we have no guarantee that there won’t be “extreme deformations” introduced by this deformation. What the Principle of Computational Equivalence says, however, is that almost always the deformations won’t have to be extreme. And indeed we can expect that particularly when multiple rules are involved, there’ll be rapid convergence almost always to a kind of “uniform equivalence” that ensures that the final structure of the ruliad is always the same. But the Principle of Computational Equivalence appears to say still more about the ruliad: it says that not only will the ruliad be the same independent of the “computational basis” used to construct it, but also that there’ll be a certain uniformity across the ruliad. Different “regions of the ruliad” might involve different specific rules or different patterns of their application. But the Principle of Computational Equivalence implies that almost always the computations that happen will be equivalent, so that—at least at a certain scale—the structure associated with them will also be equivalent. Knowing that the ruliad contains so many different computations, one might imagine that it would show no particular uniformity or homogeneity. But the Principle of Computational Equivalence seems to imply that it necessarily does, and moreover that there must be a certain coherence to its structure—that one can interpret (in the style of Grothendieck’s hypothesis) as an inevitable emergent geometry. An individual computation corresponds to a path in the ruliad, going from its “input state” to its “output state”. In an ordinary deterministic computation, the path is restricted to always use the same rule at each step. In a nondeterministic computation, there can be different rules at different steps. But now we can formulate things like the P vs. NP problem essentially in terms of the geometry of the ruliad. Here’s a picture of the same finite Turing-machine-based approximation to the ruliad as above—but now with the paths that correspond to deterministic Turing machine computations marked in red: The P vs. NP problem basically asks roughly whether the deterministic computations (shown here in red) will eventually “fill the ruliad”, or whether the general nondeterministic computations that are part of the ruliad will always “reach further”. Once again, there are many complicated and subtle issues here. But it’s interesting to see how something like the P vs. NP problem might play out in the ruliad. In physics (and mathematics), we as human observers tend to sample the ruliad in a coarse-grained way, “noticing” only certain aspects of it. So is there an analog of this in computation theory—perhaps associated with certain characteristics of the “computation-theoretic observer”? There’s a potential answer, rather similar to what we’ve already seen in both physics and mathematics. The basic point is that in computation theory we tend to study classes of computations (say P or NP) rather than individual computations. And in doing this we are in a sense always conflating many different possible inputs and possible outputs—which we assume we do in a computationally bounded way (e.g. through polynomial-time transformations, etc.) Another thing is that we tend to focus more on the “experience of the end user” than the detailed activities of the “programmer”. In other words, we’re concerned more with what computational results are obtained, with what computational resources, rather than on the details of the program constructed to achieve this. Or, put another way, we tend to think about computation in terms of things like the successive evaluation of functions—and we conflate the different paths by which this is achieved. Most likely this means that there are “effective laws” that can be derived in this computational view of the ruliad, analogous to laws of physics like general relativity. So what might some other analogies be? A computation, as we’ve mentioned, corresponds to a path in the ruliad. And whenever there’s a possibility for an infinite path in the ruliad, this is a sign of undecidability: that there may be no finite way to determine whether a computation can reach a particular result. But what about cases when many paths converge to a point at which no further rules apply, or effectively “time stops”? This is the analog of a spacelike singularity—or a black hole—in the ruliad. And in terms of computation theory, it corresponds to something decidable: every computation one does will get to a result in finite time. One can start asking questions like: What is the density of black holes in rulial space? If we construct the ruliad using Turing machines, this is basically analogous to asking “What’s the density of halting Turing machines (+initial conditions) in rulial space?” And this is essentially given by Chaitin’s Ω. But so is there some number Ω that we can just compute for the ruliad? Well, actually, no. Because the undecidability of the halting problem makes Ω noncomputable. One can get approximations to it, but—in the language of the ruliad—those will correspond to using particular samplings or particular reference frames. Or in other words, even the perceived density of “decidability black holes” in the ruliad depends on features of the observer.
The Concept of the Ruliad 22.7 What’s beyond the Ruliad? : In our Physics Project we usually talk of the universe “evolving through time” (albeit with many entangled threads of history). But if the ruliad and its structure is a matter of formal necessity, doesn’t that mean that the whole ruliad effectively “already exists”—“outside of time”? Well, in some sense it does. But ultimately that would only be relevant to us if we could “look at the ruliad from the outside”. And as observers like us within the ruliad, we necessarily have a different perception. Because our consciousness—with its computational boundedness—only gets to sample a certain sequence of pieces of the ruliad. If it were not for computational irreducibility, we might get to “jump around” in time. But computational irreducibility, together with our own computational boundedness, implies that our perception must necessarily just experience the passage of time through an irreducible process of computation. In other words, while in some sense the ruliad may all “already be there” when viewed from the outside, our own perception of it “from the inside” is necessarily a progressive one, that effectively corresponds to the passage of time. Could we experience the ruliad differently, even while being computationally bounded? If we think of the ruliad as a graph, then our usual “sequence of configurations of space at successive times” way of experiencing the ruliad is like a breadth-first traversal. But could we for example instead do a depth-first traversal, exploring all time before investigating different parts of space? (And, yes, something like this can happen in general relativity near an event horizon, or in connection with timelike singularities.) Later, we’ll discuss different ways to perceive the ruliad and the universe. But it seems to be a feature of anything we might call a coherent observer that there needs to be some form of progression in the perception. And so while we might not call it the passage of time, there’ll still be some way in which our exploration of the ruliad has a computationally irreducible process underneath. A very important claim about the ruliad is that it’s unique. Yes, it can be coordinatized and sampled in different ways. But ultimately there’s only one ruliad. And we can trace the argument for this to the Principle of Computational Equivalence. In essence there’s only one ruliad because the Principle of Computational Equivalence says that almost all rules lead to computations that are equivalent. In other words, the Principle of Computational Equivalence tells us that there’s only one ultimate equivalence class for computations. But what if we just imagine a “hypercomputation” not in that class? For example, imagine a hypercomputation (analogous, for example, to an oracle for a Turing machine) that in a finite number of steps will give us the result from an infinite number of steps of a computationally irreducible process. Such a hypercomputation isn’t part of our usual ruliad. But we could still formally imagine a hyperruliad that includes it—and indeed we could imagine a whole infinite hierarchy of successively larger and more powerful hyperruliads. But it’s a fundamental claim that we’re making—that can be thought of as a matter of natural science—that in our universe only computation can occur, not hypercomputation. At a purely formal level, there’s nothing wrong with hyperruliads. They exist as a matter of formal necessity just like the ordinary ruliad does. But the key point is that an observer embedded within the ruliad can never perceive a hyperruliad. As a matter of formal necessity there is, in a sense, a permanent event horizon that prevents anything from any hyperruliad from affecting anything in the ordinary ruliad. So now we can be a bit more precise about our statement that “hypercomputation doesn’t happen in our universe”. Really we should say that we assert that we as observers operate purely computationally and not hypercomputationally. And this means that we are embedded within the ordinary ruliad, and not the hyperruliad. Yes, we could imagine some other entity that’s embedded within the hyperruliad, and perceives what it considers to be the universe to operate hypercomputationally. But in a statement that’s in a sense more “about us” than “about the universe”, we assert that that can’t be us, and that we in a sense live purely within the ruliad—which means that for us the Principle of Computational Equivalence holds, and we perceive only computation, not hypercomputation.
The Concept of the Ruliad 22.8 Communicating across Rulial Space : What observers can there be embedded in the ruliad, and how should we characterize them? In physical spacetime we’re used to characterizing observers by their locations in physical space and by things like the spacetime reference frames they construct. And it’s very much the same for observers in the ruliad: we can characterize them by where they are in rulial space, and what rulial reference frames they use. The Principle of Computational Equivalence tells us that it’s almost always possible to “encode” one “model of how the ruliad works” in any other model—effectively just by setting up a program that emulates the rules for one model using the rules for the other model. But we can think of these different models as being associated with different possible observers in the ruliad. In other words, we can say that observers “at different places in rulial space” (or “using different rulial reference frames”) are using different description languages for what’s happening in the ruliad. And when an observer “moves” in rulial space, they’re effectively doing a translation from one description language to another. (And, yes, there’s a maximum rate of motion ρ in rulial space—which is the rulial analog of the speed of light—and which is effectively determined by the fundamental processing speed of the universe.) So far this might all seem quite abstract. But there are immediate, everyday examples that effectively correspond to being at different places in rulial space. A simple concrete one is computers with different instruction sets. Another one is different brains with different consciousnesses. We can think of a single human consciousness as having a certain thread of experience of the universe. Part of that experience is determined by the physical location of the consciousness and by the sensory apparatus with which it samples the world. But part is determined by the “internal description language” that it uses. And inevitably this internal description language depends both on the detailed physiology of the brain in which it is implemented, and on the past history of experiences that have “defined its way of looking at the world”. In the analogy of artificial neural networks, different networks will tend to have different “internal representations” because this depends not only on the network architecture, but also on the particular training data that the network has “experienced”. Why can’t one human consciousness “get inside” another? It’s not just a matter of separation in physical space. It’s also that the different consciousnesses—in particular by virtue of their different histories—are inevitably at different locations in rulial space. In principle they could be brought together; but this would require not just motion in physical space, but also motion in rulial space. But why then do different consciousnesses seem to have compatible views about “what happens in the universe”? Essentially this can be seen as a consequence of rulial relativity—which in turn depends on the inevitable causal invariance of the ruliad, which follows from the Principle of Computational Equivalence. There are certainly many issues to be worked out, but basically what seems to be going on is that because of causal invariance, different rulial reference frames will ultimately yield the same rulial multiway causal graphs, and therefore the same “fundamental description of reality”. We’ve talked about different consciousnesses. But what about just “different ways of thinking”? Well, it’s definitely more than an analogy to say that different ways of thinking correspond to different positions in rulial space. If there’s lots of common history then there’ll be common ancestry in the rulial multiway graph and one will necessarily end up close in rulial space. But without common history, one can end up with different description languages—or different ways of thinking—that are not nearby in rulial space. In physical space we expect to effectively use momentum to move our location. And it’s potentially a bizarrely similar story in rulial space. In our models of fundamental physics, energy and momentum are essentially related to the density of activity (i.e. elementary updating events) in physical space. And we can similarly define a rulial analog of energy and momentum in terms of activity in rulial space. But it’s exactly this activity that provides connections between different parts of rulial space, or in effect “enables motion” in rulial space. In other words, if you want to move in rulial space, you can do it by putting in the appropriate computational work to change your conceptual point of view (or, essentially equivalently, your language for describing things). So what about curvature (or the analog of gravity) in rulial space—say generated through an analog of Einstein’s equations from density of activity in rulial space? Presumably this relates to the difficulty—or time it takes—to get from one place in rulial space, and one way of thinking, to another. And conceivably things like “paradigm shifts” between different ways of thinking might be associated with features of rulial space like event horizons. But let’s say you’re at one place in rulial space, and you want to get to another—or at least “send a signal” there. A typical microscopic change at one point in rulial space will tend to just “spread out in all directions” and “decay quickly”. But if you want to “coherently communicate”, you need some kind of structure that will persist as it propagates through rulial space. And by analogy with the case of physical space, what this presumably means is that you effectively need a “rulial particle”. In terms of the ruliad, a rulial particle would presumably be some kind of “topological obstruction” or “topologically stable structure” that is at any moment effectively localized in rulial space and maintains its identity as it propagates across rulial space. But what might a rulial particle be in more everyday terms? Potentially it’s like what we’d normally consider a concept—or something to which we might assign a word in human language. If we have ways of thinking—or consciousnesses—whose details are different, the issue is what will be robust enough to be able to be transported between them. And what everyday experience seems to suggest is that the answer is concepts. Even though one might have a slightly different way of thinking, what one calls “a fish” (or essentially, the concept of a fish) is something that can still robustly be communicated. It’s interesting to notice that for an observer like us, there seem to be only a finite set of types of “elementary particles” that exist in physical space. And perhaps that’s not unrelated to the fact that observers like us also seem to imagine that there are in some sense only a finite number of “basic concepts” (associated, say, with distinct words in human languages). There’s lots more detail that exists in rulial space—or in the ruliad—but for observers like us, with our type of way of sampling the ruliad, these might be core coherent structures that we perceive.
The Concept of the Ruliad 22.9 So Is There a Fundamental Theory of Physics? : The concept of the ruliad arose from our efforts to find a fundamental theory of physics. But now that we know about the ruliad, what does it tell us about a fundamental theory? At the outset, we might have imagined that the end point of our project would be the identification of some particular rule of which we could say “This is the rule for the universe”. But of course then we’d be faced with the question: “Why that rule, and not another?” And perhaps we would imagine just having to say “That’s something that you have to go beyond science to answer”. But the ruliad implies a quite different—and in my view ultimately much more satisfying—picture. The ruliad itself is a construct of abstract necessity—that in a sense represents the entangled behavior of all possible rules for the universe. But instead of imagining that some particular rule out of all these possibilities is “picked from outside” as “the choice for our universe”, what we suppose is that—as observers embedded within the ruliad—we’re the ones who are implicitly picking the rule by virtue of how we sample and perceive the ruliad. At first this might seem like it’s a wimp out. We want to know how our universe works. Yet we seem to be saying “we just pick whatever rule we feel like”. But that’s not really the story at all. Because in fact observers that are even vaguely like us are in effect deeply constrained in what rules they can attribute to the universe. There’s still some freedom, but a fundamental result is that for observers like us it seems to be basically inevitable that any rule we can pick will on a large scale reproduce the central known general laws of physics, in particular general relativity and quantum mechanics. In other words, for observers generally like us it’s a matter of abstract necessity that we must observe general laws of physics that are the ones we know. But what about more specific things, like the particular spectrum of elementary particles, or the particular distribution of matter in the universe? It’s not clear how far “the general” goes—in other words, what is a matter of abstract necessity purely from the structure of the ruliad and general features of observers like us. But inevitably at some point we will run out of “the general”. And then we’ll be down to specifics. So where do those specifics enter? Ultimately they must be determined by the details of how we sample the ruliad. And a prominent part of that is simply: Where in the ruliad are we? We can ask that about our location in physical space. And we can also ask it about our location in rulial space. What does all this mean? At some level it’s saying that the way we are as observers is what makes us attribute certain rules to our universe. The ruliad is in a sense the only thing that fundamentally exists—and in fact its existence is a matter of abstract necessity. And our universe as we experience it is some “slice of the ruliad”, with what slice it is being determined by what we’re like as observers. Let’s look at the logical structure of what we’re saying. First, we’re describing the ruliad, which at the outset doesn’t have anything specifically to do with physics: it’s just a formal construct whose structure is a matter of abstract necessity, and which relates as much to mathematics as it does to physics. But what “puts the physics in” is that we in effect “live in the ruliad”, and our perception of everything is based on “experiencing the ruliad”. But that experience—and the effective laws of physics it entails—inevitably depends on “where we are in the ruliad” and how we’re able to sample it. And this is where our pieces of “falsifiable natural science” come in. The first “assertion of natural science” that we make is that we are embedded only within the ordinary ruliad, and not a hyperruliad—or in other words that our experience encompasses only computation, and not hypercomputation. This is closely related to a second assertion, which may in fact be considered to subsume this: that we are computationally bounded observers, or, in other words, that our processes of perception involve bounded computation. Relative to the whole ruliad—and all the computation it entails—we’re asserting that we as observers occupy only a tiny part. There’s one more assertion as well, again related to computational boundedness: that we as observers have a certain coherence or persistence. In general the ruliad contains all sorts of wild and computationally irreducible behavior. But what we’re asserting is that that part of the ruliad that is associated with us as observers has a certain simplicity or computational reducibility: and that as we evolve through time or move in space, we somehow maintain our identity. These assertions seem very general, and in some ways almost self-evident—at least as they apply to us. But the important and surprising discovery is that they alone seem to lead us inexorably to crucial features of physics as we know it. Where does this physics “come from”? It comes partly from the formal structure of the ruliad, and formal features of the multicomputational processes it involves. And it comes partly from the nature of us as observers. So if we ask “Why is the physics of our universe the way it is?”, an important part of the answer is “Because we observe the universe the way we do”. One might imagine that in some sense physics would give us no choice about how we observe the universe. But that’s not the case. Because in the end our “observation” of the universe is about the “abstract conceptual model” we build up for the universe. And, yes, that’s certainly informed by the particular sensory apparatus we have, and so on. But it’s something we can certainly imagine being different. We can think of ourselves as using some particular description language for the universe. The structure of that language is constrained by the assertions we gave above. But within such a description language, the laws of physics necessarily work out the way they do. But if we chose a different description language, we’d end up with different laws of physics. Much of our perception of the universe is based on our raw biological structure—the way our sensory organs (like our eyes) work, as well as the way our brains integrate the inputs we get. But that’s not all there is to it. There’s also a certain base of knowledge in our civilization that informs how we parse our “raw perception”—and in effect what description language we use. Once we have the idea of periodic behavior, say, we can use it to describe things that we’d previously have to talk about in a less “economical” way. But what if our knowledge changed? Or we had different sensory capabilities? Or we used technology to integrate our sensory input in different ways? Then we’d be able to perceive and describe the universe in different ways. One’s first impression might be that the ruliad effectively contains many possible “parallel universes”, and that we have selected ourselves into one of these, perhaps as a result of our particular characteristics. But in fact the ruliad isn’t about “parallel universes”, it’s about universes that are entangled at the finest possible level. And an important consequence of this is that it means we’re not “stuck in a particular parallel universe”. Instead, we can expect that by somehow “changing our point of view”, we can effectively find ourselves in a “different universe”. Put another way, a given description of the universe is roughly represented by being at a certain location in rulial space. But it’s possible to move in rulial space—and end up with a different description, and different effective laws for the universe. But how difficult is motion in rulial space? It could be that some impressive future technology would allow us to “move far enough” to end up with significantly different laws of physics. But it seems more likely that we’d be able to move only comparatively little—and never be able to “escape the box” of things like computational boundedness, and coherence of the observer. Of course, even changing a little might lead us to different detailed laws of physics—say attributing a different mass to the electron, or a different value of the electromagnetic coupling constant α. But actually, even in traditional physics, this is already something that happens. When viewed at different energy scales—or in a sense with different technology—these quantities have different effective values (as characterized by the renormalization group). At first it might seem a little strange to say that as our knowledge or technology change, the laws of physics change. But the whole point is that it’s really our perceived laws of physics. At the level of the raw ruliad there aren’t definite laws of physics. It’s only when we “sample our slice” of the ruliad that we perceive definite laws. What does all this mean operationally for the search for a fundamental theory of physics? At some level we could just point to the ruliad and declare victory. But this certainly wouldn’t give us specific predictions about the particulars of our perceived universe. To get that we have to go further—and we have to be able to say something about what “slice of the ruliad” we’re dealing with. But the good news is that we don’t seem to have to make many assumptions about ourselves as observers to be able to identify many physical laws that observers like us should perceive. So can we ever expect to nail down a single, specific rule for the universe, say one a particular observer would attribute to it? Given our characteristics as observers, the answer is undoubtedly no. We’re simply not that small in rulial space. But we’re not that big, either. And, importantly, we’re small enough that we can expect to “do science” and consider the universe to “behave in definite ways”. But just as in physical space we’re vastly larger than the scale associated with the atoms of space, so similarly we’re also undoubtedly vastly larger in rulial space than the individual components of the ruliad—so we can’t expect our experience to all be “concentrated in one thread” of the ruliad, following one particular rule. As we discussed above, by doing experiments we can use scientific inference to attempt to localize ourselves in rulial space. But we won’t be able to do enough to say “from our point of view, the universe is operating according to this one specific rule, and not another”. Instead, there’ll be a whole collection of rules that are “good enough”, in the sense that they’ll be sufficient to predict the results of experiments we can realistically do. People have often imagined that, try as we might, we’d never be able to “get to the bottom of physics” and find a specific rule for our universe. And in a sense our inability to localize ourselves in rulial space supports this intuition. But what our Physics Project seems to rather dramatically suggest is that we can “get close enough” in rulial space to have vast predictive power about how our universe must work, or at least how observers like us must perceive it to work.
The Concept of the Ruliad 22.10 Alien Views of the Ruliad : We’ve discussed how “observers like us” will necessarily “parse the ruliad” in ways that make us perceive the universe to follow the laws of physics as we know them. But how different could things get? We have a definite sense of what constitutes a “reasonable observer” based on our 21st-century human experience—and in particular our biology, our technology and our ways of thinking. But what other kinds of observers can we imagine? What about, for example, animals other than humans—in particular say ones whose sensory experience emphasizes olfaction or echolocation or fluid motion? We can think of such animals as operating in a different rulial reference frame or at a different place in rulial space. But how far away in rulial space will they be? How similar or not will their “world views” (and perceived laws of physics) be to ours? It’s hard to know. Presumably our basic assertions about computational boundedness and coherence still apply. But just how the specifics of something like sequentialization in time play out, say, for an ant colony, seems quite unclear. Maybe one day we’ll be able to systematically “think like other animals”. But as of now we haven’t been able to “travel that far” in rulial space. We’ve quite thoroughly explored physical space, say on the surface of our planet, but we haven’t explored very far at all in rulial space. We don’t have a way to translate our thinking into some kind of “thinking differently”—and we don’t, for example, have a common language to get there. There’s often an assumption (a kind of “human exceptionalism”) that if it wasn’t for details of the human experience—like brains and words—then we’d necessarily be dealing with something fundamentally simpler, that could not, for example, show features that we might identify as intelligence. But the Principle of Computational Equivalence tells us this isn’t correct. Because it says that there’s a certain maximal computational sophistication that’s achieved not just by us humans but also by a vast range of other systems. The restrictions of what we’ve chosen to study (in science and elsewhere) have often made us miss this, but in fact computational sophistication—and the direct generalization of our notion of intelligence that’s associated with it—seems quite ubiquitous across many different kinds of systems. So can those other kinds of systems act as “observers like us”? To do so, they need not just computational sophistication, but also a certain alignment with the features we have that lead to our coherent thread of “conscious experience”. And even given that, to actually “connect with” such systems, we need to be able to reach far enough in rulial space to sufficiently make a translation. Imagine the weather (sometimes said to “have a mind of its own”). It’s got plenty of computational sophistication. But is there any sense in which it sequentializes time like we do? Or can one only think of all those different parts of our atmosphere “running in their own time”? To know things like this, we effectively have to have a way to “translate” from the operation of the weather to our (current) way of thinking. And in some sense we can consider the whole enterprise of natural science as being an effort to find a method of translation—or a common language—between nature and our way of thinking. We as observers in effect trace out particular trajectories in rulial space; the challenge of natural science is to “reach out” in rulial space and “pull in” more of the ruliad; to be able to define a way to translate more parts of the ruliad to our processes of thinking. Every time we do an experiment, we can think of this as representing a moment of “connection” or “communication” between us and some aspect of nature. The experiment in effect defines a small piece of “common history” between us and nature—which helps “knit together” the parts of rulial space associated with us and with nature. One of the great mysteries of science has been why—in the vastness of physical space—we’ve never detected something we identify as “alien intelligence”, or an “alien civilization”. We might have thought that it was because we humans have either achieved a unique pinnacle of intelligence or computational ability—or have fundamentally not gotten far enough. But the Principle of Computational Equivalence explodes the idea of this kind of cosmic computational pecking order. So what could actually be going on? Thinking in terms of the ruliad suggests an answer. Our radio telescopes might be able to detect signals from far away in physical space. But our putative aliens might not only live far away in physical space, but also in rulial space. Put another way, the “alien civilization” might be sampling aspects of the ruliad—and in effect the universe—that are utterly different from those we’re used to. That different sampling might be happening right down at the level of atoms of space, or it might be that the rulial distance from us to the aliens is small enough that there’s enough “shared description language” that the alien civilization might rise to the level of seeming like some kind of “noise” relative to our view of “what’s important in the universe”. We might wonder how far apart what we could consider “alien civilizations” would be in physical space. But what we now realize is that we also have to consider how far apart they might be in rulial space. And just like in exploring physical space we can imagine building better spacecraft or better telescopes, so also we can imagine building better ways to reach across rulial space. We’re so used to physical space that it seems to us very concrete to reach across it. Of course, in our Physics Project, things like motion in physical space end up—like everything else—being pure computational processes. And from this point of view, reaching across rulial space is ultimately no more abstract—even though today we would describe it in terms of “doing (abstract) computations” rather than “moving in space”. Relative to our own physical size, the universe already seems like a vast place. But the full ruliad is even incredibly more vast. And we are likely much tinier in rulial space relative to the whole universe than we are in physical space. From the Principle of Computational Equivalence we can expect that there’s ultimately no lack of raw computational sophistication out there—but thinking in terms of the ruliad, the issue is whether what’s going on is close enough to us in rulial space that we can successfully see it as an “alien civilization”. One test of rulial distance might be to ask whether our putative aliens perceive the same laws of physics for the universe that we do. We know that at least the general forms of those laws depend only on what seem to us rather loose conditions. But to get good alignment presumably requires at the very least that we and the aliens are somehow “comparable in size” not only in physical space (and branchial space), but also in rulial space. It’s humbling how difficult it is to imagine the universe from the point of view of an alien at a different place in rulial space. But for example if the alien is big compared to us in rulial space, we can say that they’ll inevitably have a version of science that seems to us much “vaguer” than ours. Because if they maintain a coherent thread of experience, they’ll have to conflate more distant paths in rulial space, on which the universe will do things that are “more different” than what we’re used to. (And, yes, there should be rulial analogs of quantum phenomena, associated for example with conflated paths that diverge far in rulial space.) What would it mean operationally for there to be an alien civilization perhaps nearby in physical space but at a distance in rulial space? Basically the alien civilization will be “operating” in features of the universe that our parsing of the universe just doesn’t pick up. As a simple analogy, our view of, for example, a box of gas might be that it’s something with a certain temperature and pressure. But a different “parsing” of that system might identify a whole world of detailed motions of molecules that with respect to that parsing can be viewed as a vast “alien civilization”. Of course, the situation is much more extreme when it comes to the whole ruliad, and all the paths of history and configurations of atoms of space that it represents. Relative to the whole ruliad, our civilization and our experience have carved out an extremely tiny piece. And what we’re thinking of as “alien civilizations” might also have carved out their own tiny pieces. And while we’re all “living in the same ruliad”, we might no more be able to detect each other or communicate (and likely very much less) than we can across vast distances in physical space. What of the future? The future of our civilization might well be a story of mapping out more of rulial space. If we continue to invent new technology, explore new ideas and generally broaden our ways of thinking and perceiving, we will gradually—albeit in tiny steps—map out more of rulial space. How far can we get? The ultimate limit is determined by the maximum rulial speed. But if we expect to maintain our character as “observers like us”, we’ll no doubt be limited to something much less. Among other issues, moving in rulial space involves doing computation. (The ultimate scale is set by the “processing power” of the universe—which defines the maximum rulial speed.) But “density of computation” effectively corresponds to a generalized version of mass—and is for example a source of “generalized gravity”. And it could be that to “move any significant distance” in rulial space, we’d have to “experience enough generalized gravity” that we could never maintain things like the kind of coherence we need to be an “observer like us”. Put another way: yes, it might in principle be possible to “reach out in rulial space” and “contact the rulial aliens”. But it might be that doing so would require us to be so different from the way we currently are that we wouldn’t recognize anything like consciousness or anything that really makes us “identifiably us”. And if this is so, we are in a sense limited to experiencing the ruliad “on our own” from our particular place in rulial space, forever isolated from “alien civilizations” elsewhere in rulial space.
The Concept of the Ruliad 22.11 Conceptual Implications of the Ruliad : What does the concept of the ruliad mean for the fundamental way we think about things like science? The typical conception of “what science does” is that it’s about us figuring out—as “objectively” as we can—how the world happens to be. But the concept of the ruliad in a sense turns this on its head. Because it says that at some ultimate level, everything is a matter of abstract necessity. And it’s just our “parsing” of it that defines the subject matter of what we call science. We might have thought that the science of the universe was just something that’s “out there”. But what we’re realizing is that instead in some fundamental sense, it’s all “on us”. But does that mean that there’s no “objective truth”, and nothing that can robustly be said about the universe without “passing it through us”? Well, no. Because what we’ve discovered through our Physics Project is that actually there are quite global things that can (“objectively”) be said about our universe and the laws it follows, as perceived by observers like us. We don’t have to know in detail about us humans and the particular ways we perceive things. All we need are some general features—particularly that we are computationally bounded, and that we have a certain persistence and coherence. And this is all it takes to deduce some quite specific statements about how our universe operates, at least as we perceive it. So in a sense what this means is that there is a large “zone of objectivity”; a large set of choices for how we could be that will still lead us to the same “objective truth” about our universe. But if we go far enough away in our mechanism for “parsing the ruliad”, this will no longer be the case. From our current vantage point, we’d no doubt then be hard-pressed to recognize how we’re “doing the parsing”, but the results we’d get would no longer give us the same laws of physics or general perception of the universe that we’re used to. This view of things has all sorts of implications for various long-discussed philosophical issues. But it’s also a view that has precise scientific consequences. And these don’t just relate to physics. Because the ruliad is really a general object that represents the entangled behavior of all possible abstract rules. When we think of ourselves as observers embedded within this object, it means that for us things are actualized, and we have what we call physics. But we can also imagine sampling the ruliad in different ways. Some of those ways correspond to mathematics (or metamathematics). Some correspond to theoretical computer science. The ruliad is the single object that underlies all of them. And which of them we’re talking about just depends on how we imagine we’re sampling or parsing the ruliad, and how we’re describing what we’re observing. With this degree of generality and universality, it’s inevitable that the ruliad must be a complicated object; in fact, in a sense it must encapsulate all possible achievable complexity. But what’s important is that we now have a definite concept of the ruliad, as something we can study and analyze. It’s not simple to do this. The ruliad is at some level an object of great and perhaps supremely elegant abstract regularity. But for us to get any concrete handle on it and its structure, we need to break it down into some kind of “digestible slices” which inevitably lose much of its abstract regularity. And we’re just at the beginning of seeing how best to “unpack” and “pick through” the ruliad. With explicit computations, we can only chip away at the very simplest approximations to the ruliad. In a sense it’s a tribute to the naturalness and inevitability of the ruliad that it’s so closely related to some of the most advanced abstract mathematical methods we know so far. But again, even with these methods we’re barely scratching the surface of the ruliad and what it contains. The theoretical exploration of the ruliad will be a long and difficult journey. But the incredible generality and universality of the ruliad means that every piece of progress is likely to have exceptionally powerful consequences. In some sense the exploration of the ruliad can be seen as the encapsulated expression of everything it means to do theoretical investigation: a kind of ultimately abstract limit of theoretical science and more. For me, the ruliad in a sense builds on a tower of ideas, that include the computational paradigm in general, the exploration of the computational universe of simple programs, the Principle of Computational Equivalence, our Physics Project and the notion of multicomputation. But even with all of these it’s still a significant further jump in abstraction. And one whose consequences will take considerable time to unfold. But for now it’s exciting to have at least been able to define this thing I call the ruliad, and to start seeing some of its unprecedentedly broad and deep implications.
The Concept of the Ruliad 22.12 Appendix: The Case of the “Multiplicad” : As a very simple example of something like the ruliad, we can consider what we might call the “multiplicad”: a rulial multiway system based on integers, in which the rules simply multiply by successive integers: (Note that this kind of pure multiplication is presumably not computation universal, so the limiting object here will not be a coordinatization of the actual full ruliad.) Just like with the full ruliad, there are many different “directions” in which to build up the multiplicad. We could allow as many multipliers and steps as we want, but limit the total size of numbers generated, here say to 30: As an alternative, we can limit the number of multipliers s, say to . Then the multiplicad would build up like this: In the pictures we’ve drawn so far, we’re effectively always deduplicating different occurrences of the same integer. So, for example, the integer 12 can be generated as 1 × 3 × 4 or 1 × 6 × 2 or 1 × 3 × 2 × 2, etc. And in principle we could show each of these “different 12s” separately. But in our deduplicated graph, only a single 12 appears—with the different possible decompositions of 12 being reflected in the presence of multiple paths that lead to the 12. Sometimes the structure we get is richer—if much bigger—when we don’t immediately do deduplication. For example, if we allow any number of multipliers (i.e. take ) then after just 1 step we will get all integers—and if we do deduplication, then this will be the end of our graph, because we “already have all the integers”. But if we don’t do deduplication, we’ll get a slightly more complicated picture, that begins like this: The “topological” structure of this graph is now straightforward, but its “labeling” with numbers is less so—and if we ask, for example, where a particular number appears after t steps, this can be more complicated. Imagine that we are looking only at the subtrees associated with up to s multipliers at the first step—or, equivalently, that we are looking at the rulial multiway system “truncated” with only s rules. Which numbers will appear after steps? The answer is that it will be precisely those numbers that show up in an s × s multiplication table where we start from : Clearly no primes appear here, but some numbers can appear multiple times (e.g. 12 appears 4 times). In general, the number of times that the number will show up is the number of proper divisors it has, or DivisorSigma[0, n]–2: We can continue this, to ask how many times a given number n will occur at a particular step t: We can think of these results as being determined by the number of times that n appears in an s × s × s… (t times) multiplication array. Alternatively, to know the results for a given number n, we can look at all the ways n can be decomposed into factors. For , for example, we would have: And from this we can deduce that 12 appears once at (i.e. with 1 factor), 4 times at (i.e. with 2 factors) and 3 times at (i.e. with 3 factors). The full multiplicad is formed by taking the limits and (as well as what is essentially the limit for an infinite set of possible initial conditions). As we can see, our “finite perception” of the multiplicad will be different depending on how we sample it in s and t. As an example, let’s consider what happens for given s as a function of t. For , we simply have powers of 2: For , where can multiply by both 2 and 3, we get: In studying multiway systems, it is often of interest to ask about the growth rates of the number of states reached over the course of t steps (i.e. the growth rates of volumes of geodesic balls). In the case , the number of states reached by step t is just t. For , it’s the triangular numbers t (t – 1)/2: Here are some results for larger s: Each of these sequences is generated by a linear recurrence relation with a kernel given by a sequence of signed binomial coefficients. The values for successive t can be represented by polynomials: In our general analysis of multiway graphs, it is common to consider branchial graphs—or for a rulial multiway system what we can call rulial graphs—obtained by looking at a slice of the multiway graph, effectively for a given t, and asking what states are connected by having a common ancestor. The results for are rather trivial (here shown for , 2, 3): For we get: And for we have: In a sense these pictures show how numbers in the multiplicad can be “laid out in rulial space”. For , the “large-t graph” has a very linear form and the numbers that appear “from left to right” are arranged more or less in numerical order: For , the result is a 2D-like structure: And again the numbers that appear are roughly arranged in a kind of “numerical sequence”: It should be noted that we’ve only considered one particular way of sampling the rulial multiway graph as a function of t. In general there are many different possible foliations that could be used, all of them giving us in effect a different view of the multiplicad, from a different “reference frame”. As mentioned at the beginning, the multiplicad is presumably not on its own capable of giving us the full ruliad. But if we change the underlying rules—probably even just inserting addition as well as multiplication—we’ll potentially get a system that is capable of universal computation, and which can therefore generate the full ruliad. Needless to say, the particular representation of the ruliad obtained by the kind of “numerical processes” that we’ve used here may be utterly different from any representation that we would recognize from our perception of the physical universe. Thanks & Note Thanks for discussions of various aspects of the ruliad to Xerxes Arsiwalla, James Boyd, Elise Cawley, Hatem Elshatlawy, Jonathan Gorard and Nik Murzin. Thanks also to Ed Pegg and Joseph Stocke for input about the multiplicad. A new paper by Xerxes Arsiwalla and Jonathan Gorard discusses in a more technical way some ideas and results related to the ruliad.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.0 Abstract : Both metamathematics and physics are posited to emerge from samplings by observers of the unique ruliad structure that corresponds to the entangled limit of all possible computations. The possibility of higher-level mathematics accessible to humans is posited to be the analog for mathematical observers of the perception of physical space for physical observers. A physicalized analysis is given of the bulk limit of traditional axiomatic approaches to the foundations of mathematics, together with explicit empirical metamathematics of some examples of formalized mathematics. General physicalized laws of mathematics are discussed, associated with concepts such as metamathematical motion, inevitable dualities, proof topology and metamathematical singularities. It is argued that mathematics as currently practiced can be viewed as derived from the ruliad in a direct Platonic fashion analogous to our experience of the physical world, and that axiomatic formulation, while often convenient, does not capture the ultimate character of mathematics. Among the implications of this view is that only certain collections of axioms may be consistent with inevitable features of human mathematical observers. A discussion is included of historical and philosophical connections, as well as of foundational implications for the future of mathematics.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.1 Mathematics and Physics Have the Same Foundations : One of the many surprising (and to me, unexpected) implications of our Physics Project is its suggestion of a very deep correspondence between the foundations of physics and mathematics. We might have imagined that physics would have certain laws, and mathematics would have certain theories, and that while they might be historically related, there wouldn’t be any fundamental formal correspondence between them. But what our Physics Project suggests is that underneath everything we physically experience there is a single very general abstract structure—that we call the ruliad—and that our physical laws arise in an inexorable way from the particular samples we take of this structure. We can think of the ruliad as the entangled limit of all possible computations—or in effect a representation of all possible formal processes. And this then leads us to the idea that perhaps the ruliad might underlie not only physics but also mathematics—and that everything in mathematics, like everything in physics, might just be the result of sampling the ruliad. Of course, mathematics as it’s normally practiced doesn’t look the same as physics. But the idea is that they can both be seen as views of the same underlying structure. What makes them different is that physical and mathematical observers sample this structure in somewhat different ways. But since in the end both kinds of observers are associated with human experience they inevitably have certain core characteristics in common. And the result is that there should be “fundamental laws of mathematics” that in some sense mirror the perceived laws of physics that we derive from our physical observation of the ruliad. So what might those fundamental laws of mathematics be like? And how might they inform our conception of the foundations of mathematics, and our view of what mathematics really is? The most obvious manifestation of the mathematics that we humans have developed over the course of many centuries is the few million mathematical theorems that have been published in the literature of mathematics. But what can be said in generality about this thing we call mathematics? Is there some notion of what mathematics is like “in bulk”? And what might we be able to say, for example, about the structure of mathematics in the limit of infinite future development? When we do physics, the traditional approach has been to start from our basic sensory experience of the physical world, and of concepts like space, time and motion—and then to try to formalize our descriptions of these things, and build on these formalizations. And in its early development—for example by Euclid—mathematics took the same basic approach. But beginning a little more than a century ago there emerged the idea that one could build mathematics purely from formal axioms, without necessarily any reference to what is accessible to sensory experience. And in a way our Physics Project begins from a similar place. Because at the outset it just considers purely abstract structures and abstract rules—typically described in terms of hypergraph rewriting—and then tries to deduce their consequences. Many of these consequences are incredibly complicated, and full of computational irreducibility. But the remarkable discovery is that when sampled by observers with certain general characteristics that make them like us, the behavior that emerges must generically have regularities that we can recognize, and in fact must follow exactly known core laws of physics. And already this begins to suggest a new perspective to apply to the foundations of mathematics. But there’s another piece, and that’s the idea of the ruliad. We might have supposed that our universe is based on some particular chosen underlying rule, like an axiom system we might choose in mathematics. But the concept of the ruliad is in effect to represent the entangled result of “running all possible rules”. And the key point is then that it turns out that an “observer like us” sampling the ruliad must perceive behavior that corresponds to known laws of physics. In other words, without “making any choice” it’s inevitable—given what we’re like as observers—that our “experience of the ruliad” will show fundamental laws of physics. But now we can make a bridge to mathematics. Because in embodying all possible computational processes the ruliad also necessarily embodies the consequences of all possible axiom systems. As humans doing physics we’re effectively taking a certain sampling of the ruliad. And we realize that as humans doing mathematics we’re also doing essentially the same kind of thing. But will we see “general laws of mathematics” in the same kind of way that we see “general laws of physics”? It depends on what we’re like as “mathematical observers”. In physics, there turn out to be general laws—and concepts like space and motion—that we humans can assimilate. And in the abstract it might not be that anything similar would be true in mathematics. But it seems as if the thing mathematicians typically call mathematics is something for which it is—and where (usually in the end leveraging our experience of physics) it’s possible to successfully carve out a sampling of the ruliad that’s again one we humans can assimilate. When we think about physics we have the idea that there’s an actual physical reality that exists—and that we experience physics within this. But in the formal axiomatic view of mathematics, things are different. There’s no obvious “underlying reality” there; instead there’s just a certain choice we make of axiom system. But now, with the concept of the ruliad, the story is different. Because now we have the idea that “deep underneath” both physics and mathematics there’s the same thing: the ruliad. And that means that insofar as physics is “grounded in reality”, so also must mathematics be. When most working mathematicians do mathematics it seems to be typical for them to reason as if the constructs they’re dealing with (whether they be numbers or sets or whatever) are “real things”. But usually there’s a concept that in principle one could “drill down” and formalize everything in terms of some axiom system. And indeed if one wants to get a global view of mathematics and its structure as it is today, it seems as if the best approach is to work from the formalization that’s been done with axiom systems. In starting from the ruliad and the ideas of our Physics Project we’re in effect positing a certain “theory of mathematics”. And to validate this theory we need to study the “phenomena of mathematics”. And, yes, we could do this in effect by directly “reading the whole literature of mathematics”. But it’s more efficient to start from what’s in a sense the “current prevailing underlying theory of mathematics” and to begin by building on the methods of formalized mathematics and axiom systems. Over the past century a certain amount of metamathematics has been done by looking at the general properties of these methods. But most often when the methods are systematically used today, it’s to set up some particular mathematical derivation, normally with the aid of a computer. But here what we want to do is think about what happens if the methods are used “in bulk”. Underneath there may be all sorts of specific detailed formal derivations being done. But somehow what emerges from this is something higher level, something “more human”—and ultimately something that corresponds to our experience of pure mathematics. How might this work? We can get an idea from an analogy in physics. Imagine we have a gas. Underneath, it consists of zillions of molecules bouncing around in detailed and complicated patterns. But most of our “human” experience of the gas is at a much more coarse-grained level—where we perceive not the detailed motions of individual molecules, but instead continuum fluid mechanics. And so it is, I think, with mathematics. All those detailed formal derivations—for example of the kind automated theorem proving might do—are like molecular dynamics. But most of our “human experience of mathematics”—where we talk about concepts like integers or morphisms—is like fluid dynamics. The molecular dynamics is what builds up the fluid, but for most questions of “human interest” it’s possible to “reason at the fluid dynamics level”, without dropping down to molecular dynamics. It’s certainly not obvious that this would be possible. It could be that one might start off describing things at a “fluid dynamics” level—say in the case of an actual fluid talking about the motion of vortices—but that everything would quickly get “shredded”, and that there’d soon be nothing like a vortex to be seen, only elaborate patterns of detailed microscopic molecular motions. And similarly in mathematics one might imagine that one would be able to prove theorems in terms of things like real numbers but actually find that everything gets “shredded” to the point where one has to start talking about elaborate issues of mathematical logic and different possible axiomatic foundations. But in physics we effectively have the Second Law of thermodynamics—which we now understand in terms of computational irreducibility—that tells us that there’s a robust sense in which the microscopic details are systematically “washed out” so that things like fluid dynamics “work”. Just sometimes—like in studying Brownian motion, or hypersonic flow—the molecular dynamics level still “shines through”. But for most “human purposes” we can describe fluids just using ordinary fluid dynamics. So what’s the analog of this in mathematics? Presumably it’s that there’s some kind of “general law of mathematics” that explains why one can so often do mathematics “purely in the large”. Just like in fluid mechanics there can be “corner-case” questions that probe down to the “molecular scale”—and indeed that’s where we can expect to see things like undecidability, as a rough analog of situations where we end up tracing the potentially infinite paths of single molecules rather than just looking at “overall fluid effects”. But somehow in most cases there’s some much stronger phenomenon at work—that effectively aggregates low-level details to allow the kind of “bulk description” that ends up being the essence of what we normally in practice call mathematics. But is such a phenomenon something formally inevitable, or does it somehow depend on us humans “being in the loop”? In the case of the Second Law it’s crucial that we only get to track coarse-grained features of a gas—as we humans with our current technology typically do. Because if instead we watched and decoded what every individual molecule does, we wouldn’t end up identifying anything like the usual bulk “Second-Law” behavior. In other words, the emergence of the Second Law is in effect a direct consequence of the fact that it’s us humans—with our limitations on measurement and computation—who are observing the gas. So is something similar happening with mathematics? At the underlying “molecular level” there’s a lot going on. But the way we humans think about things, we’re effectively taking just particular kinds of samples. And those samples turn out to give us “general laws of mathematics” that give us our usual experience of “human-level mathematics”. To ultimately ground this we have to go down to the fully abstract level of the ruliad, but we’ll already see many core effects by looking at mathematics essentially just at a traditional “axiomatic level”, albeit “in bulk”. The full story—and the full correspondence between physics and mathematics—requires in a sense “going below” the level at which we have recognizable formal axiomatic mathematical structures; it requires going to a level at which we’re just talking about making everything out of completely abstract elements, which in physics we might interpret as “atoms of space” and in mathematics as some kind of “symbolic raw material” below variables and operators and everything else familiar in traditional axiomatic mathematics. The deep correspondence we’re describing between physics and mathematics might make one wonder to what extent the methods we use in physics can be applied to mathematics, and vice versa. In axiomatic mathematics the emphasis tends to be on looking at particular theorems and seeing how they can be knitted together with proofs. And one could certainly imagine an analogous “axiomatic physics” in which one does particular experiments, then sees how they can “deductively” be knitted together. But our impression that there’s an “actual reality” to physics makes us seek broader laws. And the correspondence between physics and mathematics implied by the ruliad now suggests that we should be doing this in mathematics as well. What will we find? Some of it in essence just confirms impressions that working pure mathematicians already have. But it provides a definite framework for understanding these impressions and for seeing what their limits may be. It also lets us address questions like why undecidability is so comparatively rare in practical pure mathematics, and why it is so common to discover remarkable correspondences between apparently quite different areas of mathematics. And beyond that, it suggests a host of new questions and approaches both to mathematics and metamathematics—that help frame the foundations of the remarkable intellectual edifice that we call mathematics.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.2 The Underlying Structure of Mathematics and Physics : If we “drill down” to what we’ve called above the “molecular level” of mathematics, what will we find there? There are many technical details (some of which we’ll discuss later) about the historical conventions of mathematics and its presentation. But in broad outline we can think of there as being a kind of “gas” of “mathematical statements”—like 1 + 1 = 2 or x + y = y + x—represented in some specified symbolic language. (And, yes, Wolfram Language provides a well-developed example of what that language can be like.) But how does the “gas of statements” behave? The essential point is that new statements are derived from existing ones by “interactions” that implement laws of inference (like that q can be derived from the statement p and the statement “p implies q”). And if we trace the paths by which one statement can be derived from others, these correspond to proofs. And the whole graph of all these derivations is then a representation of the possible historical development of mathematics—with slices through this graph corresponding to the sets of statements reached at a given stage. By talking about things like a “gas of statements” we’re making this sound a bit like physics. But while in physics a gas consists of actual, physical molecules, in mathematics our statements are just abstract things. But this is where the discoveries of our Physics Project start to be important. Because in our project we’re “drilling down” beneath for example the usual notions of space and time to an “ultimate machine code” for the physical universe. And we can think of that ultimate machine code as operating on things that are in effect just abstract constructs—very much like in mathematics. In particular, we imagine that space and everything in it is made up of a giant network (hypergraph) of “atoms of space”—with each “atom of space” just being an abstract element that has certain relations with other elements. The evolution of the universe in time then corresponds to the application of computational rules that (much like laws of inference) take abstract relations and yield new relations—thereby progressively updating the network that represents space and everything in it. But while the individual rules may be very simple, the whole detailed pattern of behavior to which they lead is normally very complicated—and typically shows computational irreducibility, so that there’s no way to systematically find its outcome except in effect by explicitly tracing each step. But despite all this underlying complexity it turns out—much like in the case of an ordinary gas—that at a coarse-grained level there are much simpler (“bulk”) laws of behavior that one can identify. And the remarkable thing is that these turn out to be exactly general relativity and quantum mechanics (which, yes, end up being the same theory when looked at in terms of an appropriate generalization of the notion of space). But down at the lowest level, is there some specific computational rule that’s “running the universe”? I don’t think so. Instead, I think that in effect all possible rules are always being applied. And the result is the ruliad: the entangled structure associated with performing all possible computations. But what then gives us our experience of the universe and of physics? Inevitably we are observers embedded within the ruliad, sampling only certain features of it. But what features we sample are determined by the characteristics of us as observers. And what seem to be critical to have “observers like us” are basically two characteristics. First, that we are computationally bounded. And second, that we somehow persistently maintain our coherence—in the sense that we can consistently identify what constitutes “us” even though the detailed atoms of space involved are continually changing. But we can think of different “observers like us” as taking different specific samples, corresponding to different reference frames in rulial space, or just different positions in rulial space. These different observers may describe the universe as evolving according to different specific underlying rules. But the crucial point is that the general structure of the ruliad implies that so long as the observers are “like us”, it’s inevitable that their perception of the universe will be that it follows things like general relativity and quantum mechanics. It’s very much like what happens with a gas of molecules: to an “observer like us” there are the same gas laws and the same laws of fluid dynamics essentially independent of the detailed structure of the individual molecules. So what does all this mean for mathematics? The crucial and at first surprising point is that the ideas we’re describing in physics can in effect immediately be carried over to mathematics. And the key is that the ruliad represents not only all physics, but also all mathematics—and it shows that these are not just related, but in some sense fundamentally the same. In the traditional formulation of axiomatic mathematics, one talks about deriving results from particular axiom systems—say Peano Arithmetic, or ZFC set theory, or the axioms of Euclidean geometry. But the ruliad in effect represents the entangled consequences not just of specific axiom systems but of all possible axiom systems (as well as all possible laws of inference). But from this structure that in a sense corresponds to all possible mathematics, how do we pick out any particular mathematics that we’re interested in? The answer is that just as we are limited observers of the physical universe, so we are also limited observers of the “mathematical universe”. But what are we like as “mathematical observers”? As I’ll argue in more detail later, we inherit our core characteristics from those we exhibit as “physical observers”. And that means that when we “do mathematics” we’re effectively sampling the ruliad in much the same way as when we “do physics”. We can operate in different rulial reference frames, or at different locations in rulial space, and these will correspond to picking out different underlying “rules of mathematics”, or essentially using different axiom systems. But now we can make use of the correspondence with physics to say that we can also expect there to be certain “overall laws of mathematics” that are the result of general features of the ruliad as perceived by observers like us. And indeed we can expect that in some formal sense these overall laws will have exactly the same structure as those in physics—so that in effect in mathematics we’ll have something like the notion of space that we have in physics, as well as formal analogs of things like general relativity and quantum mechanics. What does this mean? It implies that—just as it’s possible to have coherent “higher-level descriptions” in physics that don’t just operate down at the level of atoms of space, so also this should be possible in mathematics. And this in a sense is why we can expect to consistently do what I described above as “human-level mathematics”, without usually having to drop down to the “molecular level” of specific axiomatic structures (or below). Say we’re talking about the Pythagorean theorem. Given some particular detailed axiom system for mathematics we can imagine using it to build up a precise—if potentially very long and pedantic—representation of the theorem. But let’s say we change some detail of our axioms, say associated with the way they talk about sets, or real numbers. We’ll almost certainly still be able to build up something we consider to be “the Pythagorean theorem”—even though the details of the representation will be different. In other words, this thing that we as humans would call “the Pythagorean theorem” is not just a single point in the ruliad, but a whole cloud of points. And now the question is: what happens if we try to derive other results from the Pythagorean theorem? It might be that each particular representation of the theorem—corresponding to each point in the cloud—would lead to quite different results. But it could also be that essentially the whole cloud would coherently lead to the same results. And the claim from the correspondence with physics is that there should be “general laws of mathematics” that apply to “observers like us” and that ensure that there’ll be coherence between all the different specific representations associated with the cloud that we identify as “the Pythagorean theorem”. In physics it could have been that we’d always have to separately say what happens to every atom of space. But we know that there’s a coherent higher-level description of space—in which for example we can just imagine that objects can move while somehow maintaining their identity. And we can now expect that it’s the same kind of thing in mathematics: that just as there’s a coherent notion of space in physics where things can for example move without being “shredded”, so also this will happen in mathematics. And this is why it’s possible to do “higher-level mathematics” without always dropping down to the lowest level of axiomatic derivations. It’s worth pointing out that even in physical space a concept like “pure motion” in which objects can move while maintaining their identity doesn’t always work. For example, close to a spacetime singularity, one can expect to eventually be forced to see through to the discrete structure of space—and for any “object” to inevitably be “shredded”. But most of the time it’s possible for observers like us to maintain the idea that there are coherent large-scale features whose behavior we can study using “bulk” laws of physics. And we can expect the same kind of thing to happen with mathematics. Later on, we’ll discuss more specific correspondences between phenomena in physics and mathematics—and we’ll see the effects of things like general relativity and quantum mechanics in mathematics, or, more precisely, in metamathematics. But for now, the key point is that we can think of mathematics as somehow being made of exactly the same stuff as physics: they’re both just features of the ruliad, as sampled by observers like us. And in what follows we’ll see the great power that arises from using this to combine the achievements and intuitions of physics and mathematics—and how this lets us think about new “general laws of mathematics”, and view the ultimate foundations of mathematics in a different light.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.3 The Metamodeling of Axiomatic Mathematics : Consider all the mathematical statements that have appeared in mathematical books and papers. We can view these in some sense as the “observed phenomena” of (human) mathematics. And if we’re going to make a “general theory of mathematics” a first step is to do something like we’d typically do in natural science, and try to “drill down” to find a uniform underlying model—or at least representation—for all of them. At the outset, it might not be clear what sort of representation could possibly capture all those different mathematical statements. But what’s emerged over the past century or so—with particular clarity in Mathematica and the Wolfram Language—is that there is in fact a rather simple and general representation that works remarkably well: a representation in which everything is a symbolic expression. And presumably it’s this familiarity from human natural language that’s caused “human natural mathematics” to develop in a way that can so readily be represented by symbolic expressions. But in typical mathematics there’s an important wrinkle. One often wants to make statements not just about particular things but about whole classes of things. And it’s common to then just declare that some of the “symbols” (like, say, x) that appear in an expression are “variables”, while others (like, say, Plus) are not. But in our effort to capture the essence of mathematics as uniformly as possible it seems much better to burn the idea of an object representing a whole class of things right into the structure of the symbolic expression. And indeed this is a core idea in the Wolfram Language, where something like x or f is just a “symbol that stands for itself”, while x_ is a pattern (named x) that can stand for anything. (More precisely, _ on its own is what stands for “anything”, and x_—which can also be written x:_—just says that whatever _ stands for in a particular instance will be called x.) Then with this notation an example of a “mathematical statement” might be: which can be viewed as a shorthand for the pair of Wolfram Language rules: OK, so let’s say we have the expression . Now we can just apply the rules defined by our statement. Here’s what happens if we do this just once in all possible ways: And here we see, for example, that can be transformed to . Continuing this we build up a whole multiway graph. After just one more step we get: Continuing for a few more steps we then get or in a different rendering: But what does this graph mean? Essentially it gives us a map of equivalences between expressions—with any pair of expressions that are connected being equivalent. So, for example, it turns out that the expressions and are equivalent, and we can “prove this” by exhibiting a path between them in the graph: The steps on the path can then be viewed as steps in the proof, where here at each step we’ve indicated where the transformation in the expression took place: In mathematical terms, we can then say that starting from the “axiom” we were able to prove a certain equivalence theorem between two expressions. We gave a particular proof. But there are others, for example the “less efficient” 35-step one corresponding to the path: For our later purposes it’s worth talking in a little bit more detail here about how the steps in these proofs actually proceed. Consider the expression: We can think of this as a tree: Our axiom can then be represented as: In terms of trees, our first proof becomes: where we’re indicating at each step which piece of tree gets “substituted for” using the axiom. What we’ve done so far is to generate a multiway graph for a certain number of steps, and then to see if we can find a “proof path” in it for some particular statement. But what if we are given a statement, and asked whether it can be proved within the specified axiom system? In effect this asks whether if we make a sufficiently large multiway graph we can find a path of any length that corresponds to the statement. If our system was computationally reducible we could expect always to be able to find a finite answer to this question. But in general—with the Principle of Computational Equivalence and the ubiquitous presence of computational irreducibility—it’ll be common that there is no fundamentally better way to determine whether a path exists than effectively to try explicitly generating it. If we knew, for example, that the intermediate expressions generated always remained of bounded length, then this would still be a bounded problem. But in general the expressions can grow to any size—with the result that there is no general upper bound on the length of path necessary to prove even a statement about equivalence between small expressions. For example, for the axiom we are using here, we can look at statements of the form . Then this shows how many expressions expr of what sizes have shortest proofs of with progressively greater lengths: And for example if we look at the statement: its shortest proof is: where, as is often the case, there are intermediate expressions that are longer than the final result.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.4 Some Simple Examples with Mathematical Interpretations : The multiway graphs in the previous section are in a sense fundamentally metamathematical. Their “raw material” is mathematical statements. But what they represent are the results of operations—like substitution—that are defined at a kind of meta level, that “talks about mathematics” but isn’t itself immediately “representable as mathematics”. But to help understand this relationship it’s useful to look at simple cases where it’s possible to make at least some kind of correspondence with familiar mathematical concepts. Consider for example the axiom that we can think of as representing commutativity of the binary operator ∘. Now consider using substitution to “apply this axiom”, say starting from the expression . The result is the (finite) multiway graph: Conflating the pairs of edges going in opposite directions, the resulting graphs starting from any expression involving s ∘’s (and distinct variables) are: And these are just the Boolean hypercubes, each with nodes. If instead of commutativity we consider the associativity axiom: then we get a simple “ring” multiway graph: With both associativity and commutativity we get: What is the mathematical significance of this object? We can think of our axioms as being the general axioms for a commutative semigroup. And if we build a multiway graph—say starting with —we’ll find out what expressions are equivalent to in any commutative semigroup—or, in other words, we’ll get a collection of theorems that are “true for any commutative semigroup”: But what if we want to deal with a “specific semigroup” rather than a generic one? We can think of our symbols a and b as generators of the semigroup, and then we can add relations, as in: And the result of this will be that we get more equivalences between expressions: The multiway graph here is still finite, however, giving a finite number of equivalences. But let’s say instead that we add the relations: Then if we start from a we get a multiway graph that begins like: but just keeps growing forever (here shown after 6 steps): And what this then means is that there are an infinite number of equivalences between expressions. We can think of our basic symbols and as being generators of our semigroup. Then our expressions correspond to “words” in the semigroup formed from these generators. The fact that the multiway graph is infinite then tells us that there are an infinite number of equivalences between words. But when we think about the semigroup mathematically we’re typically not so interested in specific words as in the overall “distinct elements” in the semigroup, or in other words, in those “clusters of words” that don’t have equivalences between them. And to find these we can imagine starting with all possible expressions, then building up multiway graphs from them. Many of the graphs grown from different expressions will join up. But what we want to know in the end is how many disconnected graph components are ultimately formed. And each of these will correspond to an element of the semigroup. As a simple example, let’s start from all words of length 2: The multiway graphs formed from each of these after 1 step are: But these graphs in effect “overlap”, leaving three disconnected components: After 2 steps the corresponding result has two components: And if we start with longer (or shorter) words, and run for more steps, we’ll keep finding the same result: that there are just two disconnected “droplets” that “condense out” of the “gas” of all possible initial words: And what this means is that our semigroup ultimately has just two distinct elements—each of which can be represented by any of the different (“equivalent”) words in each “droplet”. (In this particular case the droplets just contain respectively all words with an odd or even number of b’s.) In the mathematical analysis of semigroups (as well as groups), it’s common ask what happens if one forms products of elements. In our setting what this means is in effect that one wants to “combine droplets using ∘”. The simplest words in our two droplets are respectively and . And we can use these as “representatives of the droplets”. Then we can see how multiplication by and by transforms words from each droplet: With only finite words the multiplications will sometimes not “have an immediate target” (so they are not indicated here). But in the limit of an infinite number of multiway steps, every multiplication will “have a target” and we’ll be able to summarize the effect of multiplication in our semigroup by the graph: More familiar as mathematical objects than semigroups are groups. And while their axioms are slightly more complicated, the basic setup we’ve discussed for semigroups also applies to groups. And indeed the graph we’ve just generated for our semigroup is very much like a standard Cayley graph that we might generate for a group—in which the nodes are elements of the group and the edges define how one gets from one element to another by multiplying by a generator. (One technical detail is that in Cayley graphs identity-element self-loops are normally dropped.) Consider the group (the “Klein four-group”). In our notation the axioms for this group can be written: Given these axioms we do the same construction as for the semigroup above. And what we find is that now four “droplets” emerge, corresponding to the four elements of and the pattern of connections between them in the limit yields exactly the Cayley graph for : We can view what’s happening here as a first example of something we’ll return to at length later: the idea of “parsing out” recognizable mathematical concepts (here things like elements of groups) from lower-level “purely metamathematical” structures.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.5 Metamathematical Space : In multiway graphs like those we’ve shown in previous sections we routinely generate very large numbers of “mathematical” expressions. But how are these expressions related to each other? And in some appropriate limit can we think of them all being embedded in some kind of “metamathematical space”? It turns out that this is the direct analog of what in our Physics Project we call branchial space, and what in that case defines a map of the entanglements between branches of quantum history. In the mathematical case, let’s say we have a multiway graph generated using the axiom: After a few steps starting from we have: Now—just as in our Physics Project—let’s form a branchial graph by looking at the final expressions here and connecting them if they are “entangled” in the sense that they share an ancestor on the previous step: There’s some trickiness here associated with loops in the multiway graph (which are the analog of closed timelike curves in physics) and what it means to define different “steps in evolution”. But just iterating once more the construction of the multiway graph, we get a branchial graph: After a couple more iterations the structure of the branchial graph is (with each node sized according to the size of expression it represents): Continuing another iteration, the structure becomes: And in essence this structure can indeed be thought of as defining a kind of “metamathematical space” in which the different expressions are embedded. But what is the “geography” of this space? This shows how expressions (drawn as trees) are laid out on a particular branchial graph and we see that there is at least a general clustering of similar trees on the graph—indicating that “similar expressions” tend to be “nearby” in the metamathematical space defined by this axiom system. An important feature of branchial graphs is that effects are—essentially by construction—always local in the branchial graph. For example, if one changes an expression at a particular step in the evolution of a multiway system, it can only affect a region of the branchial graph that essentially expands by one edge per step. One can think of the affected region—in analogy with a light cone in spacetime—as being the “entailment cone” of a particular expression. The edge of the entailment cone in effect expands at a certain “maximum metamathematical speed” in metamathematical (i.e. branchial) space—which one can think of as being measured in units of “expression change per multiway step”. By analogy with physics one can start talking in general about motion in metamathematical space. A particular proof path in the multiway graph will progressively “move around” in the branchial graph that defines metamathematical space. (Yes, there are many subtle issues here, not least the fact that one has to imagine a certain kind of limit being taken so that the structure of the branchial graph is “stable enough” to “just be moving around” in something like a “fixed background space”.) By the way, the shortest proof path in the multiway graph is the analog of a geodesic in spacetime. And later we’ll talk about how the “density of activity” in the branchial graph is the analog of energy in physics, and how it can be seen as “deflecting” the path of geodesics, just as gravity does in spacetime. It’s worth mentioning just one further subtlety. Branchial graphs are in effect associated with “transverse slices” of the multiway graph—but there are many consistent ways to make these slices. In physics terms one can think of the foliations that define different choices of sequences of slices as being like “reference frames” in which one is specifying a sequence of “simultaneity surfaces” (here “branchtime hypersurfaces”). The particular branchial graphs we’ve shown here are ones associated with what in physics might be called the cosmological rest frame in which every node is the result of the same number of updates since the beginning.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.6 The Issue of Generated Variables : A rule like: defines transformations for any expressions and . So, for example, if we use the rule from left to right on the expression the “pattern variable” will be taken to be a while will be taken to be b ∘ a, and the result of applying the rule will be . But consider instead the case where our rule is: Applying this rule (from left to right) to we’ll now get . And applying the rule to we’ll get . But what should we make of those ’s? And in particular, are they “the same”, or not? A pattern variable like z_ can stand for any expression. But do two different z_’s have to stand for the same expression? In a rule like   … we’re assuming that, yes, the two z_’s always stand for the same expression. But if the z_’s appear in different rules it’s a different story. Because in that case we’re dealing with two separate and unconnected z_’s—that can stand for completely different expressions. To begin seeing how this works, let’s start with a very simple example. Consider the (for now, one-way) rule where is the literal symbol , and x_ is a pattern variable. Applying this to we might think we could just write the result as: Then if we apply the rule again both branches will give the same expression , so there’ll be a merge in the multiway graph: But is this really correct? Well, no. Because really those should be two different x_’s, that could stand for two different expressions. So how can we indicate this? One approach is just to give every “generated” x_ a new name: But this result isn’t really correct either. Because if we look at the second step we see the two expressions and . But what’s really the difference between these? The names are arbitrary; the only constraint is that within any given expression they have to be different. But between expressions there’s no such constraint. And in fact and both represent exactly the same class of expressions: any expression of the form . So in fact it’s not correct that there are two separate branches of the multiway system producing two separate expressions. Because those two branches produce equivalent expressions, which means they can be merged. And turning both equivalent expressions into the same canonical form we get: It’s important to notice that this isn’t the same result as what we got when we assumed that every x_ was the same. Because then our final result was the expression which can match but not —whereas now the final result is which can match both and . This may seem like a subtle issue. But it’s critically important in practice. Not least because generated variables are in effect what make up all “truly new stuff” that can be produced. With a rule like one’s essentially just taking whatever one started with, and successively rearranging the pieces of it. But with a rule like there’s something “truly new” generated every time z_ appears. By the way, the basic issue of “generated variables” isn’t something specific to the particular symbolic expression setup we’ve been using here. For example, there’s a direct analog of it in the hypergraph rewriting systems that appear in our Physics Project. But in that case there’s a particularly clear interpretation: the analog of “generated variables” are new “atoms of space” produced by the application of rules. And far from being some kind of footnote, these “generated atoms of space” are what make up everything we have in our universe today. The issue of generated variables—and especially their naming—is the bane of all sorts of formalism for mathematical logic and programming languages. As we’ll see later, it’s perfectly possible to “go to a lower level” and set things up with no names at all, for example using combinators. But without names, things tend to seem quite alien to us humans—and certainly if we want to understand the correspondence with standard presentations of mathematics it’s pretty necessary to have names. So at least for now we’ll keep names, and handle the issue of generated variables by uniquifying their names, and canonicalizing every time we have a complete expression. Let’s look at another example to see the importance of how we handle generated variables. Consider the rule: If we start with a ∘ a and do no uniquification, we’ll get: With uniquification, but not canonicalization, we’ll get a pure tree: But with canonicalization this is reduced to: A confusing feature of this particular example is that this same result would have been obtained just by canonicalizing the original “assume-all-x_’s-are-the-same” case. But things don’t always work this way. Consider the rather trivial rule starting from . If we don’t do uniquification, and don’t do canonicalization, we get: If we do uniquification (but not canonicalization), we get a pure tree: But if we now canonicalize this, we get: And this is now not the same as what we would get by canonicalizing, without uniquifying:.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.7 Rules Applied to Rules : In what we’ve done so far, we’ve always talked about applying rules (like ) to expressions (like or ). But if everything is a symbolic expression there shouldn’t really need to be a distinction between “rules” and “ordinary expressions”. They’re all just expressions. And so we should as well be able to apply rules to rules as to ordinary expressions. And indeed the concept of “applying rules to rules” is something that has a familiar analog in standard mathematics. The “two-way rules” we’ve been using effectively define equivalences—which are very common kinds of statements in mathematics, though in mathematics they’re usually written with rather than with . And indeed, many axioms and many theorems are specified as equivalences—and in equational logic one takes everything to be defined using equivalences. And when one’s dealing with theorems (or axioms) specified as equivalences, the basic way one derives new theorems is by applying one theorem to another—or in effect by applying rules to rules. As a specific example, let’s say we have the “axiom”: We can now apply this to the rule: to get (where since is equivalent to we’re sorting each two-way rule that arises): or after a few more steps: In this example all that’s happening is that the substitutions specified by the axiom are getting separately applied to the left- and right-hand sides of each rule that is generated. But if we really take seriously the idea that everything is a symbolic expression, things can get a bit more complicated. Consider for example the rule: If we apply this to: then if x_ “matches any expression” it can match the whole expression giving the result: Standard mathematics doesn’t have an obvious meaning for something like this—although as soon as one “goes metamathematical” it’s fine. But in an effort to maintain contact with standard mathematics we’ll for now have the “meta rule” that x_ can’t match an expression whose top-level operator is . (As we’ll discuss later, including such matches would allow us to do exotic things like encode set theory within arithmetic, which is again something usually considered to be “syntactically prevented” in mathematical logic.) Another—still more obscure—meta rule we have is that x_ can’t “match inside a variable”. In Wolfram Language, for example, a_ has the full form Pattern[a,Blank[]], and one could imagine that x_ could match “internal pieces” of this. But for now, we’re going to treat all variables as atomic—even though later on, when we “descend below the level of variables”, the story will be different. When we apply a rule like to we’re taking a rule with pattern variables, and doing substitutions with it on a “literal expression” without pattern variables. But it’s also perfectly possible to apply pattern rules to pattern rules—and indeed that’s what we’ll mostly do below. But in this case there’s another subtle issue that can arise. Because if our rule generates variables, we can end up with two different kinds of variables with “arbitrary names”: generated variables, and pattern variables from the rule we’re operating on. And when we canonicalize the names of these variables, we can end up with identical expressions that we need to merge. Here’s what happens if we apply the rule to the literal rule : If we apply it to the pattern rule but don’t do canonicalization, we’ll just get the same basic result: But if we canonicalize we get instead: The effect is more dramatic if we go to two steps. When operating on the literal rule we get: Operating on the pattern rule, but without canonicalization, we get: while if we include canonicalization many rules merge and we get.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.8 Accumulative Evolution : We can think of “ordinary expressions” like as being like “data”, and rules as being like “code”. But when everything is a symbolic expression, it’s perfectly possible—as we saw above—to “treat code like data”, and in particular to generate rules as output. But this now raises a new possibility. When we “get a rule as output”, why not start “using it like code” and applying it to things? In mathematics we might apply some theorem to prove a lemma, and then we might subsequently use that lemma to prove another theorem—eventually building up a whole “accumulative structure” of lemmas (or theorems) being used to prove other lemmas. In any given proof we can in principle always just keep using the axioms over and over again—but it’ll be much more efficient to progressively build a library of more and more lemmas, and use these. And in general we’ll build up a richer structure by “accumulating lemmas” than always just going back to the axioms. In the multiway graphs we’ve drawn so far, each edge represents the application of a rule, but that rule is always a fixed axiom. To represent accumulative evolution we need a slightly more elaborate structure—and it’ll be convenient to use token-event graphs rather than pure multiway graphs. Every time we apply a rule we can think of this as an event. And with the setup we’re describing, that event can be thought of as taking two tokens as input: one the “code rule” and the other the “data rule”. The output from the event is then some collection of rules, which can then serve as input (either “code” or “data”) to other events. Let’s start with the very simple example of the rule: where for now there are no patterns being used. Starting from this rule, we get the token-event graph (where now we’re indicating the initial “axiom” statement using a slightly different color): One subtlety here is that the is applied to itself—so there are two edges going into the event from the node representing the rule. Another subtlety is that there are two different ways the rule can be applied, with the result that there are two output rules generated. Here’s another example, based on the two rules: Continuing for another step we get: Typically we will want to consider as “defining an equivalence”, so that means the same as , and can be conflated with it—yielding in this case: Now let’s consider the rule: After one step we get: After 2 steps we get: The token-event graphs after 3 and 4 steps in this case are (where now we’ve deduplicated events): Let’s now consider a rule with the same structure, but with pattern variables instead of literal symbols: Here’s what happens after one step (note that there’s canonicalization going on, so a_’s in different rules aren’t “the same”) and we see that there are different theorems from the ones we got without patterns. After 2 steps with the pattern rule we get where now the complete set of “theorems that have been derived” is (dropping the _’s for readability) or as trees: After another step one gets: where now there are 2860 “theorems”, roughly exponentially distributed across sizes according to: and with a typical “size-19” theorem being: In effect we can think of our original rule (or “axiom”) as having initiated some kind of “mathematical Big Bang” from which an increasing number of theorems are generated. Early on we described having a “gas” of mathematical theorems that—a little like molecules—can interact and create new theorems. So now we can view our accumulative evolution process as a concrete example of this. Let’s consider the rule from previous sections: After one step of accumulative evolution according to this rule we get: After 2 and 3 steps the results are: What is the significance of all this complexity? At a basic level, it’s just an example of the ubiquitous phenomenon in the computational universe (captured in the Principle of Computational Equivalence) that even systems with very simple rules can generate behavior as complex as anything. But the question is whether—on top of all this complexity—there are simple “coarse-grained” features that we can identify as “higher-level mathematics”; features that we can think of as capturing the “bulk” behavior of the accumulative evolution of axiomatic mathematics.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.9 Accumulative String Systems : As we’ve just seen, the accumulative evolution of even very simple transformation rules for expressions can quickly lead to considerable complexity. And in an effort to understand the essence of what’s going on, it’s useful to look at the slightly simpler case not of rules for “tree-structured expressions” but instead at rules for strings of characters. Consider the seemingly trivial case of the rule: After one step this gives: while after 2 steps we get: though treating as the same as this just becomes: Here’s what happens with the rule: After 2 steps we get: and after 3 steps: where now there are a total of 25 “theorems”, including (unsurprisingly) things like: It’s worth noting that despite the “lexical similarity” of the string rule we’re now using to the expression rule from the previous section, these rules actually work in very different ways. The string rule can apply to characters anywhere within a string, but what it inserts is always of fixed size. The expression rule deals with trees, and only applies to “whole subtrees”, but what it inserts can be a tree of any size. (One can align these setups by thinking of strings as expressions in which characters are “bound together” by an associative operator, as in A·B·A·A. But if one explicitly gives associativity axioms these will lead to additional pieces in the token-event graph.) A rule like also has the feature of involving patterns. In principle we could include patterns in strings too—both for single characters (as with _) and for sequences of characters (as with __)—but we won’t do this here. (We can also consider one-way rules, using → instead of .) To get a general sense of the kinds of things that happen in accumulative (string) systems, we can consider enumerating all possible distinct two-way string transformation rules. With only a single character A, there are only two distinct cases because systematically generates all possible rules and at t steps gives a total number of rules equal to: With characters A and B the distinct token-event graphs generated starting from rules with a total of at most 5 characters are: Note that when the strings in the initial rule are the same length, only a rather trivial finite token-event graph is ever generated, as in the case of : But when the strings are of different lengths, there is always unbounded growth.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.10 The Case of Hypergraphs : We’ve looked at accumulative versions of expression and string rewriting systems. So what about accumulative versions of hypergraph rewriting systems of the kind that appear in our Physics Project? Consider the very simple hypergraph rule: or pictorially: (Note that the nodes that are named 1 here are really like pattern variables, that could be named for example x_.) We can now do accumulative evolution with this rule, at each step combining results that involve equivalent (i.e. isomorphic) hypergraphs: After two steps this gives: And after 3 steps: How does all this compare to “ordinary” evolution by hypergraph rewriting? Here’s a multiway graph based on applying the same underlying rule repeatedly, starting from an initial condition formed from the rule: What we see is that the accumulative evolution in effect “shortcuts” the ordinary multiway evolution, essentially by “caching” the result of every piece of every transformation between states (which in this case are rules), and delivering a given state in fewer steps. In our typical investigation of hypergraph rewriting for our Physics Project we consider one-way transformation rules. Inevitably, though, the ruliad contains rules that go both ways. And here, in an effort to understand the correspondence with our metamodel of mathematics, we can consider two-way hypergraph rewriting rules. An example is the tw0-way version of the rule above: Now the token-event graph becomes or after 2 steps (where now the transformations from “later states” to “earlier states” have started to fill in): Just like in ordinary hypergraph evolution, the only way to get hypergraphs with additional hyperedges is to start with a rule that involves the addition of new hyperedges—and the same is true for the addition of new elements. Consider the rule: After 1 step this gives: while after 2 steps it gives: The general appearance of this token-event graph is not much different from what we saw with string rewrite or expression rewrite systems. So what this suggests is that it doesn’t matter much whether we’re starting from our metamodel of axiomatic mathematics or from any other reasonably rich rewriting system: we’ll always get the same kind of “large-scale” token-event graph structure. And this is an example of what we’ll use to argue for general laws of metamathematics.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.11 Proofs in Accumulative Systems : In an earlier section, we discussed how paths in a multiway graph can represent proofs of “equivalence” between expressions (or the “entailment” of one expression by another). For example, with the rule (or “axiom”) this shows a path that “proves” that “BA entails AAB”: But once we know this, we can imagine adding this result (as what we can think of as a “lemma”) to our original rule: And now (the “theorem”) “BA entails AAB” takes just one step to prove—and all sorts of other proofs are also shortened: It’s perfectly possible to imagine evolving a multiway system with a kind of “caching-based” speed-up mechanism where every new entailment discovered is added to the list of underlying rules. And, by the way, it’s also possible to use two-way rules throughout the multiway system: But accumulative systems provide a much more principled way to progressively “add what’s discovered”. So what do proofs look like in such systems? Consider the rule: Running it for 2 steps we get the token-event graph: Now let’s say we want to prove that the original “axiom” implies (or “entails”) the “theorem” . Here’s the subgraph that demonstrates the result: And here it is as a separate “proof graph” where each event takes two inputs—the “rule to be applied” and the “rule to apply to”—and the output is the derived (i.e. entailed or implied) new rule or rules. If we run the accumulative system for another step, we get: Now there are additional “theorems” that have been generated. An example is: And now we can find a proof of this theorem: This proof exists as a subgraph of the token-event graph: The proof just given has the fewest events—or “proof steps”—that can be used. But altogether there are 50 possible proofs, other examples being: These correspond to the subgraphs: How much has the accumulative character of these token-event graphs contributed to the structure of these proofs? It’s perfectly possible to find proofs that never use “intermediate lemmas” but always “go back to the original axiom” at every step. In this case examples are which all in effect require at least one more “sequential event” than our shortest proof using intermediate lemmas. A slightly more dramatic example occurs for the theorem: where now without intermediate lemmas the shortest proof is: but with intermediate lemmas it becomes: What we’ve done so far here is to generate a complete token-event graph for a certain number of steps, and then to see if we can find a proof in it for some particular statement. The proof is a subgraph of the “relevant part” of the full token-event graph. Often—in analogy to the simpler case of finding proofs of equivalences between expressions in a multiway graph—we’ll call this subgraph a “proof path”. But in addition to just “finding a proof” in a fully constructed token-event graph, we can ask whether, given a statement, we can directly construct a proof for it. As discussed in the context of proofs in ordinary multiway graphs, computational irreducibility implies that in general there’s no “shortcut” way to find a proof. In addition, for any statement, there may be no upper bound on the length of proof that will be required (or on the size or number of intermediate “lemmas” that will have to be used). And this, again, is the shadow of undecidability in our systems: that there can be statements whose provability may be arbitrarily difficult to determine.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.12 Beyond Substitution: Cosubstitution and Bisubstitution : In making our “metamodel” of mathematics we’ve been discussing the rewriting of expressions according to rules. But there’s a subtle issue that we’ve so far avoided, that has to do with the fact that the expressions we’re rewriting are often themselves patterns that stand for whole classes of expressions. And this turns out to allow for additional kinds of transformations that we’ll call cosubstitution and bisubstitution. In general, the point is that ordinary substitution specializes patterns that appear in rules—while what one can think of as the “dual operation” of cosubstitution specializes patterns that appear in the expressions to which the rules are being applied. If one thinks of the rule that’s being applied as like an operator, and the expression to which the rule is being applied as an operand, then in effect substitution is about making the operator fit the operand, and cosubstitution is about making the operand fit the operator. It’s important to realize that as soon as one’s operating on expressions involving patterns, cosubstitution is not something “optional”: it’s something that one has to include if one is really going to interpret patterns—wherever they occur—as standing for classes of expressions. When one’s operating on a literal expression (without patterns) only substitution is ever possible, as in corresponding to this fragment of a token-event graph: There’s an additional wrinkle when the same pattern (such as ) appears multiple times: In all cases, x_ is matched to a. But which of the x_’s is actually replaced is different in each case. Here’s a slightly more complicated example: In ordinary substitution, replacements for patterns are in effect always made “locally”, with each specific pattern separately being replaced by some expression. But in cosubstitution, a “special case” found for a pattern will get used throughout when the replacement is done. Let’s see how this all works in an accumulative axiomatic system. Consider the very simple rule: One step of substitution gives the token-event graph (where we’ve canonicalized the names of pattern variables to a_ and b_): But one step of cosubstitution gives instead: Here are the individual transformations that were made (with the rule at least nominally being applied only in one direction): The token-event graph above is then obtained by canonicalizing variables, and combining identical expressions (though for clarity we don’t merge rules of the form and ). If we go another step with this particular rule using only substitution, there are additional events (i.e. transformations) but no new theorems produced: Cosubstitution, however, produces another 27 theorems: or altogether: or as trees: We’ve now seen examples of both substitution and cosubstitution in action. But in our metamodel for mathematics we’re ultimately dealing not with each of these individually, but rather with the “symmetric” concept of bisubstitution, in which both substitution and cosubstitution can be mixed together, and applied even to parts of the same expression. In the particular case of , bisubstitution adds nothing beyond cosubstitution. But often it does. Consider the rule: Here’s the result of applying this to three different expressions using substitution, cosubstitution and bisubstitution (where we consider only matches for “whole ∘ expressions”, not subparts): Cosubstitution very often yields substantially more transformations than substitution—bisubstitution then yielding modestly more than cosubstitution. For example, for the axiom system the number of theorems derived after 1 and 2 steps is given by: In some cases there are theorems that can be produced by full bisubstitution, but not—even after any number of steps—by substitution or cosubstitution alone. However, it is also common to find that theorems can in principle be produced by substitution alone, but that this just takes more steps (and sometimes vastly more) than when full bisubstitution is used. (It’s worth noting, however, that the notion of “how many steps” it takes to “reach” a given theorem depends on the foliation one chooses to use in the token-event graph.) The various forms of substitution that we’ve discussed here represent different ways in which one theorem can entail others. But our overall metamodel of mathematics—based as it is purely on the structure of symbolic expressions and patterns—implies that bisubstitution covers all entailments that are possible. In the history of metamathematics and mathematical logic, a whole variety of “laws of inference” or “methods of entailment” have been considered. But with the modern view of symbolic expressions and patterns (as used, for example, in the Wolfram Language), bisubstitution emerges as the fundamental form of entailment, with other forms of entailment corresponding to the use of particular types of expressions or the addition of further elements to the pure substitutions we’ve used here. It should be noted, however, that when it comes to the ruliad different kinds of entailments correspond merely to different foliations—with the form of entailment that we’re using representing just a particularly straightforward case. The concept of bisubstitution has arisen in the theory of term rewriting, as well as in automated theorem proving (where it is often viewed as a particular “strategy”, and called “paramodulation”). In term rewriting, bisubstitution is closely related to the concept of unification—which essentially asks what assignment of values to pattern variables is needed in order to make different subterms of an expression be identical.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.13 Some First Metamathematical Phenomenology : Now that we’ve finished describing the many technical issues involved in constructing our metamodel of mathematics, we can start looking at its consequences. We discussed above how multiway graphs formed from expressions can be used to define a branchial graph that represents a kind of “metamathematical space”. We can now use a similar approach to set up a metamathematical space for our full metamodel of the “progressive accumulation” of mathematical statements. Let’s start by ignoring cosubstitution and bisubstitution and considering only the process of substitution—and beginning with the axiom: Doing accumulative evolution from this axiom we get the token-event graph or after 2 steps: From this we can derive an “effective multiway graph” by directly connecting all input and output tokens involved in each event: And then we can produce a branchial graph, which in effect yields an approximation to the “metamathematical space” generated by our axiom: Showing the statements produced in the form of trees we get (with the top node representing ⟷): If we do the same thing with full bisubstitution, then even after one step we get a slightly larger token-event graph: After two steps, we get: which contains 46 statements, compared to 42 if only substitution is used. The corresponding branchial graph is: The adjacency matrices for the substitution and bisubstitution cases are then which have 80% and 85% respectively of the number of edges in complete graphs of these sizes. Branchial graphs are usually quite dense, but they nevertheless do show definite structure. Here are some results after 2 steps.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.14 Relations to Automated Theorem Proving : We’ve discussed at some length what happens if we start from axioms and then build up an “entailment cone” of all statements that can be derived from them. But in the actual practice of mathematics people often want to just look at particular target statements, and see if they can be derived (i.e. proved) from the axioms. But what can we say “in bulk” about this process? The best source of potential examples we have right now come from the practice of automated theorem proving—as for example implemented in the Wolfram Language function FindEquationalProof. As a simple example of how this works, consider the axiom and the theorem: Automated theorem proving (based on FindEquationalProof) finds the following proof of this theorem: Needless to say, this isn’t the only possible proof. And in this very simple case, we can construct the full entailment cone—and determine that there aren’t any shorter proofs, though there are two more of the same length: All three of these proofs can be seen as paths in the entailment cone: How “complicated” are these proofs? In addition to their lengths, we can for example ask how big the successive intermediate expressions they involve become, where here we are including not only the proofs already shown, but also some longer ones as well: In the setup we’re using here, we can find a proof of by starting with lhs, building up an entailment cone, and seeing whether there’s any path in it that reaches rhs. In general there’s no upper bound on how far one will have to go to find such a path—or how big the intermediate expressions may need to get. One can imagine all kinds of optimizations, for example where one looks at multistep consequences of the original axioms, and treats these as “lemmas” that we can “add as axioms” to provide new rules that jump multiple steps on a path at a time. Needless to say, there are lots of tradeoffs in doing this. (Is it worth the memory to store the lemmas? Might we “jump” past our target? etc.) But typical actual automated theorem provers tend to work in a way that is much closer to our accumulative rewriting systems—in which the “raw material” on which one operates is statements rather than expressions. Once again, we can in principle always construct a whole entailment cone, and then look to see whether a particular statement occurs there. But then to give a proof of that statement it’s sufficient to find the subgraph of the entailment cone that leads to that statement. For example, starting with the axiom we get the entailment cone (shown here as a token-event graph, and dropping _’s): After 2 steps the statement: shows up in this entailment cone: where we’re indicating the subgraph that leads from the original axiom to this statement. Extracting this subgraph we get: which we can view as a proof of the statement within this axiom system. But now let’s use traditional automated theorem proving (in the form of FindEquationalProof) to get a proof of this same statement. Here’s what we get: This is again a token-event graph, but its structure is slightly different from the one we “fished out of” the entailment cone. Instead of starting from the axiom and “progressively deriving” our statement we start from both the statement and the axiom and then show that together they lead “merely via substitution” to a statement of the form , which we can take as an “obviously derivable tautology”. Sometimes the minimal “direct proof” found from the entailment cone can be considerably simpler than the one found by automated theorem proving. For example, for the statement the minimal direct proof is: while the one found by FindEquationalProof is: But the great advantage of automated theorem proving is that it can “directedly” search for proofs instead of just “fishing them out of” the entailment cone that contains all possible exhaustively generated proofs. To use automated theorem proving you have to “know where you want to go”—and in particular identify the theorem you want to prove. Consider the axiom: and the statement: This statement doesn’t show up in the first few steps of the entailment cone for the axiom, even though millions of other theorems do. But automated theorem proving finds a proof of it—and rearranging the “prove-a-tautology proof” so that we just have to feed in a tautology somewhere in the proof, we get: The model-theoretic methods we’ll discuss a little later allow one effectively to “guess” theorems that might be derivable from a given axiom system. So, for example, for the axiom system here’s a “guess” at a theorem: and here’s a representation of its proof found by automated theorem proving—where now the length of an intermediate “lemma” is indicated by the size of the corresponding node: and in this case the longest intermediate lemma is of size 67 and is: In principle it’s possible to rearrange token-event graphs generated by automated theorem proving to have the same structure as the ones we get directly from the entailment cone—with axioms at the beginning and the theorem being proved at the end. But typical strategies for automated theorem proving don’t naturally produce such graphs. In principle automated theorem proving could work by directly searching for a “path” that leads to the theorem one’s trying to prove. But usually it’s much easier instead to have as the “target” a simple tautology. At least conceptually automated theorem proving must still try to “navigate” through the full token-event graph that makes up the entailment cone. And the main issue in doing this is that there are many places where one does not know “which branch to take”. But here there’s a crucial—if at first surprising—fact: at least so long as one is using full bisubstitution it ultimately doesn’t matter which branch one takes; there’ll always be a way to “merge back” to any other branch. This is a consequence of the fact that the accumulative systems we’re using automatically have the property of confluence which says that every branch is accompanied by a subsequent merge. There’s an almost trivial way in which this is true by virtue of the fact that for every edge the system also includes the reverse of that edge. But there’s a more substantial reason as well: that given any two statements on two different branches, there’s always a way to combine them using a bisubstitution to get a single statement. In our Physics Project, the concept of causal invariance—which effectively generalizes confluence—is an important one, that leads among other things to ideas like relativistic invariance. Later on we’ll discuss the idea that “regardless of what order you prove theorems in, you’ll always get the same math”, and its relationship to causal invariance and to the notion of relativity in metamathematics. But for now the importance of confluence is that it has the potential to simplify automated theorem proving—because in effect it says one can never ultimately “make a wrong turn” in getting to a particular theorem, or, alternatively, that if one keeps going long enough every path one might take will eventually be able to reach every theorem. And indeed this is exactly how things work in the full entailment cone. But the challenge in automated theorem proving is to generate only a tiny part of the entailment cone, yet still “get to” the theorem we want. And in doing this we have to carefully choose which “branches” we should try to merge using bisubstitution events. In automated theorem proving these bisubstitution events are typically called “critical pair lemmas”, and there are a variety of strategies for defining an order in which critical pair lemmas should be tried. It’s worth pointing out that there’s absolutely no guarantee that such procedures will find the shortest proof of any given theorem (or in fact that they’ll find a proof at all with a given amount of computational effort). One can imagine “higher-order proofs” in which one attempts to transform not just statements of the form , but full proofs (say represented as token-event graphs). And one can imagine using such transformations to try to simplify proofs. A general feature of the proofs we’ve been showing is that they are accumulative, in the sense they continually introduce lemmas which are then reused. But in principle any proof can be “unrolled” into one that just repeatedly uses the original axioms (and in fact, purely by substitution)—and never introduces other lemmas. The necessary “cut elimination” can effectively be done by always recreating each lemma from the axioms whenever it’s needed—a process which can become exponentially complex. As an example, from the axiom above we can generate the proof: where for example the first lemma at the top is reused in four events. But now by cut elimination we can “unroll” this whole proof into a “straight-line” sequence of substitutions on expressions done just using the original axiom and we see that our final theorem is the statement that the first expression in the sequence is equivalent under the axiom to the last one. As is fairly evident in this example, a feature of automated theorem proving is that its result tends to be very “non-human”. Yes, it can provide incontrovertible evidence that a theorem is valid. But that evidence is typically far away from being any kind of “narrative” suitable for human consumption. In the analogy to molecular dynamics, an automated proof gives detailed “turn-by-turn instructions” that show how a molecule can reach a certain place in a gas. Typical “human-style” mathematics, on the other hand, operates on a higher level, analogous to talking about overall motion in a fluid. And a core part of what’s achieved by our physicalization of metamathematics is understanding why it’s possible for mathematical observers like us to perceive mathematics as operating at this higher level.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.15 Axiom Systems of Present-Day Mathematics : The axiom systems we’ve been talking about so far were chosen largely for their axiomatic simplicity. But what happens if we consider axiom systems that are used in practice in present-day mathematics? The simplest common example are the axioms (actually, a single axiom) of semigroup theory, stated in our notation as: Using only substitution, all we ever get after any number of steps is the token-event graph (i.e. “entailment cone”): But with bisubstitution, even after one step we already get the entailment cone which contains such theorems as: After 2 steps, the entailment cone becomes: which contains 1617 theorems such as: with sizes distributed as follows: Looking at these theorems we can see that—in fact by construction—they are all just statements of the associativity of ∘. Or, put another way, they state that under this axiom all expression trees that have the same sequence of leaves are equivalent. What about group theory? The standard axioms can be written: where ∘ is interpreted as the binary group multiplication operation, overbar as the unary inverse operation, and 1 as the constant identity element (or, equivalently, zero-argument function). One step of substitution already gives: It’s notable that in this picture one can already see “different kinds of theorems” ending up in different “metamathematical locations”. One also sees some “obvious” tautological “theorems”, like and . If we use full bisubstitution, we get 56 rather than 27 theorems, and many of the theorems are more complicated: After 2 steps of pure substitution, the entailment cone in this case becomes: which includes 792 theorems with sizes distributed according to: But among all these theorems, do straightforward “textbook theorems” appear, like? The answer is no. It’s inevitable that in the end all such theorems must appear in the entailment cone. But it turns out that it takes quite a few steps. And indeed with automated theorem proving we can find “paths” that can be taken to prove these theorems—involving significantly more than two steps: So how about logic, or, more specifically Boolean algebra? A typical textbook axiom system for this (represented in terms of And ∧, Or ∨ and Not ) is: After one step of substitution from these axioms we get: or in our more usual rendering: So what happens here with “named textbook theorems” (excluding commutativity and distributivity, which already appear in the particular axioms we’re using)? Once again none of these appear in the first step of the entailment cone. But at step 2 with full bisubstitution the idempotence laws show up where here we’re only operating on theorems with leaf count below 14 (of which there are a total of 27,953). And if we go to step 3—and use leaf count below 9—we see the law of excluded middle and the law of noncontradiction show up: How are these reached? Here’s the smallest fragment of token-event graph (“shortest path”) within this entailment cone from the axioms to the law of excluded middle: There are actually many possible “paths” (476 in all with our leaf count restriction); the next smallest ones with distinct structures are: Here’s the “path” for this theorem found by automated theorem proving: Most of the other “named theorems” involve longer proofs—and so won’t show up until much later in the entailment cone: The axiom system we’ve used for Boolean algebra here is by no means the only possible one. For example, it’s stated in terms of And, Or and Not—but one doesn’t need all those operators; any Boolean expression (and thus any theorem in Boolean algebra) can also be stated just in terms of the single operator Nand. And in terms of that operator the very simplest axiom system for Boolean algebra contains (as I found in 2000) just one axiom (where here ∘ is now interpreted as Nand): Here’s one step of the substitution entailment cone for this axiom: After 2 steps this gives an entailment cone with 5486 theorems with size distribution: When one’s working with Nand, it’s less clear what one should consider to be “notable theorems”. But an obvious one is the commutativity of Nand: Here’s a proof of this obtained by automated theorem proving (tipped on its side for readability): Eventually it’s inevitable that this theorem must show up in the entailment cone for our axiom system. But based on this proof we would expect it only after something like 102 steps. And with the entailment cone growing exponentially this means that by the time shows up, perhaps other theorems would have done so—though most vastly more complicated. We’ve looked at axioms for group theory and for Boolean algebra. But what about other axiom systems from present-day mathematics? In a sense it’s remarkable how few of these there are—and indeed I was able to list essentially all of them in just two pages in A New Kind of Science: The longest axiom system listed here is a precise version of Euclid’s original axioms where we are listing everything (even logic) in explicit (Wolfram Language) functional form. Given these axioms we should now be able to prove all theorems in Euclidean geometry. As an example (that’s already complicated enough) let’s take Euclid’s very first “proposition” (Book 1, Proposition 1) which states that it’s possible “with a ruler and compass” (i.e. with lines and circles) to construct an equilateral triangle based on any line segment—as in: We can write this theorem by saying that given the axioms together with the “setup”: it’s possible to derive: We can now use automated theorem proving to generate a proof and in this case the proof takes 272 steps. But the fact that it’s possible to generate this proof shows that (up to various issues about the “setup conditions”) the theorem it proves must eventually “occur naturally” in the entailment cone of the original axioms—though along with an absolutely immense number of other theorems that Euclid didn’t “call out” and write down in his books. Looking at the collection of axiom systems from A New Kind of Science (and a few related ones) for many of them we can just directly start generating entailment cones—here shown after one step, using substitution only: But if we’re going to make entailment cones for all axiom systems there are a few other technical wrinkles we have to deal with. The axiom systems shown above are all “straightforwardly equational” in the sense that they in effect state what amount to “algebraic relations” (in the sense of universal algebra) universally valid for all choices of variables. But some axiom systems traditionally used in mathematics also make other kinds of statements. In the traditional formalism and notation of mathematical logic these can look quite complicated and abstruse. But with a metamodel of mathematics like ours it’s possible to untangle things to the point where these different kinds of statements can also be handled in a streamlined way. In standard mathematical notation one might write: which we can read as “for all a and b, equals ”—and which we can interpret in our “metamodel” of mathematics as the (two-way) rule: What this says is just that any time we see an expression that matches the pattern we can replace it by (or in Wolfram Language notation just ), and vice versa, so that in effect can be said to entail . But what if we have axioms that involve not just universal statements (“for all …”) but also existential statements (“there exists…”)? In a sense we’re already dealing with these. Whenever we write —or in explicit functional form, say o[a_, b_]—we’re effectively asserting that there exists some operator o that we can do operations with. It’s important to note that once we introduce o (or ∘) we imagine that it represents the same thing wherever it appears (in contrast to a pattern variable like a_ that can represent different things in different instances). Now consider an “explicit existential statement” like: which we can read as “there exists something a for which equals a”. To represent the “something” we just introduce a “constant”, or equivalently an expression with head, say, α, and zero arguments: α[ ]. Now we can write out existential statement as We can operate on this using rules like , with α[] always “passing through” unchanged—but with its mere presence asserting that “it exists”. A very similar setup works even if we have both universal and existential quantifiers. For example, we can represent: as just: where now there isn’t just a single object, say β[], that we assert exists; instead there are “lots of different β’s”, “parametrized” in this case by a. We can apply our standard accumulative bisubstitution process to this statement—and after one step we get: Note that this is a very different result from the one for the “purely universal” statement: In general, we can “compile” any statement in terms of quantifiers into our metamodel, essentially using the standard technique of Skolemization from mathematical logic. Thus for example can be “compiled into”: while: can be compiled into: If we look at the actual axiom systems used in current mathematics there’s one more issue to deal with—which doesn’t affect the axioms for logic or group theory, but does show up, for example, in the Peano axioms for arithmetic. And the issue is that in addition to quantifying over “variables”, we also need to quantify over “functions”. Or formulated differently, we need to set up not just individual axioms, but a whole “axiom schema” that can generate an infinite sequence of “ordinary axioms”, one for each possible “function”. Using this setup we can then “compile” the standard induction axiom of Peano arithmetic into the (Wolfram Language) metamodel form: where the “implications” in the original axiom have been converted into one-way rules, so that what the axiom can now be seen to do is to define a transformation for something that is not an “ordinary mathematical-style expression” but rather an expression that is itself a rule. But the important point is that our whole setup of doing substitutions in symbolic expressions—like Wolfram Language—makes no fundamental distinction between dealing with “ordinary expressions” and with “rules” (in Wolfram Language, for example, is just Rule[a,b]). And as a result we can expect to be able to construct token-event graphs, build entailment cones, etc. just as well for axiom systems like Peano arithmetic, as for ones like Boolean algebra and group theory. The actual number of nodes that appear even in what might seem like simple cases can be huge, but the whole setup makes it clear that exploring an axiom system like this is just another example—that can be uniformly represented with our metamodel of mathematics—of a form of sampling of the ruliad.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.16 The Model-Theoretic Perspective : We’ve so far considered something like: just as an abstract statement about arbitrary symbolic variables x and y, and some abstract operator ∘. But can we make a “model” of what x, y, and ∘ could “explicitly be”? Let’s imagine for example that x and y can take 2 possible values, say 0 or 1. (We’ll use numbers for notational convenience, though in principle the values could be anything we want.) Now we have to ask what ∘ can be in order to have our original statement always hold. It turns out in this case that there are several possibilities, that can be specified by giving possible “multiplication tables” for ∘: If we allow, say, 3 possible values for x and y, there turn out to be 221 possible forms for ∘. The first few are: As another example, let’s consider the simplest axiom for Boolean algebra (that I discovered in 2000): Here are the “size-2” models for this: and these, as expected, are the truth tables for Nand and Nor respectively. (In this particular case, there are no size-3 models, 12 size-4 models, and in general models of size 2n—and no finite models of any other size.) Looking at this example suggests a way to talk about models for axiom systems. We can think of an axiom system as defining a collection of abstract constraints. But what can we say about objects that might satisfy those constraints? A model is in effect telling us about these objects. Or, put another way, it’s telling what “things” the axiom system “describes”. And in the case of my axiom for Boolean algebra, those “things” would be Boolean variables, operated on using Nand or Nor. As another example, consider the axioms for group theory: Is there a mathematical interpretation of these? Well, yes. They essentially correspond to (representations of) particular finite groups. The original axioms define constraints to be satisfied by any group. These models now correspond to particular groups with specific finite numbers of elements (and in fact specific representations of these groups). And just like in the Boolean algebra case this interpretation now allows us to start saying what the models are “about”. The first three, for example, correspond to cyclic groups which can be thought of as being “about” addition of integers mod k. For axiom systems that haven’t traditionally been studied in mathematics, there typically won’t be any such preexisting identification of what they’re “about”. But we can still think of models as being a way that a mathematical observer can characterize—or summarize—an axiom system. And in a sense we can see the collection of possible finite models for an axiom system as being a kind of “model signature” for the axiom system. But let’s now consider what models tell us about “theorems” associated with a given axiom system. Take for example the axiom: Here are the size-2 models for this axiom system: Let’s now pick the last of these models. Then we can take any symbolic expression involving ∘, and say what its values would be for every possible choice of the values of the variables that appear in it: The last row here gives an “expression code” that summarizes the values of each expression in this particular model. And if two expressions have different codes in the model then this tells us that these expressions cannot be equivalent according to the underlying axiom system. But if the codes are the same, then it’s at least possible that the expressions are equivalent in the underlying axiom system. So as an example, let’s take the equivalences associated with pairs of expressions that have code 3 (according to the model we’re using): So now let’s compare with an actual entailment cone for our underlying axiom system (where to keep the graph of modest size we have dropped expressions involving more than 3 variables): So far this doesn’t establish equivalence between any of our code-3 expressions. But if we generate a larger entailment cone (here using a different initial expression) we get where the path shown corresponds to the statement: demonstrating that this is an equivalence that holds in general for the axiom system. But let’s take another statement implied by the model, such as: Yes, it’s valid in the model. But it’s not something that’s generally valid for the underlying axiom system, or could ever be derived from it. And we can see this for example by picking another model for the axiom system, say the second-to-last one in our list above and finding out that the values for the two expressions here are different in that model: The definitive way to establish that a particular statement follows from a particular axiom system is to find an explicit proof for it, either directly by picking it out as a path in the entailment cone or by using automated theorem proving methods. But models in a sense give one a way to “get an approximate result”. As an example of how this works, consider a collection of possible expressions, with pairs of them joined whenever they can be proved equal in the axiom system we’re discussing: Now let’s indicate what codes two models of the axiom system assign to the expressions: The expressions within each connected graph component are equal according to the underlying axiom system, and in both models they are always assigned the same codes. But sometimes the models “overshoot”, assigning the same codes to expressions not in the same connected component—and therefore not equal according to the underlying axiom system. The models we’ve shown so far are ones that are valid for the underlying axiom system. If we use a model that isn’t valid we’ll find that even expressions in the same connected component of the graph (and therefore equal according to the underlying axiom system) will be assigned different codes (note the graphs have been rearranged to allow expressions with the same code to be drawn in the same “patch”): We can think of our graph of equivalences between expressions as corresponding to a slice through an entailment graph—and essentially being “laid out in metamathematical space”, like a branchial graph, or what we’ll later call an “entailment fabric”. And what we see is that when we have a valid model different codes yield different patches that in effect cover metamathematical space in a way that respects the equivalences implied by the underlying axiom system. But now let’s see what happens if we make an entailment cone, tagging each node with the code corresponding to the expression it represents, first for a valid model, and then for non-valid ones: With the valid model, the whole entailment cone is tagged with the same code (and here also same color). But for the non-valid models, different “patches” in the entailment cone are tagged with different codes. Let’s say we’re trying to see if two expressions are equal according to the underlying axiom system. The definitive way to tell this is to find a “proof path” from one expression to the other. But as an “approximation” we can just “evaluate” these two expressions according to a model, and see if the resulting codes are the same. Even if it’s a valid model, though, this can only definitively tell us that two expressions aren’t equal; it can’t confirm that they are. In principle we can refine things by checking in multiple models—particularly ones with more elements. But without essentially pre-checking all possible equalities we can’t in general be sure that this will give us the complete story. Of course, generating explicit proofs from the underlying axiom system can also be hard—because in general the proof can be arbitrarily long. And in a sense there is a tradeoff. Given a particular equivalence to check we can either search for a path in the entailment graph, often effectively having to try many possibilities. Or we can “do the work up front” by finding a model or collection of models that we know will correctly tell us whether the equivalence is correct. Later we’ll see how these choices relate to how mathematical observers can “parse” the structure of metamathematical space. In effect observers can either explicitly try to trace out “proof paths” formed from sequences of abstract symbolic expressions—or they can “globally predetermine” what expressions “mean” by identifying some overall model. In general there may be many possible choices of models—and what we’ll see is that these different choices are essentially analogous to different choices of reference frames in physics. One feature of our discussion of models so far is that we’ve always been talking about making models for axioms, and then applying these models to expressions. But in the accumulative systems we’ve discussed above (and that seem like closer metamodels of actual mathematics), we’re only ever talking about “statements”—with “axioms” just being statements we happen to start with. So how do models work in such a context? Here’s the beginning of the token-event graph starting with: produced using one step of entailment by substitution: For each of the statements given here, there are certain size-2 models (indicated here by their multiplication tables) that are valid—or in some cases all models are valid: We can summarize this by indicating in a 4×4 grid which of the 16 possible size-2 models are consistent with each statement generated so far in the entailment cone: Continuing one more step we get: It’s often the case that statements generated on successive steps in the entailment cone in essence just “accumulate more models”. But—as we can see from the right-hand edge of this graph—it’s not always the case—and sometimes a model valid for one statement is no longer valid for a statement it entails. (And the same is true if we use full bisubstitution rather than just substitution.) Everything we’ve discussed about models so far here has to do with expressions. But there can also be models for other kinds of structures. For strings it’s possible to use something like the same setup, though it doesn’t work quite so well. One can think of transforming the string and then trying to find appropriate “multiplication tables” for ∘, but here operating on the specific elements A and B, not on a collection of elements defined by the model. Defining models for a hypergraph rewriting system is more challenging, if interesting. One can think of the expressions we’ve used as corresponding to trees—which can be “evaluated” as soon as definite “operators” associated with the model are filled in at each node. If we try to do the same thing with graphs (or hypergraphs) we’ll immediately be thrust into issues of the order in which we scan the graph. At a more general level, we can think of a “model” as being a way that an observer tries to summarize things. And we can imagine many ways to do this, with differing degrees of fidelity, but always with the feature that if the summaries of two things are different, then those two things can’t be transformed into each other by whatever underlying process is being used. Put another way, a model defines some kind of invariant for the underlying transformations in a system. The raw material for computing this invariant may be operators at nodes, or may be things like overall graph properties (like cycle counts).
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.17 Axiom Systems in the Wild : We’ve talked about what happens with specific, sample axiom systems, as well as with various axiom systems that have arisen in present-day mathematics. But what about “axiom systems in the wild”—say just obtained by random sampling, or by systematic enumeration? In effect, each possible axiom system can be thought of as “defining a possible field of mathematics”—just in most cases not one that’s actually been studied in the history of human mathematics. But the ruliad certainly contains all such axiom systems. And in the style of A New Kind of Science we can do ruliology to explore them. As an example, let’s look at axiom systems with just one axiom, one binary operator and one or two variables. Here are the smallest few: For each of these axiom systems, we can then ask what theorems they imply. And for example we can enumerate theorems—just as we have enumerated axiom systems—then use automated theorem proving to determine which theorems are implied by which axiom systems. This shows the result, with possible axiom systems going down the page, possible theorems going across, and a particular square being filled in (darker for longer proofs) if a given theorem can be proved from a given axiom system: The diagonal on the left is axioms “proving themselves”. The lines across are for axiom systems like that basically say that any two expressions are equal—so that any theorem that is stated can be proved from the axiom system. But what if we look at the whole entailment cone for each of these axiom systems? Here are a few examples of the first two steps: With our method of accumulative evolution the axiom doesn’t on its own generate a growing entailment cone (though if combined with any axiom containing ∘ it does, and so does on its own). But in all the other cases shown the entailment cone grows rapidly (typically at least exponentially)—in effect quickly establishing many theorems. Most of those theorems, however, are “not small”—and for example after 2 steps here are the distributions of their sizes: So let’s say we generate only one step in the entailment cone. This is the pattern of “small theorems” we establish: And here is the corresponding result after two steps: Superimposing this on our original array of theorems we get: In other words, there are many small theorems that we can establish “if we look for them”, but which won’t “naturally be generated” quickly in the entailment cone (though eventually it’s inevitable that they will be generated). (Later we’ll see how this relates to the concept of “entailment fabrics” and the “knitting together of pieces of mathematics”.) In the previous section we discussed the concept of models for axiom systems. So what models do typical “axiom systems from the wild” have? The number of possible models of a given size varies greatly for different axiom systems: But for each model we can ask what theorems it implies are valid. And for example combining all models of size 2 yields the following “predictions” for what theorems are valid (with the actual theorems indicated by dots): Using instead models of size 3 gives “more accurate predictions”: As expected, looking at a fixed number of steps in the entailment cone “underestimates” the number of valid theorems, while looking at finite models overestimates it. So how does our analysis for “axiom systems from the wild” compare with what we’d get if we considered axiom systems that have been explicitly studied in traditional human mathematics? Here are some examples of “known” axiom systems that involve just a single binary operator and here’s the distribution of theorems they give: As must be the case, all the axiom systems for Boolean algebra yield the same theorems. But axiom systems for “different mathematical theories” yield different collections of theorems. What happens if we look at entailments from these axiom systems? Eventually all theorems must show up somewhere in the entailment cone of a given axiom system. But here are the results after one step of entailment: Some theorems have already been generated, but many have not: Just as we did above, we can try to “predict” theorems by constructing models. Here’s what happens if we ask what theorems hold for all valid models of size 2: For several of the axiom systems, the models “perfectly predict” at least the theorems we show here. And for Boolean algebra, for example, this isn’t surprising: the models just correspond to identifying ∘ as Nand or Nor, and to say this gives a complete description of Boolean algebra. But in the case of groups, “size-2 models” just capture particular groups that happen to be of size 2, and for these particular groups there are special, extra theorems that aren’t true for groups in general. If we look at models specifically of size 3 there aren’t any examples for Boolean algebra so we don’t predict any theorems. But for group theory, for example, we start to get a slightly more accurate picture of what theorems hold in general: Based on what we’ve seen here, is there something “obviously special” about the axiom systems that have traditionally been used in human mathematics? There are cases like Boolean algebra where the axioms in effect constrain things so much that we can reasonably say that they’re “talking about definite things” (like Nand and Nor). But there are plenty of other cases, like group theory, where the axioms provide much weaker constraints, and for example allow an infinite number of possible specific groups. But both situations occur among axiom systems “from the wild”. And in the end what we’re doing here doesn’t seem to reveal anything “obviously special” (say in the statistics of models or theorems) about “human” axiom systems. And what this means is that we can expect that conclusions we draw from looking at the “general case of all axiom systems”—as captured in general by the ruliad—can be expected to hold in particular for the specific axiom systems and mathematical theories that human mathematics has studied.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.18 The Topology of Proof Space : In the typical practice of pure mathematics the main objective is to establish theorems. Yes, one wants to know that a theorem has a proof (and perhaps the proof will be helpful in understanding the theorem), but the main focus is on theorems and not on proofs. In our effort to “go underneath” mathematics, however, we want to study not only what theorems there are, but also the process by which the theorems are reached. We can view it as an important simplifying assumption of typical mathematical observers that all that matters is theorems—and that different proofs aren’t relevant. But to explore the underlying structure of metamathematics, we need to unpack this—and in effect look directly at the structure of proof space. Let’s consider a simple system based on strings. Say we have the rewrite rule and we want to establish the theorem . To do this we have to find some path from A to ABA in the multiway system (or, effectively, in the entailment cone for this axiom system): But this isn’t the only possible path, and thus the only possible proof. In this particular case, there are 20 distinct paths, each corresponding to at least a slightly different proof: But one feature here is that all these different proofs can in a sense be “smoothly deformed” into each other, in this case by progressively changing just one step at a time. So this means that in effect there is no nontrivial topology to proof space in this case—and “distinctly inequivalent” collections of proofs: But consider instead the rule . With this “axiom system” there are 15 possible proofs for the theorem : Pulling out just the proofs we get: And we see that in a sense there’s a “hole” in proof space here—so that there are two distinctly different kinds of proofs that can be done. One place it’s common to see a similar phenomenon is in games and puzzles. Consider for example the Towers of Hanoi puzzle. We can set up a multiway system for the possible moves that can be made. Starting from all disks on the left peg, we get after 1 step: After 2 steps we have: And after 8 steps (in this case) we have the whole “game graph”: The corresponding result for 4 disks is: And in each case we see the phenomenon of nontrivial topology. What fundamentally causes this? In a sense it reflects the possibility for distinctly different strategies that lead to the same result. Here, for example, different sides of the “main loop” correspond to the “foundational choice” of whether to move the biggest disk first to the left or to the right. And the same basic thing happens with 4 disks on 4 pegs, though the overall structure is more complicated there: If two paths diverge in a multiway system it could be that it will never be possible for them to merge again. But whenever the system has the property of confluence, it’s guaranteed that eventually the paths will merge. And, as it turns out, our accumulative evolution setup guarantees that (at least ignoring generation of new variables) confluence will always be achieved. But the issue is how quickly. If branches always merge after just one step, then in a sense there’ll always be topologically trivial proof space. But if the merging can take awhile (and in a continuum limit, arbitrarily long) then there’ll in effect be nontrivial topology. And one consequence of the nontrivial topology we’re discussing here is that it leads to disconnection in branchial space. Here are the branchial graphs for the first 3 steps in our original 3-disk 3-peg case: For the first two steps, the branchial graphs stay connected; but on the third step there’s disconnection. For the 4-disk 4-peg case the sequence of branchial graphs begins: At the beginning (and also the end) there’s a single component, that we might think of as a coherent region of metamathematical space. But in the middle it breaks into multiple disconnected components—in effect reflecting the emergence of multiple distinct regions of metamathematical space with something like event horizons temporarily existing between them. How should we interpret this? First and foremost, it’s something that reveals that there’s structure “below” the “fluid dynamics” level of mathematics; it’s something that depends on the discrete “axiomatic infrastructure” of metamathematics. And from the point of view of our Physics Project, we can think of it as a kind of metamathematical analog of a “quantum effect”. In our Physics Project we imagine different paths in the multiway system to correspond to different possible quantum histories. The observer is in effect spread over multiple paths, which they coarse grain or conflate together. An “observable quantum effect” occurs when there are paths that can be followed by the system, but that are somehow “too far apart” to be immediately coarse-grained together by the observer. Put another way, there is “noticeable quantum interference” when the different paths corresponding to different histories that are “simultaneously happening” are “far enough apart” to be distinguished by the observer. “Destructive interference” is presumably associated with paths that are so far apart that to conflate them would effectively require conflating essentially every possible path. (And our later discussion of the relationship between falsity and the “principle of explosion” then suggests a connection between destructive interference in physics and falsity in mathematics.) In essence what determines the extent of “quantum effects” is then our “size” as observers in branchial space relative to the size of features in branchial space such as the “topological holes” we’ve been discussing. In the metamathematical case, the “size” of us as observers is in effect related to our ability (or choice) to distinguish slight differences in axiomatic formulations of things. And what we’re saying here is that when there is nontrivial topology in proof space, there is an intrinsic dynamics in metamathematical entailment that leads to the development of distinctions at some scale—though whether these become “visible” to us as mathematical observers depends on how “strong a metamathematical microscope” we choose to use relative to the scale of the “topological holes”.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.19 Time, Timelessness and Entailment Fabrics : A fundamental feature of our metamodel of mathematics is the idea that a given set of mathematical statements can entail others. But in this picture what does “mathematical progress” look like? In analogy with physics one might imagine it would be like the evolution of the universe through time. One would start from some limited set of axioms and then—in a kind of “mathematical Big Bang”—these would lead to a progressively larger entailment cone containing more and more statements of mathematics. And in analogy with physics, one could imagine that the process of following chains of successive entailments in the entailment cone would correspond to the passage of time. But realistically this isn’t how most of the actual history of human mathematics has proceeded. Because people—and even their computers—basically never try to extend mathematics by axiomatically deriving all possible valid mathematical statements. Instead, they come up with particular mathematical statements that for one reason or another they think are valid and interesting, then try to prove these. Sometimes the proof may be difficult, and may involve a long chain of entailments. Occasionally—especially if automated theorem proving is used—the entailments may approximate a geodesic path all the way from the axioms. But the practical experience of human mathematics tends to be much more about identifying “nearby statements” and then trying to “fit them together” to deduce the statement one’s interested in. And in general human mathematics seems to progress not so much through the progressive “time evolution” of an entailment graph as through the assembly of what one might call an “entailment fabric” in which different statements are being knitted together by entailments. In physics, the analog of the entailment graph is basically the causal graph which builds up over time to define the content of a light cone (or, more accurately, an entanglement cone). The analog of the entailment fabric is basically the (more-or-less) instantaneous state of space (or, more accurately, branchial space). In our Physics Project we typically take our lowest-level structure to be a hypergraph—and informally we often say that this hypergraph “represents the structure of space”. But really we should be deducing the “structure of space” by taking a particular time slice from the “dynamic evolution” represented by the causal graph—and for example we should think of two “atoms of space” as “being connected” in the “instantaneous state of space” if there’s a causal connection between them defined within the slice of the causal graph that occurs within the time slice we’re considering. In other words, the “structure of space” is knitted together by the causal connections represented by the causal graph. (In traditional physics, we might say that space can be “mapped out” by looking at overlaps between lots of little light cones.) Let’s look at how this works out in our metamathematical setting, using string rewrites to simplify things. If we start from the axiom this is the beginning of the entailment cone it generates: But instead of starting with one axiom and building up a progressively larger entailment cone, let’s start with multiple statements, and from each one generate a small entailment cone, say applying each rule at most twice. Here are entailment cones started from several different statements: But the crucial point is that these entailment cones overlap—so we can knit them together into an “entailment fabric”: Or with more pieces and another step of entailment: And in a sense this is a “timeless” way to imagine building up mathematics—and metamathematical space. Yes, this structure can in principle be viewed as part of the branchial graph obtained from a slice of an entailment graph (and technically this will be a useful way to think about it). But a different view—closer to the practice of human mathematics—is that it’s a “fabric” formed by fitting together many different mathematical statements. It’s not something where one’s tracking the overall passage of time, and seeing causal connections between things—as one might in “running a program”. Rather, it’s something where one’s fitting pieces together in order to satisfy constraints—as one might in creating a tiling. Underneath everything is the ruliad. And entailment cones and entailment fabrics can be thought of just as different samplings or slicings of the ruliad. The ruliad is ultimately the entangled limit of all possible computations. But one can think of it as being built up by starting from all possible rules and initial conditions, then running them for an infinite number of steps. An entailment cone is essentially a “slice” of this structure where one’s looking at the “time evolution” from a particular rule and initial condition. An entailment fabric is an “orthogonal” slice, looking “at a particular time” across different rules and initial conditions. (And, by the way, rules and initial conditions are essentially equivalent, particularly in an accumulative system.) One can think of these different slices of the ruliad as being what different kinds of observers will perceive within the ruliad. Entailment cones are essentially what observers who persist through time but are localized in rulial space will perceive. Entailment fabrics are what observers who ignore time but explore more of rulial space will perceive. Elsewhere I’ve argued that a crucial part of what makes us perceive the laws of physics we do is that we are observers who consider ourselves to be persistent through time. But now we’re seeing that in the way human mathematics is typically done, the “mathematical observer” will be of a different character. And whereas for a physical observer what’s crucial is causality through time, for a mathematical observer (at least one who’s doing mathematics the way it’s usually done) what seems to be crucial is some kind of consistency or coherence across metamathematical space. In physics it’s far from obvious that a persistent observer would be possible. It could be that with all those detailed computationally irreducible processes happening down at the level of atoms of space there might be nothing in the universe that one could consider consistent through time. But the point is that there are certain “coarse-grained” attributes of the behavior that are consistent through time. And it is by concentrating on these that we end up describing things in terms of the laws of physics we know. There’s something very analogous going on in mathematics. The detailed branchial structure of metamathematical space is complicated, and presumably full of computational irreducibility. But once again there are “coarse-grained” attributes that have a certain consistency and coherence across it. And it is on these that we concentrate as human “mathematical observers”. And it is in terms of these that we end up being able to do “human-level mathematics”—in effect operating at a “fluid dynamics” level rather than a “molecular dynamics” one. The possibility of “doing physics in the ruliad” depends crucially on the fact that as physical observers we assume that we have certain persistence and coherence through time. The possibility of “doing mathematics (the way it’s usually done) in the ruliad” depends crucially on the fact that as “mathematical observers” we assume that the mathematical statements we consider will have a certain coherence and consistency—or, in effect, that it’s possible for us to maintain and grow a coherent body of mathematical knowledge, even as we try to include all sorts of new mathematical statements.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.20 The Notion of Truth : Logic was originally conceived as a way to characterize human arguments—in which the concept of “truth” has always seemed quite central. And when logic was applied to the foundations of mathematics, “truth” was also usually assumed to be quite central. But the way we’ve modeled mathematics here has been much more about what statements can be derived (or entailed) than about any kind of abstract notion of what statements can be “tagged as true”. In other words, we’ve been more concerned with “structurally deriving” that “” than in saying that “1 + 1 = 2 is true”. But what is the relation between this kind of “constructive derivation” and the logical notion of truth? We might just say that “if we can construct a statement then we should consider it true”. And if we’re starting from axioms, then in a sense we’ll never have an “absolute notion of truth”—because whatever we derive is only “as true as the axioms we started from”. One issue that can come up is that our axioms might be inconsistent—in the sense that from them we can derive two obviously inconsistent statements. But to get further in discussing things like this we really need not only to have a notion of truth, but also a notion of falsity. In traditional logic it has tended to be assumed that truth and falsity are very much “the same kind of thing”—like 1 and 0. But one feature of our view of mathematics here is that actually truth and falsity seem to have a rather different character. And perhaps this is not surprising—because in a sense if there’s one true statement about something there are typically an infinite number of false statements about it. So, for example, the single statement is true, but the infinite collection of statements for any other are all false. There is another aspect to this, discussed since at least the Middle Ages, often under the name of the “principle of explosion”: that as soon as one assumes any statement that is false, one can logically derive absolutely any statement at all. In other words, introducing a single “false axiom” will start an explosion that will eventually “blow up everything”. So within our model of mathematics we might say that things are “true” if they can be derived, and are “false” if they lead to an “explosion”. But let’s say we’re given some statement. How can we tell if it’s true or false? One thing we can do to find out if it’s true is to construct an entailment cone from our axioms and see if the statement appears anywhere in it. Of course, given computational irreducibility there’s in general no upper bound on how far we’ll need to go to determine this. But now to find out if a statement is false we can imagine introducing the statement as an additional axiom, and then seeing if the entailment cone that’s now produced contains an explosion—though once again there’ll in general be no upper bound on how far we’ll have to go to guarantee that we have a “genuine explosion” on our hands. So is there any alternative procedure? Potentially the answer is yes: we can just try to see if our statement is somehow equivalent to “true” or “false”. But in our model of mathematics where we’re just talking about transformations on symbolic expressions, there’s no immediate built-in notion of “true” and “false”. To talk about these we have to add something. And for example what we can do is to say that “true” is equivalent to what seems like an “obvious tautology” such as , or in our computational notation, , while “false” is equivalent to something “obviously explosive”, like (or in our particular setup something more like ). But even though something like “Can we find a way to reach from a given statement?” seems like a much more practical question for an actual theorem-proving system than “Can we fish our statement out of a whole entailment cone?”, it runs into many of the same issues—in particular that there’s no upper limit on the length of path that might be needed. Soon we’ll return to the question of how all this relates to our interpretation of mathematics as a slice of the ruliad—and to the concept of the entailment fabric perceived by a mathematical observer. But to further set the context for what we’re doing let’s explore how what we’ve discussed so far relates to things like Gödel’s theorem, and to phenomena like incompleteness. From the setup of basic logic we might assume that we could consider any statement to be either true or false. Or, more precisely, we might think that given a particular axiom system, we should be able to determine whether any statement that can be syntactically constructed with the primitives of that axiom system is true or false. We could explore this by asking whether every statement is either derivable or leads to an explosion—or can be proved equivalent to an “obvious tautology” or to an “obvious explosion”. But as a simple “approximation” to this, let’s consider a string rewriting system in which we define a “local negation operation”. In particular, let’s assume that given a statement like the “negation” of this statement just exchanges A and B, in this case yielding . Now let’s ask what statements are generated from a given axiom system. Say we start with . After one step of possible substitutions we get while after 2 steps we get: And in our setup we’re effectively asserting that these are “true” statements. But now let’s “negate” the statements, by exchanging A and B. And if we do this, we’ll see that there’s never a statement where both it and its negation occur. In other words, there’s no obvious inconsistency being generated within this axiom system. But if we consider instead the axiom then this gives: And since this includes both and its “negation” , by our criteria we must consider this axiom system to be inconsistent. In addition to inconsistency, we can also ask about incompleteness. For all possible statements, does the axiom system eventually generate either the statement or its negation? Or, in other words, can we always decide from the axiom system whether any given statement is true or false? With our simple assumption about negation, questions of inconsistency and incompleteness become at least in principle very simple to explore. Starting from a given axiom system, we generate its entailment cone, then we ask within this cone what fraction of possible statements, say of a given length, occur. If the answer is more than 50% we know there’s inconsistency, while if the answer is less than 50% that’s evidence of incompleteness. So what happens with different possible axiom systems? Here are some results from A New Kind of Science, in each case showing both what amounts to the raw entailment cone (or, in this case, multiway system evolution from “true”), and the number of statements of a given length reached after progressively more steps: At some level this is all rather straightforward. But from the pictures above we can already get a sense that there’s a problem. For most axiom systems the fraction of statements reached of a given length changes as we increase the number of steps in the entailment cone. Sometimes it’s straightforward to see what fraction will be achieved even after an infinite number of steps. But often it’s not. And in general we’ll run into computational irreducibility—so that in effect the only way to determine whether some particular statement is generated is just to go to ever more steps in the entailment cone and see what happens. In other words, there’s no guaranteed-finite way to decide what the ultimate fraction will be—and thus whether or not any given axiom system is inconsistent, or incomplete, or neither. For some axiom systems it may be possible to tell. But for some axiom systems it’s not, in effect because we don’t in general know how far we’ll have to go to determine whether a given statement is true or not. A certain amount of additional technical detail is required to reach the standard versions of Gödel’s incompleteness theorems. (Note that these theorems were originally stated specifically for the Peano axioms for arithmetic, but the Principle of Computational Equivalence suggests that they’re in some sense much more general, and even ubiquitous.) But the important point here is that given an axiom system there may be statements that either can or cannot be reached—but there’s no upper bound on the length of path that might be needed to reach them even if one can. OK, so let’s come back to talking about the notion of truth in the context of the ruliad. We’ve discussed axiom systems that might show inconsistency, or incompleteness—and the difficulty of determining if they do. But the ruliad in a sense contains all possible axiom systems—and generates all possible statements. So how then can we ever expect to identify which statements are “true” and which are not? When we talked about particular axiom systems, we said that any statement that is generated can be considered true (at least with respect to that axiom system). But in the ruliad every statement is generated. So what criterion can we use to determine which we should consider “true”? The key idea is any computationally bounded observer (like us) can perceive only a tiny slice of the ruliad. And it’s a perfectly meaningful question to ask whether a particular statement occurs within that perceived slice. One way of picking a “slice” is just to start from a given axiom system, and develop its entailment cone. And with such a slice, the criterion for the truth of a statement is exactly what we discussed above: does the statement occur in the entailment cone? But how do typical “mathematical observers” actually sample the ruliad? As we discussed in the previous section, it seems to be much more by forming an entailment fabric than by developing a whole entailment cone. And in a sense progress in mathematics can be seen as a process of adding pieces to an entailment fabric: pulling in one mathematical statement after another, and checking that they fit into the fabric. So what happens if one tries to add a statement that “isn’t true”? The basic answer is that it produces an “explosion” in which the entailment fabric can grow to encompass essentially any statement. From the point of view of underlying rules—or the ruliad—there’s really nothing wrong with this. But the issue is that it’s incompatible with an “observer like us”—or with any realistic idealization of a mathematician. Our view of a mathematical observer is essentially an entity that accumulates mathematical statements into an entailment fabric. But we assume that the observer is computationally bounded, so in a sense they can only work with a limited collection of statements. So if there’s an explosion in an entailment fabric that means the fabric will expand beyond what a mathematical observer can coherently handle. Or, put another way, the only kind of entailment fabrics that a mathematical observer can reasonably consider are ones that “contain no explosions”. And in such fabrics, it’s reasonable to take the generation or entailment of a statement as a signal that the statement can be considered true. The ruliad is in a sense a unique and absolute thing. And we might have imagined that it would lead us to a unique and absolute definition of truth in mathematics. But what we’ve seen is that that’s not the case. And instead our notion of truth is something based on how we sample the ruliad as mathematical observers. But now we must explore what this means about what mathematics as we perceive it can be like.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.21 What Can Human Mathematics Be Like? : The ruliad in a sense contains all structurally possible mathematics—including all mathematical statements, all axiom systems and everything that follows from them. But mathematics as we humans conceive of it is never the whole ruliad; instead it is always just some tiny part that we as mathematical observers sample. We might imagine, however, that this would mean that there is in a sense a complete arbitrariness to our mathematics—because in a sense we could just pick any part of the ruliad we want. Yes, we might want to start from a specific axiom system. But we might imagine that that axiom system could be chosen arbitrarily, with no further constraint. And that the mathematics we study can therefore be thought of as an essentially arbitrary choice, determined by its detailed history, and perhaps by cognitive or other features of humans. But there is a crucial additional issue. When we “sample our mathematics” from the ruliad we do it as mathematical observers and ultimately as humans. And it turns out that even very general features of us as mathematical observers turn out to put strong constraints on what we can sample, and how. When we discussed physics, we said that the central features of observers are their computational boundedness and their assumption of their own persistence through time. In mathematics, observers are again computationally bounded. But now it is not persistence through time that they assume, but rather a certain coherence of accumulated knowledge. We can think of a mathematical observer as progressively expanding the entailment fabric that they consider to “represent mathematics”. And the question is what they can add to that entailment fabric while still “remaining coherent” as observers. In the previous section, for example, we argued that if the observer adds a statement that can be considered “logically false” then this will lead to an “explosion” in the entailment fabric. Such a statement is certainly present in the ruliad. But if the observer were to add it, then they wouldn’t be able to maintain their coherence—because, whimsically put, their mind would necessarily explode. In thinking about axiomatic mathematics it’s been standard to say that any axiom system that’s “reasonable to use” should at least be consistent (even though, yes, for a given axiom system it’s in general ultimately undecidable whether this is the case). And certainly consistency is one criterion that we now see is necessary for a “mathematical observer like us”. But one can expect that it’s not the only criterion. In other words, although it’s perfectly possible to write down any axiom system, and even start generating its entailment cone, only some axiom systems may be compatible with “mathematical observers like us”. And so, for example, something like the Continuum Hypothesis—which is known to be independent of the “established axioms” of set theory—may well have the feature that, say, it has to be assumed to be true in order to get a metamathematical structure compatible with mathematical observers like us. In the case of physics, we know that the general characteristics of observers lead to certain key perceived features and laws of physics. In statistical mechanics, we’re dealing with “coarse-grained observers” who don’t trace and decode the paths of individual molecules, and therefore perceive the Second Law of thermodynamics, fluid dynamics, etc. And in our Physics Project we’re also dealing with coarse-grained observers who don’t track all the details of the atoms of space, but instead perceive space as something coherent and effectively continuous. And it seems as if in metamathematics there’s something very similar going on. As we began to discuss in the very first section above, mathematical observers tend to “coarse grain” metamathematical space. In operational terms, one way they do this is by talking about something like the Pythagorean theorem without always going down to the detailed level of axioms, and for example saying just how real numbers should be defined. And something related is that they tend to concentrate more on mathematical statements and theorems than on their proofs. Later we’ll see how in the context of the ruliad there’s an even deeper level to which one can go. But the point here is that in actually doing mathematics one tends to operate at the “human scale” of talking about mathematical concepts rather than the “molecular-scale details” of axioms. But why does this work? Why is one not continually “dragged down” to the detailed axiomatic level—or below? How come it’s possible to reason at what we described above as the “fluid dynamics” level, without always having to go down to the detailed “molecular dynamics” level? The basic claim is that this works for mathematical observers for essentially the same reason as the perception of space works for physical observers. With the “coarse-graining” characteristics of the observer, it’s inevitable that the slice of the ruliad they sample will have the kind of coherence that allows them to operate at a higher level. In other words, mathematics can be done “at a human level” for the same basic reason that we have a “human-level experience” of space in physics. The fact that it works this way depends both on necessary features of the ruliad—and in general of multicomputation—as well as on characteristics of us as observers. Needless to say, there are “corner cases” where what we’ve described starts to break down. In physics, for example, the “human-level experience” of space breaks down near spacetime singularities. And in mathematics, there are cases where for example undecidability forces one to take a lower-level, more axiomatic and ultimately more metamathematical view. But the point is that there are large regions of physical space—and metamathematical space—where these kinds of issues don’t come up, and where our assumptions about physical—and mathematical—observers can be maintained. And this is what ultimately allows us to have the “human-scale” views of physics and mathematics that we do.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.22 Going below Axiomatic Mathematics : In the traditional view of the foundations of mathematics one imagines that axioms—say stated in terms of symbolic expressions—are in some sense the lowest level of mathematics. But thinking in terms of the ruliad suggests that in fact there is a still-lower “ur level”—a kind of analog of machine code in which everything, including axioms, is broken down into ultimate “raw computation”. Take an axiom like , or, in more precise computational language: Compared to everything we’re used to seeing in mathematics this looks simple. But actually it’s already got a lot in it. For example, it assumes the notion of a binary operator, which it’s in effect naming “∘”. And for example it also assumes the notion of variables, and has two distinct pattern variables that are in effect “tagged” with the names x and y. So how can we define what this axiom ultimately “means”? Somehow we have to go from its essentially textual symbolic representation to a piece of actual computation. And, yes, the particular representation we’ve used here can immediately be interpreted as computation in the Wolfram Language. But the ultimate computational concept we’re dealing with is more general than that. And in particular it can exist in any universal computational system. Different universal computational systems (say particular languages or CPUs or Turing machines) may have different ways to represent computations. But ultimately any computation can be represented in any of them—with the differences in representation being like different “coordinatizations of computation”. And however we represent computations there is one thing we can say for sure: all possible computations are somewhere in the ruliad. Different representations of computations correspond in effect to different coordinatizations of the ruliad. But all computations are ultimately there. For our Physics Project it’s been convenient use a “parametrization of computation” that can be thought of as being based on rewriting of hypergraphs. The elements in these hypergraphs are ultimately purely abstract, but we tend to talk about them as “atoms of space” to indicate the beginnings of our interpretation. It’s perfectly possible to use hypergraph rewriting as the “substrate” for representing axiom systems stated in terms of symbolic expressions. But it’s a bit more convenient (though ultimately equivalent) to instead use systems based on expression rewriting—or in effect tree rewriting. At the outset, one might imagine that different axiom systems would somehow have to be represented by “different rules” in the ruliad. But as one might expect from the phenomenon of universal computation, it’s actually perfectly possible to think of different axiom systems as just being specified by different “data” operated on by a single set of rules. There are many rules and structures that we could use. But one set that has the benefit of a century of history are S, K combinators. The basic concept is to represent everything in terms of “combinator expressions” containing just the two objects S and K. (It’s also possible to have just one fundamental object, and indeed S alone may be enough.) It’s worth saying at the outset that when we go this “far down” things get pretty non-human and obscure. Setting things up in terms of axioms may already seem pedantic and low level. But going to a substrate below axioms—that we can think of as getting us to raw “atoms of existence”—will lead us to a whole other level of obscurity and complexity. But if we’re going to understand how mathematics can emerge from the ruliad this is where we have to go. And combinators provide us with a more-or-less-concrete example. Here’s an example of a small combinator expression: which corresponds to the “expression tree”: We can write the combinator expression without explicit “function application” [ ... ] by using a (left) application operator • and it’s always unambiguous to omit this operator, yielding the compact representation: By mapping S, K and the application operator to codewords it’s possible to represent this as a simple binary sequence: But what does our combinator expression mean? The basic combinators are defined to have the rules: These rules on their own don’t do anything to our combinator expression. But if we form the expression which we can write as: then repeated application of the rules gives: We can think of this as “feeding” c, x and y into our combinator expression, then using the “plumbing” defined by the combinator expression to assemble a particular expression in terms of c, x and y. But what does this expression now mean? Well, that depends on what we think c, x and y mean. We might notice that c always appears in the configuration c[_][_]. And this means we can interpret it as a binary operator, which we could write in infix form as ∘ so that our expression becomes: And, yes, this is all incredibly low level. But we need to go even further. Right now we’re feeding in names like c, x and y. But in the end we want to represent absolutely everything purely in terms of S and K. So we need to get rid of the “human-readable names” and just replace them with “lumps” of S, K combinators that—like the names—get “carried around” when the combinator rules are applied. We can think about our ultimate expressions in terms of S and K as being like machine code. “One level up” we have assembly language, with the same basic operations, but explicit names. And the idea is that things like axioms—and the laws of inference that apply to them—can be “compiled down” to this assembly language. But ultimately we can always go further, to the very lowest-level “machine code”, in which only S and K ever appear. Within the ruliad as “coordinatized” by S, K combinators, there’s an infinite collection of possible combinator expressions. But how do we find ones that “represent something recognizably mathematical”? As an example let’s consider a possible way in which S, K can represent integers, and arithmetic on integers. The basic idea is that an integer n can be input as the combinator expression which for n = 5 gives: But if we now apply this to [S][K] what we get reduces to: which contains 4 S’s. But with this representation of integers it’s possible to find combinator expressions that represent arithmetic operations. For example, here’s a representation of an addition operator: At the “assembly language” level we might call this plus, and apply it to integers i and j using: But at the “pure machine code” level can be represented simply by: which when applied to [S][K] reduces to the “output representation” of 3: As a slightly more elaborate example: represents the operation of raising to a power. Then becomes: Applying this to [S][K] repeated application of the combinator rules gives: eventually yielding the output representation of 8: We could go on and construct any other arithmetic or computational operation we want, all just in terms of the “universal combinators” S and K. But how should we think about this in terms of our conception of mathematics? Basically what we’re seeing is that in the “raw machine code” of S, K combinators it’s possible to “find” a representation for something we consider to be a piece of mathematics. Earlier we talked about starting from structures like axiom systems and then “compiling them down” to raw machine code. But what about just “finding mathematics” in a sense “naturally occurring” in “raw machine code”? We can think of the ruliad as containing “all possible machine code”. And somewhere in that machine code must be all the conceivable “structures of mathematics”. But the question is: in the wildness of the raw ruliad, what structures can we as mathematical observers successfully pick out? The situation is quite directly analogous to what happens at multiple levels in physics. Consider for example a fluid full of molecules bouncing around. As we’ve discussed several times, observers like us usually aren’t sensitive to the detailed dynamics of the molecules. But we can still successfully pick out large-scale structures—like overall fluid motions, vortices, etc. And—much like in mathematics—we can talk about physics just at this higher level. In our Physics Project all this becomes much more extreme. For example, we imagine that space and everything in it is just a giant network of atoms of space. And now within this network we imagine that there are “repeated patterns”—that correspond to things like electrons and quarks and black holes. In a sense it is the big achievement of natural science to have managed to find these regularities so that we can describe things in terms of them, without always having to go down to the level of atoms of space. But the fact that these are the kinds of regularities we have found is also a statement about us as physical observers. And the point is that even at the level of the raw ruliad our characteristics as physical observers will inevitably lead us to such regularities. The fact that we are computationally bounded and assume ourselves to have a certain persistence will lead us to consider things that are localized and persistent—that in physics we identify for example as particles. And it’s very much the same thing in mathematics. As mathematical observers we’re interested in picking out from the raw ruliad “repeated patterns” that are somehow robust. But now instead of identifying them as particles, we’ll identify them as mathematical constructs and definitions. In other words, just as a repeated pattern in the ruliad might in physics be interpreted as an electron, in mathematics a repeated pattern in the ruliad might be interpreted as an integer. We might think of physics as something “emergent” from the structure of the ruliad, and now we’re thinking of mathematics the same way. And of course not only is the “underlying stuff” of the ruliad the same in both cases, but also in both cases it’s “observers like us” that are sampling and perceiving things. There are lots of analogies to the process we’re describing of “fishing constructs out of the raw ruliad”. As one example, consider the evolution of a (“class 4”) cellular automaton in which localized structures emerge: Underneath, just as throughout the ruliad, there’s lots of detailed computation going on, with rules repeatedly getting applied to each cell. But out of all this underlying computation we can identify a certain set of persistent structures—which we can use to make a “higher-level description” that may capture the aspects of the behavior that we care about. Given an “ocean” of S, K combinator expressions, how might we set about “finding mathematics” in them? One straightforward approach is just to identify certain “mathematical properties” we want, and then go searching for S, K combinator expressions that satisfy these. For example, if we want to “search for (propositional) logic” we first need to pick combinator expressions to symbolically represent “true” and “false”. There are many pairs of expressions that will work. As one example, let’s pick: Now we can just search for combinator expressions which, when applied to all possible pairs of “true” and “false” give truth tables corresponding to particular logical functions. And if we do this, here are examples of the smallest combinator expressions we find: Here’s how we can then reproduce the truth table for And: If we just started picking combinator expressions at random, then most of them wouldn’t be “interpretable” in terms of this representation of logic. But if we ran across for example we could recognize in it the combinators for And, Or, etc. that we identified above, and in effect “disassemble” it to give: It’s worth noting, though, that even with the choices we made above for “true” and “false”, there’s not just a single possible combinator, say for And. Here are a few possibilities: And there’s also nothing unique about the choices for “true” and “false”. With the alternative choices: here are the smallest combinator expressions for a few logical functions: So what can we say in general about the “interpretability” of an arbitrary combinator expression? Obviously any combinator expression does what it does at the level of raw combinators. But the question is whether it can be given a “higher-level”—and potentially “mathematical”—interpretation. And in a sense this is directly an issue of what a mathematical observer “perceives” in it. Does it contain some kind of robust structure—say a kind of analog for mathematics of a particle in physics? Axiom systems can be viewed as a particular way to “summarize” certain “raw machine code” in the ruliad. But from the point of a “raw coordinatization of the ruliad” like combinators there doesn’t seem to be anything immediately special about them. At least for us humans, however, they do seem to be an obvious “waypoint”. Because by distinguishing operators and variables, establishing arities for operators and introducing names for things, they reflect the kind of structure that’s familiar from human language. But now that we think of the ruliad as what’s “underneath” both mathematics and physics there’s a different path that’s suggested. With the axiomatic approach we’re effectively trying to leverage human language as a way of summarizing what’s going on. But an alternative is to leverage our direct experience of the physical world, and our perception and intuition about things like space. And as we’ll discuss later, this is likely in many ways a better “metamodel” of the way pure mathematics is actually practiced by us humans. In some sense, this goes straight from the “raw machine code” of the ruliad to “human-level mathematics”, sidestepping the axiomatic level. But given how much “reductionist” work has already been done in mathematics to represent its results in axiomatic form, there is definitely still great value in seeing how the whole axiomatic setup can be “fished out” of the “raw ruliad”. And there’s certainly no lack of complicated technical issues in doing this. As one example, how should one deal with “generated variables”? If one “coordinatizes” the ruliad in terms of something like hypergraph rewriting this is fairly straightforward: it just involves creating new elements or hypergraph nodes (which in physics would be interpreted as atoms of space). But for something like S, K combinators it’s a bit more subtle. In the examples we’ve given above, we have combinators that, when “run”, eventually reach a fixed point. But to deal with generated variables we probably also need combinators that never reach fixed points, making it considerably more complicated to identify correspondences with definite symbolic expressions. Another issue involves rules of entailment, or, in effect, the metalogic of an axiom system. In the full axiomatic setup we want to do things like create token-event graphs, where each event corresponds to an entailment. But what rule of entailment should be used? The underlying rules for S, K combinators, for example, define a particular choice—though they can be used to emulate others. But the ruliad in a sense contains all choices. And, once again, it’s up to the observer to “fish out” of the raw ruliad a particular “slice”—which captures not only the axiom system but also the rules of entailment used. It may be worth mentioning a slightly different existing “reductionist” approach to mathematics: the idea of describing things in terms of types. A type is in effect an equivalence class that characterizes, say, all integers, or all functions from tuples of reals to truth values. But in our terms we can interpret a type as a kind of “template” for our underlying “machine code”: we can say that some piece of machine code represents something of a particular type if the machine code matches a particular pattern of some kind. And the issue is then whether that pattern is somehow robust “like a particle” in the raw ruliad. An important part of what made our Physics Project possible is the idea of going “underneath” space and time and other traditional concepts of physics. And in a sense what we’re doing here is something very similar, though for mathematics. We want to go “underneath” concepts like functions and variables, and even the very idea of symbolic expressions. In our Physics Project a convenient “parametrization” of what’s “underneath” is a hypergraph made up of elements that we often refer to as “atoms of space”. In mathematics we’ve discussed using combinators as our “parametrization” of what’s “underneath”. But what are these “made of”? We can think of them as corresponding to raw elements of metamathematics, or raw elements of computation. But in the end, they’re “made of” whatever the ruliad is “made of”. And perhaps the best description of the elements of the ruliad is that they are “atoms of existence”—the smallest units of anything, from which everything, in mathematics and physics and elsewhere, must be made. The atoms of existence aren’t bits or points or anything like that. They’re something fundamentally lower level that’s come into focus only with our Physics Project, and particularly with the identification of the ruliad. And for our purposes here I’ll call such atoms of existence “emes” (pronounced “eemes”, like phonemes etc.). Everything in the ruliad is made of emes. The atoms of space in our Physics Project are emes. The nodes in our combinator trees are emes. An eme is a deeply abstract thing. And in a sense all it has is an identity. Every eme is distinct. We could give it a name if we wanted to, but it doesn’t intrinsically have one. And in the end the structure of everything is built up simply from relations between emes.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.23 The Physicalized Laws of Mathematics : The concept of the ruliad suggests there is a deep connection between the foundations of mathematics and physics. And now that we have discussed how some of the familiar formalism of mathematics can “fit into” the ruliad, we are ready to use the “bridge” provided by the ruliad to start exploring how to apply some of the successes and intuitions of physics to mathematics. A foundational part of our everyday experience of physics is our perception that we live in continuous space. But our Physics Project implies that at sufficiently small scales space is actually made of discrete elements—and it is only because of the coarse-grained way in which we experience it that we perceive it as continuous. In mathematics—unlike physics—we’ve long thought of the foundations as being based on things like symbolic expressions that have a fundamentally discrete structure. Normally, though, the elements of those expressions are, for example, given human-recognizable names (like 2 or Plus). But what we saw in the previous section is that these recognizable forms can be thought of as existing in an “anonymous” lower-level substrate made of what we can call atoms of existence or emes. But the crucial point is that this substrate is directly based on the ruliad. And its structure is identical between the foundations of mathematics and physics. In mathematics the emes aggregate up to give us our universe of mathematical statements. In physics they aggregate up to give us our physical universe. But now the commonality of underlying “substrate” makes us realize that we should be able to take our experience of physics, and apply it to mathematics. So what is the analog in mathematics of our perception of the continuity of space in physics? We’ve discussed the idea that we can think of mathematical statements as being laid out in a metamathematical space—or, more specifically, in what we’ve called an entailment fabric. We initially talked about “coordinatizing” this using axioms, but in the previous section we saw how to go “below axioms” to the level of “pure emes”. When we do mathematics, though, we’re sampling this on a much higher level. And just like as physical observers we coarse grain the emes (that we usually call “atoms of space”) that make up physical space, so too as “mathematical observers” we coarse grain the emes that make up metamathematical space. Foundational approaches to mathematics—particularly over the past century or so—have almost always been based on axioms and on their fundamentally discrete symbolic structure. But by going to a lower level and seeing the correspondence with physics we are led to consider what we might think of as a higher-level “experience” of mathematics—operating not at the “molecular dynamics” level of specific axioms and entailments, but rather at what one might call the “fluid dynamics” level of larger-scale concepts. At the outset one might not have any reason to think that this higher-level approach could consistently be applied. But this is the first big place where ideas from physics can be used. If both physics and mathematics are based on the ruliad, and if our general characteristics as observers apply in both physics and mathematics, then we can expect that similar features will emerge. And in particular, we can expect that our everyday perception of physical space as continuous will carry over to mathematics, or, more accurately, to metamathematical space. The picture is that we as mathematical observers have a certain “size” in metamathematical space. We identify concepts—like integers or the Pythagorean theorem—as “regions” in the space of possible configurations of emes (and ultimately of slices of the ruliad). At an axiomatic level we might think of ways to capture what a typical mathematician might consider “the same concept” with slightly different formalism (say, different large cardinal axioms or different models of real numbers). But when we get down to the level of emes there’ll be vastly more freedom in how we capture a given concept—so that we’re in effect using a whole region of “emic space” to do so. But now the question is what happens if we try to make use of the concept defined by this “region”? Will the “points in the region” behave coherently, or will everything be “shredded”, with different specific representations in terms of emes leading to different conclusions? The expectation is that in most cases it will work much like physical space, and that what we as observers perceive will be quite independent of the detailed underlying behavior at the level of emes. Which is why we can expect to do “higher-level mathematics”, without always having to descend to the level of emes, or even axioms. And this we can consider as the first great “physicalized law of mathematics”: that coherent higher-level mathematics is possible for us for the same reason that physical space seems coherent to observers like us. We’ve discussed several times before the analogy to the Second Law of thermodynamics—and the way it makes possible a higher-level description of things like fluids for “observers like us”. There are certainly cases where the higher-level description breaks down. Some of them may involve specific probes of molecular structure (like Brownian motion). Others may be slightly more “unwitting” (like hypersonic flow). In our Physics Project we’re very interested in where similar breakdowns might occur—because they’d allow us to “see below” the traditional continuum description of space. Potential targets involve various extreme or singular configurations of spacetime, where in effect the “coherent observer” gets “shredded”, because different atoms of space “within the observer” do different things. In mathematics, this kind of “shredding” of the observer will tend to be manifest in the need to “drop below” higher-level mathematical concepts, and go down to a very detailed axiomatic, metamathematical or even eme level—where computational irreducibility and phenomena like undecidability are rampant. It’s worth emphasizing that from the point of view of pure axiomatic mathematics it’s not at all obvious that higher-level mathematics should be possible. It could be that there’d be no choice but to work through every axiomatic detail to have any chance of making conclusions in mathematics. But the point is that we now know there could be exactly the same issue in physics. Because our Physics Project implies that at the lowest level our universe is effectively made of emes that have all sorts of complicated—and computationally irreducible—behavior. Yet we know that we don’t have to trace through all the details of this to make conclusions about what will happen in the universe—at least at the level we normally perceive it. In other words, the fact that we can successfully have a “high-level view” of what happens in physics is something that fundamentally has the same origin as the fact that we can successfully have a high-level view of what happens in mathematics. Both are just features of how observers like us sample the ruliad that underlies both physics and mathematics.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.24 Uniformity and Motion in Metamathematical Space : We’ve discussed how the basic concept of space as we experience it in physics leads us to our first great physicalized law of mathematics—and how this provides for the very possibility of higher-level mathematics. But this is just the beginning of what we can learn from thinking about the correspondences between physical and metamathematical space implied by their common origin in the structure of the ruliad. A key idea is to think of a limit of mathematics in which one is dealing with so many mathematical statements that one can treat them “in bulk”—as forming something we could consider a continuous metamathematical space. But what might this space be like? Our experience of physical space is that at our scale and with our means of perception it seems to us for the most part quite simple and uniform. And this is deeply connected to the concept that pure motion is possible in physical space—or, in other words, that it’s possible for things to move around in physical space without fundamentally changing their character. Looked at from the point of view of the atoms of space it’s not at all obvious that this should be possible. After all, whenever we move we’ll almost inevitably be made up of different atoms of space. But it’s fundamental to our character as observers that the features we end up perceiving are ones that have a certain persistence—so that we can imagine that we, and objects around us, can just “move unchanged”, at least with respect to those aspects of the objects that we perceive. And this is why, for example, we can discuss laws of mechanics without having to “drop down” to the level of the atoms of space. So what’s the analog of all this in metamathematical space? At the present stage of our physical universe, we seem to be able to experience physical space as having features like being basically three-dimensional. Metamathematical space probably doesn’t have such familiar mathematical characterizations. But it seems very likely (and we’ll see some evidence of this from empirical metamathematics below) that at the very least we’ll perceive metamathematical space as having a certain uniformity or homogeneity. In our Physics Project we imagine that we can think of physical space as beginning “at the Big Bang” with what amounts to some small collection of atoms of space, but then growing to the vast number of atoms in our current universe through the repeated application of particular rules. But with a small set of rules being applied a vast number of times, it seems almost inevitable that some kind of uniformity must result. But then the same kind of thing can be expected in metamathematics. In axiomatic mathematics one imagines the mathematical analog of the Big Bang: everything starts from a small collection of axioms, and then expands to a huge number of mathematical statements through repeated application of laws of inference. And from this picture (which gets a bit more elaborate when one considers emes and the full ruliad) one can expect that at least after it’s “developed for a while” metamathematical space, like physical space, will have a certain uniformity. The idea that physical space is somehow uniform is something we take very much for granted, not least because that’s our lifelong experience. But the analog of this idea for metamathematical space is something we don’t have immediate everyday intuition about—and that in fact may at first seem surprising or even bizarre. But actually what it implies is something that increasingly rings true from modern experience in pure mathematics. Because by saying that metamathematical space is in a sense uniform, we’re saying that different parts of it somehow seem similar—or in other words that there’s parallelism between what we see in different areas of mathematics, even if they’re not “nearby” in terms of entailments. But this is exactly what, for example, the success of category theory implies. Because it shows us that even in completely different areas of mathematics it makes sense to set up the same basic structures of objects, morphisms and so on. As such, though, category theory defines only the barest outlines of mathematical structure. But what our concept of perceived uniformity in metamathematical space suggests is that there should in fact be closer correspondences between different areas of mathematics. We can view this as another fundamental “physicalized law of mathematics”: that different areas of mathematics should ultimately have structures that are in some deep sense “perceived the same” by mathematical observers. For several centuries we’ve known there’s a certain correspondence between, for example, geometry and algebra. But it’s been a major achievement of recent mathematics to identify more and more such correspondences or “dualities”. Often the existence of these has seemed remarkable, and surprising. But what our view of metamathematics here suggests is that this is actually a general physicalized law of mathematics—and that in the end essentially all different areas of mathematics must share a deep structure, at least in some appropriate “bulk metamathematical limit” when enough statements are considered. But it’s one thing to say that two places in metamathematical space are “similar”; it’s another to say that “motion between them” is possible. Once again we can make an analogy with physical space. We’re used to the idea that we can move around in space, maintaining our identity and structure. But this in a sense requires that we can maintain some kind of continuity of existence on our path between two positions. In principle it could have been that we would have to be “atomized” at one end, then “reconstituted” at the other end. But our actual experience is that we perceive ourselves to continually exist all the way along the path. In a sense this is just an assumption about how things work that physical observers like us make; but what’s nontrivial is that the underlying structure of the ruliad implies that this will always be consistent. And so we expect it will be in metamathematics. Like a physical observer, the way a mathematical observer operates, it’ll be possible to “move” from one area of mathematics to another “at a high level”, without being “atomized” along the way. Or, in other words, that a mathematical observer will be able to make correspondences between different areas of mathematics without having to go down to the level of emes to do so. It’s worth realizing that as soon as there’s a way of representing mathematics in computational terms the concept of universal computation (and, more tightly, the Principle of Computational Equivalence) implies that at some level there must always be a way to translate between any two mathematical theories, or any two areas of mathematics. But the question is whether it’s possible to do this in “high-level mathematical terms” or only at the level of the underlying “computational substrate”. And what we’re saying is that there’s a general physicalized law of mathematics that implies that higher-level translation should be possible. Thinking about mathematics at a traditional axiomatic level can sometimes obscure this, however. For example, in axiomatic terms we usually think of Peano arithmetic as not being as powerful as ZFC set theory (for example, it lacks transfinite induction)—and so nothing like “dual” to it. But Peano arithmetic can perfectly well support universal computation, so inevitably a “formal emulator” for ZFC set theory can be built in it. But the issue is that to do this essentially requires going down to the “atomic” level and operating not in terms of mathematical constructs but instead directly in terms of “metamathematical” symbolic structure (and, for example, explicitly emulating things like equality predicates). But the issue, it seems, is that if we think at the traditional axiomatic level, we’re not dealing with a “mathematical observer like us”. In the analogy we’ve used above, we’re operating at the “molecular dynamics” level, not at the human-scale “fluid dynamics” level. And so we see all sorts of details and issues that ultimately won’t be relevant in typical approaches to actually doing pure mathematics. It’s somewhat ironic that our physicalized approach shows this by going below the axiomatic level—to the level of emes and the raw ruliad. But in a sense it’s only at this level that there’s the uniformity and coherence to conveniently construct a general picture that can encompass observers like us. Much as with ordinary matter we can say that “everything is made of atoms”, we’re now saying that everything is “made of computation” (and its structure and behavior is ultimately described by the ruliad). But the crucial idea that emerged from our Physics Project—and that is at the core of what I’m calling the multicomputational paradigm—is that when we ask what observers perceive there is a whole additional level of inexorable structure. And this is what makes it possible to do both human-scale physics and higher-level mathematics—and for there to be what amounts to “pure motion”, whether in physical or metamathematical space. There’s another way to think about this, that we alluded to earlier. A key feature of an observer is to have a coherent identity. In physics, that involves having a consistent thread of experience in time. In mathematics, it involves bringing together a consistent view of “what’s true” in the space of mathematical statements. In both cases the observer will in effect involve many separate underlying elements (ultimately, emes). But in order to maintain the observer’s view of having a coherent identity, the observer must somehow conflate all these elements, effectively treating them as “the same”. In physics, this means “coarse-graining” across physical or branchial (or, in fact, rulial) space. In mathematics, this means “coarse-graining” across metamathematical space—or in effect treating different mathematical statements as “the same”. In practice, there are several ways this happens. First of all, one tends to be more concerned about mathematical results than their proofs, so two statements that have the same form can be considered the same even if the proofs (or other processes) that generated them are different (and indeed this is something we have routinely done in constructing entailment cones here). But there’s more. One can also imagine that any statements that entail each other can be considered “the same”. In a simple case, this means that if and then one can always assume . But there’s a much more general version of this embodied in the univalence axiom of homotopy type theory—that in our terms can be interpreted as saying that mathematical observers consider equivalent things the same. There’s another way that mathematical observers conflate different statements—that’s in many ways more important, but less formal. As we mentioned above, when mathematicians talk, say, about the Pythagorean theorem, they typically think they have a definite concept in mind. But at the axiomatic level—and even more so at the level of emes—there are a huge number of different “metamathematical configurations” that are all “considered the same” by the typical working mathematician, or by our “mathematical observer”. (At the level of axioms, there might be different axiom systems for real numbers; at the level of emes there might be different ways of representing concepts like addition or equality.) In a sense we can think of mathematical observers as having a certain “extent” in metamathematical space. And much like human-scale physical observers see only the aggregate effects of huge numbers of atoms of space, so also mathematical observers see only the “aggregate effects” of huge numbers of emes of metamathematical space. But now the key question is whether a “whole mathematical observer” can “move in metamathematical space” as a single “rigid” entity, or whether it will inevitably be distorted—or shredded—by the structure of metamathematical space. In the next section we’ll discuss the analog of gravity—and curvature—in metamathematical space. But our physicalized approach tends to suggest that in “most” of metamathematical space, a typical mathematical observer will be able to “move around freely”, implying that there will indeed be paths or “bridges” between different areas of mathematics, that involve only higher-level mathematical constructs, and don’t require dropping down to the level of emes and the raw ruliad.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.25 Gravitational and Relativistic Effects in Metamathematics : If metamathematical space is like physical space, does that mean that it has analogs of gravity, and relativity? The answer seems to be “yes”—and these provide our next examples of physicalized laws of mathematics. In the end, we’re going to be able to talk about at least gravity in a largely “static” way, referring mostly to the “instantaneous state of metamathematics”, captured as an entailment fabric. But in leveraging ideas from physics, it’s important to start off formulating things in terms of the analog of time for metamathematics—which is entailment. As we’ve discussed above, the entailment cone is the direct analog of the light cone in physics. Starting with some mathematical statement (or, more accurately, some event that transforms it) the forward entailment cone contains all statements (or, more accurately, events) that follow from it. Any possible “instantaneous state of metamathematics” then corresponds to a “transverse slice” through this entailment cone—with the slice in effect being laid out in metamathematical space. An individual entailment of one statement by another corresponds to a path in the entailment cone, and this path (or, more accurately for accumulative evolution, subgraph) can be thought of as a proof of one statement given another. And in these terms the shortest proof can be thought of as a geodesic in the entailment cone. (In practical mathematics, it’s very unlikely one will find—or care about—the strictly shortest proof. But even having a “fairly short proof” will be enough to give the general conclusions we’ll discuss here.) Given a path in the entailment cone, we can imagine projecting it onto a transverse slice, i.e. onto an entailment fabric. Being able to consistently do this depends on having a certain uniformity in the entailment cone, and in the sequence of “metamathematical hypersurfaces” that are defined by whatever “metamathematical reference frame” we’re using. But assuming, for example, that underlying computational irreducibility successfully generates a kind of “statistical uniformity” that cannot be “decoded” by the observer, we can expect to have meaningful paths—and geodesics—on entailment fabrics. But what these geodesics are like then depends on the emergent geometry of entailment fabrics. In physics, the limiting geometry of the analog of this for physical space is presumably a fairly simple 3D manifold. For branchial space, it’s more complicated, probably for example being “exponential dimensional”. And for metamathematics, the limiting geometry is also undoubtedly more complicated—and almost certainly exponential dimensional. We’ve argued that we expect metamathematical space to have a certain perceived uniformity. But what will affect this, and therefore potentially modify the local geometry of the space? The basic answer is exactly the same as in our Physics Project. If there’s “more activity” somewhere in an entailment fabric, this will in effect lead to “more local connections”, and thus effective “positive local curvature” in the emergent geometry of the network. Needless to say, exactly what “more activity” means is somewhat subtle, especially given that the fabric in which one is looking for this is itself defining the ambient geometry, measures of “area”, etc. In our Physics Project we make things more precise by associating “activity” with energy density, and saying that energy effectively corresponds to the flux of causal edges through spacelike hypersurfaces. So this suggests that we think about an analog of energy in metamathematics: essentially defining it to be the density of update events in the entailment fabric. Or, put another way, energy in metamathematics depends on the “density of proofs” going through a region of metamathematical space, i.e. involving particular “nearby” mathematical statements. There are lots of caveats, subtleties and details. But the notion that “activity AKA energy” leads to increasing curvature in an emergent geometry is a general feature of the whole multicomputational paradigm that the ruliad captures. And in fact we expect a quantitative relationship between energy density (or, strictly, energy-momentum) and induced curvature of the “transversal space”—that corresponds exactly to Einstein’s equations in general relativity. It’ll be more difficult to see this in the metamathematical case because metamathematical space is geometrically more complicated—and less familiar—than physical space. But even at a qualitative level, it seems very helpful to think in terms of physics and spacetime analogies. The basic phenomenon is that geodesics are deflected by the presence of “energy”, in effect being “attracted to it”. And this is why we can think of regions of higher energy (or energy-momentum/mass)—in physics and in metamathematics—as “generating gravity”, and deflecting geodesics towards them. (Needless to say, in metamathematics, as in physics, the vast majority of overall activity is just devoted to knitting together the structure of space, and when gravity is produced, it’s from slightly increased activity in a particular region.) (In our Physics Project, a key result is that the same kind of dependence of “spatial” structure on energy happens not only in physical space, but also in branchial space—where there’s a direct analog of general relativity that basically yields the path integral of quantum mechanics.) What does this mean in metamathematics? Qualitatively, the implication is that “proofs will tend to go through where there’s a higher density of proofs”. Or, in an analogy, if you want to drive from one place to another, it’ll be more efficient if you can do at least part of your journey on a freeway. One question to ask about metamathematical space is whether one can always get from any place to any other. In other words, starting from one area of mathematics, can one somehow derive all others? A key issue here is whether the area one starts from is computation universal. Propositional logic is not, for example. So if one starts from it, one is essentially trapped, and cannot reach other areas. But results in mathematical logic have established that most traditional areas of axiomatic mathematics are in fact computation universal (and the Principle of Computational Equivalence suggests that this will be ubiquitous). And given computation universality there will at least be some “proof path”. (In a sense this is a reflection of the fact that the ruliad is unique, so everything is connected in “the same ruliad”.) But a big question is whether the “proof path” is “big enough” to be appropriate for a “mathematical observer like us”. Can we expect to get from one part of metamathematical space to another without the observer being “shredded”? Will we be able to start from any of a whole collection of places in metamathematical space that are considered “indistinguishably nearby” to a mathematical observer and have all of them “move together” to reach our destination? Or will different specific starting points follow quite different paths—preventing us from having a high-level (“fluid dynamics”) description of what’s going on, and instead forcing us to drop down to the “molecular dynamics” level? In practical pure mathematics, this tends to be an issue of whether there is an “elegant proof using high-level concepts”, or whether one has to drop down to a very detailed level that’s more like low-level computer code, or the output of an automated theorem proving system. And indeed there’s a very visceral sense of “shredding” in cases where one’s confronted with a proof that consists of page after page of “machine-like details”. But there’s another point here as well. If one looks at an individual proof path, it can be computationally irreducible to find out where the path goes, and the question of whether it ever reaches a particular destination can be undecidable. But in most of the current practice of pure mathematics, one’s interested in “higher-level conclusions”, that are “visible” to a mathematical observer who doesn’t resolve individual proof paths. Later we’ll discuss the dichotomy between explorations of computational systems that routinely run into undecidability—and the typical experience of pure mathematics, where undecidability is rarely encountered in practice. But the basic point is that what a typical mathematical observer sees is at the “fluid dynamics level”, where the potentially circuitous path of some individual molecule is not relevant. Of course, by asking specific questions—about metamathematics, or, say, about very specific equations—it’s still perfectly possible to force tracing of individual “low-level” proof paths. But this isn’t what’s typical in current pure mathematical practice. And in a sense we can see this as an extension of our first physicalized law of mathematics: not only is higher-level mathematics possible, but it’s ubiquitously so, with the result that, at least in terms of the questions a mathematical observer would readily formulate, phenomena like undecidability are not generically seen. But even though undecidability may not be directly visible to a mathematical observer, its underlying presence is still crucial in coherently “knitting together” metamathematical space. Because without undecidability, we won’t have computation universality and computational irreducibility. But—just like in our Physics Project—computational irreducibility is crucial in producing the low-level apparent randomness that is needed to support any kind of “continuum limit” that allows us to think of large collections of what are ultimately discrete emes as building up some kind of coherent geometrical space. And when undecidability is not present, one will typically not end up with anything like this kind of coherent space. An extreme example occurs in rewrite systems that eventually terminate—in the sense that they reach a “fixed-point” (or “normal form”) state where no more transformations can be applied. In our Physics Project, this kind of termination can be interpreted as a spacelike singularity at which “time stops” (as at the center of a non-rotating black hole). But in general decidability is associated with “limits on how far paths can go”—just like the limits on causal paths associated with event horizons in physics. There are many details to work out, but the qualitative picture can be developed further. In physics, the singularity theorems imply that in essence the eventual formation of spacetime singularities is inevitable. And there should be a direct analog in our context that implies the eventual formation of “metamathematical singularities”. In qualitative terms, we can expect that the presence of proof density (which is the analog of energy) will “pull in” more proofs until eventually there are so many proofs that one has decidability and a “proof event horizon” is formed. In a sense this implies that the long-term future of mathematics is strangely similar to the long-term future of our physical universe. In our physical universe, we expect that while the expansion of space may continue, many parts of the universe will form black holes and essentially be “closed off”. (At least ignoring expansion in branchial space, and quantum effects in general.) The analog of this in mathematics is that while there can be continued overall expansion in metamathematical space, more and more parts of it will “burn out” because they’ve become decidable. In other words, as more work and more proofs get done in a particular area, that area will eventually be “finished”—and there will be no more “open-ended” questions associated with it. In physics there’s sometimes discussion of white holes, which are imagined to effectively be time-reversed black holes, spewing out all possible material that could be captured in a black hole. In metamathematics, a white hole is like a statement that is false and therefore “leads to an explosion”. The presence of such an object in metamathematical space will in effect cause observers to be shredded—making it inconsistent with the coherent construction of higher-level mathematics. We’ve talked at some length about the “gravitational” structure of metamathematical space. But what about seemingly simpler things like special relativity? In physics, there’s a notion of basic, flat spacetime, for which it’s easy to construct families of reference frames, and in which parallel trajectories stay parallel. In metamathematics, the analog is presumably metamathematical space in which “parallel proof geodesics” remain “parallel”—so that in effect one can continue “making progress in mathematics” by just “keeping on doing what you’ve been doing”. And somehow relativistic invariance is associated with the idea that there are many ways to do math, but in the end they’re all able to reach the same conclusions. Ultimately this is something one expects as a consequence of fundamental features of the ruliad—and the inevitability of causal invariance in it resulting from the Principle of Computational Equivalence. It’s also something that might seem quite familiar from practical mathematics and, say, from the ability to do derivations using different methods—like from either geometry or algebra—and yet still end up with the same conclusions. So if there’s an analog of relativistic invariance, what about analogs of phenomena like time dilation? In our Physics Project time dilation has a rather direct interpretation. To “progress in time” takes a certain amount of computational work. But motion in effect also takes a certain amount of computational work—in essence to continually recreate versions of something in different places. But from the ruliad on up there is ultimately only a certain amount of computational work that can be done—and if computational work is being “used up” on motion, there is less available to devote to progress in time, and so time will effectively run more slowly, leading to the experience of time dilation. So what is the metamathematical analog of this? Presumably it’s that when you do derivations in math you can either stay in one area and directly make progress in that area, or you can “base yourself in some other area” and make progress only by continually translating back and forth. But ultimately that translation process will take computational work, and so will slow down your progress—leading to an analog of time dilation. In physics, the speed of light defines the maximum amount of motion in space that can occur in a certain amount of time. In metamathematics, the analog is that there’s a maximum “translation distance” in metamathematical space that can be “bridged” with a certain amount of derivation. In physics we’re used to measuring spatial distance in meters—and time in seconds. In metamathematics we don’t yet have familiar units in which to measure, say, distance between mathematical concepts—or, for that matter, “amount of derivation” being done. But with the empirical metamathematics we’ll discuss in the next section we actually have the beginnings of a way to define such things, and to use what’s been achieved in the history of human mathematics to at least imagine “empirically measuring” what we might call “maximum metamathematical speed”. It should be emphasized that we are only at the very beginning of exploring things like the analogs of relativity in metamathematics. One important piece of formal structure that we haven’t really discussed here is causal dependence, and causal graphs. We’ve talked at length about statements entailing other statements. But we haven’t talked about questions like which part of which statement is needed for some event to occur that will entail some other statement. And—while there’s no fundamental difficulty in doing it—we haven’t concerned ourselves with constructing causal graphs to represent causal relationships and causal dependencies between events. When it comes to physical observers, there is a very direct interpretation of causal graphs that relates to what a physical observer can experience. But for mathematical observers—where the notion of time is less central—it’s less clear just what the interpretation of causal graphs should be. But one certainly expects that they will enter in the construction of any general “observer theory” that characterizes “observers like us” across both physics and mathematics.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.26 Empirical Metamathematics : We’ve discussed the overall structure of metamathematical space, and the general kind of sampling that we humans do of it (as “mathematical observers”) when we do mathematics. But what can we learn from the specifics of human mathematics, and the actual mathematical statements that humans have published over the centuries? We might imagine that these statements are just ones that—as “accidents of history”—humans have “happened to find interesting”. But there’s definitely more to it—and potentially what’s there is a rich source of “empirical data” relevant to our physicalized laws of mathematics, and to what amounts to their “experimental validation”. The situation with “human settlements” in metamathematical space is in a sense rather similar to the situation with human settlements in physical space. If we look at where humans have chosen to live and build cities, we’ll find a bunch of locations in 3D space. The details of where these are depend on history and many factors. But there’s a clear overarching theme, that’s in a sense a direct reflection of underlying physics: all the locations lie on the more-or-less spherical surface of the Earth. It’s not so straightforward to see what’s going on in the metamathematical case, not least because any notion of coordinatization seems to be much more complicated for metamathematical space than for physical space. But we can still begin by doing “empirical metamathematics” and asking questions about for example what amounts to where in metamathematical space we humans have so far established ourselves. And as a first example, let’s consider Boolean algebra. Even to talk about something called “Boolean algebra” we have to be operating at a level far above the raw ruliad—where we’ve already implicitly aggregated vast numbers of emes to form notions of, for example, variables and logical operations. But once we’re at this level we can “survey” metamathematical space just by enumerating possible symbolic statements that can be created using the operations we’ve set up for Boolean algebra (here And ∧, Or ∨ and Not ): But so far these are just raw, structural statements. To connect with actual Boolean algebra we must pick out which of these can be derived from the axioms of Boolean algebra, or, put another way, which of them are in the entailment cone of these axioms: Of all possible statements, it’s only an exponentially small fraction that turn out to be derivable: But in the case of Boolean algebra, we can readily collect such statements: We’ve typically explored entailment cones by looking at slices consisting of collections of theorems generated after a specified number of proof steps. But here we’re making a very different sampling of the entailment cone—looking in effect instead at theorems in order of their structural complexity as symbolic expressions. In doing this kind of systematic enumeration we’re in a sense operating at a “finer level of granularity” than typical human mathematics. Yes, these are all “true theorems”. But mostly they’re not theorems that a human mathematician would ever write down, or specifically “consider interesting”. And for example only a small fraction of them have historically been given names—and are called out in typical logic textbooks: The reduction from all “structurally possible” theorems to just “ones we consider interesting” can be thought of as a form of coarse graining. And it could well be that this coarse graining would depend on all sorts of accidents of human mathematical history. But at least in the case of Boolean algebra there seems to be a surprisingly simple and “mechanical” procedure that can reproduce it. Go through all theorems in order of increasing structural complexity, in each case seeing whether a given theorem can be proved from ones earlier in the list: It turns out that the theorems identified by humans as “interesting” coincide almost exactly with “root theorems” that cannot be proved from earlier theorems in the list. Or, put another way, the “coarse graining” that human mathematicians do seems (at least in this case) to essentially consist of picking out only those theorems that represent “minimal statements” of new information—and eliding away those that involve “extra ornamentation”. But how are these “notable theorems” laid out in metamathematical space? Earlier we saw how the simplest of them can be reached after just a few steps in the entailment cone of a typical textbook axiom system for Boolean algebra. The full entailment cone rapidly gets unmanageably large but we can get a first approximation to it by generating individual proofs (using automated theorem proving) of our notable theorems, and then seeing how these “knit together” through shared intermediate lemmas in a token-event graph: Looking at this picture we see at least a hint that clumps of notable theorems are spread out across the entailment cone, only modestly building on each other—and in effect “staking out separated territories” in the entailment cone. But of the 11 notable theorems shown here, 7 depend on all 6 axioms, while 4 depend only on various different sets of 3 axioms—suggesting at least a certain amount of fundamental interdependence or coherence. From the token-event graph we can derive a branchial graph that represents a very rough approximation to how the theorems are “laid out in metamathematical space”: We can get a potentially slightly better approximation by including proofs not just of notable theorems, but of all theorems up to a certain structural complexity. The result shows separation of notable theorems both in the multiway graph and in the branchial graph: In doing this empirical metamathematics we’re including only specific proofs rather than enumerating the whole entailment cone. We’re also using only a specific axiom system. And even beyond this, we’re using specific operators to write our statements in Boolean algebra. In a sense each of these choices represents a particular “metamathematical coordinatization”—or particular reference frame or slice that we’re sampling in the ruliad. For example, in what we’ve done above we’ve built up statements from And, Or and Not. But we can just as well use any other functionally complete sets of operators, such as the following (here each shown representing a few specific Boolean expressions): For each set of operators, there are different axiom systems that can be used. And for each axiom system there will be different proofs. Here are a few examples of axiom systems with a few different sets of operators—in each case giving a proof of the law of double negation (which has to be stated differently for different operators): Boolean algebra (or, equivalently, propositional logic) is a somewhat desiccated and thin example of mathematics. So what do we find if we do empirical metamathematics on other areas? Let’s talk first about geometry—for which Euclid’s Elements provided the very first large-scale historical example of an axiomatic mathematical system. The Elements started from 10 axioms (5 “postulates” and 5 “common notions”), then gave 465 theorems. Each theorem was proved from previous ones, and ultimately from the axioms. Thus, for example, the “proof graph” (or “theorem dependency graph”) for Book 1, Proposition 5 (which says that angles at the base of an isosceles triangle are equal) is: One can think of this as a coarse-grained version of the proof graphs we’ve used before (which are themselves in turn “slices” of the entailment graph)—in which each node shows how a collection of “input” theorems (or axioms) entails a new theorem. Here’s a slightly more complicated example (Book 1, Proposition 48) that ultimately depends on all 10 of the original axioms: And here’s the full graph for all the theorems in Euclid’s Elements: Of the 465 theorems here, 255 (i.e. 55%) depend on all 10 axioms. (For the much smaller number of notable theorems of Boolean algebra above we found that 64% depended on all 6 of our stated axioms.) And the general connectedness of this graph in effect reflects the idea that Euclid’s theorems represent a coherent body of connected mathematical knowledge. The branchial graph gives us an idea of how the theorems are “laid out in metamathematical space”: One thing we notice is that theorems about different areas—shown here in different colors—tend to be separated in metamathematical space. And in a sense the seeds of this separation are already evident if we look “textually” at how theorems in different books of Euclid’s Elements refer to each other: Looking at the overall dependence of one theorem on others in effect shows us a very coarse form of entailment. But can we go to a finer level—as we did above for Boolean algebra? As a first step, we have to have an explicit symbolic representation for our theorems. And beyond that, we have to have a formal axiom system that describes possible transformations between these. At the level of “whole theorem dependency” we can represent the entailment of Euclid’s Book 1, Proposition 1 from axioms as: But if we now use the full, formal axiom system for geometry that we discussed in a previous section we can use automated theorem proving to get a full proof of Book 1, Proposition 1: In a sense this is “going inside” the theorem dependency graph to look explicitly at how the dependencies in it work. And in doing this we see that what Euclid might have stated in words in a sentence or two is represented formally in terms of hundreds of detailed intermediate lemmas. (It’s also notable that whereas in Euclid’s version, the theorem depends only on 3 out of 10 axioms, in the formal version the theorem depends on 18 out of 20 axioms.) How about for other theorems? Here is the theorem dependency graph from Euclid’s Elements for the Pythagorean theorem (which Euclid gives as Book 1, Proposition 47): The theorem depends on all 10 axioms, and its stated proof goes through 28 intermediate theorems (i.e. about 6% of all theorems in the Elements). In principle we can “unroll” the proof dependency graph to see directly how the theorem can be “built up” just from copies of the original axioms. Doing a first step of unrolling we get: And “flattening everything out” so that we don’t use any intermediate lemmas but just go back to the axioms to “re-prove” everything we can derive the theorem from a “proof tree” with the following number of copies of each axiom (and a certain “depth” to reach that axiom): So how about a more detailed and formal proof? We could certainly in principle construct this using the axiom system we discussed above. But an important general point is that the thing we in practice call “the Pythagorean theorem” can actually be set up in all sorts of different axiom systems. And as an example let’s consider setting it up in the main actual axiom system that working mathematicians typically imagine they’re (usually implicitly) using, namely ZFC set theory. Conveniently, the Metamath formalized math system has accumulated about 40,000 theorems across mathematics, all with hand-constructed proofs based ultimately on ZFC set theory. And within this system we can find the theorem dependency graph for the Pythagorean theorem: Altogether it involves 6970 intermediate theorems, or about 18% of all theorems in Metamath—including ones from many different areas of mathematics. But how does it ultimately depend on the axioms? First, we need to talk about what the axioms actually are. In addition to “pure ZFC set theory”, we need axioms for (predicate) logic, as well as ones that define real and complex numbers. And the way things are set up in Metamath’s “set.mm” there are (essentially) 49 basic axioms (9 for pure set theory, 15 for logic and 25 related to numbers). And much as in Euclid’s Elements we found that the Pythagorean theorem depended on all the axioms, so now here we find that the Pythagorean theorem depends on 48 of the 49 axioms—with the only missing axiom being the Axiom of Choice. Just like in the Euclid’s Elements case, we can imagine “unrolling” things to see how many copies of each axiom are used. Here are the results—together with the “depth” to reach each axiom: And, yes, the numbers of copies of most of the axioms required to establish the Pythagorean theorem are extremely large. There are several additional wrinkles that we should discuss. First, we’ve so far only considered overall theorem dependency—or in effect “coarse-grained entailment”. But the Metamath system ultimately gives complete proofs in terms of explicit substitutions (or, effectively, bisubstitutions) on symbolic expressions. So, for example, while the first-level “whole-theorem-dependency” graph for the Pythagorean theorem is the full first-level entailment structure based on the detailed proof is (where the black vertices indicate “internal structural elements” in the proof—such as variables, class specifications and “inputs”): Another important wrinkle has to do with the concept of definitions. The Pythagorean theorem, for example, refers to squaring numbers. But what is squaring? What are numbers? Ultimately all these things have to be defined in terms of the “raw data structures” we’re using. In the case of Boolean algebra, for example, we could set things up just using Nand (say denoted ∘), but then we could define And and Or in terms of Nand (say as and respectively). We could still write expressions using And and Or—but with our definitions we’d immediately be able to convert these to pure Nands. Axioms—say about Nand—give us transformations we can use repeatedly to make derivations. But definitions are transformations we use “just once” (like macro expansion in programming) to reduce things to the point where they involve only constructs that appear in the axioms. In Metamath’s “set.mm” there are about 1700 definitions that effectively build up from “pure set theory” (as well as logic, structural elements and various axioms about numbers) to give the mathematical constructs one needs. So, for example, here is the definition dependency graph for addition (“+” or Plus): At the bottom are the basic constructs of logic and set theory—in terms of which things like order relations, complex numbers and finally addition are defined. The definition dependency graph for GCD, for example, is somewhat larger, though has considerable overlap at lower levels: Different constructs have definition dependency graphs of different sizes—in effect reflecting their “definitional distance” from set theory and the underlying axioms being used: In our physicalized approach to metamathematics, though, something like set theory is not our ultimate foundation. Instead, we imagine that everything is eventually built up from the raw ruliad, and that all the constructs we’re considering are formed from what amount to configurations of emes in the ruliad. We discussed above how constructs like numbers and logic can be obtained from a combinator representation of the ruliad. We can view the definition dependency graph above as being an empirical example of how somewhat higher-level definitions can be built up. From a computer science perspective, we can think of it as being like a type hierarchy. From a physics perspective, it’s as if we’re starting from atoms, then building up to molecules and beyond. It’s worth pointing out, however, that even the top of the definition hierarchy in something like Metamath is still operating very much at an axiomatic kind of level. In the analogy we’ve been using, it’s still for the most part “formulating math at the molecular dynamics level” not at the more human “fluid dynamics” level. We’ve been talking about “the Pythagorean theorem”. But even on the basis of set theory there are many different possible formulations one can give. In Metamath, for example, there is the pythag version (which is what we’ve been using), and there is also a (somewhat more general) pythi version. So how are these related? Here’s their combined theorem dependency graph (or at least the first two levels in it)—with red indicating theorems used only in deriving pythag, blue indicating ones used only in deriving pythi, and purple indicating ones used in both: And what we see is there’s a certain amount of “lower-level overlap” between the derivations of these variants of the Pythagorean theorem, but also some discrepancy—indicating a certain separation between these variants in metamathematical space. So what about other theorems? Here’s a table of some famous theorems from all over mathematics, sorted by the total number of theorems on which proofs of them formulated in Metamath depend—giving also the number of axioms and definitions used in each case: The Pythagorean theorem (here the pythi formulation) occurs solidly in the second half. Some of the theorems with the fewest dependencies are in a sense very structural theorems. But it’s interesting to see that theorems from all sorts of different areas soon start appearing, and then are very much mixed together in the remainder of the list. One might have thought that theorems involving “more sophisticated concepts” (like Ramsey’s theorem) would appear later than “more elementary” ones (like the sum of angles of a triangle). But this doesn’t seem to be true. There’s a distribution of what amount to “proof sizes” (or, more strictly, theorem dependency sizes)—from the Schröder–Bernstein theorem which relies on less than 4% of all theorems, to Dirichlet’s theorem that relies on 25%: If we look not at “famous” theorems, but at all theorems covered by Metamath, the distribution becomes broader, with many short-to-prove “glue” or essentially “definitional” lemmas appearing: But using the list of famous theorems as an indication of the “math that mathematicians care about” we can conclude that there is a kind of “metamathematical floor” of results that one needs to reach before “things that we care about” start appearing. It’s a bit like the situation in our Physics Project—where the vast majority of microscopic events that happen in the universe seem to be devoted merely to knitting together the structure of space, and only “on top of that” can events which can be identified with things like particles and motion appear. And indeed if we look at the “prerequisites” for different famous theorems, we indeed find that there is a large overlap (indicated by lighter colors)—supporting the impression that in a sense one first has “knit together metamathematical space” and only then can one start generating “interesting theorems”: Another way to see “underlying overlap” is to look at what axioms different theorems ultimately depend on (the colors indicate the “depth” at which the axioms are reached): The theorems here are again sorted in order of “dependency size”. The “very-set-theoretic” ones at the top don’t depend on any of the various number-related axioms. And quite a few “integer-related theorems” don’t depend on complex number axioms. But otherwise, we see that (at least according to the proofs in set.mm) most of the “famous theorems” depend on almost all the axioms. The only axiom that’s rarely used is the Axiom of Choice—on which only things like “analysis-related theorems” such as the Fundamental Theorem of Calculus depend. If we look at the “depth of proof” at which axioms are reached, there’s a definite distribution. And this may be about as robust as any a “statistical characteristic” of the sampling of metamathematical space corresponding to mathematics that is “important to humans”. If we were, for example, to consider all possible theorems in the entailment cone we’d get a very different picture. But potentially what we see here may be a characteristic signature of what’s important to a “mathematical observer like us”. Going beyond “famous theorems” we can ask, for example, about all the 42,000 or so identified theorems in the Metamath set.mm collection. Here’s a rough rendering of their theorem dependency graph, with different colors indicating theorems in different fields of math (and with explicit edges removed): There’s some evidence of a certain overall uniformity, but we can see definite “patches of metamathematical space” dominated by different areas of mathematics. And here’s what happens if we zoom in on the central region, and show where famous theorems lie: A bit like we saw for the named theorems of Boolean algebra clumps of famous theorems appear to somehow “stake out their own separate metamathematical territory”. But notably the famous theorems seem to show some tendency to congregate near “borders” between different areas of mathematics. To get more of a sense of the relation between these different areas, we can make what amounts to a highly coarsened branchial graph, effectively laying out whole areas of mathematics in metamathematical space, and indicating their cross-connections: We can see “highways” between certain areas. But there’s also a definite “background entanglement” between areas, reflecting at least a certain background uniformity in metamathematical space, as sampled with the theorems identified in Metamath. It’s not the case that all these areas of math “look the same”—and for example there are differences in their distributions of theorem dependency sizes: In areas like algebra and number theory, most proofs are fairly long, as revealed by the fact that they have many dependencies. But in set theory there are plenty of short proofs, and in logic all the proofs of theorems that have been included in Metamath are short. What if we look at the overall dependency graph for all theorems in Metamath? Here’s the adjacency matrix we get: The results are triangular because theorems in the Metamath database are arranged so that later ones only depend on earlier ones. And while there’s considerable patchiness visible, there still seems to be a certain overall background level of uniformity. In doing this empirical metamathematics we’re sampling metamathematical space just through particular “human mathematical settlements” in it. But even from the distribution of these “settlements” we potentially begin to see evidence of a certain background uniformity in metamathematical space. Perhaps in time as more connections between different areas of mathematics are found human mathematics will gradually become more “uniformly settled” in metamathematical space—and closer to what we might expect from entailment cones and ultimately from the raw ruliad. But it’s interesting to see that even with fairly basic empirical metamathematics—operating on a current corpus of human mathematical knowledge—it may already be possible to see signs of some features of physicalized metamathematics. One day, no doubt, we’ll be able do experiments in physics that take our “parsing” of the physical universe in terms of things like space and time and quantum mechanics—and reveal “slices” of the raw ruliad underneath. But perhaps something similar will also be possible in empirical metamathematics: to construct what amounts to a metamathematical microscope (or telescope) through which we can see aspects of the ruliad.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.27 Invented or Discovered? How Mathematics Relates to Humans : It’s an old and oft-asked question: is mathematics ultimately something that is invented, or something that is discovered? Or, put another way: is mathematics something arbitrarily set up by us humans, or something inevitable and fundamental and in a sense “preexisting”, that we merely get to explore? In the past it’s seemed as if these were two fundamentally incompatible possibilities. But the framework we’ve built here in a sense blends them both into a rather unexpected synthesis. The starting point is the idea that mathematics—like physics—is rooted in the ruliad, which is a representation of formal necessity. Actual mathematics as we “experience” it is—like physics—based on the particular sampling we make of the ruliad. But then the crucial point is that very basic characteristics of us as “observers” are sufficient to constrain that experience to be our general mathematics—or our physics. At some level we can say that “mathematics is always there”—because every aspect of it is ultimately encoded in the ruliad. But in another sense we can say that the mathematics we have is all “up to us”—because it’s based on how we sample the ruliad. But the point is that that sampling is not somehow “arbitrary”: if we’re talking about mathematics for us humans then it’s us ultimately doing the sampling, and the sampling is inevitably constrained by general features of our nature. A major discovery from our Physics Project is that it doesn’t take much in the way of constraints on the observer to deeply constrain the laws of physics they will perceive. And similarly we posit here that for “observers like us” there will inevitably be general (“physicalized”) laws of mathematics, that make mathematics inevitably have the general kinds of characteristics we perceive it to have (such as the possibility of doing mathematics at a high level, without always having to drop down to an “atomic” level). Particularly over the past century there’s been the idea that mathematics can be specified in terms of axiom systems, and that these axiom systems can somehow be “invented at will”. But our framework does two things. First, it says that “far below” axiom systems is the raw ruliad, which in a sense represents all possible axiom systems. And second, it says that whatever axiom systems we perceive to be “operating” will be ones that we as observers can pick out from the underlying structure of the ruliad. At a formal level we can “invent” an arbitrary axiom system (and it’ll be somewhere in the ruliad), but only certain axiom systems will be ones that describe what we as “mathematical observers” can perceive. In a physics setting we might construct some formal physical theory that talks about detailed patterns in the atoms of space (or molecules in a gas), but the kind of “coarse-grained” observations that we can make won’t capture these. Put another way, observers like us can perceive certain kinds of things, and can describe things in terms of these perceptions. But with the wrong kind of theory—or “axioms”—these descriptions won’t be sufficient—and only an observer who’s “shredded” down to a more “atomic” level will be able to track what’s going on. There’s lots of different possible math—and physics—in the ruliad. But observers like us can only “access” a certain type. Some putative alien not like us might access a different type—and might end up with both a different math and a different physics. Deep underneath they—like us—would be talking about the ruliad. But they’d be taking different samples of it, and describing different aspects of it. For much of the history of mathematics there was a close alignment between the mathematics that was done and what we perceive in the world. For example, Euclidean geometry—with its whole axiomatic structure—was originally conceived just as an idealization of geometrical things that we observe about the world. But by the late 1800s the idea had emerged that one could create “disembodied” axiomatic systems with no particular grounding in our experience in the world. And, yes, there are many possible disembodied axiom systems that one can set up. And in doing ruliology and generally exploring the computational universe it’s interesting to investigate what they do. But the point is that this is something quite different from mathematics as mathematics is normally conceived. Because in a sense mathematics—like physics—is a “more human” activity that’s based on what “observers like us” make of the raw formal structure that is ultimately embodied in the ruliad. When it comes to physics there are, it seems, two crucial features of “observers like us”. First, that we’re computationally bounded. And second, that we have the perception that we’re persistent—and have a definite and continuous thread of experience. At the level of atoms of space, we’re in a sense constantly being “remade”. But we nevertheless perceive it as always being the “same us”. This single seemingly simple assumption has far-reaching consequences. For example, it leads us to experience a single thread of time. And from the notion that we maintain a continuity of experience from every successive moment to the next we are inexorably led to the idea of a perceived continuum—not only in time, but also for motion and in space. And when combined with intrinsic features of the ruliad and of multicomputation in general, what comes out in the end is a surprisingly precise description of how we’ll perceive our universe to operate—that seems to correspond exactly with known core laws of physics. What does that kind of thinking tell us about mathematics? The basic point is that—since in the end both relate to humans—there is necessarily a close correspondence between physical and mathematical observers. Both are computationally bounded. And the assumption of persistence in time for physical observers becomes for mathematical observers the concept of maintaining coherence as more statements are accumulated. And when combined with intrinsic features of the ruliad and multicomputation this then turns out to imply the kind of physicalized laws of mathematics that we’ve discussed. In a formal axiomatic view of mathematics one just imagines that one invents axioms and sees their consequences. But what we’re describing here is a view of mathematics that is ultimately just about the ways that we as mathematical observers sample and experience the ruliad. And if we use axiom systems it has to be as a kind of “intermediate language” that helps us make a slightly higher-level description of some corner of the raw ruliad. But actual “human-level” mathematics—like human-level physics—operates at a higher level. Our everyday experience of the physical world gives us the impression that we have a kind of “direct access” to many foundational features of physics, like the existence of space and the phenomenon of motion. But our Physics Project implies that these are not concepts that are in any sense “already there”; they are just things that emerge from the raw ruliad when you “parse” it in the kinds of ways observers like us do. In mathematics it’s less obvious (at least to all but perhaps experienced pure mathematicians) that there’s “direct access” to anything. But in our view of mathematics here, it’s ultimately just like physics—and ultimately also rooted in the ruliad, but sampled not by physical observers but by mathematical ones. So from this point view there’s just as much that’s “real” underneath mathematics as there is underneath physics. The mathematics is sampled slightly differently (though very similarly)—but we should not in any sense consider it “fundamentally more abstract”. When we think of ourselves as entities within the ruliad, we can build up what we might consider a “fully abstract” description of how we get our “experience” of physics. And we can basically do the same thing for mathematics. So if we take the commonsense point of view that physics fundamentally exists “for real”, we’re forced into the same point of view for mathematics. In other words, if we say that the physical universe exists, so must we also say that in some fundamental sense, mathematics also exists. It’s not something we as humans “just make”, but it is something that is made through our particular way of observing the ruliad, that is ultimately defined by our particular characteristics as observers, with our particular core assumptions about the world, our particular kinds of sensory experience, and so on. So what can we say in the end about whether mathematics is “invented” or “discovered”? It is neither. Its underpinnings are the ruliad, whose structure is a matter of formal necessity. But its perceived form for us is determined by our intrinsic characteristics as observers. We neither get to “arbitrarily invent” what’s underneath, nor do we get to “arbitrarily discover” what’s already there. The mathematics we see is the result of a combination of formal necessity in the underlying ruliad, and the particular forms of perception that we—as entities like us—have. Putative aliens could have quite different mathematics, but not because the underlying ruliad is any different for them, but because their forms of perception might be different. And it’s the same with physics: even though they “live in the same physical universe” their perception of the laws of physics could be quite different.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.28 What Axioms Can There Be for Human Mathematics? : When they were first developed in antiquity the axioms of Euclidean geometry were presumably intended basically as a kind of “tightening” of our everyday impressions of geometry—that would aid in being able to deduce what was true in geometry. But by the mid-1800s—between non-Euclidean geometry, group theory, Boolean algebra and quaternions—it had become clear that there was a range of abstract axiom systems one could in principle consider. And by the time of Hilbert’s program around 1900 the pure process of deduction was in effect being viewed as an end in itself—and indeed the core of mathematics—with axiom systems being seen as “starter material” pretty much just “determined by convention”. In practice even today very few different axiom systems are ever commonly used—and indeed in A New Kind of Science I was able to list essentially all of them comfortably on a couple of pages. But why these axiom systems and not others? Despite the idea that axiom systems could ultimately be arbitrary, the concept was still that in studying some particular area of mathematics one should basically have an axiom system that would provide a “tight specification” of whatever mathematical object or structure one was trying to talk about. And so, for example, the Peano axioms are what became used for talking about arithmetic-style operations on integers. In 1931, however, Gödel’s theorem showed that actually these axioms weren’t strong enough to constrain one to be talking only about integers: there were also other possible models of the axiom system, involving all sorts of exotic “non-standard arithmetic”. (And moreover, there was no finite way to “patch” this issue.) In other words, even though the Peano axioms had been invented—like Euclid’s axioms for geometry—as a way to describe a definite “intuitive” mathematical thing (in this case, integers) their formal axiomatic structure “had a life of its own” that extended (in some sense, infinitely) beyond its original intended purpose. Both geometry and arithmetic in a sense had foundations in everyday experience. But for set theory dealing with infinite sets there was never an obvious intuitive base rooted in everyday experience. Some extrapolations from finite sets were clear. But in covering infinite sets various axioms (like the Axiom of Choice) were gradually added to capture what seemed like “reasonable” mathematical assertions. But one example whose status for a long time wasn’t clear was the Continuum Hypothesis—which asserts that the “next distinct possible cardinality” after the cardinality of the integers is : the cardinality of real numbers (i.e. of “the continuum”). Was this something that followed from previously accepted axioms of set theory? And if it was added, would it even be consistent with them? In the early 1960s it was established that actually the Continuum Hypothesis is independent of the other axioms. With the axiomatic view of the foundations of mathematics that’s been popular for the past century or so it seems as if one could, for example, just choose at will whether to include the Continuum Hypothesis (or its negation) as an axiom in set theory. But with the approach to the foundations of mathematics that we’ve developed here, this is no longer so clear. Recall that in our approach, everything is ultimately rooted in the ruliad—with whatever mathematics observers like us “experience” just being the result of the particular sampling we do of the ruliad. And in this picture, axiom systems are a particular representation of fairly low-level features of the sampling we do of the raw ruliad. If we could do any kind of sampling we want of the ruliad, then we’d presumably be able to get all possible axiom systems—as intermediate-level “waypoints” representing different kinds of slices of the ruliad. But in fact by our nature we are observers capable of only certain kinds of sampling of the ruliad. We could imagine “alien observers” not like us who could for example make whatever choice they want about the Continuum Hypothesis. But given our general characteristics as observers, we may be forced into a particular choice. Operationally, as we’ve discussed above, the wrong choice could, for example, be incompatible with an observer who “maintains coherence” in metamathematical space. Let’s say we have a particular axiom stated in standard symbolic form. “Underneath” this axiom there will typically be at the level of the raw ruliad a huge cloud of possible configurations of emes that can represent the axiom. But an “observer like us” can only deal with a coarse-grained version in which all these different configurations are somehow considered equivalent. And if the entailments from “nearby configurations” remain nearby, then everything will work out, and the observer can maintain a coherent view of what’s going, for example just in terms of symbolic statements about axioms. But if instead different entailments of raw configurations of emes lead to very different places, the observer will in effect be “shredded”—and instead of having definite coherent “single-minded” things to say about what happens, they’ll have to separate everything into all the different cases for different configurations of emes. Or, as we’ve said it before, the observer will inevitably end up getting “shredded”—and not be able to come up with definite mathematical conclusions. So what specifically can we say about the Continuum Hypothesis? It’s not clear. But conceivably we can start by thinking of as characterizing the “base cardinality” of the ruliad, while characterizes the base cardinality of a first-level hyperruliad that could for example be based on Turing machines with oracles for their halting problems. And it could be that for us to conclude that the Continuum Hypothesis is false, we’d have to somehow be straddling the ruliad and the hyperruliad, which would be inconsistent with us maintaining a coherent view of mathematics. In other words, the Continuum Hypothesis might somehow be equivalent to what we’ve argued before is in a sense the most fundamental “contingent fact”—that just as we live in a particular location in physical space—so also we live in the ruliad and not the hyperruliad. We might have thought that whatever we might see—or construct—in mathematics would in effect be “entirely abstract” and independent of anything about physics, or our experience in the physical world. But particularly insofar as we’re thinking about mathematics as done by humans we’re dealing with “mathematical observers” that are “made of the same stuff” as physical observers. And this means that whatever general constraints or features exist for physical observers we can expect these to carry over to mathematical observers—so it’s no coincidence that both physical and mathematical observers have the same core characteristics, of computational boundedness and “assumption of coherence”. And what this means is that there’ll be a fundamental correlation between things familiar from our experience in the physical world and what shows up in our mathematics. We might have thought that the fact that Euclid’s original axioms were based on our human perceptions of physical space would be a sign that in some “overall picture” of mathematics they should be considered arbitrary and not in any way central. But the point is that in fact our notions of space are central to our characteristics as observers. And so it’s inevitable that “physical-experience-informed” axioms like those for Euclidean geometry will be what appear in mathematics for “observers like us”.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.29 Counting the Emes of Mathematics and Physics : How does the “size of mathematics” compare to the size of our physical universe? In the past this might have seemed like an absurd question, that tries to compare something abstract and arbitrary with something real and physical. But with the idea that both mathematics and physics as we experience them emerge from our sampling of the ruliad, it begins to seem less absurd. At the lowest level the ruliad can be thought of as being made up of atoms of existence that we call emes. As physical observers we interpret these emes as atoms of space, or in effect the ultimate raw material of the physical universe. And as mathematical observers we interpret them as the ultimate elements from which the constructs of mathematics are built. As the entangled limit of all possible computations, the whole ruliad is infinite. But we as physical or mathematical observers sample only limited parts of it. And that means we can meaningfully ask questions like how the number of emes in these parts compare—or, in effect, how big is physics as we experience it compared to mathematics. In some ways an eme is like a bit. But the concept of emes is that they’re “actual atoms of existence”—from which “actual stuff” like the physical universe and its history are made—rather than just “static informational representations” of it. As soon as we imagine that everything is ultimately computational we are immediately led to start thinking of representing it in terms of bits. But the ruliad is not just a representation. It’s in some way something lower level. It’s the “actual stuff” that everything is made of. And what defines our particular experience of physics or of mathematics is the particular samples we as observers take of what’s in the ruliad. So the question is now how many emes there are in those samples. Or, more specifically, how many emes “matter to us” in building up our experience. Let’s return to an analogy we’ve used several times before: a gas made of molecules. In the volume of a room there might be individual molecules, each on average colliding every seconds. So that means that our “experience of the room” over the course of a minute or so might sample collisions. Or, in terms closer to our Physics Project, we might say that there are perhaps “collision events” in the causal graph that defines what we experience. But these “collision events” aren’t something fundamental; they have what amounts to “internal structure” with many associated parameters about location, time, molecular configuration, etc. Our Physics Project, however, suggests that—far below for example our usual notions of space and time—we can in fact have a truly fundamental definition of what’s happening in the universe, ultimately in terms of emes. We don’t yet know the “physical scale” for this—and in the end we presumably need experiments to determine that. But rather rickety estimates based on a variety of assumptions suggest that the elementary length might be around meters, with the elementary time being around seconds. And with these estimates we might conclude that our “experience of a room for a minute” would involve sampling perhaps update events, that create about this number of atoms of space. But it’s immediately clear that this is in a sense a gross underestimate of the total number of emes that we’re sampling. And the reason is that we’re not accounting for quantum mechanics, and for the multiway nature of the evolution of the universe. We’ve so far only considered one “thread of time” at one “position in branchial space”. But in fact there are many threads of time, constantly branching and merging. So how many of these do we experience? In effect that depends on our size in branchial space. In physical space “human scale” is of order a meter—or perhaps elementary lengths. But how big is it in branchial space? The fact that we’re so large compared to the elementary length is the reason that we consistently experience space as something continuous. And the analog in branchial space is that if we’re big compared to the “elementary branchial distance between branches” then we won’t experience the different individual histories of these branches, but only an aggregate “objective reality” in which we conflate together what happens on all the branches. Or, put another way, being large in branchial space is what makes us experience classical physics rather than quantum mechanics. Our estimates for branchial space are even more rickety than for physical space. But conceivably there are on the order of “instantaneous parallel threads of time” in the universe, and encompassed by our instantaneous experience—implying that in our minute-long experience we might sample a total of on the order of close to emes. But even this is a vast underestimate. Yes, it tries to account for our extent in physical space and in branchial space. But then there’s also rulial space—which in effect is what “fills out” the whole ruliad. So how big are we in that space? In essence that’s like asking how many different possible sequences of rules there are that are consistent with our experience. The total conceivable number of sequences associated with emes is roughly the number of possible hypergraphs with nodes—or around . But the actual number consistent with our experience is smaller, in particular as reflected by the fact that we attribute specific laws to our universe. But when we say “specific laws” we have to recognize that there is a finiteness to our efforts at inductive inference which inevitably makes these laws at least somewhat uncertain to us. And in a sense that uncertainty is what represents our “extent in rulial space”. But if we want to count the emes that we “absorb” as physical observers, it’s still going to be a huge number. Perhaps the base may be lower—say —but there’s still a vast exponent, suggesting that if we include our extent in rulial space, we as physical observers may experience numbers of emes like . But let’s say we go beyond our “everyday human-scale experience”. For example, let’s ask about “experiencing” our whole universe. In physical space, the volume of our current universe is about times larger than “human scale” (while human scale is perhaps times larger than the “scale of the atoms of space”). In branchial space, conceivably our current universe is times larger than “human scale”. But these differences absolutely pale in comparison to the sizes associated with rulial space. We might try to go beyond “ordinary human experience” and for example measure things using tools from science and technology. And, yes, we could then think about “experiencing” lengths down to meters, or something close to “single threads” of quantum histories. But in the end, it’s still the rulial size that dominates, and that’s where we can expect most of the vast number of emes that form of our experience of the physical universe to come from. OK, so what about mathematics? When we think about what we might call human-scale mathematics, and talk about things like the Pythagorean theorem, how many emes are there “underneath”? “Compiling” our theorem down to typical traditional mathematical axioms, we’ve seen that we’ll routinely end up with expressions containing, say, symbolic elements. But what happens if we go “below that”, compiling these symbolic elements—which might include things like variables and operators—into “pure computational elements” that we can think of as emes? We’ve seen a few examples, say with combinators, that suggest that for the traditional axiomatic structures of mathematics, we might need another factor of maybe roughly . These are incredibly rough estimates, but perhaps there’s a hint that there’s “further to go” to get from human-scale for a physical observer down to atoms of space that correspond to emes, than there is to get from human-scale for a mathematical observer down to emes. Just like in physics, however, this kind of “static drill-down” isn’t the whole story for mathematics. When we talk about something like the Pythagorean theorem, we’re really referring to a whole cloud of “human-equivalent” points in metamathematical space. The total number of “possible points” is basically the size of the entailment cone that contains something like the Pythagorean theorem. The “height” of the entailment cone is related to typical lengths of proofs—which for current human mathematics might be perhaps hundreds of steps. And this would lead to overall sizes of entailment cones of very roughly theorems. But within this “how big” is the cloud of variants corresponding to particular “human-recognized” theorems? Empirical metamathematics could provide additional data on this question. But if we very roughly imagine that half of every proof is “flexible”, we’d end up with things like variants. So if we asked how many emes correspond to the “experience” of the Pythagorean theorem, it might be, say, . To give an analogy of “everyday physical experience” we might consider a mathematician thinking about mathematical concepts, and maybe in effect pondering a few tens of theorems per minute—implying according to our extremely rough and speculative estimates that while typical “specific human-scale physics experience” might involve emes, specific human-scale mathematics experience might involve emes (a number comparable, for example, to the number of physical atoms in our universe). What if instead of considering “everyday mathematical experience” we consider all humanly explored mathematics? On the scales we’re describing, the factors are not large. In the history of human mathematics, only a few million theorems have been published. If we think about all the computations that have been done in the service of mathematics, it’s a somewhat larger factor. I suspect Mathematica is the dominant contributor here—and we can estimate that the total number of Wolfram Language operations corresponding to “human-level mathematics” done so far is perhaps. But just like for physics, all these numbers pale in comparison with those introduced by rulial sizes. We’ve talked essentially about a particular path from emes through specific axioms to theorems. But the ruliad in effect contains all possible axiom systems. And if we start thinking about enumerating these—and effectively “populating all of rulial space”—we’ll end up with exponentially more emes. But as with the perceived laws of physics, in mathematics as done by humans it’s actually just a narrow slice of rulial space that we’re sampling. It’s like a generalization of the idea that something like arithmetic as we imagine it can be derived from a whole cloud of possible axiom systems. It’s not just one axiom system; but it’s also not all possible axiom systems. One can imagine doing some combination of ruliology and empirical metamathematics to get an estimate of “how broad” human-equivalent axiom systems (and their construction from emes) might be. But the answer seems likely to be much smaller than the kinds of sizes we have been estimating for physics. It’s important to emphasize that what we’ve discussed here is extremely rough—and speculative. And indeed I view its main value as being to provide an example of how to imagine thinking through things in the context of the ruliad and the framework around it. But on the basis of what we’ve discussed, we might make the very tentative conclusion that “human-experienced physics” is bigger than “human-experienced mathematics”. Both involve vast numbers of emes. But physics seems to involve a lot more. In a sense—even with all its abstraction—the suspicion is that there’s “less ultimately in mathematics” as far as we’re concerned than there is in physics. Though by any ordinary human standards, mathematics still involves absolutely vast numbers of emes.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.30 Some Historical (and Philosophical) Background : The human activity that we now call “mathematics” can presumably trace its origins into prehistory. What might have started as “a single goat”, “a pair of goats”, etc. became a story of abstract numbers that could be indicated purely by things like tally marks. In Babylonian times the practicalities of a city-based society led to all sorts of calculations involving arithmetic and geometry—and basically everything we now call “mathematics” can ultimately be thought of as a generalization of these ideas. The tradition of philosophy that emerged in Greek times saw mathematics as a kind of reasoning. But while much of arithmetic (apart from issues of infinity and infinitesimals) could be thought of in explicit calculational ways, precise geometry immediately required an idealization—specifically the concept of a point having no extent, or equivalently, the continuity of space. And in an effort to reason on top of this idealization, there emerged the idea of defining axioms and making abstract deductions from them. But what kind of a thing actually was mathematics? Plato talked about things we sense in the external world, and things we conceptualize in our internal thoughts. But he considered mathematics to be at its core an example of a third kind of thing: something from an abstract world of ideal forms. And with our current thinking, there is an immediate resonance between this concept of ideal forms and the concept of the ruliad. But for most of the past two millennia of the actual development of mathematics, questions about what it ultimately was lay in the background. An important step was taken in the late 1600s when Newton and others “mathematicized” mechanics, at first presenting what they did in the form of axioms similar to Euclid’s. Through the 1700s mathematics as a practical field was viewed as some kind of precise idealization of features of the world—though with an increasingly elaborate tower of formal derivations constructed in it. Philosophy, meanwhile, typically viewed mathematics—like logic—mostly as an example of a system in which there was a formal process of derivation with a “necessary” structure not requiring reference to the real world. But in the first half of the 1800s there arose several examples of systems where axioms—while inspired by features of the world—ultimately seemed to be “just invented” (e.g. group theory, curved space, quaternions, Boolean algebra, …). A push towards increasing rigor (especially for calculus and the nature of real numbers) led to more focus on axiomatization and formalization—which was still further emphasized by the appearance of a few non-constructive “purely formal” proofs. But if mathematics was to be formalized, what should its underlying primitives be? One obvious choice seemed to be logic, which had originally been developed by Aristotle as a kind of catalog of human arguments, but two thousand years later felt basic and inevitable. And so it was that Frege, followed by Whitehead and Russell, tried to start “constructing mathematics” from “pure logic” (along with set theory). Logic was in a sense a rather low-level “machine code”, and it took hundreds of pages of unreadable (if impressive-looking) “code” for Whitehead and Russell, in their 1910 Principia Mathematica, to get to 1 + 1 = 2. Meanwhile, starting around 1900, Hilbert took a slightly different path, essentially representing everything with what we would now call symbolic expressions, and setting up axioms as relations between these. But what axioms should be used? Hilbert seemed to feel that the core of mathematics lay not in any “external meaning” but in the pure formal structure built up from whatever axioms were used. And he imagined that somehow all the truths of mathematics could be “mechanically derived” from axioms, a bit, as he said in a certain resonance with our current views, like the “great calculating machine, Nature” does it for physics. Not all mathematicians, however, bought into this “formalist” view of what mathematics is. And in 1931 Gödel managed to prove from inside the formal axiom system traditionally used for arithmetic that this system had a fundamental incompleteness that prevented it from ever having anything to say about certain mathematical statements. But Gödel seems to have maintained a more Platonic belief about mathematics: that even though the axiomatic method falls short, the truths of mathematics are in some sense still “all there”, and it’s potentially possible for the human mind to have “direct access” to them. And while this is not quite the same as our picture of the mathematical observer accessing the ruliad, there’s again some definite resonance here. But, OK, so how has mathematics actually conducted itself over the past century? Typically there’s at least lip service paid to the idea that there are “axioms underneath”—usually assumed to be those from set theory. There’s been significant emphasis placed on the idea of formal deduction and proof—but not so much in terms of formally building up from axioms as in terms of giving narrative expositions that help humans understand why some theorem might follow from other things they know. There’s been a field of “mathematical logic” concerned with using mathematics-like methods to explore mathematics-like aspects of formal axiomatic systems. But (at least until very recently) there’s been rather little interaction between this and the “mainstream” study of mathematics. And for example phenomena like undecidability that are central to mathematical logic have seemed rather remote from typical pure mathematics—even though many actual long-unsolved problems in mathematics do seem likely to run into it. But even if formal axiomatization may have been something of a sideshow for mathematics, its ideas have brought us what is without much doubt the single most important intellectual breakthrough of the twentieth century: the abstract concept of computation. And what’s now become clear is that computation is in some fundamental sense much more general than mathematics. At a philosophical level one can view the ruliad as containing all computation. But mathematics (at least as it’s done by humans) is defined by what a “mathematical observer like us” samples and perceives in the ruliad. The most common “core workflow” for mathematicians doing pure mathematics is first to imagine what might be true (usually through a process of intuition that feels a bit like making “direct access to the truths of mathematics”)—and then to “work backwards” to try to construct a proof. As a practical matter, though, the vast majority of “mathematics done in the world” doesn’t follow this workflow, and instead just “runs forward”—doing computation. And there’s no reason for at least the innards of that computation to have any “humanized character” to it; it can just involve the raw processes of computation. But the traditional pure mathematics workflow in effect depends on using “human-level” steps. Or if, as we described earlier, we think of low-level axiomatic operations as being like molecular dynamics, then it involves operating at a “fluid dynamics” level. A century ago efforts to “globally understand mathematics” centered on trying to find common axiomatic foundations for everything. But as different areas of mathematics were explored (and particularly ones like algebraic topology that cut across existing disciplines) it began to seem as if there might also be “top-down” commonalities in mathematics, in effect directly at the “fluid dynamics” level. And within the last few decades, it’s become increasingly common to use ideas from category theory as a general framework for thinking about mathematics at a high level. But there’s also been an effort to progressively build up—as an abstract matter—formal “higher category theory”. A notable feature of this has been the appearance of connections to both geometry and mathematical logic—and for us a connection to the ruliad and its features. The success of category theory has led in the past decade or so to interest in other high-level structural approaches to mathematics. A notable example is homotopy type theory. The basic concept is to characterize mathematical objects not by using axioms to describe properties they should have, but instead to use “types” to say “what the objects are” (for example, “mapping from reals to integers”). Such type theory has the feature that it tends to look much more “immediately computational” than traditional mathematical structures and notation—as well as making explicit proofs and other metamathematical concepts. And in fact questions about types and their equivalences wind up being very much like the questions we’ve discussed for the multiway systems we’re using as metamodels for mathematics. Homotopy type theory can itself be set up as a formal axiomatic system—but with axioms that include what amount to metamathematical statements. A key example is the univalence axiom which essentially states that things that are equivalent can be treated as the same. And now from our point of view here we can see this being essentially a statement of metamathematical coarse graining—and a piece of defining what should be considered “mathematics” on the basis of properties assumed for a mathematical observer. When Plato introduced ideal forms and their distinction from the external and internal world the understanding of even the fundamental concept of computation—let alone multicomputation and the ruliad—was still more than two millennia in the future. But now our picture is that everything can in a sense be viewed as part of the world of ideal forms that is the ruliad—and that not only mathematics but also physical reality are in effect just manifestations of these ideal forms. But a crucial aspect is how we sample the “ideal forms” of the ruliad. And this is where the “contingent facts” about us as human “observers” enter. The formal axiomatic view of mathematics can be viewed as providing one kind of low-level description of the ruliad. But the point is that this description isn’t aligned with what observers like us perceive—or with what we will successfully be able to view as human-level mathematics. A century ago there was a movement to take mathematics (as well, as it happens, as other fields) beyond its origins in what amount to human perceptions of the world. But what we now see is that while there is an underlying “world of ideal forms” embodied in the ruliad that has nothing to do with us humans, mathematics as we humans do it must be associated with the particular sampling we make of that underlying structure. And it’s not as if we get to pick that sampling “at will”; the sampling we do is the result of fundamental features of us as humans. And an important point is that those fundamental features determine our characteristics both as mathematical observers and as physical observers. And this fact leads to a deep connection between our experience of physics and our definition of mathematics. Mathematics historically began as a formal idealization of our human perception of the physical world. Along the way, though, it began to think of itself as a more purely abstract pursuit, separated from both human perception and the physical world. But now, with the general idea of computation, and more specifically with the concept of the ruliad, we can in a sense see what the limit of such abstraction would be. And interesting though it is, what we’re now discovering is that it’s not the thing we call mathematics. And instead, what we call mathematics is something that is subtly but deeply determined by general features of human perception—in fact, essentially the same features that also determine our perception of the physical world. The intellectual foundations and justification are different now. But in a sense our view of mathematics has come full circle. And we can now see that mathematics is in fact deeply connected to the physical world and our particular perception of it. And we as humans can do what we call mathematics for basically the same reason that we as humans manage to parse the physical world to the point where we can do science about it.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.31 Implications for the Future of Mathematics : Having talked a bit about historical context let’s now talk about what the things we’ve discussed here mean for the future of mathematics—both in theory and in practice. At a theoretical level we’ve characterized the story of mathematics as being the story of a particular way of exploring the ruliad. And from this we might think that in some sense the ultimate limit of mathematics would be to just deal with the ruliad as a whole. But observers like us—at least doing mathematics the way we normally do it—simply can’t do that. And in fact, with the limitations we have as mathematical observers we can inevitably sample only tiny slices of the ruliad. But as we’ve discussed, it is exactly this that leads us to experience the kinds of “general laws of mathematics” that we’ve talked about. And it is from these laws that we get a picture of the “large-scale structure of mathematics”—that turns out to be in many ways similar to the picture of the large-scale structure of our physical universe that we get from physics. As we’ve discussed, what corresponds to the coherent structure of physical space is the possibility of doing mathematics in terms of high-level concepts—without always having to drop down to the “atomic” level. Effective uniformity of metamathematical space then leads to the idea of “pure metamathematical motion”, and in effect the possibility of translating at a high level between different areas of mathematics. And what this suggests is that in some sense “all high-level areas of mathematics” should ultimately be connected by “high-level dualities”—some of which have already been seen, but many of which remain to be discovered. Thinking about metamathematics in physicalized terms also suggests another phenomenon: essentially an analog of gravity for metamathematics. As we discussed earlier, in direct analogy to the way that “larger densities of activity” in the spatial hypergraph for physics lead to a deflection in geodesic paths in physical space, so also larger “entailment density” in metamathematical space will lead to deflection in geodesic paths in metamathematical space. And when the entailment density gets sufficiently high, it presumably becomes inevitable that these paths will all converge, leading to what one might think of as a “metamathematical singularity”. In the spacetime case, a typical analog would be a place where all geodesics have finite length, or in effect “time stops”. In our view of metamathematics, it corresponds to a situation where “all proofs are finite”—or, in other words, where everything is decidable, and there is no more “fundamental difficulty” left. Absent other effects we might imagine that in the physical universe the effects of gravity would eventually lead everything to collapse into black holes. And the analog in metamathematics would be that everything in mathematics would “collapse” into decidable theories. But among the effects not accounted for is continued expansion—or in effect the creation of new physical or metamathematical space, formed in a sense by underlying raw computational processes. What will observers like us make of this, though? In statistical mechanics an observer who does coarse graining might perceive the “heat death of the universe”. But at a molecular level there is all sorts of detailed motion that reflects a continued irreducible process of computation. And inevitably there will be an infinite collection of possible “slices of reducibility” to be found in this—just not necessarily ones that align with any of our current capabilities as observers. What does this mean for mathematics? Conceivably it might suggest that there’s only so much that can fundamentally be discovered in “high-level mathematics” without in effect “expanding our scope as observers”—or in essence changing our definition of what it is we humans mean by doing mathematics. But underneath all this is still raw computation—and the ruliad. And this we know goes on forever, in effect continually generating “irreducible surprises”. But how should we study “raw computation”? In essence we want to do unfettered exploration of the computational universe, of the kind I did in A New Kind of Science, and that we now call the science of ruliology. It’s something we can view as more abstract and more fundamental than mathematics—and indeed, as we’ve argued, it’s for example what’s underneath not only mathematics but also physics. Ruliology is a rich intellectual activity, important for example as a source of models for many processes in nature and elsewhere. But it’s one where computational irreducibility and undecidability are seen at almost every turn—and it’s not one where we can readily expect “general laws” accessible to observers like us, of the kind we’ve seen in physics, and now see in mathematics. We’ve argued that with its foundation in the ruliad mathematics is ultimately based on structures lower level than axiom systems. But given their familiarity from the history of mathematics, it’s convenient to use axiom systems—as we have done here—as a kind of “intermediate-scale metamodel” for mathematics. But what is the “workflow” for using axiom systems? One possibility in effect inspired by ruliology is just to systematically construct the entailment cone for an axiom system, progressively generating all possible theorems that the axiom system implies. But while doing this is of great theoretical interest, it typically isn’t something that will in practice reach much in the way of (currently) familiar mathematical results. But let’s say one’s thinking about a particular result. A proof of this would correspond to a path within the entailment cone. And the idea of automated theorem proving is to systematically find such a path—which, with a variety of tricks, can usually be done vastly more efficiently than just by enumerating everything in the entailment cone. In practice, though, despite half a century of history, automated theorem proving has seen very little use in mainstream mathematics. Of course it doesn’t help that in typical mathematical work a proof is seen as part of the high-level exposition of ideas—but automated proofs tend to operate at the level of “axiomatic machine code” without any connection to human-level narrative. But if one doesn’t already know the result one’s trying to prove? Part of the intuition that comes from A New Kind of Science is that there can be “interesting results” that are still simple enough that they can conceivably be found by some kind of explicit search—and then verified by automated theorem proving. But so far as I know, only one significant unexpected result has so far ever been found in this way with automated theorem proving: my 2000 result on the simplest axiom system for Boolean algebra. And the fact is that when it comes to using computers for mathematics, the overwhelming fraction of the time they’re used not to construct proofs, but instead to do “forward computations” and “get results” (yes, often with Mathematica). Of course, within those forward computations, there are many operations—like Reduce, SatisfiableQ, PrimeQ, etc.—that essentially work by internally finding proofs, but their output is “just results” not “why-it’s-true explanations”. (FindEquationalProof—as its name suggests—is a case where an actual proof is generated.) Whether one’s thinking in terms of axioms and proofs, or just in terms of “getting results”, one’s ultimately always dealing with computation. But the key question is how that computation is “packaged”. Is one dealing with arbitrary, raw, low-level constructs, or with something higher level and more “humanized”? As we’ve discussed, at the lowest level, everything can be represented in terms of the ruliad. But when we do both mathematics and physics what we’re perceiving is not the raw ruliad, but rather just certain high-level features of it. But how should these be represented? Ultimately we need a language that we humans understand, that captures the particular features of the underlying raw computation that we’re interested in. From our computational point of view, mathematical notation can be thought of as a rough attempt at this. But the most complete and systematic effort in this direction is the one I’ve worked towards for the past several decades: what’s now the full-scale computational language that is the Wolfram Language (and Mathematica). Ultimately the Wolfram Language can represent any computation. But the point is to make it easy to represent the computations that people care about: to capture the high-level constructs (whether they’re polynomials, geometrical objects or chemicals) that are part of modern human thinking. The process of language design (on which, yes, I’ve spent immense amounts of time) is a curious mixture of art and science, that requires both drilling down to the essence of things, and creatively devising ways to make those things accessible and cognitively convenient for humans. At some level it’s a bit like deciding on words as they might appear in a human language—but it’s something more structured and demanding. And it’s our best way of representing “high-level” mathematics: mathematics not at the axiomatic (or below) “machine code” level, but instead at the level human mathematicians typically think about it. We’ve definitely not “finished the job”, though. Wolfram Language currently has around 7000 built-in primitive constructs, of which at least a couple of thousand can be considered “primarily mathematical”. But while the language has long contained constructs for algebraic numbers, random walks and finite groups, it doesn’t (yet) have built-in constructs for algebraic topology or K-theory. In recent years we’ve been slowly adding more kinds of pure-mathematical constructs—but to reach the frontiers of modern human mathematics might require perhaps a thousand more. And to make them useful all of them will have to be carefully and coherently designed. The great power of the Wolfram Language comes not only from being able to represent things computationally, but also being able to compute with things, and get results. And it’s one thing to be able to represent some pure mathematical construct—but quite another to be able to broadly compute with it. The Wolfram Language in a sense emphasizes the “forward computation” workflow. Another workflow that’s achieved some popularity in recent years is the proof assistant one—in which one defines a result and then as a human one tries to fill in the steps to create a proof of it, with the computer verifying that the steps correctly fit together. If the steps are low level then what one has is something like typical automated theorem proving—though now being attempted with human effort rather than being done automatically. In principle one can build up to much higher-level “steps” in a modular way. But now the problem is essentially the same as in computational language design: to create primitives that are both precise enough to be immediately handled computationally, and “cognitively convenient” enough to be usefully understood by humans. And realistically once one’s done the design (which, after decades of working on such things, I can say is hard), there’s likely to be much more “leverage” to be had by letting the computer just do computations than by expending human effort (even with computer assistance) to put together proofs. One might think that a proof would be important in being sure one’s got the right answer. But as we’ve discussed, that’s a complicated concept when one’s dealing with human-level mathematics. If we go to a full axiomatic level it’s very typical that there will be all sorts of pedantic conditions involved. Do we have the “right answer” if underneath we assume that 1/0=0? Or does this not matter at the “fluid dynamics” level of human mathematics? One of the great things about computational language is that—at least if it’s written well—it provides a clear and succinct specification of things, just like a good “human proof” is supposed to. But computational language has the great advantage that it can be run to create new results—rather than just being used to check something. It’s worth mentioning that there’s another potential workflow beyond “compute a result” and “find a proof”. It’s “here’s an object or a set of constraints for creating one; now find interesting facts about this”. Type into Wolfram|Alpha something like sin^4(x) (and, yes, there’s “natural math understanding” needed to translate something like this to precise Wolfram Language). There’s nothing obvious to “compute” here. But instead what Wolfram|Alpha does is to “say interesting things” about this—like what its maximum or its integral over a period is. In principle this is a bit like exploring the entailment cone—but with the crucial additional piece of picking out which entailments will be “interesting to humans”. (And implementationally it’s a very deeply constrained exploration.) It’s interesting to compare these various workflows with what one can call experimental mathematics. Sometimes this term is basically just applied to studying explicit examples of known mathematical results. But the much more powerful concept is to imagine discovering new mathematical results by “doing experiments”. Usually these experiments are not done at the level of axioms, but rather at a considerably higher level (e.g. with things specified using the primitives of Wolfram Language). But the typical pattern is to enumerate a large number of cases and to see what happens—with the most exciting result being the discovery of some unexpected phenomenon, regularity or irregularity. This type of approach is in a sense much more general than mathematics: it can be applied to anything computational, or anything described by rules. And indeed it is the core methodology of ruliology, and what it does to explore the computational universe—and the ruliad. One can think of the typical approach in pure mathematics as representing a gradual expansion of the entailment fabric, with humans checking (perhaps with a computer) statements they consider adding. Experimental mathematics effectively strikes out in some “direction” in metamathematical space, potentially jumping far away from the entailment fabric currently within the purview of some mathematical observer. And one feature of this—very common in ruliology—is that one may run into undecidability. The “nearby” entailment fabric of the mathematical observer is in a sense “filled in enough” that it doesn’t typically have infinite proof paths of the kind associated with undecidability. But something reached by experimental mathematics has no such guarantee. What’s good of course is that experimental mathematics can discover phenomena that are “far away” from existing mathematics. But (like in automated theorem proving) there isn’t necessarily any human-accessible “narrative explanation” (and if there’s undecidability there may be no “finite explanation” at all). So how does this all relate to our whole discussion of new ideas about the foundations of mathematics? In the past we might have thought that mathematics must ultimately progress just by working out more and more consequences of particular axioms. But what we’ve argued is that there’s a fundamental infrastructure even far below axiom systems—whose low-level exploration is the subject of ruliology. But the thing we call mathematics is really something higher level. Axiom systems are some kind of intermediate modeling layer—a kind of “assembly language” that can be used as a wrapper above the “raw ruliad”. In the end, we’ve argued, the details of this language won’t matter for typical things we call mathematics. But in a sense the situation is very much like in practical computing: we want an “assembly language” that makes it easiest to do the typical high-level things we want. In practical computing that’s often achieved with RISC instruction sets. In mathematics we typically imagine using axiom systems like ZFC. But—as reverse mathematics has tended to indicate—there are probably much more accessible axiom systems that could be used to reach the mathematics we want. (And ultimately even ZFC is limited in what it can reach.) But if we could find such a “RISC” axiom system for mathematics it has the potential to make practical more extensive exploration of the entailment cone. It’s also conceivable—though not guaranteed—that it could be “designed” to be more readily understood by humans. But in the end actual human-level mathematics will typically operate at a level far above it. And now the question is whether the “physicalized general laws of mathematics” that we’ve discussed can be used to make conclusions directly about human-level mathematics. We’ve identified a few features—like the very possibility of high-level mathematics, and the expectation of extensive dualities between mathematical fields. And we know that basic commonalities in structural features can be captured by things like category theory. But the question is what kinds of deeper general features can be found, and used. In physics our everyday experience immediately makes us think about “large-scale features” far above the level of atoms of space. In mathematics our typical experience so far has been at a lower level. So now the challenge is to think more globally, more metamathematically and, in effect, more like in physics. In the end, though, what we call mathematics is what mathematical observers perceive. So if we ask about the future of mathematics we must also ask about the future of mathematical observers. If one looks at the history of physics there was already much to understand just on the basis of what we humans could “observe” with our unaided senses. But gradually as more kinds of detectors became available—from microscopes to telescopes to amplifiers and so on—the domain of the physical observer was expanded, and the perceived laws of physics with it. And today, as the practical computational capability of observers increases, we can expect that we’ll gradually see new kinds of physical laws (say associated with hitherto “it’s just random” molecular motion or other features of systems). As we’ve discussed above, we can see our characteristics as physical observers as being associated with “experiencing” the ruliad from one particular “vantage point” in rulial space (just as we “experience” physical space from one particular vantage point in physical space). Putative “aliens” might experience the ruliad from a different vantage point in rulial space—leading them to have laws of physics utterly incoherent with our own. But as our technology and ways of thinking progress, we can expect that we’ll gradually be able to expand our “presence” in rulial space (just as we do with spacecraft and telescopes in physical space). And so we’ll be able to “experience” different laws of physics. We can expect the story to be very similar for mathematics. We have “experienced” mathematics from a certain vantage point in the ruliad. Putative aliens might experience it from another point, and build their own “paramathematics” utterly incoherent with our mathematics. The “natural evolution” of our mathematics corresponds to a gradual expansion in the entailment fabric, and in a sense a gradual spreading in rulial space. Experimental mathematics has the potential to launch a kind of “metamathematical space probe” which can discover quite different mathematics. At first, though, this will tend to be a piece of “raw ruliology”. But, if pursued, it potentially points the way to a kind of “colonization of rulial space” that will gradually expand the domain of the mathematical observer. The physicalized general laws of mathematics we’ve discussed here are based on features of current mathematical observers (which in turn are highly based on current physical observers). What these laws would be like with “enhanced” mathematical observers we don’t yet know. Mathematics as it is today is a great example of the “humanization of raw computation”. Two other examples are theoretical physics and computational language. And in all cases there is the potential to gradually expand our scope as observers. It’ll no doubt be a mixture of technology and methods along with expanded cognitive frameworks and understanding. We can use ruliology—or experimental mathematics—to “jump out” into the raw ruliad. But most of what we’ll see is “non-humanized” computational irreducibility. But perhaps somewhere there’ll be another slice of computational reducibility: a different “island” on which “alien” general laws can be built. But for now we exist on our current “island” of reducibility. And on this island we see the particular kinds of general laws that we’ve discussed. We saw them first in physics. But there we discovered that they could emerge quite generically from a lower-level computational structure—and ultimately from the very general structure that we call the ruliad. And now, as we’ve discussed here, we realize that the thing we call mathematics is actually based on exactly the same foundations—with the result that it should show the same kinds of general laws. It’s a rather different view of mathematics—and its foundations—than we’ve been able to form before. But the deep connection with physics that we’ve discussed allows us to now have a physicalized view of metamathematics, which informs both what mathematics really is now, and what the future can hold for the remarkable pursuit that we call mathematics.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.32 Some Personal History: The Evolution of These Ideas : It’s been a long personal journey to get to the ideas described here—stretching back nearly 45 years. Parts have been quite direct, steadily building over the course of time. But other parts have been surprising—even shocking. And to get to where we are now has required me to rethink some very long-held assumptions, and adopt what I had believed was a rather different way of thinking—even though, ironically, I’ve realized in the end that many aspects of this way of thinking pretty much mirror what I’ve done all along at a practical and technological level. Back in the late 1970s as a young theoretical physicist I had discovered the “secret weapon” of using computers to do mathematical calculations. By 1979 I had outgrown existing systems and decided to build my own. But what should its foundations be? A key goal was to represent the processes of mathematics in a computational way. I thought about the methods I’d found effective in practice. I studied the history of mathematical logic. And in the end I came up with what seemed to me at the time the most obvious and direct approach: that everything should be based on transformations for symbolic expressions. I was pretty sure this was actually a good general approach to computation of all kinds—and the system we released in 1981 was named SMP (“Symbolic Manipulation Program”) to reflect this generality. History has indeed borne out the strength of the symbolic expression paradigm—and it’s from that we’ve been able to build the huge tower of technology that is the modern Wolfram Language. But all along mathematics has been an important use case—and in effect we’ve now seen four decades of validation that the core idea of transformations on symbolic expressions is a good metamodel of mathematics. When Mathematica was first released in 1988 we called it “A System for Doing Mathematics by Computer”, where by “doing mathematics” we meant doing computations in mathematics and getting results. People soon did all sorts of experiments on using Mathematica to create and present proofs. But the overwhelming majority of actual usage was for directly computing results—and almost nobody seemed interested in seeing the inner workings, presented as a proof or otherwise. But in the 1980s I had started my work on exploring the computational universe of simple programs like cellular automata. And doing this was all about looking at the ongoing behavior of systems—or in effect the (often computationally irreducible) history of computations. And even though I sometimes talked about using my computational methods to do “experimental mathematics”, I don’t think I particularly thought about the actual progress of the computations I was studying as being like mathematical processes or proofs. In 1991 I started working on what became A New Kind of Science, and in doing so I tried to systematically study possible forms of computational processes—and I was soon led to substitution systems and symbolic systems which I viewed in their different ways as being minimal idealizations of what would become Wolfram Language, as well as to multiway systems. There were some areas to which I was pretty sure the methods of A New Kind of Science would apply. Three that I wasn’t sure about were biology, physics and mathematics. But by the late 1990s I had worked out quite a bit about the first two, and started looking at mathematics. I knew that Mathematica and what would become Wolfram Language were good representations of “practical mathematics”. But I assumed that to understand the foundations of mathematics I should look at the traditional low-level representation of mathematics: axiom systems. And in doing this I was soon able to simplify to multiway systems—with proofs being paths: I had long wondered what the detailed relationships between things like my idea of computational irreducibility and earlier results in mathematical logic were. And I was pleased at how well many things could be clarified—and explicitly illustrated—by thinking in terms of multiway systems. My experience in exploring simple programs in general had led to the conclusion that computational irreducibility and therefore undecidability were quite ubiquitous. So I considered it quite a mystery why undecidability seemed so rare in the mathematics that mathematicians typically did. I suspected that in fact undecidability was lurking close at hand—and I got some evidence of that by doing experimental mathematics. But why weren’t mathematicians running into this more? I came to suspect that it had something to do with the history of mathematics, and with the idea that mathematics had tended to expand its subject matter by asking “How can this be generalized while still having such-and-such a theorem be true?” But I also wondered about the particular axiom systems that had historically been used for mathematics. They all fit easily on a couple of pages. But why these and not others? Following my general “ruliological” approach of exploring all possible systems I started just enumerating possible axiom systems—and soon found out that many of them had rich and complicated implications. But where among these possible systems did the axiom systems historically used in mathematics lie? I did searches, and at about the 50,000th axiom was able to find the simplest axiom system for Boolean algebra. Proving that it was correct gave me my first serious experience with automated theorem proving. But what kind of a thing was the proof? I made some attempt to understand it, but it was clear that it wasn’t something a human could readily understand—and reading it felt a bit like trying to read machine code. I recognized that the problem was in a sense a lack of “human connection points”—for example of intermediate lemmas that (like words in a human language) had a contextualized significance. I wondered about how one could find lemmas that “humans would care about”? And I was surprised to discover that at least for the “named theorems” of Boolean algebra a simple criterion could reproduce them. Quite a few years went by. Off and on I thought about two ultimately related issues. One was how to represent the execution histories of Wolfram Language programs. And the other was how to represent proofs. In both cases there seemed to be all sorts of detail, and it seemed difficult to have a structure that would capture what would be needed for further computation—or any kind of general understanding. Meanwhile, in 2009, we released Wolfram|Alpha. One of its features was that it had “step-by-step” math computations. But these weren’t “general proofs”: rather they were narratives synthesized in very specific ways for human readers. Still, a core concept in Wolfram|Alpha—and the Wolfram Language—is the idea of integrating in knowledge about as many things as possible in the world. We’d done this for cities and movies and lattices and animals and much more. And I thought about doing it for mathematical theorems as well. We did a pilot project—on theorems about continued fractions. We trawled through the mathematical literature assessing the difficulty of extending the “natural math understanding” we’d built for Wolfram|Alpha. I imagined a workflow which would mix automated theorem generation with theorem search—in which one would define a mathematical scenario, then say “tell me interesting facts about this”. And in 2014 we set about engaging the mathematical community in a large-scale curation effort to formalize the theorems of mathematics. But try as we might, only people already involved in math formalization seemed to care; with few exceptions working mathematicians just didn’t seem to consider it relevant to what they did. We continued, however, to push slowly forward. We worked with proof assistant developers. We curated various kinds of mathematical structures (like function spaces). I had estimated that we’d need more than a thousand new Wolfram Language functions to cover “modern pure mathematics”, but without a clear market we couldn’t motivate the huge design (let alone implementation) effort that would be needed—though, partly in a nod to the intellectual origins of mathematics, we did for example do a project that has succeeded in finally making Euclid-style geometry computable. Then in the latter part of the 2010s a couple more “proof-related” things happened. Back in 2002 we’d started using equational logic automated theorem proving to get results in functions like FullSimplify. But we hadn’t figured out how to present the proofs that were generated. In 2018 we finally introduced FindEquationalProof—allowing programmatic access to proofs, and making it feasible for me to explore collections of proofs in bulk. I had for decades been interested in what I’ve called “symbolic discourse language”: the extension of the idea of computational language to “everyday discourse”—and to the kind of thing one might want for example to express in legal contracts. And between this and our involvement in the idea of computational contracts, and things like blockchain technology, I started exploring questions of AI ethics and “constitutions”. At this point we’d also started to introduce machine-learning-based functions into the Wolfram Language. And—with my “human incomprehensible” Boolean algebra proof as “empirical data”—I started exploring general questions of explainability, and in effect proof. And not long after that came the surprise breakthrough of our Physics Project. Extending my ideas from the 1990s about computational foundations for fundamental physics it suddenly became possible finally to understand the underlying origins of the main known laws of physics. And core to this effort—and particularly to the understanding of quantum mechanics—were multiway systems. At first we just used the knowledge that multiway systems could also represent axiomatic mathematics and proofs to provide analogies for our thinking about physics (“quantum observers might in effect be doing critical-pair completions”, “causal graphs are like higher categories”, etc.) But then we started wondering whether the phenomenon of the emergence that we’d seen for the familiar laws of physics might also affect mathematics—and whether it could give us something like a “bulk” version of metamathematics. I had long studied the transition from discrete “computational” elements to “bulk” behavior, first following my interest in the Second Law of thermodynamics, which stretched all the way back to age 12 in 1972, then following my work on cellular automaton fluids in the mid-1980s, and now with the emergence of physical space from underlying hypergraphs in our Physics Project. But what might “bulk” metamathematics be like? One feature of our Physics Project—in fact shared with thermodynamics—is that certain aspects of its observed behavior depend very little on the details of its components. But what did they depend on? We realized that it all had to do with the observer—and their interaction (according to what I’ve described as the 4th paradigm for science) with the general “multicomputational” processes going on underneath. For physics we had some idea what characteristics an “observer like us” might have (and actually they seemed to be closely related to our notion of consciousness). But what might a “mathematical observer” be like? In its original framing we talked about our Physics Project as being about “finding the rule for the universe”. But right around the time we launched the project we realized that that wasn’t really the right characterization. And we started talking about rulial multiway systems that instead “run every rule”—but in which an observer perceives only some small slice, that in particular can show emergent laws of physics. But what is this “run every rule” structure? In the end it’s something very fundamental: the entangled limit of all possible computations—that I call the ruliad. The ruliad basically depends on nothing: it’s unique and its structure is a matter of formal necessity. So in a sense the ruliad “necessarily exists”—and, I argued, so must our universe. But we can think of the ruliad not only as the foundation for physics, but also as the foundation for mathematics. And so, I concluded, if we believe that the physical universe exists, then we must conclude—a bit like Plato—that mathematics exists too. But how did all this relate to axiom systems and ideas about metamathematics? I had two additional pieces of input from the latter half of 2020. First, following up on a note in A New Kind of Science, I had done an extensive study of the “empirical metamathematics” of the network of the theorems in Euclid, and in a couple of math formalization systems. And second, in celebration of the 100th anniversary of their invention essentially as “primitives for mathematics”, I had done an extensive ruliological and other study of combinators. I began to work on this current piece in the fall of 2020, but felt there was something I was missing. Yes, I could study axiom systems using the formalism of our Physics Project. But was this really getting at the essence of mathematics? I had long assumed that axiom systems really were the “raw material” of mathematics—even though I’d long gotten signals they weren’t really a good representation of how serious, aesthetically oriented pure mathematicians thought about things. In our Physics Project we’d always had as a target to reproduce the known laws of physics. But what should the target be in understanding the foundations of mathematics? It always seemed like it had to revolve around axiom systems and processes of proof. And it felt like validation when it became clear that the same concepts of “substitution rules applied to expressions” seemed to span my earliest efforts to make math computational, the underlying structure of our Physics Project, and “metamodels” of axiom systems. But somehow the ruliad—and the idea that if physics exists so must math—made me realize that this wasn’t ultimately the right level of description. And that axioms were some kind of intermediate level, between the “raw ruliad”, and the “humanized” level at which pure mathematics is normally done. At first I found this hard to accept; not only had axiom systems dominated thinking about the foundations of mathematics for more than a century, but they also seemed to fit so perfectly into my personal “symbolic rules” paradigm. But gradually I got convinced that, yes, I had been wrong all this time—and that axiom systems were in many respects missing the point. The true foundation is the ruliad, and axiom systems are a rather-hard-to-work-with “machine-code-like” description below the inevitable general “physicalized laws of metamathematics” that emerge—and that imply that for observers like us there’s a fundamentally higher-level approach to mathematics. At first I thought this was incompatible with my general computational view of things. But then I realized: “No, quite the opposite!” All these years I’ve been building the Wolfram Language precisely to connect “at a human level” with computational processes—and with mathematics. Yes, it can represent and deal with axiom systems. But it’s never felt particularly natural. And it’s because they’re at an awkward level—neither at the level of the raw ruliad and raw computation, nor at the level where we as humans define mathematics. But now, I think, we begin to get some clarity on just what this thing we call mathematics really is. What I’ve done here is just a beginning. But between its explicit computational examples and its conceptual arguments I feel it’s pointing the way to a broad and incredibly fertile new understanding that—even though I didn’t see it coming—I’m very excited is now here.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.33 Notes & Thanks : For more than 25 years Elise Cawley has been telling me her thematic (and rather Platonic) view of the foundations of mathematics—and that basing everything on constructed axiom systems is a piece of modernism that misses the point. From what’s described here, I now finally realize that, yes, despite my repeated insistence to the contrary, what she’s been telling me has been on the right track all along! I’m grateful for extensive help on this project from James Boyd and Nik Murzin, with additional contributions by Brad Klee and Mano Namuduri. Some of the early core technical ideas here arose from discussions with Jonathan Gorard, with additional input from Xerxes Arsiwalla and Hatem Elshatlawy. (Xerxes and Jonathan have now also been developing connections with homotopy type theory.) I’ve had helpful background discussions (some recently and some longer ago) with many people, including Richard Assar, Jeremy Avigad, Andrej Bauer, Kevin Buzzard, Mario Carneiro, Greg Chaitin, Harvey Friedman, Tim Gowers, Tom Hales, Lou Kauffman, Maryanthe Malliaris, Norm Megill, Assaf Peretz, Dana Scott, Matthew Szudzik, Michael Trott and Vladimir Voevodsky. I’d like to recognize Norm Megill, creator of the Metamath system used for some of the empirical metamathematics here, who died in December 2021. (Shortly before his death he was also working on simplifying the proof of my axiom for Boolean algebra.) Most of the specific development of this report has been livestreamed or otherwise recorded, and is available—along with archives of working notebooks—at the Wolfram Physics Project website. The Wolfram Language code to produce all the images here is directly available by clicking each image. And I should add that this project would have been impossible without the Wolfram Language, both its practical manifestation, and the ideas that it has inspired and clarified. So thanks to everyone involved in the 40+ years of its development and gestation.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: accumulative system : A system in which states are rules and rules update rules. Successive steps in the evolution of such a system are collections of rules that can be applied to each other.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: axiomatic level : The traditional foundational way to represent mathematics using axioms, viewed here as being intermediate between the raw ruliad and human-scale mathematics.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: bisubstitution : The combination of substitution and cosubstitution that corresponds to the complete set of possible transformations to make on expressions containing patterns.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: branchial space : Space corresponding to the limit of a branchial graph that provides a map of common ancestry (or entanglement) in a multiway graph.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: cosubstitution : The dual operation to substitution, in which a pattern expression that is to be transformed is specialized to allow a given rule to match it.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: eme : The smallest element of existence according to our framework. In physics it can be identified as an “atom of space”, but in general it is an entity whose only internal attribute is that it is distinct from others.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: entailment cone : The expanding region of a multiway graph or token-event graph affected by a particular node. The entailment cone is the analog in metamathematical space of a light cone in physical space.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: entailment fabric : A piece of metamathematical space constructed by knitting together many small entailment cones. An entailment fabric is a rough model for what a mathematical observer might effectively perceive.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: entailment graph : A combination of entailment cones starting from a collection of initial nodes.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: expression rewriting : The process of rewriting (tree-structured) symbolic expressions according to rules for symbolic patterns. (Called “operator systems” in A New Kind of Science. Combinators are a special case..
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: mathematical observer : An entity sampling the ruliad as a mathematician might effectively do it. Mathematical observers are expected to have certain core human-derived characteristics in common with physical observers.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: metamathematical space : The space in which mathematical expressions or mathematical statements can be considered to lie. The space can potentially acquire a geometry as a limit of its construction through a branchial graph.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: multiway graph : A graph that represents an evolution process in which there are multiple outcomes from a given state at each step. Multiway graphs are central to our Physics Project and to the multicomputational paradigm in general.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: paramathematics : Parallel analogs of mathematics corresponding to different samplings of the ruliad by putative aliens or others.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: pattern expression : A symbolic expression that involves pattern variables (x_ etc. in Wolfram Language, or ∀ quantifiers in mathematical logic).
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: physicalization of metamathematics : The concept of treating metamathematical constructs like elements of the physical universe.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: proof cone : Another term for the entailment cone.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: proof graph : The subgraph in a token-event graph that leads from axioms to a given statement.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: proof path : The path in a multiway graph that shows equivalence between expressions, or the subgraph in a token-event graph that shows the constructibility of a given statement.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: ruliad : The entangled limit of all possible computational processes, that is posited to be the ultimate foundation of both physics and mathematics.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: rulial space : The limit of rulelike slices taken from a foliation of the ruliad in time. The analog in the rulelike “direction” of branchial space or physical space.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: shredding of observers : The process by which an observer who has aggregated statements in a localized region of metamathematical space is effectively pulled apart by trying to cover consequences of these statements.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: statement : A symbolic expression, often containing a two-way rule, and often derivable from axioms, and thus representing a lemma or theorem.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: substitution event : An update event in which a symbolic expression (which may be a rule) is transformed by substitution according to a given rule.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: token-event graph : A graph indicating the transformation of expressions or statements (“tokens”) through updating events.
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: two-way rule : A transformation rule for pattern expressions that can be applied in both directions (indicated with ).
The Physicalization of Metamathematics and Its Implications for the Foundations of Mathematics 23.34 Glossary: uniquification : The process of giving different names to variables generated through different events.
On the Concept of Motion 24.1 How Is It That Things Can Move? : It seems like the kind of question that might have been hotly debated by ancient philosophers, but would have been settled long ago: how is it that things can move? And indeed with the view of physical space that’s been almost universally adopted for the past two thousand years it’s basically a non-question. As crystallized by the likes of Euclid it’s been assumed that space is ultimately just a kind of “geometrical background” into which any physical thing can be put—and then moved around. But in our Physics Project we’ve developed a fundamentally different view of space—in which space is not just a background, but has its own elaborate composition and structure. And in fact, we posit that space is in a sense everything that exists, and that all “things” are ultimately just features of the structure of space. We imagine that at the lowest level, space consists of large numbers of abstract “atoms of space” connected in a hypergraph that’s continually getting updated according to definite rules and that’s a huge version of something like this: But with this setup, what even is motion? It’s no longer something baked into our basic ideas about space. Instead—much like the ancient philosophers imagined—it’s something we can try to derive from a lower level of description. It’s not something we can take for granted—and indeed it’s going to turn out that its character depends in fundamental ways on issues like our nature as observers. To have a concept of motion, one has to have not only a concept of space—and time—but also a concept of “things”. One has to have something definite that one can imagine moves through space with time. And in effect the concept of “pure motion” is that there can be a “thing” that “just moves” without “changing its character”. But if the thing is “made of atoms of space” that are continually getting updated, what does this mean? Somehow the identity of the “thing” has to be associated with some collective characteristic that doesn’t depend on the particular atoms of space from which it’s made. There’s an immediate analogy here. Consider something like a vortex in a fluid. The vortex can move around as a “thing” even though “underneath” it’s made of an ever-changing collection of lots of discrete molecules. If we looked in microscopic detail, we’d see effects from those discrete molecules. But at the scale at which we humans typically operate, we just consider there to be a definite “thing” we describe as a vortex—that at this level of description exhibits “pure motion”. Our fundamental model of space is not so different from this. At the lowest level there’s continual activity associated with the application of rules that create new atoms of space and new connections between them. And just as continual collisions between molecules in a fluid “knit together” the structure of the fluid, so also the continual rewriting of the hypergraph that connects atoms of space knits together the structure of space. But then on top of this there can be “localized collective features” that have a certain persistence. And these are the “things” (or “objects”) that we can consider to “show pure motion”. Physics suggests two kinds of things like this. The first are particles, like electrons or photons or quarks. And the second are black holes. As of now, we have no specific evidence that particles like electrons are “made of anything”; they just seem to act like geometrical points. But in our Physics Project we posit that they are ultimately “made of space” and actually contain large numbers of atoms of space that collectively form some kind of persistent structure a bit like a vortex in a fluid. Black holes operate on a very different scale—though I suspect they’re actually very similar in character to particles. And in fact for black holes we already have a sense from traditional general relativity that they can just be “made of space”—though without our discrete underlying model there are some inevitable mathematical hacks involved. So what is it that leads to persistent structures? Often one can identify it as something “topological”. There’s an underlying “medium” in which all sorts of essentially continuous changes can be made. But then there are structures that can’t be created or destroyed by such continuous changes—in effect because they are “topologically distinct”. Vortices are one such example—because around the core of the vortex, independent of what “continuous deformations” one makes, there’s always a constant circulation of fluid, that can’t be gotten rid of except by some kind of discontinuous change. (In reality, of course, vortices are eventually damped out by viscosity generated as a result of microscopic motion, but the point is that this takes a while, and until it’s happened, the vortex can reasonably be considered to persistently be a “thing”.) In our Physics Project, we’ve already been able to figure out quite a bit about how black holes work. We know less about the specifics of how particles work. But the basic idea is that somehow there are features that are local and persistent that we can identify as particles—and perhaps these features have topological origins that make it inevitable that, for example, all electrons “intrinsically seem the same”, and that there are only a discrete set of possible types of particles (at least at our energy scales). So in the end what we imagine is that there are certain “carriers of pure motion”: certain collective features of space that are persistent enough that we can consider them to “just move”, without changing. At the outset it’s not obvious that any such features should exist at all, and that pure motion should ever be possible. Unlike in the traditional “pure geometrical” view of space, in our Physics Project it’s something one has to explicitly derive from the underlying structure of the model—though it seems quite likely that it’s ultimately an inevitable and ubiquitous consequence of rather general “topological” features of hypergraph rewriting. We keep on talking about “features that persist”. But what does this really mean? As soon as something moves it’ll be made of different atoms of space. So what does it mean for it to “persist”? In the end it’s all about what observers perceive. Do we view it as being the “same thing” but in a different place? Or do we say it’s different because some detail of it is different? And actually this kind of issue already comes up even before we’re talking about motion and the persistence of “objects”: it’s crucial just in the emergence of the basic notion of space itself. At the level of individual atoms of space there isn’t anything we can really call “space”, just like at the level of individual molecules there isn’t anything we can reasonably call a fluid. And instead, the notion of space—or of fluids—emerges when we look at things in the kind of way that observers like us do. We’re not tracking what’s happening at the level of individual atoms of space—or individual molecules; we’re looking at things in a more coarse-grained way, that it turns out we can summarize in terms of what amount to continuum concepts. Once again, it’s not obvious things will work like this. Down at the level of atoms of space—or, for that matter, molecules—there are definite computational rules being followed. And from the Principle of Computational Equivalence it’s almost inevitable that there’ll be computational irreducibility, implying that there’s no way to find the outcome except in effect by doing an irreducible amount of computational work. If we as observers were computationally unbounded then, yes, we could always “decode” what’s going on, and “see down” to the behavior of individual atoms of space or individual molecules. But if we’re computationally bounded we can’t do this. And, as I’ve argued elsewhere, that’s both why we believe in the Second Law of thermodynamics, and why we perceive there to be something like ordinary “geometrical space”. In other words, our inability to track the details means that in a first approximation we can summarize what’s going on just by saying we’ve got something that seems like our ordinary notion of space. And going one step beyond that is what has us talking about “persistent objects in space”. But now we’re back to discussing what it means for an object to “be persistent”. Ultimately it’s that we as observers somehow perceive it to “be the same”, even though perhaps in a “different place”. A key finding of our Physics Project is that certain basic laws of physics—in particular general relativity and quantum mechanics—inevitably seem to emerge as soon as we assume that observers have two basic characteristics: first, that they are computationally bounded, and second, that they are persistent in time. In our Physics Project the passage of time corresponds to the inexorable (and irreducible) computational process of updating the “spatial hypergraph” that represents the lowest-level structure of the universe. And when we talk formally we can imagine looking at this “from the outside”. But in reality we as observers must be embedded within the system, being continually updated and changed just like the rest of the system. But here there’s a crucial point. Even though the particular configuration of atoms in our brains is continually changing, we think it’s “still us”. Or, in other words, we have the perception that we persist through time. Now it could be that this wouldn’t be a consistent thing to imagine, and that if we imagined it, we’d never be able to form a coherent view of the world. But in fact what our Physics Project implies is that with this assumption we can (subject to various conditions) form a coherent view of the world, and it’s one where the core known laws of physics are in evidence. OK, so we ourselves are persistent essentially because we assume that we are (and in most situations nothing goes wrong if we do this). But the persistence of something like a particle, or a black hole, is a different story. From our point of view, we’re not “inside” things like these; instead we’re “looking at them from the outside”. But what do we notice in them? Well, that depends on our “powers of observation”. The basic idea of particles, for example, is that they should be objects that can somehow be separated from each other and from everything else. In our Physics Project, though, any particle must ultimately be “embedded as a part of space”. So when we say that it’s a “separable object” what we’re imagining is just that there’s some attribute of it that we can identify and observe independent of its “environment”. But just what this is can depend on our characteristics as observers, and the fact that we operate on certain scales of length and time. If we were able to go down to the level of individual atoms of space we probably wouldn’t be able to “see” that there’s anything like a particle there at all. That’s something that emerges for observers with our kinds of characteristics. Quite what the full spectrum of “conceivable persistent features” might be isn’t clear (though we’ll see some exotic possibilities below). But as soon as one can identify a persistent feature, one can ask about motion. Is it possible for that feature to “move” from being embedded at one “place” to another? There’s yet another subtlety here, though. Our ordinary experience of motion involves things going from one place to another by progressively “visiting every place in between”. But ultimately, as soon as we’re dealing with discrete atoms of space, this can’t be how things work. And instead what we need to discuss is whether something somehow “maintains its form” at intermediate stages as it “moves”. For example, we probably wouldn’t consider it motion in the ordinary sense if what we had was a kind of Star Trek–like “transporter” in which objects get completely disassembled, then get “transmitted to a different place” and reassembled. But somehow it does seem more like “ordinary motion” if there’s a collection of pixel values that move across a computer screen—even if at intermediate moments they are distorted by all sorts of aliasing effects. Even in ordinary general relativity there are issues with the idea of motion—at least for extended objects. If we’re in a region of space that’s reasonably flat it’s fine. But if we’re near a spacetime singularity then inevitably objects won’t be able to “maintain their integrity”—and instead they’ll effectively be “shredded”—and so can’t be interpreted as “just moving”. When we’re dealing not with geometric continuum spacetime but instead with our spatial hypergraph, there’ll always be something analogous to “shredding” on a small enough scale, and the question is whether at the level we perceive things we’ll be able to tell that there’s something persistent that isn’t shredded. So, in the end, how is it that things can move? Ultimately it’s something that has to be formally derived from the underlying model, based on the characteristics of the observer. At least conceptually the first step is to identify what kinds of things the observer considers “the same”, and what details make them “seem different”. Then one needs to determine whether there are structures that would be considered the same by the observer, but which progressively change ”where they’re embedded”. And if so, we’ve identified “motion”. For us humans with our current state of technological development, particles and objects made of them are the most obvious things to consider. So in a sense the question reduces to whether there are “lumps of space” that persist in maintaining (perhaps topological) features recognized by our powers of perception. And to determine this is a formal question that’s important to explore as our Physics Project progresses.
On the Concept of Motion 24.2 Motion Can Be a Complicated Story : We’ve talked about “persistent structures” as “carriers of pure motion”. But how do such structures actually work? Ultimately it can be a very complicated story. But here we’ll consider a simplified case that begins to illustrate some of the issues. We’ll be talking not about the actual model of space in our Physics Project, but instead about the cellular automaton systems I’ve studied for many years in which space is effectively predefined to consist of a rigid array of cells, each with a discrete value updated according to a local rule. Here’s an example in which there quickly emerge obvious “localized persistent structures” that we can think of as being roughly like particles: Some “stay still” relative to the fixed cellular automaton background; others “move”. With this specific cellular automaton, it’s easy to identify certain possible “particles”, some “staying still” and some “showing motion”: But consider instead a cellular automaton with very different behavior: Does this support the concept of motion? Certainly not as obviously as the previous case. And in fact there doesn’t seem to be anything identifiable that systematically propagates across the system. Or in other words, at least with our typical “powers of perception” we don’t “see motion” here. There’s a whole spectrum of more complicated cases, however. Consider for example: Here one can easily identify “particle-like” structures, but they never seem to “keep moving forever”; instead they always fairly quickly interact and “annihilate”. But to expect otherwise is to imagine an idealization in which there is at some level “only one object” in the whole system. As soon as there are multiple objects it’s basically inevitable that they’ll eventually interact. Or, put another way, motion in any real situation will never be about “persistently moving” forever; it’s just about persisting for at least long enough to be identified as something separate and definite. (This is very similar to the situation in quantum field theory where actual particles eventually interact, even though their formal definition assumes no interaction.) Here’s another case, where on a large scale there’s no “obvious motion” to be seen: but where locally one can identify rather simple “particle-like” structures: that on their own can be thought of as “exhibiting motion”, even though there are other structures that for example just expand, apparently without bound: Sometimes there can be lots of “particle-like” activity, but with other things consistently mixed in: Here’s a slightly more exotic example, where continual “streams of particles” are produced: In all the examples we’ve seen so far the “particles” exist on a “blank” or otherwise simple background. But it’s also perfectly possible for them to be on a background with more elaborate structure: But what about a seemingly random background? Here’s at least a partial example where there are both structures that “respond to the background” and ones that have “intrinsic particle-like form”: What does all this mean for the concept of motion? The most important point is that we’ve seen that “objects” that can be thought of as “showing pure motion” can emerge even in underlying systems that don’t seem to have any particular “built-in concept of motion”. But what we’ve also seen is that along with “objects that show pure motion” there can be all sorts of other effects and phenomena. And in our actual Physics Project these can necessarily in a sense be much more extreme. The cellular automaton systems we’ve been discussing so far have a built-in underlying notion of space, which exists even if the system basically “doesn’t do anything”. But in our Physics Project the structure of space itself is created through activity. So—as we discussed in the previous section—“objects” or particles have to somehow exist “on top” of this. It’s fairly clear roughly how such particles must work, being based for example on essentially topological features of the system. But we don’t yet know the details, and there’s probably quite a depth of mathematical formalism that needs to be built to clarify them. It’s still possible, though, to explore at least some toy examples. Consider the hypergraph rewriting rule: It maintains a very simple (effectively 1D and cyclic) form of space (with rewrites shown in red): If the initial conditions contain a feature that can be interpreted as something like a “particle” then the rules are such that this can “move around”, but can’t be destroyed: It’s a little clearer what’s going on if instead of looking at an explicit sequence of hypergraphs we instead generate causal graphs (see the next section) that show the “spacetime” network of causal relationships between updating events. Here’s the causal graph for the “space only, no particles” case (where here we can think of time as effectively running from left to right): Here’s the causal graph when there’s a “particle” included: And here’s the result when there are “two particles”—where things begin to get more complicated.
On the Concept of Motion 24.3 The Observer Is Actually inside the System : We’ve discussed what it takes for an observer to identify something as “moving” in a system. But so far there’s an important piece we’ve left out. Because in effect we’ve assumed that the observer is “outside the system” and “looking in”. But if we imagine that we’re dealing with a complete model of the physical universe the observer necessarily has to “be inside”. And ultimately the observer has got to be “made of the same stuff” as whatever thing it is to which we’re attributing motion. How does an observer observe? Ultimately whatever is “happening in the outside world” must affect the observer, and the observer must change as a result. Our Physics Project has a fundamental way to think about change, in terms of elementary “updating events”. In addition to imagining that space is made up of discrete “atoms of space”, we imagine that change is made up of discrete “atoms of change” or “events”. In the hypergraph that represents space and everything in it, each event updates (or “rewrites”) the hypergraph, by “consuming” some collection of hyperedges, and generating a new collection. But actually events are a more general concept that don’t for example depend on having an underlying hypergraph. We can just think of them as consuming collections of “tokens”, whatever they may be, and generating new ones. But events satisfy a very important constraint, which in some sense is responsible for the very existence of what we think of as time. And the constraint is that for any event to happen, all the tokens it’s going to consume have to exist. But those tokens have to have “come from somewhere”. And at least if we ignore what happens “at the very beginning” every token that’s going to be consumed has to have been generated by some other event. In other words, there’s a certain necessary ordering among events. And we can capture this by constructing a causal graph that captures the causal relationships that must exist between events. As a simple example, here’s a system that consists of a string of As and Bs, and in which each “updating event” (indicated as a yellow box) corresponds to an application of the rule BA→AB: Here’s the causal graph for this superimposed: Imagine that some collection of characters on the left-hand side represents “an observer”. The only way this observer can be affected by what happens on the right-hand side is as a result of its events being affected by events on the right-hand side. But what event is affected by what other event is exactly what the causal graph defines. And so in the end we can say that what the observer can “perceive” is just the causal graph of causal relationships between events. “From the outside” we might see some particular “absolute” arrangement of events in the cellular-automaton-like picture above. But the point is that “from the inside” the observer can’t perceive this “absolute arrangement”. All they can perceive is the causal graph. Or, put another way, the observer doesn’t have any “absolute knowledge” of the system; all they “know about” is “effects on them”. So what does this imply about motion? In something like a cellular automaton there’s a fixed concept of space that we typically “look at from the outside”—and we can readily “see what’s moving” relative to that fixed, absolute “background space”. But in something like our Physics Project we imagine that any observer must be inside the system, able to “tell what’s going on” only from the causal graph. In standard physics we might posit that to find out “where something is” we’d have to probe it, say with light signals. Here we’ve broken everything down to the level of elementary events and we’re in some sense “representing everything that can happen” in terms of the causal graph of relationships between events. And in fact as soon as we assume that our “perceived reality” has to be based on the causal graph, we’ve inevitably abandoned any absolute notion of space. All we as observers can know is “relative information”, defined for us by the causal graph. Looking at our BA→AB system above we can see that “viewed from the outside” there’s a lot of arbitrariness in “when we do” each update. But it turns out that none of this matters to the causal graph we construct—because this particular underlying system has the property of causal invariance, which makes the causal graph have the same structure independent of these choices. And in general whenever there’s causal invariance (which there inevitably will be at least at the ultimate level of the ruliad) this has the important implication that there’s relativistic invariance in the system. We won’t go into this in detail here. Because while it certainly affects the specifics of how motion works there are more fundamental issues to discuss about the underlying concept of motion itself. We’ve already discussed the idea that observers like us posit our own persistence through time. But now we can be a bit more precise—and say that what we really posit is that we “follow the causal graph”. It could be that our perception samples all sorts of events—that we might think of as being “all over spacetime”. But in fact we assume that we don’t “jump around the causal graph”, and that instead our experiences are based on “coherent paths” through the causal graph. We never in any absolute sense “know where we are”. But we construct our notion of place by positing that we exist at a definite—and in a sense “coherent”—place, relative to which we perceive other things. If our perception of “where we are” could “jump around” the causal graph, we’d never be able to define a coherent concept of pure motion. To make this a little bit “more practical” let’s discuss (as I did some time ago) the question of faster-than-light travel in our Physics Project. By the very definition of the causal graph the effect of one event on another is represented by the presence of a “causal path” between the events within the graph. We can assume that “traversing” each “causal edge” (i.e. going from one event to the next) takes a certain elementary time. But to work out “how fast the effect propagated” we need to know how “far away in space” the event that was affected is. But recall that all the observer ultimately has available is the causal graph. So any questions about “distances in space” have to be deduced from the causal graph. And the nature of the observer—and the assumptions they make about themselves—inevitably affect the deductions they make. Imagine a causal graph that is mostly a grid, but suppose there is a single edge that “jumps across the grid”, connecting events that would otherwise be distant in the graph. If we as observers were sensitive to that single edge it’d make us think that the two events it joins are “very close together”. But if we look only at the “bulk structure” of the causal graph, we’d ignore that edge in our definition of the “layout of space”, and consider it only as some kind of “microscopic anomaly”. So should we in fact include that single edge when we define our concept of motion? If we posit that we “exist at a definite place” then the presence of such an edge in what “constitutes us” means the “place we’re at” must extend to wherever in the causal graph the edge reaches. But if there are enough “stray edges” (or in general what I call “space tunnels”) we as observers would inevitably get very “delocalized”. To be able to “observe motion” we’d better be observers who can coherently form a notion of space in which there can be consistent “local places”. And if there’s some elaborate pattern of space tunnels this could potentially be broken. Although ultimately it won’t be unless the space tunnels are somehow coherent enough to “get observers like us through them”. Earlier we saw that the concept of motion depends on the idea that we as observers can identify “things” as “persistent” relative to the “background structure of space”. And now we can see that in fact motion depends on a certain persistence in time and “coherence” in place not only for the “thing” we posit is moving, but also for us as observers observing it. In our Physics Project we imagine that both time and space are fundamentally discrete. But the concept of persistence—or “coherence”—implies that at least at the level of our perception there must be a certain effectively continuous character to them. There’s a certain resonance with things like Zeno’s paradoxes. Yes, our models may define only what happens at a sequence of discrete steps. But the perception that we persistently exist will make us effectively fill in all the “intervening moments”—to form what we experience as a “continuous thread of existence”. The idea that pure motion is possible is thus intimately connected to the idea of the continuum. Pure motion in a sense posits that there is some kind of “thread of existence” for “things” that leads from one place and time to another. But ultimately all that’s relevant is that observers like us perceive there to be such a thread. And the whole point is that the possibility of such perception can be deduced as a matter of formal derivation from the structure of the underlying model and general characteristics of us as observers. But in describing our perception what we’ll tend to do is to talk in terms of the continuum. Because that’s the level of description at which we can abstractly discuss pure motion, without having to get into the mechanics of how it happens. And in effect the “derivation of pure motion” is thus directly connected to the “derivation of the continuum”: pure motion is in a sense an operational consequence not necessarily of an actual continuum world, but of a continuum perception of the world by an embedded observer like us.
On the Concept of Motion 24.4 Motion beyond Physical Space: The Branchial Case : Our everyday experience of motion has to do with ordinary, physical space. But the multicomputational paradigm inspired by our Physics Project inevitably leads to other kinds of space—that are different in character and interpretation from ordinary, physical space, but have deep analogies to it. So in the context of these other kinds of space, what analogs of the concept of “pure motion” might there be? Let’s talk first about branchial space, which in our Physics Project is interpreted as the space of quantum states. To approach this from a simple example, let’s consider the multiway graph generated by applying the rule {A→AB,B→A} in all possible ways to each “state”: We can think of each path through this graph as defining a possible history for the system, leading to a complicated pattern of possible “threads of history”, sometimes branching and sometimes merging. But now consider taking a “branchial slice” across this system—and then characterizing the “multicomputational behavior” of the system by constructing what we call the branchial graph by joining states that share an ancestor on the step before: For physics, we interpret the nodes of these graphs as quantum states, so that the branchial graph effectively gives us a “map of quantum entanglements” between states. And just like for the hypergraph that we imagine defines the relations between the atoms of physical space, we think about the limit of a very large branchial graph—that gives us what we can call branchial space. As we’ve discussed elsewhere, branchial space is in many ways much wilder than ordinary, physical space, and is for example probably exponential-dimensional. In basic quantum mechanics, distances in branchial space are probably related to differences in phase between quantum amplitudes. In more complicated cases they probably correspond to more complicated transformations between quantum states. So how might we think about “motion” in branchial space? Although we’ve discussed it at length elsewhere, we didn’t above talk about what we might call “bulk motion” in physical space, as effectively produced by the curvature of space associated with gravity. But in branchial space there seems to be a directly analogous phenomenon—in which the presence of energy (which corresponds to the density of activity in the system) leads to an effective curvature in branchial space which deflects all paths, in a way that seems to produce the change of quantum phase specified by the path integral. But can we identify specific things moving and preserving their identity in branchial space, as we can identify things like particles moving in physical space? It’s a tricky story, incompletely figured out, and deeply connected to issues of quantum measurement. But just like in physical space, an important issue is to define what “observers like us” are like. And a crucial first step is to realize that—as entities embedded in the universe—we must inevitably have multiple histories. So to ask how we perceive what happens in the universe is in effect to ask how a “branching mind” perceives a branching universe. And the crucial point—directly analogous to what we’ve discussed in the case of physical space—is that whatever one might be able to “see from outside”, we “internally” assume that we as observers have a certain persistence and coherence. In particular, even though “from the outside” the multiway graph might show many branching threads of history, our perception is that we have a single thread of experience. In ordinary quantum mechanics, it’s quite tricky to see how this “conflation of threads of history” interacts even with “bulk motion” in branchial space. Typically, as in traditional quantum measurement, one just considers “snapshots” at particular times. Yes, one can imagine that things like wave packets spread out in branchial space, but—a bit like discussing “motion” for gravitational fields or even gravitational waves in spacetime—there isn’t the same kind of systematic concept of pure motion that we’ve encountered with things like particles in physical space. When we get to quantum field theory—or the full quantum gravity associated with our models—it will probably be a different story. Perhaps we can view certain configurations of quantum fields as being like structures in branchial space, that an observer will consider to be localized and persistent. Indeed, it’s easy to imagine that in the branchial graph—or even more so the multiway causal graph—there may be things like “topologically stable” structures that we can reasonably think of as “things that move”. But just what the character and interpretation of such things might be, we don’t yet know.
On the Concept of Motion 24.5 Motion in Rulial Space : There’s physical space, and there’s branchial space. But in a sense the ultimate kind of space is rulial space. The story begins with the ruliad, which represents the entangled limit of all possible computations. The ruliad is what we imagine underlies not only physics but also mathematics. When we “experience physics” we’re sampling a certain slice of the ruliad that’s accessible to physical observers like us. And when we “experience mathematics” we’re sampling a slice of the ruliad that’s accessible to “mathematical observers” like us. So what do different “places” in rulial space correspond to? Fundamentally they’re different choices for the rules we sample from the ruliad. Ultimately everything is part of the unique object that is the ruliad. But at different places in the ruliad we’ll have different specific experiences as observers. Inevitably, though, there’s a translation that can be made. It’s basically like the situation with different computational systems that—according to the Principle of Computational Equivalence—are generically universal: there’s always an “interpreter” that can be created in one system that can translate to the other. In a sense the idea of different places in rulial space is quite familiar from our everyday experience. Because it’s directly analogous to the idea that different minds “parse” and “experience” the world differently. Whether one’s talking about a human brain or an artificial neural net, the details of its past experience will cause it to represent things in the world in different ways, and to process them differently. At the very lowest level, the components of the systems will—like any other universal computer—be able to emulate the detailed operations of other systems. But at this level there are no “things that are moving from one place to another in rulial space”; everything is just being “atomized”. So are there in fact robust structures that can “move across rulial space”? The answer, I think, is yes. But it’s a strange story. I suspect that the analog in rulial space of particles in physical space is basically concepts—say of the kind that might be represented by words in a human (or computational) language. Imagine thinking about a cat. There’s a particular representation of a cat in one’s brain—and in detail it’ll be different from the representation in anyone else’s brain. But now imagine using the word “cat”, or in some way communicating the concept of “cat”. The “cat” concept is something robust, that we’re used to seeing “transmitted” from one brain to another—even though different brains represent it differently. Things might not work this way. It could be that there’d be no robust way to transmit anything about the thinking going on in one brain to another brain. But that’s where the idea of concepts comes in. They’re an abstracted way to “transport” some feature of thinking in one brain to another. And in a sense they’re a reflection of the possibility of pure motion in rulial space: they’re a way to have some kind of persistent “thing” that can be traced across rulial space. But just like our examples of motion, the way this works depends on the characteristics of the observers observing it—and insofar as we are the observers, it therefore depends on us. We know from experience that we form concepts, and that they have a certain robustness. But why is this? In a sense, concepts are a way of coarse-graining things so that we—as computationally bounded entities—can deal with them. And the fact that we take concepts to maintain some kind of fixed meaning is part of our perception that we maintain a single persistent thread of experience. It is strange to think that something as explicit and concrete as an electron in physical space could in some sense be similar to an abstract concept like “cat”. But this is the kind of thing that happens when one has something as fundamental and general as the ruliad underlying everything. We know that our general characteristics as observers inevitably lead to certain general laws of physics. And so similarly we can expect that our general characteristics as observers will lead to certain general laws about the overall representation of things. Perhaps we’ll be able to identify analogs of energy and gravity and quantum mechanics. But a first step is to identify the analog of motion, and the kinds of things which can exhibit pure motion. In physical space, particles like electrons are our basic “carriers of motion”. In rulial space “concepts” seem to be our best description of the “carriers of motion” (though there are presumably higher-level constructs too, like analogies and syntactic structures). And, yes, it might seem very odd to say that something as apparently human-centered as “concepts” can be associated with something as fundamental as motion. But as we’ve emphasized several times here, “pure motion” is something that relies on the observer, and on the observer having what amounts to a “sensory apparatus” that considers a “thing” to maintain a persistent character. So when it comes to the representation of “arbitrary content” it’s not surprising that we as observers have to talk about the fundamental way we think about things, and about constructs like concepts. But are things like concepts the only kind of persistent structures that can exist in rulial space? They’re ones that we as observers can readily parse out of the ruliad—based for example on the particular ways of thinking that we’ve embraced so far in our intellectual development. But we can certainly imagine that there’s the possibility for “robust communication” independent, for example, of human minds. There’s a great tendency, though, to try to relate things back to human constructs. For example, we might consider a machine-learning system that’s successfully discovered a distinction that can repeatedly be used for some purpose. And, yes, we can imagine “transporting” that to a different system. But we’ll tend to think of this again in terms of some “feature” or “concept”, even though, for example, we might not happen (at least yet) to have some word for it in a human language, or a computational language intended for use by humans. We can similarly talk about communication with or between other animals, or, more ambitiously, we can discuss communications with or between “alien intelligences”. We might assume that we would be able to say nothing about such cases. But ultimately we imagine that everything is represented somewhere in the ruliad. And in effect by doing things like exploring arbitrarily chosen programs we can investigate possible “raw material” for “alien intelligence”. And it’s then at some level a matter of science—or, more specifically, ruliology—to try to identify “transportable elements” between different programs, or, in effect, between different places in rulial space. At a simple level we might say we’re looking for “common principles”—which puts us back to something like “concepts”. But in general we can imagine a more elaborate computational structure for our “transportable elements” in rulial space. In physical space we know that we can make “material objects” out of particles like electrons and quarks, and then “move these around” in physical space. Within the domain of “human-thinking rulial space” we can do something analogous with descriptions “made from known concepts”. And in both cases we can imagine that there are more general constructs that are “possible”, even though we human observers as we are now might not be able to “parse them out of the ruliad”. The constraints of computational boundedness and perception of persistence are probably pretty fundamental to any form of experience that can be connected to us. But as we develop what amount to new sensory capabilities or new ways of thinking we can expect that our “range” as observers will at least somewhat increase. And in a sense our very exploration of the concept of motion here can be thought of as a way to make possible a little bit more motion in rulial space. The concept of motion is a very general one. And one that we now see is deeply tied into ideas about observers and multicomputation. The question of how things can move is the same one that was asked in antiquity. But the tower of ideas that we can now bring to bear in answering is very different, and it’s sobering to see just how far we really were earlier in intellectual history from being able to meaningfully address it.
Twenty Years Later: The Surprising Greater Implications of A New Kind of Science 25.1 : See also: “A New Kind of Science: A 15-Year View”. From the Foundations Laid by A New Kind of Science When A New Kind of Science was published twenty years ago I thought what it had to say was important. But what’s become increasingly clear—particularly in the last few years—is that it’s actually even much more important than I ever imagined. My original goal in A New Kind of Science was to take a step beyond the mathematical paradigm that had defined the state of the art in science for three centuries—and to introduce a new paradigm based on computation and on the exploration of the computational universe of possible programs. And already in A New Kind of Science one can see that there’s immense richness to what can be done with this new paradigm. There’s a new abstract basic science—that I now call ruliology—that’s concerned with studying the detailed properties of systems with simple rules. There’s a vast new source of “raw material” to “mine” from the computational universe, both for making models of things and for developing technology. And there are new, computational ways to think about fundamental features of how systems in nature and elsewhere work. But what’s now becoming clear is that there’s actually something still bigger, still more overarching that the paradigm of A New Kind of Science lays the foundations for. In a sense, A New Kind of Science defines how one can use computation to think about things. But what we’re now realizing is that actually computation is not just a way to think about things: it is at a very fundamental level what everything actually is. One can see this as a kind of ultimate limit of A New Kind of Science. What we call the ruliad is the entangled limit of all possible computations. And what we, for example, experience as physical reality is in effect just our particular sampling of the ruliad. And it’s the ideas of A New Kind of Science—and particularly things like the Principle of Computational Equivalence—that lay the foundations for understanding how this works. When I wrote A New Kind of Science I discussed the possibility that there might be a way to find a fundamental model of physics based on simple programs. And from that seed has now come the Wolfram Physics Project, which, with its broad connections to existing mathematical physics, now seems to show that, yes, it’s really true that our physical universe is “computational all the way down”. But there’s more. It’s not just that at the lowest level there’s some specific rule operating on a vast network of atoms of space. It’s that underneath everything is all possible computation, encapsulated in the single unique construct that is the ruliad. And what determines our experience—and the science we use to summarize it—is what characteristics we as observers have in sampling the ruliad. There is a tower of ideas that relate to fundamental questions about the nature of existence, and the foundations not only of physics, but also of mathematics, computer science and a host of other fields. And these ideas build crucially on the paradigm of A New Kind of Science. But they need something else as well: what I now call the multicomputational paradigm. There were hints of it in A New Kind of Science when I discussed multiway systems. But it has only been within the past couple of years that this whole new paradigm has begun to come into focus. In A New Kind of Science I explored some of the remarkable things that individual computations out in the computational universe can do. What the multicomputational paradigm now does is to consider the aggregate of multiple computations—and in the end the entangled limit of all possible computations, the ruliad. The Principle of Computational Equivalence is in many ways the intellectual culmination of A New Kind of Science—and it has many deep consequences. And one of them is the idea—and uniqueness—of the ruliad. The Principle of Computational Equivalence provides a very general statement about what all possible computational systems do. What the ruliad then does is to pull together the behaviors and relationships of all these systems into a single object that is, in effect, an ultimate representation of everything computational, and indeed in a certain sense simply of everything.
Twenty Years Later: The Surprising Greater Implications of A New Kind of Science 25.2 The Intellectual Journey: From Physics to Physics, and Beyond : The publication of A New Kind of Science 20 years ago was for me already the culmination of an intellectual journey that had begun more than 25 years earlier. I had started in theoretical physics as a teenager in the 1970s. And stimulated by my needs in physics, I had then built my first computational language. A couple of years later I returned to basic science, now interested in some very fundamental questions. And from my blend of experience in physics and computing I was led to start trying to formulate things in terms of computation, and computational experiments. And soon discovered the remarkable fact that in the computational universe, even very simple programs can generate immensely complex behavior. For several years I studied the basic science of the particular class of simple programs known as cellular automata—and the things I saw led me to identify some important general phenomena, most notably computational irreducibility. Then in 1986—having “answered most of the obvious questions I could see”—I left basic science again, and for five years concentrated on creating Mathematica and what’s now the Wolfram Language. But in 1991 I took the tools I’d built, and again immersed myself in basic science. The decade that followed brought a long string of exciting and unexpected discoveries about the computational universe and its implications—leading finally in 2002 to the publication of A New Kind of Science. In many ways, A New Kind of Science is a very complete book—that in its 1280 pages does well at “answering all the obvious questions”, save, notably, for some about the “application area” of fundamental physics. For a couple of years after the book was published, I continued to explore some of these remaining questions. But pretty soon I was swept up in the building of Wolfram|Alpha and then the Wolfram Language, and in all the complicated and often deep questions involved in for the first time creating a full-scale computational language. And so for nearly 17 years I did almost no basic science. The ideas of A New Kind of Science nevertheless continued to exert a deep influence—and I came to see my decades of work on computational language as ultimately being about creating a bridge between the vast capabilities of the computational universe revealed by A New Kind of Science, and the specific kinds of ways we humans are able to think about things. This point of view led me to all kinds of important conclusions about the role of computation and its implications for the future. But through all this I kept on thinking that one day I should look at physics again. And finally in 2019, stimulated by a small technical breakthrough, as well as enthusiasm from physicists of a new generation, I decided it was time to try diving into physics again. My practical tools had developed a lot since I’d worked on A New Kind of Science. And—as I have found so often—the passage of years had given me greater clarity and perspective about what I’d discovered in A New Kind of Science. And it turned out we were rather quickly able to make spectacular progress. A New Kind of Science had introduced definite ideas about how fundamental physics might work. Now we could see that these ideas were very much on the right track, but on their own they did not go far enough. Something else was needed. In A New Kind of Science I’d introduced what I called multiway systems, but I’d treated them as a kind of sideshow. Now—particularly tipped off by quantum mechanics—we realized that multiway systems were not a sideshow but were actually in a sense the main event. They had come out of the computational paradigm of A New Kind of Science, but they were really harbingers of a new paradigm: the multicomputational paradigm. In A New Kind of Science, I’d already talked about space—and everything else in the universe—ultimately being made up of a network of discrete elements that I’d now call “atoms of space”. And I’d talked about time being associated with the inexorable progressive application of computationally irreducible rules. But now we were thinking not just of a single thread of computation, but instead of a whole multiway system of branching and merging threads—representing in effect a multicomputational history for the universe. In A New Kind of Science I’d devoted a whole chapter to “Processes of Perception and Analysis”, recognizing the importance of the observer in computational systems. But with multicomputation there was yet more focus on this, and on how a physical observer knits things together to form a coherent thread of experience. Indeed, it became clear that it’s certain features of the observer that ultimately determine the laws of physics we perceive. And in particular it seems that as soon as we—somehow reflecting core features of our conscious experience—believe that we exist persistently through time, but are computationally bounded, then it follows that we will attribute to the universe the central known laws of spacetime and quantum mechanics. At the level of atoms of space and individual threads of history everything is full of computational irreducibility. But the key point is that observers like us don’t experience this; instead we sample certain computationally reducible features—that we can describe in terms of meaningful “laws of physics”. I never expected it would be so easy, but by early 2020—only a few months into our Wolfram Physics Project—we seemed to have successfully identified how the “machine code” of our universe must work. A New Kind of Science had established that computation was a powerful way of thinking about things. But now it was becoming clear that actually our whole universe is in a sense “computational all the way down”. But where did this leave the traditional mathematical view? To my surprise, far from being at odds it seemed as if our computation-all-the-way-down model of physics perfectly plugged into a great many of the more abstract existing mathematical approaches. Mediated by multicomputation, the concepts of A New Kind of Science—which began as an effort to go beyond mathematics—seemed now to be finding a kind of ultimate convergence with mathematics. But despite our success in working out the structure of the “machine code” for our universe, a major mystery remained. Let’s say we could find a particular rule that could generate everything in our universe. Then we’d have to ask “Why this rule, and not another?” And if “our rule” was simple, how come we’d “lucked out” like that? Ever since I was working on A New Kind of Science I’d wondered about this. And just as we were getting ready to announce the Physics Project in May 2020 the answer began to emerge. It came out of the multicomputational paradigm. And in a sense it was an ultimate version of it. Instead of imagining that the universe follows some particular rule—albeit applying it multicomputationally in all possible ways—what if the universe follows all possible rules? And then we realized: this is something much more general than physics. And in a sense it’s the ultimate computational construct. It’s what one gets if one takes all the programs in the computational universe that I studied in A New Kind of Science and runs them together—as a single, giant, multicomputational system. It’s a single, unique object that I call the ruliad, formed as the entangled limit of all possible computations. There’s no choice about the ruliad. Everything about it is abstractly necessary—emerging as it does just from the formal concept of computation. A New Kind of Science developed the abstraction of thinking about things in terms of computation. The ruliad takes this to its ultimate limit—capturing the whole entangled structure of all possible computations—and defining an object that in some sense describes everything. Once we believe—as the Principle of Computational Equivalence implies—that things like our universe are computational, it then inevitably follows that they are described by the ruliad. But the observer has a crucial role here. Because while as a matter of theoretical science we can discuss the whole ruliad, our experience of it inevitably has to be based on sampling it according to our actual capabilities of perception. In the end, it’s deeply analogous to something that—as I mention in A New Kind of Science—first got me interested in fundamental questions in science 50 years ago: the Second Law of thermodynamics. The molecules in a gas move around and interact according to certain rules. But as A New Kind of Science argues, one can think about this as a computational process, which can show computational irreducibility. If one didn’t worry about the “mechanics” of the observer, one might imagine that one could readily “see through” this computational irreducibility, to the detailed behavior of the molecules underneath. But the point is that a realistic, computationally bounded observer—like us—will be forced by computational irreducibility to perceive only certain “coarse-grained” aspects of what’s going on, and so will consider the gas to be behaving in a standard large-scale thermodynamic way. And so it is, at a grander level, with the ruliad. Observers like us can only perceive certain aspects of what’s going on in the ruliad, and a key result of our Physics Project is that with only quite loose constraints on what we’re like as observers, it’s inevitable that we will perceive our universe to operate according to particular precise known laws of physics. And indeed the attributes that we associate with “consciousness” seem closely tied to what’s needed to get the features of spacetime and quantum mechanics that we know from physics. In A New Kind of Science one of the conclusions is that the Principle of Computational Equivalence implies a fundamental equivalence between systems (like us) that we consider “intelligent” or “conscious”, and systems that we consider “merely computational”. But what’s now become clear in the multicomputational paradigm is that there’s more to this story. It’s not (as people have often assumed) that there’s something more powerful about “conscious observers” like us. Actually, it’s rather the opposite: that in order to have consistent “conscious experience” we have to have certain limitations (in particular, computational boundedness, and a belief of persistence in time), and these limitations are what make us “see the ruliad” in the way that corresponds to our usual view of the physical world. The concept of the ruliad is a powerful one, with implications that significantly transcend the traditional boundaries of science. For example, last year I realized that thinking in terms of the ruliad potentially provides a meaningful answer to the ultimate question of why our universe exists. The answer, I posit, is that the ruliad—as a “purely formal” object—“necessarily exists”. And what we perceive as “our universe” is then just the “slice” that corresponds to what we can “see” from the particular place in “rulial space” at which we happen to be. There has to be “something there”—and the remarkable fact is that for an observer with our general characteristics, that something has to have features that are like our usual laws of physics. In A New Kind of Science I discussed how the Principle of Computational Equivalence implies that almost any system can be thought of as being “like a mind” (as in, “the weather has a mind of its own”). But the issue—that for example is of central importance in talking about extraterrestrial intelligence—is how similar to us that mind is. And now with the ruliad we have a more definite way to discuss this. Different minds (even different human ones) can be thought of as being at different places in the ruliad, and thus in effect attributing different rules to the universe. The Principle of Computational Equivalence implies that there must ultimately be a way to translate (or, in effect, move) from one place to another. But the question is how far it is. Our senses and measuring devices—together with our general paradigms for thinking about things—define the basic area over which our understanding extends, and for which we can readily produce a high-level narrative description of what’s going on. And in the past we might have assumed that this was all we’d ever need to reach with whatever science we built. But what A New Kind of Science—and now the ruliad—show us is that there’s much more out there. There’s a whole computational universe of possible programs—many of which behave in ways that are far from our current domain of high-level understanding. Traditional science we can view as operating by gradually expanding our domain of understanding. But in a sense the key methodological idea that launched A New Kind of Science is to do computational experiments, which in effect just “jump without prior understanding” out into the wilds of the computational universe. And that’s in the end why all that ruliology in A New Kind of Science at first looks so alien: we’ve effectively jumped quite far from our familiar place in rulial space, so there’s no reason to expect we’ll recognize anything. And in effect, as the title of the book says, we need to be doing a new kind of science. In A New Kind of Science, an important part of the story has to do with the phenomenon of computational irreducibility, and the way in which it prevents any computationally bounded observer (like us) from being able to “reduce” the behavior of systems, and thereby perceive them as anything other than complex. But now that we’re thinking not just about computation, but about multicomputation, other attributes of other observers start to be important too. And with the ruliad ultimately representing everything, the question of what will be perceived in any particular case devolves into one about the characteristics of observers. In A New Kind of Science I give examples of how the same kinds of simple programs (such as cellular automata) can provide good “metamodels” for a variety of kinds of systems in nature and elsewhere, that show up in very different areas of science. But one feature of different areas of science is that they’re often concerned with different kinds of questions. And with the focus on the characteristics of the observer this is something we get to capture—and we get to discuss, for example, what the chemical observer, or the economic observer, might be like, and how that affects their perception of what’s ultimately in the ruliad. In Chapter 12 of A New Kind of Science there’s a long section on “Implications for Mathematics and Its Foundations”, which begins with the observation that just as many models in science seem to be able to start from simple rules, mathematics is traditionally specifically set up to start from simple axioms. I then analyzed how multiway systems could be thought of as defining possible derivations (or proofs) of new mathematical theorems from axioms or other theorems—and I discussed how the difficulty of doing mathematics can be thought of as a reflection of computational irreducibility. But informed by our Physics Project I realized that there’s much more to say about the foundations of mathematics—and this has led to our recently launched Metamathematics Project. At the core of this project is the idea that mathematics, like physics, is ultimately just a sampling of the ruliad. And just as the ruliad defines the lowest-level machine code of physics, so does it also for mathematics. The traditional axiomatic level of mathematics (with its built-in notions of variables and operators and so on) is already higher level than the “raw ruliad”. And a crucial observation is that just like physical observers operate at a level far above things like the atoms of space, so “mathematical observers” mostly operate at a level far above the raw ruliad, or even the “assembly code” of axioms. In an analogy with gases, the ruliad—or even axiom systems—are talking about the “molecular dynamics” level; but “mathematical observers” operate more at the “fluid dynamics” level. And the result of this is what I call the physicalization of metamathematics: the realization that our “perception” of mathematics is like our perception of physics. And that, for example, the very possibility of consistently doing higher-level mathematics where we don’t always have to drop down to the level of axioms or the raw ruliad has the same origin as the fact that “observers like us” typically view space as something continuous, rather than something made up of lots of atoms of space. In A New Kind of Science I considered it a mystery why phenomena like undecidability are not more common in typical pure mathematics. But now our Metamathematics Project provides an answer that’s based on the character of mathematical observers. My stated goal at the beginning of A New Kind of Science was to go beyond the mathematical paradigm, and that’s exactly what was achieved. But now there’s almost a full circle—because we see that building on A New Kind of Science and the computational paradigm we reach the multicomputational paradigm and the ruliad, and then we realize that mathematics, like physics, is part of the ruliad. Or, put another way, mathematics, like physics—and like everything else—is “made of computation”, and all computation is in the ruliad. And that means that insofar as we consider there to be physical reality, so also we must consider there to be “mathematical reality”. Physical reality arises from the sampling of the ruliad by physical observers; so similarly mathematical reality must arise from the sampling of the ruliad by mathematical observers. Or, in other words, if we believe that the physical world exists, so we must—essentially like Plato—also believe that the mathematics exists, and that there is an underlying reality to mathematics. All of these ideas rest on what was achieved in A New Kind of Science but now go significantly beyond it. In an “Epilog” that I eventually cut from the final version of A New Kind of Science I speculated that “major new directions” might be built in 15–30 years. And when I wrote that, I wasn’t really expecting that I would be the one to be central in doing that. And indeed I suspect that had I simply continued the direct path in basic science defined by my work on A New Kind of Science, it wouldn’t have been me. It’s not something I’ve explicitly planned, but at this point I can look back on my life so far and see it as a repeated alternation between technology and basic science. Each builds on the other, giving me both ideas and tools—and creating in the end a taller and taller intellectual tower. But what’s crucial is that every alternation is in many ways a fresh start, where I’m able to use what I’ve done before, but have a chance to reexamine everything from a new perspective. And so it has been in the past few years with A New Kind of Science: having returned to basic science after 17 years away, it’s been possible to make remarkably rapid and dramatic progress that’s taken things to a new and wholly unexpected level.
Twenty Years Later: The Surprising Greater Implications of A New Kind of Science 25.3 The Arrival of a Fourth Scientific Paradigm : In the course of intellectual history, there’ve been very few fundamentally different paradigms introduced for theoretical science. The first is what one might call the “structural paradigm”, in which one’s basically just concerned with what things are made of. And beginning in antiquity—and continuing for two millennia—this was pretty much the only paradigm on offer. But in the 1600s there was, as I described it in the opening sentence of A New Kind of Science, a “dramatic new idea”—that one could describe not just how things are, but also what they can do, in terms of mathematical equations. And for three centuries this “mathematical paradigm” defined the state of the art for theoretical science. But as I went on to explain in the opening paragraph of A New Kind of Science, my goal was to develop a new “computational paradigm” that would describe things not in terms of mathematical equations but instead in terms of computational rules or programs. There’d been precursors to this in my own work in the 1980s, but despite the practical use of computers in applying the mathematical paradigm, there wasn’t much of a concept of describing things, say in nature, in a fundamentally computational way. One feature of a mathematical equation is that it aims to encapsulate “in one fell swoop” the whole behavior of a system. Solve the equation and you’ll know everything about what the system will do. But in the computational paradigm it’s a different story. The underlying computational rules for a system in principle determine what it will do. But to actually find out what it does, you have to run those rules—which is often a computationally irreducible process. Put another way: in the structural paradigm, one doesn’t talk about time at all. In the mathematical paradigm, time is there, but it’s basically just a parameter, that if you can solve the equations you can set to whatever value you want. In the computational paradigm, however, time is something more fundamental: it’s associated with the actual irreducible progression of computation in a system. It’s an important distinction that cuts to the core of theoretical science. Heavily influenced by the mathematical paradigm, it’s often been assumed that science is fundamentally about being able to make predictions, or in a sense having a model that can “outrun” the system you’re studying, and say what it’s going to do with much less computational effort than the system itself. But computational irreducibility implies that there’s a fundamental limit to this. There are systems whose behavior is in effect “too complex” for us to ever be able to “find a formula for it”. And this is not something we could, for example, resolve just by increasing our mathematical sophistication: it is a fundamental limit that arises from the whole structure of the computational paradigm. In effect, from deep inside science we’re learning that there are fundamental limitations on what science can achieve. But as I mentioned in A New Kind of Science, computational irreducibility has an upside as well. If everything were computationally reducible, the passage of time wouldn’t in any fundamental sense add up to anything; we’d always be able to “jump ahead” and see what the outcome of anything would be without going through the steps, and we’d never have something we could reasonably experience as free will. In practical computing it’s pretty common to want to go straight from “question” to “answer”, and not be interested in “what happened inside”. But in A New Kind of Science there is in a sense an immediate emphasis on “what happens inside”. I don’t just show the initial input and final output for a cellular automaton. I show its whole “spacetime” history. And now that we have a computational theory of fundamental physics we can see that all the richness of our physical experience is contained in the “process inside”. We don’t just want to know the endpoint of the universe; we want to live the ongoing computational process that corresponds to our experience of the passage of time. But, OK, so in A New Kind of Science we reached what we might identify as the third major paradigm for theoretical science. But the exciting—and surprising—thing is that inspired by our Physics Project we can now see a fourth paradigm: the multicomputational paradigm. And while the computational paradigm involves considering the progression of particular computations, the multicomputational paradigm involves considering the entangled progression of many computations. The computational paradigm involves a single thread of time. The multicomputational paradigm involves multiple threads of time that branch and merge. What in a sense forced us into the multicomputational paradigm was thinking about quantum mechanics in our Physics Project, and realizing that multicomputation was inevitable in our models. But the idea of multicomputation is vastly more general, and in fact immediately applies to any system where at any given step multiple things can happen. In A New Kind of Science I studied many kinds of computational systems—like cellular automata and Turing machines—where one definite thing happens at each step. I looked a little at multiway systems—primarily ones based on string rewriting. But now in general in the multicomputational paradigm one is interested in studying multiway systems of all kinds. They can be based on simple iterations, say involving numbers, in which multiple functions can be applied at each step. They can be based on systems like games where there are multiple moves at each step. And they can be based on a whole range of systems in nature, technology and elsewhere where there are multiple “asynchronous” choices of events that can occur. Given the basic description of multicomputational systems, one might at first assume that whatever difficulties there are in deducing the behavior of computational systems, they would only be greater for multicomputational systems. But the crucial point is that whereas with a purely computational system (like a cellular automaton) it’s perfectly reasonable to imagine “experiencing” its whole evolution—say just by seeing a picture of it, the same is not true of a multicomputational system. Because for observers like us, who fundamentally experience time in a single thread, we have no choice but to somehow “sample” or “coarse grain” a multicomputational system if we are to reduce its behavior to something we can “experience”. And there’s then a remarkable formal fact: if one has a system that shows fundamental computational irreducibility, then computationally bounded “single-thread-of-time” observers inevitably perceive certain effective behavior in the system, that follows something like the typical laws of physics. Once again we can make an analogy with gases made from large numbers of molecules. Large-scale (computationally bounded) observers will essentially inevitably perceive gases to follow, say, the standard gas laws, quite independent of the detailed properties of individual molecules. In other words, the interplay between an “observer like us” and a multicomputational system will effectively select out a slice of computational reducibility from the underlying computational irreducibility. And although I didn’t see this coming, it’s in the end fairly obvious that something like this has to happen. The Principle of Computational Equivalence makes it basically inevitable that the underlying processes in the universe will be computationally irreducible. But somehow the particular features of the universe that we perceive and care about have to be ones that have enough computational reducibility that we can, for example, make consistent decisions about what to do, and we’re not just continually confronted by irreducible unpredictability. So how general can we expect this picture of multicomputation to be, with its connection to the kinds of things we’ve seen in physics? It seems to be extremely general, and to provide a true fourth paradigm for theoretical science. There are many kinds of systems for which the multicomputational paradigm seems to be immediately relevant. Beyond physics and metamathematics, there seems to be near-term promise in chemistry, molecular biology, evolutionary biology, neuroscience, immunology, linguistics, economics, machine learning, distributed computing and more. In each case there are underlying low-level elements (such as molecules) that interact through some kind of events (say collisions or reactions). And then there’s a big question of what the relevant observer is like. In chemistry, for example, the observer could just measure the overall concentration of some kind of molecule, coarse-graining together all the individual instances of those molecules. Or the observer could be sensitive, for example, to detailed causal relationships between collisions among molecules. In traditional chemistry, things like this generally aren’t “observed”. But in biology (for example in connection with membranes), or in molecular computing, they may be crucial. When I began the project that became A New Kind of Science the central question I wanted to answer is why we see so much complexity in so many kinds of systems. And with the computational paradigm and the ubiquity of computational irreducibility we had an answer, which also in a sense told us why it was difficult to make certain kinds of progress in a whole range of areas. But now we’ve got a new paradigm, the multicomputational paradigm. And the big surprise is that through the intermediation of the observer we can tap into computational reducibility, and potentially find “physics-like” laws for all sorts of fields. This may not work for the questions that have traditionally been asked in these fields. But the point is that with the “right kind of observer” there’s computational reducibility to be found. And that computational reducibility may be something we can tap into for understanding, or to use some kind of system for technology. It can all be seen as starting with the ruliad, and involving almost philosophical questions of what one can call “observer theory”. But in the end it gives us very practical ideas and methods that I think have the potential to lead to unexpectedly dramatic progress in a remarkable range of fields. I knew that A New Kind of Science would have practical applications, particularly in modeling, in technology and in producing creative material. And indeed it has. But for our Physics Project applications seemed much further away, perhaps centuries. But a great surprise has been that through the multicomputational paradigm it seems as if there are going to be some quite immediate and very practical applications of the Physics Project. In a sense the reason for this is that through the intermediation of multicomputation we see that many kinds of systems share the same underlying “metastructure”. And this means that as soon as there are things to say about one kind of system these can be applied to other systems. And in particular the great successes of physics can be applied to a whole range of systems that share the same multicomputational metastructure. An immediate example is in practical computing, and particularly in the Wolfram Language. It’s something of a personal irony that the Wolfram Language is based on transformation rules for symbolic expressions, which is a structure very similar to what ends up being what’s involved in the Physics Project. But there’s a crucial difference: in the usual case of the Wolfram Language, everything works in a purely computational way, with a particular transformation being done at each step. But now there’s the potential to generalize that to the multicomputational case, and in effect to trace the multiway system of every possible transformation. It’s not easy to pick out of that structure things that we can readily understand. But there are important lessons from physics for this. And as we build out the multicomputational capabilities of the Wolfram Language I fully expect that the “notational clarity” it will bring will help us to formulate much more in terms of the multicomputational paradigm. I built the Wolfram Language as a tool that would help me explore the computational paradigm, and from that paradigm there emerged principles like the Principle of Computational Equivalence, which in turn led me to see the possibility of something like Wolfram|Alpha. But now from the latest basic science built on the foundations of A New Kind of Science, together with the practical tooling of the Wolfram Language, it’s becoming possible again to see how to make conceptual advances that can drive technology that will again in turn let us make—likely dramatic—progress in basic science.
Twenty Years Later: The Surprising Greater Implications of A New Kind of Science 25.4 Harvesting Seeds from A New Kind of Science : A New Kind of Science is full of intellectual seeds. And in the past few years—having now returned to basic science—I’ve been harvesting a few of those seeds. The Physics Project and the Metamathematics Project are two major results. But there’s been quite a bit more. And in fact it’s rather remarkable how many things that were barely more than footnotes in A New Kind of Science have turned into major projects, with important results. Back in 2018—a year before beginning the Physics Project—I returned, for example, to what’s become known as the Wolfram Axiom: the axiom that I found in A New Kind of Science that is the very simplest possible axiom for Boolean algebra. But my focus now was not so much on the axiom itself as on the automated process of proving its correctness, and the effort to see the relation between “pure computation” and what one might consider a human-absorbable “narrative proof”. Computational irreducibility appeared many times, notably in my efforts to understand AI ethics and the implications of computational contracts. I’ve no doubt that in the years to come, the concept of computational irreducibility will become increasingly important in everyday thinking—a bit like how concepts such as energy and momentum from the mathematical paradigm have become important. And in 2019, for example, computational irreducibility made an appearance in government affairs, as a result of me testifying about its implications for legislation about AI selection of content on the internet. In A New Kind of Science I explored many specific systems about which one can ask all sorts of questions. And one might think that after 20 years “all the obvious questions” would have been answered. But they have not. And in a sense the fact that they have not is a direct reflection of the ubiquity of computational irreducibility. But it’s a fundamental feature that whenever there’s computational irreducibility, there must also be pockets of computational reducibility: in other words, the very existence of computational irreducibility implies an infinite frontier of potential progress. Back in 2007, we’d had great success with our Turing Machine Prize, and the Turing machine that I’d suspected was the very simplest possible universal Turing machine was indeed proved universal—providing another piece of evidence for the Principle of Computational Equivalence. And in a sense there’s a general question that’s raised by A New Kind of Science about where the threshold of universality—or computational equivalence—really is in different kinds of systems. But there are simpler-to-define questions as well. And ever since I first studied rule 30 in 1984 I’d wondered about many questions related to it. And in October 2019 I decided to launch the Rule 30 Prizes, defining three specific easy-to-state questions about rule 30. So far I don’t know of progress on them. And for all I know they’ll be open problems for centuries. From the point of view of the ruliad we can think of them as distant explorations in rulial space, and the question of when they can be answered is like the question of when we’ll have the technology to get to some distant place in physical space. Having launched the Physics Project in April 2020, it was rapidly clear that its ideas could also be applied to metamathematics. And it even seemed as if it might be easier to make relevant “real-world” observations in metamathematics than in physics. And the seed for this was in a note in A New Kind of Science entitled “Empirical Metamathematics”. That note contained one picture of the theorem-dependency graph of Euclid’s Elements, which in the summer of 2020 expanded into a 70-page study. And in my recent “Physicalization of Metamathematics” there’s a continuation of that—beginning to map out empirical metamathematical space, as explored in the practice of mathematics, with the idea that multicomputational phenomena that in physics may take technically infeasible particle accelerators or telescopes might actually be within reach. In addition to being the year we launched our Physics Project, 2020 was also the 100th anniversary of combinators—the first concrete formalization of universal computation. In A New Kind of Science I devoted a few pages and some notes to combinators, but I decided to do a deep dive and use both what I’d learned from A New Kind of Science and from the Physics Project to take a new look at them. Among other things the result was another application of multicomputation, as well as the realization that even though the S, K combinators from 1920 seemed very minimal, it was possible that S alone might also be universal, though with something different than the usual input → output “workflow” of computation. In A New Kind of Science a single footnote mentions multiway Turing machines. And early last year I turned this seed into a long and detailed study that provides further foundational examples of multicomputation, and explores the question of just what it means to “do a computation” multicomputationally—something which I believe is highly relevant not only for practical distributed computing but also for things like molecular computing. In 2021 it was the centenary of Post tag systems, and again I turned a few pages in A New Kind of Science into a long and detailed study. And what’s important about both this and my study of combinators is that they provide foundational examples (much like cellular automata in A New Kind of Science), which even in the past year or so I’ve used multiple times in different projects. In mid-2021, yet another few-page discussion in A New Kind of Science turned into a detailed study of “The Problem of Distributed Consensus”. And once again, this turned out to have a multicomputational angle, at first in understanding the multiway character of possible outcomes, but later with the realization that the formation of consensus is deeply related to the process of measurement and the coarse-graining involved in it—and the fundamental way that observers extract “coherent experiences” from systems. In A New Kind of Science, there’s a short note about multiway systems based on numbers. And once again, in fall 2021 I expanded on this to produce an extensive study of such systems, as a certain kind of very minimal example of multicomputation, that at least in some cases connects with traditional mathematical ideas. From the vantage point of multicomputation and our Physics Project it’s interesting to look back at A New Kind of Science, and see some of what it describes with more clarity. In the fall of 2021, for example, I reviewed what had become of the original goal of “understanding complexity”, and what methodological ideas had emerged from that effort. I identified two primary ones, which I called “ruliology” and “metamodeling”. Ruliology, as I’ve mentioned above, is my new name for the pure, basic science of studying the behavior of systems with simple rules: in effect, it’s the science of exploring the computational universe. Metamodeling is the key to making connections to systems in nature and elsewhere that one wants to study. Its goal is to find the “minimal models for models”. Often there are existing models for systems. But the question is what the ultimate essence of those models is. Can everything be reduced to a cellular automaton? Or a multiway system? What is the minimal “computational essence” of a system? And as we begin to apply the multicomputational paradigm to different fields, a key step will be metamodeling. Ruliology and metamodeling are in a sense already core concepts in A New Kind of Science, though not under those names. Observer theory is much less explicitly covered. And many concepts—like branchial space, token-event graphs, the multiway causal graph and the ruliad—have only emerged now, with the Physics Project and the arrival of the multicomputational paradigm. Multicomputation, the Physics Project and the Metamathematics Project are sowing their own seeds. But there are still many more seeds to harvest even from A New Kind of Science. And just as the multicomputational paradigm was not something that I, for one, could foresee from A New Kind of Science, no doubt there will in time be other major new directions that will emerge. But, needless to say, one should expect that it will be computationally irreducible to determine what will happen: a metacontribution of the science to the consideration of its own future.
Twenty Years Later: The Surprising Greater Implications of A New Kind of Science 25.5 The Doing of Science : The creation of A New Kind of Science took me a decade of intense work, none of which saw the light of day until the moment the book was published on May 14, 2002. Returning to basic science 17 years later the world had changed and it was possible for me to adopt a quite different approach, in a sense making the process of doing science as open and incremental as possible. It’s helped that there’s the web, the cloud and livestreaming. But in a sense the most crucial element has been the Wolfram Language, and its character as a full-scale computational language. Yes, I use English to tell the story of what we’re doing. But fundamentally I’m doing science in the Wolfram Language, using it both as a practical tool, and as a medium for organizing my thoughts, and sharing and communicating what I’m doing. Starting in 2003, we’ve had an annual Wolfram Summer School at which a long string of talented students have explored ideas based on A New Kind of Science, always through the medium of the Wolfram Language. In the last couple of years we’ve added a Physics track, connected to the Physics Project, and this year we’re adding a Metamathematics track, connected to the Metamathematics Project. During the 17 years that I wasn’t focused on basic science, I was doing technology development. And I think it’s fair to say that at Wolfram Research over the past 35 years we’ve created a remarkably effective “machine” for doing innovative research and development. Mostly it’s been producing technology and products. But one of the very interesting features of the Physics Project and the projects that have followed it is that we’ve been applying the same managed approach to innovation to them that we have been using so successfully for so many years at our company. And I consider the results to be quite spectacular: in a matter of weeks or months I think we’ve managed to deliver what might otherwise have taken years, if it could have been done at all. And particularly with the arrival of the multicomputational paradigm there’s quite a challenge. There are a huge number of exceptionally promising directions to follow, that have the potential to deliver revolutionary results. And with our concepts of managed research, open science and broad connection to talent it should be possible to make great progress even fairly quickly. But to do so requires significant scaling up of our efforts so far, which is why we’re now launching the Wolfram Institute to serve as a focal point for these efforts. When I think about A New Kind of Science, I can’t help but be struck by all the things that had to align to make it possible. My early experiences in science and technology, the personal environment I’d created—and the tools I built. I wondered at the time whether the five years I took “away from basic science” to launch Mathematica and what’s now the Wolfram Language might have slowed down what became A New Kind of Science. Looking back I can say that the answer was definitively no. Because without the Wolfram Language the creation of A New Kind of Science would have needed “not just a decade”, but likely more than a lifetime. And a similar pattern has repeated now, though even more so. The Physics Project and everything that has developed from it has been made possible by a tower of specific circumstances that stretch back nearly half a century—including my 17-year hiatus from basic science. Had all these circumstances not aligned, it is hard to say when something like the Physics Project would have happened, but my guess is that it would have been at least a significant part of a century away. It is a lesson of the history of science that the absorption of major new paradigms is a slow process. And normally the timescales are long compared to the 20 years since A New Kind of Science was published. But in a sense we’ve managed to jump far ahead of schedule with the Physics Project and with the development of the multicomputational paradigm. Five years ago, when I summarized the first 15 years of A New Kind of Science I had no idea that any of this would happen. But now that it has—and with all the methodology we’ve developed for getting science done—it feels as if we have a certain obligation to see just what can be achieved. And to see just what can be built in the years to come on the foundations laid down by A New Kind of Science.
Games and Puzzles as Multicomputational Systems 26.1 Humanizing Multicomputational Processes : Multicomputation is one of the core ideas of the Wolfram Physics Project—and in particular is at the heart of our emerging understanding of quantum mechanics. But how can one get an intuition for what is initially the rather abstract idea of multicomputation? A good approach, I believe, is to see it in action in familiar systems and situations. And I explore here what seems like a particularly good example: games and puzzles. One might not imagine that something as everyday as well-known games and puzzles would have any connection to the formalism for something like quantum mechanics. But the idea of multicomputation provides a link. And indeed one can view the very possibility of being able to have “interesting” games and puzzles as being related to a core phenomenon of multicomputation: multicomputational irreducibility. In an ordinary computational system each state of the system has a unique successor, and ultimately there is a single thread of time that defines a process of computation. But in a multicomputational system the key idea is that states can have multiple successors—and tracing their behavior defines a whole multiway graph of branching and merging threads of time. And the point is that this is directly related to how one can think about typical games and puzzles. Given a particular state of a game or puzzle, a player must typically decide what to do next. And where the idea of multicomputation comes in is that there are usually several choices that they can make. In any particular instance of the game, they’ll make a particular choice. But the point of the multicomputational paradigm is to look globally at the consequences of all possible choices—and to produce a multiway graph that represents them. The notion of making what we call a multiway graph has actually existed—usually under the name of “game graphs”—for games and puzzles for a bit more than a hundred years. But with the multicomputational paradigm there are now some more general concepts that can be applied to these constructs. And in turn understanding the relation to games and puzzles has the potential to provide a new level of intuition and familiarity about multiway graphs. My particular goal here is to investigate—fairly systematically—a sequence of well-known games and puzzles using the general methods we’ve been developing for studying multicomputational systems. As is typical in investigations that connect with everyday things, we’ll encounter all sorts of specific details. And while these may not immediately seem relevant to larger-scale discussions, they are important in our effort to provide a realistic and relatable picture of actual games and puzzles—and in allowing the connections we make with multicomputation to be on a solid foundation. It’s worth mentioning that the possibility of relating games and puzzles to physics is basically something that wouldn’t make sense without our Physics Project. For games and puzzles are normally at some fundamental level discrete—especially in the way that they involve discrete branching of possibilities. And if one assumes physics is fundamentally continuous, there’s no reason to expect a connection. But a key idea of our Physics Project is that the physical world is at the lowest level discrete—like games and puzzles. And what’s more, our Physics Project posits that physics—like games and puzzles—has discrete possibilities to explore. At the outset each of the games and puzzles I discuss here may seem rather different in their structure and operation. But what we’ll see is that when viewed in a multicomputational way, there is remarkable—and almost monotonous—uniformity across our different examples. I won’t comment too much on the significance of what we see until the end, when I’ll begin to discuss how various important multicomputational phenomena may play out in the context of games and puzzles. And how the very difficulty of conceptualizing multicomputation in straightforward human terms is what fundamentally leads to the engaging character of games and puzzles.
Games and Puzzles as Multicomputational Systems 26.2 Tic-Tac-Toe : Consider a simplified version of tic-tac-toe (AKA “noughts and crosses”) played on a 2×2 board. Assume X plays first. Then one can represent the possible moves by the graph: On the next turn one gets: So far this graph is a simple tree. But if we play another turn we’ll see that different branches can merge, and “playing until the board is full” we get a multiway graph—or “game graph”—of the form: Every path through this graph represents a possible complete game: In our setup so far, the total number of board configurations that can ever be reached in any game (i.e. the total number of nodes in the graph) is 35, while the total number of possible complete games (i.e. the number of possible paths from the root of the graph) is 24. If one renders the graph in 3D one can see that it has a very regular structure: And now if we define “winning” 2×2 tic-tac-toe as having two identical elements in a horizontal row, then we can annotate the multiway graph to indicate wins—removing cases where the “game is already over”: Much of the core structure of the multiway graph is actually already evident even in the seemingly trivial case of “one-player tic-tac-toe”, in which one is simply progressively filling in squares on the board: But what makes this not completely trivial is the existence of distinct paths that lead to equivalent states. Rendered differently the graph (which has 24 = 16 nodes and 4! = 24 “game paths”) has an obvious 4D hypercube form (where now we have dropped the explicit X’s in each cell): For a 3×3 board the graph is a 9D hypercube with 29 = 512 nodes and 9! = 362880 “game paths”, or in “move-layered” form: This basic structure is already visible in “1-player 1D tic-tac-toe” in which the multiway graph for a “length-n” board just corresponds to an n-dimensional hypercube: With 2 players the graphs become slightly more complicated: The total number of states in these graphs is: which is asymptotically . (Note that for n = 4 the result is the same as for the 2×2 board discussed above.) At move t the number of distinct states is given by OK, so what about standard 2-player 3×3 tic-tac-toe? Its multiway graph begins: After 2 steps (i.e. one move by X and one by O) the graph is still a tree (with the initial state now at the center): After 3 steps there is starting to be merging: And continuing for all 9 moves the full layered graph—with 6046 states—is: At the level of this graph, the results are exactly the same as for a 2-player 1D version with a total of 9 squares. But for actual 2D 3×3 tic-tac-toe there is an additional element to the story: the concept of winning a game, and thereby terminating it. With the usual rules, a game is considered won when a player gets a horizontal, vertical or diagonal line of three squares, as in for example: Whenever a “win state” such as these is reached, the game is considered over, so that subsequent states in the multiway graph are pruned, and what was previously a 6046-node graph becomes a 5478-node graph with examples of the 568 pruned states including (where the “win” that terminated the game is marked): Wins can occur at different steps: anywhere from 5 to 9. The total numbers of distinct wins are as follows (yielding 626 wins at any step for X and 316 for O). One can’t explicitly tell that a game has ended in a draw until every square has been filled in—and there are ultimately only 16 final “draw configurations” that can be reached: We can annotate the full (“game-over-pruned”) multiway graph, indicating win and draw states: To study this further, let’s start by looking at a subgraph that includes only “end games” starting with a board that already has 4 squares filled in: We see here that from our initial board: it’s possible to get a final win for both X and O: But in many of these cases the outcome is already basically determined a step or more before the actual win occurs—in the sense that unless a given player “makes a mistake” they will always be able to force a win. So, for example, if it is X’s turn and the state is: then X is guaranteed to win if they play as follows: We can represent the “pre-forcing” of wins by coloring subgraphs (or in effect “light cones”) in the multiway graph: At the very beginning of the game, when X makes the first move, nothing is yet forced. But after just one move, it’s already possible to get to configurations where X can always force a win: Starting from a state obtained after 1 step, we can see that after 2 steps there are configurations where O can force a win: Going to more moves leads to more “forced-win” configurations: Annotating the whole multiway graph we get: We can think of this graph as a representation of the “solution” to the game: given any state the coloring in the graph tells us which player can force a win from that state, and the graph defines what moves they can make to do so. Here’s a summary of possible game states at each move: Here we’re just counting the number of possible states of various kinds at each step. But is there a way to think about these states as somehow being laid out in “game state space”? Branchial graphs provide a potential way to do this. The basic branchial graph at a particular step is obtained by joining pairs of states that share a common ancestor on the step before. For the case of 2-player 2×2 tic-tac-toe the branchial graphs we get on successive steps are as follows: Things get more complicated for ordinary 3×3 tic-tac-toe. But since the multiway graph for the first two steps is a pure tree, the branchial graphs at these steps still have a rather trivial structure: In general the number of connected components on successive steps is as follows: and these are broken down across different graph structures as follows: Here in more detail are the forms of some typical components of branchial graphs achieved at particular steps: Within the branchial graph at a particular step, there can be different numbers of wins in different components: It’s notable that the wins are quite broadly distributed across branchial graphs. And this is in a sense why tic-tac-toe is not (more) trivial. If just by knowing what component of the branchial graph one was in one could immediately know the outcome, there would be even less “suspense” in the game. But with broad distribution across branchial space, “knowing roughly where you are” doesn’t help much in determining whether you’re going to win. So far we’ve always been talking about what states can be reached, but not “how often” they’re reached. Imagine that rather than playing a specific game, we instead at each step just make every possible move with equal probability. The setup for tic-tac-toe is symmetrical enough that for most of the game the probability of every possible configuration at a given step is equal. But as soon as there start to be “wins”, and there is a “cone” of “game-over-pruned” states, then the remaining states no longer have equal probabilities. For standard 3×3 tic-tac-toe this happens after 7 moves, where there are two classes of states, that occur with slightly different probabilities: At the end of the game, there are several classes of final states with different probabilities: And what this means for the probabilities of different outcomes of the game is as follows: Not surprisingly, the player who plays first has an advantage in winning. Perhaps more surprising is that in this kind of “strategyless” play, ties are comparatively uncommon—even though if one player actively tries to block the other, they often force a tie. We’ve looked at “classic tic-tac-toe” and a few specific variants. But there are ultimately all sorts of possible variants. And a convenient general way to represent the “board” for any tic-tac-toe-like game is just to give a “flattened” list of values—with 0 representing a blank position, and i representing a symbol added by player i. In standard “2D representation” one might have a board like: which in flattened form would be: Typical winning patterns can then be represented: where in each case we have framed the relevant “winning symbols”, and then given their positions in the flattened list. In ordinary tic-tac-toe it’s clear that the positions of “winning symbols” must always form an arithmetic progression. And it seems as if a good way to generalize tic-tac-toe is always to define a win for i to be associated with the presence of i symbols at positions that form an arithmetic progression of a certain length s. For ordinary tic-tac-toe s = 3, but for generalizations it could have other values. Consider now the case of a length-5 list (i.e. 5-position “1D board”). The complete multiway graph is as follows, with “winning states” that contain arithmetic progressions of length s = 3 highlighted: In a more symmetrical rendering this is: Here’s the analogous result for a 7-position board, and 2 players: For each size of board n, we can compute the total number of winning states for any given player, as well as the total number of states altogether. The result when winning is based on arithmetic progressions of length 3 (i.e. s = 3) is: The 2-player n = 9 (= 3×3) case here is similar to ordinary tic-tac-toe, but not the same. In particular, states like: are considered wins for X in the flattened setup, but not in ordinary tic-tac-toe. If we increase the length of progression needed in order to declare a win, say to s = 4, we get: The total number of game states is unchanged, but—as expected—there are “fewer ways to win”. But let’s say we have boards that are completely filled in. For small board sizes there may well not be an arithmetic progression of positions for any player—so that the game has to be considered a tie—as we see in this n = 5 case: But it is a result related to Ramsey theory that it turns out that for n ≥ 9, it’s inevitable that there will be an “arithmetic progression win” for at least one of the players—so that there is never a tie—as these examples illustrate.
Games and Puzzles as Multicomputational Systems 26.3 Walks and Their Multiway Graphs : A game like tic-tac-toe effectively involves at each step moving to one of several possible new board configurations—which we can think of as being at different “places” in “game state space”. But what if instead of board configurations we just consider our states to be positions on a lattice such as and then we look at possible walks, that at each step can, in this case, go one unit in any of 4 directions? Starting at a particular point, the multiway graph after 1 step is just: where we have laid out this graph so that the “states” are placed at their geometrical positions on the lattice. After 2 steps we get: And in general the structure of the multiway graph just “recapitulates” the structure of the lattice: We can think of the paths in the multiway graph as representing all possible random walks of a certain length in the lattice. We can lay the graph out in 3D, with the vertical position representing the first step at which a given point can be reached: We can also lay out the graph more like we laid out multiway graphs for tic-tac-toe: One feature of these “random-walk” multiway graphs is that they contain loops, that record the possibility of “returning to places one’s already been”. And this is different from what happens for example in tic-tac-toe, in which at each step one is just adding an element to the board, and it’s never possible to go back. But we can set up a similar “never-go-back rule” for walks, by considering “self-avoiding walks” in which any point that’s been visited can never be visited again. Let’s consider first the very trivial lattice: Now indicate the “current place we’ve reached” by a red dot, and the places we’ve visited before by blue dots—and start from one corner: There are only two possible walks here, one going clockwise, the other counterclockwise. Allowing one to start in each possible position yields a slightly more complicated multiway graph: With a 2×3 grid we get: while with a 3×3 grid we get: Starting in the center, and with a different layout for the multiway graph, we get: Note the presence of large “holes”, in which paths on each side basically “get to the same place” in “opposite ways”. Note that of the 2304 possible ways to have 1 red dot and up to 8 blue ones, this actual multiway graph reaches only 57. (Starting from the corner reaches 75 and from all possible initial positions 438.) With a 4×4 lattice (starting the walker in the corner) the multiway graph has the form: or in an alternative layout: where now 1677 states out of 524,288 are eventually visited, and the number of new states visited at each step (i.e. the number of nodes in successive layers in the graph) is: For a 5×5 grid 89,961 states are reached, distributed across steps according to: (For a grid with n vertices, there are a total of n 2n–1 possible states, but the number actually reached is always much smaller.) In talking about walks, an obvious question to ask is about mazes. Consider the maze: As far as traversing this maze is concerned, it is equivalent to “walking” on the graph: which in another embedding is just: But just as before, the multiway graph that represents all possible walks essentially just “recapitulates” this graph. And that means that “solving” the maze can in a sense equally be thought of as finding a path directly in the maze graph, or in the multiway graph.
Games and Puzzles as Multicomputational Systems 26.4 The Icosian Game & Some Relatives : Our discussion of self-avoiding walks turns out to be immediately related to the “Icosian game” of William Rowan Hamilton from 1857 (which is somewhat related to the early computer game Hunt the Wumpus): The object of the “game” (or, more properly, puzzle) is to find a path (yes, a Hamiltonian path) around the icosahedron graph that visits every node (and returns back to where it started from). And once again we can construct a multiway graph that represents all possible sequences of “moves” in the game. Let’s start with the simpler case of an underlying tetrahedron graph: From this we get the multiway graph: The “combined multiway graph” from all possible starting positions on the tetrahedron graph gives a truncated cuboctahedron multiway graph: And following this graph we see that from any initial state it’s always possible to reach a state where every node in the tetrahedron graph has been visited. In fact, because the tetrahedron graph is a complete graph it’s even guaranteed that the last node in the sequence will be “adjacent” to the starting node—so that one has formed a Hamiltonian cycle and solved the puzzle. Things are less trivial for the cube graph: The multiway graph (starting from a particular state) in this case is: Now there are 13 configurations where no further moves are possible: In some of these, one’s effectively “boxed in” with no adjacent node to visit. In others, all the nodes have been filled in. But only 3 ultimately achieve a true Hamiltonian cycle that ends adjacent to the starting node: It turns out that one can reach each of these states through 4 distinct paths from the root of the multiway graph. An example of such a path is: We can summarize this path as a Hamiltonian circuit of the original cube graph: In the multiway graph, the 12 “winning paths” are: In a different rendering this becomes: and keeping only “winning paths” the subgraph of the multiway graph has the symmetrical form: The actual Hamiltonian circuits through the underlying cube graph corresponding to these winning paths are: For the dodecahedral graph (i.e. the original Icosian game), the multiway graph is larger and more complicated. It begins: and has its first merge after 11 steps (and 529 in all), and ends up with a total of 11,093 nodes—of which 2446 are “end states” where no further move is possible. This shows the number of end (below) and non-end (above) states at each successive step: The successive fractions of “on-track-to-succeed” states are as follows, indicating that the puzzle is in a sense harder at the beginning than at the end: There are 13 “end states” which fill in every position of the underlying dodecahedral graph, with 3 of these corresponding to Hamiltonian cycles: The total number of paths from the root of the multiway graph leading to end states (in effect the total number of ways to try to solve the puzzle) is 3120. Of these, 60 lead to the 3 Hamiltonian cycle end states. An example of one of these “winning paths” is: Examples of underlying Hamiltonian cycles corresponding to each of the 3 Hamiltonian cycle end states are: And this now shows all 60 paths through the multiway graph that reach Hamiltonian cycle end states—and thus correspond to solutions to the puzzle: In effect, solving the puzzle consists in successfully finding these paths out of all the possibilities in the multiway graph. In practice, though—much as in theorem-proving, for example—there are considerably more efficient ways to find “winning paths” than to look directly at all possibilities in the multiway graph (e.g. FindHamiltonianCycle in Wolfram Language). But for our purpose of understanding games and puzzles in a multicomputational framework, it’s useful to see how solutions to this puzzle lay out in the multiway graph. The Icosian game from Hamilton was what launched the idea of Hamiltonian cycles on graphs. But already in 1736 Leonhard Euler had discussed what are now called Eulerian cycles in connection with the puzzle of the Bridges of Königsberg. In modern terms, we can state the puzzle as the problem of finding a path that visits once and only once all the edges in the graph (in which the “double bridges” from the original puzzle have been disambiguated by extra nodes): We can create a multiway graph that represents all possible paths starting from a particular vertex: But now we see that the end states here are: and since none of them have visited every edge, there is no Eulerian circuit here. To completely resolve the puzzle we need to make a multiway graph in which we start from all possible underlying vertices. The result is a disconnected multiway graph whose end states again never visit every edge in the underlying graph (as one can tell from the fact that the number of “levels” in each subgraph is less than 10).
Games and Puzzles as Multicomputational Systems 26.5 The Geography Game : In the Geography Game one has a collection of words (say place names) and then one attempts to “string the words together”, with the last letter of one word being the same as the first letter of the next. The game typically ends when nobody can come up with a word that works and hasn’t been used before. Usually in practice the game is played with multiple players. But one can perfectly well consider a version with just one player. And as an example let’s take our “words” to be the abbreviations for the states in the US. Then we can make a graph of what can follow what: Let’s at first ignore the question of whether a state has “already been used”. Then, starting, say, from Massachusetts (MA), we can construct the beginning of a multiway graph that gives us all possible sequences: After 10 steps the graph is: or in a different rendering: This shows the total number of paths as a function of length through this graph, assuming one doesn’t allow any state to be repeated: The maximum length of path is 23—and there are 256 such paths, 88 ending with TX and 168 ending with AZ. A few sample such paths are and all these paths can be represented by what amounts to a finite state machine: By the way, the starting state that leads to the longest path is MN—which achieves length 24 in 2336 different ways, with possible endings being AZ, DE, KY and TX. A few samples are: Drawing these paths in the first few steps of the multiway graph starting from MN we get.
Games and Puzzles as Multicomputational Systems 26.6 Groups and (Simplified) Rubik’s Cubes : We’ve talked about puzzles that effectively involve walks on graphs. A particularly famous example of a puzzle that can be thought about in this way is the Rubik’s Cube. The graph in question is then the Cayley graph for the group formed by the transformations that can be applied to the cube. As a very simple analog, we can consider the symmetry group of the square, D4, based on the operations of reflection and 90° rotation. We generate the Cayley graph just like a multiway graph: by applying each operation at each step. And in this example the Cayley graph is: This graph is small enough that it is straightforward to see how to get from any configuration to any other. But while this Cayley graph has 8 nodes and maximum path length from any one node to any other of 3, the Cayley graph for the Rubik’s Cube has nodes and a maximum shortest path length of 20. To get some sense of the structure of an object like this, we can consider the very simplified case of a “2×2×2 cube”—colored only on its corners—in which each face can be rotated by 90°: The first step in the multiway graph—starting from the configuration above—is then (note that the edges in the graph are not directed, since the underlying transformations are always reversible): Going another step gives: The complete multiway graph (which is also the Cayley graph for the group—which turns out to be S8—generated by the transformations) has 8! = 40,320 nodes (and 483,840 edges). Starting from a state (i.e. node in the Cayley graph) the number of new states reached at successive steps is: The maximum shortest paths in the graph consist of 8 steps; an example is: Between these particular two endpoints there are actually 3216 “geodesic” paths—which spread out quite far in the multiway graph Picking out only geodesic paths we see there are many ways to get from one configuration of the cube to one of its “antipodes”.
Games and Puzzles as Multicomputational Systems 26.7 Peg Solitaire : Whereas something like tic-tac-toe involves progressively filling in a board, a large class of puzzles that have been used since at least the 1600s involve basically removing pegs from a board. The typical rules involve pegs being able to jump over a single intermediate peg into a hole, with the intermediate peg then being removed. The goal is to end up with just a single peg on the board. Here’s a very simple example based on a T arrangement of pegs: In this case, there’s only one way to “solve the puzzle”. But in general there’s a multiway graph: A more complicated example is the “Tricky Triangle” (AKA the “Cracker Barrel puzzle”). Its multiway graph begins: After another couple of steps it becomes: There are a total of 3016 states in the final multiway graph, of which 118 are “dead-end” configurations from which no further moves are possible. The “earliest” of these dead-end configurations are: There are just 4 “winning states” that can be reached, and the “end games” that lead to them are: Starting from the initial configuration, the number of possible states reached at each step is given as follows, where the states that can lead to winning configurations is shown in yellow: This shows the complete multiway graph, with “winning paths” highlighted: At successive steps, the fraction of states that can lead to a winning state is as follows: The branchial graphs are highly connected, implying that in a sense the puzzle remains “well mixed” and “unpredictable” until the very end.
Games and Puzzles as Multicomputational Systems 26.8 Checkers : Peg solitaire is a one-player “game”. Checkers (AKA draughts) is a two-player game with a somewhat similar setup. “Black” and “red” pieces move diagonally in different directions on a board, “taking” each other by jumping over when they are adjacent. Let’s consider the rather minimal example of a 4×4 board. The basic set of possible moves for “black” and “red” is defined by the graphs (note that a 4×4 board is too small to support “multiple jumps”): With this setup we can immediately start to generate a multiway graph, based on alternating black and red moves: With the rules as defined so far, the full 161-node multiway graph is: It’s not completely clear what it means to “win” in this simple 4×4 case. But one possibility is to say that it happens when the other player can’t do anything at their next move. This corresponds to “dead ends” in the multiway graph. There are 26 of these, of which only 3 occur when it is red’s move next, and the rest all occur when it is black’s move: As before, any particular checkers game corresponds to a path in the multiway graph from the root to one of these end states. If we look at branchial graphs in this case, we find that they have many disconnected pieces, indicating that there are many largely independent “game paths” for this simple game—so there is not much “mixing” of outcomes: The rules we’ve used so far don’t account for what amounts to the second level of rules for checkers: the fact that when a piece reaches the other side of the board it becomes a “king” that’s allowed to move backwards as well as forwards. Even with a single piece and single player this already generates a multiway graph—notably now with loops: or in an alternative layout (with explicitly undirected edges): With two pieces (and two players taking turns) the “kings” multiway graph begins: With this initial configuration, but without backward motion, the whole multiway graph is just: The full “kings” multiway graph in this case also only has 62 nodes—but includes all sorts of loops (though with this few pieces and black playing first it’s inevitable that any win will be for black): What about the ordinary + kings multiway graph from our original initial conditions? The combined graph has 161 nodes from the “pre-king” phase, and 4302 from the “post-king” phase—giving the final form.
Games and Puzzles as Multicomputational Systems 26.9 (Very Simplified) Go : The full game of Go is sophisticated and its multiway graph in any realistic case is far too big for us to generate at all explicitly (though one can certainly wonder if there are meaningful “continuum limit” results). However, to get some flavor of Go we can consider a vastly simplified version in which black and white “stones” are progressively placed on nodes of a graph, and the game is considered “won” if one player has successfully surrounded a connected collection of the other player’s stones. Imagine that we start with a blank “board” consisting of a 2×2 square of positions, then on a sequence of “turns” add black and white stones in all possible ways. The resulting multiway graph is: Every state that has no successor here is a win for either black or white. The “black wins” (with the surrounded stone highlighted) are: while the “white wins” are: At this level what we have is basically equivalent to 2×2 tic-tac-toe, albeit with a “diagonal” win condition. With a 3×2 “board”, the first two steps in the multiway graph are: The final multiway graph is: The graph has 235 nodes, of which 24 are wins for white, and 34 for black: The successive branchial graphs in this case are (with wins for black and white indicated): For a 3×3 “board” the multiway graph has 5172 states, with 604 being wins for white and 684 being wins for black. As another example of a simple game, we’ll now consider Nim. In Nim, there are k piles of objects, and at each step p players alternate in removing as many objects as they want from whatever single pile they choose. The loser of the game is the player who is forced to have 0 objects in all the piles. Starting off with 2 piles each containing 2 objects, one can construct a multiway graph for the game: With 3 piles this becomes: These graphs show all the different possible moves that relate different configurations of the piles. However, they do not indicate which player moves when. Adding this we get in the 22 case and in the 222 case: Even though these graphs look somewhat complicated, it turns out there is a very straightforward criterion for when a particular state has the property that its “opponent” can force a lose: just take the list of numbers and see if Apply[BitXor, list] is 0. Highlighting when this occurs we get: It turns that for Nim, the sequence of branchial graphs we get have a rather regular structure. In the 22 case, with the same highlighting as before, we get: In the 222 case the sequence of branchial graphs becomes: Here are results for some other cases.
Games and Puzzles as Multicomputational Systems 26.10 Sliding Block Puzzles : They go under many names—with many different kinds of theming. But many puzzles are ultimately sliding block puzzles. A simple example might ask to go from: by sliding blocks into the empty (darker) square. A solution to this is: One can use a multiway graph to represent all possible transformations: (Note that only 12 of the 4! = 24 possible configurations of the blocks appear here; a configuration like cannot be reached.) Since blocks can always be “slid both ways” every edge in a sliding-block-puzzle multiway graph has an inverse—so going forward we’ll just draw these multiway graphs as undirected. Here are some simple cases: With a 3×2 board, things quickly get more complicated: Rendered in 3D this becomes: When all the blocks are distinct, one tends to get multiway graphs with a kind of spherical structure: (Note that in the first three cases here, it’s possible to reach all 30, 120, 360 conceivable arrangements of the blocks, while in the last case one can only reach “even permutations” of the blocks, or 360 of the 720 conceivable arrangements.) This shows how one gets from to : With many identical blocks one tends to build up a simple lattice: Making one block different basically just “adds decoration”: As the number of “1” and “2” blocks becomes closer to equal, the structure fills in: Adding a third type of block rapidly leads to a very complicated structure: This summarizes a few of the graphs obtained.
Games and Puzzles as Multicomputational Systems 26.11 Towers of Hanoi, etc. : Another well-known puzzle is the Towers of Hanoi. And once again we can construct a multiway graph for it. Starting with all disks on the left peg the first step in the multiway graph is: Going two steps we get: The complete multiway graph is then (showing undirected edges in place of pairs of directed edges): It is rather easy to see how the recursive structure of this multiway graph builds up. Here’s the “base case” of 2 disks (and 3 pegs): And as each disk is added, the number of nodes in the multiway graph increases by a factor of 3—yielding for example with 4 disks (and still 3 pegs): With 4 pegs, things at first look more complicated, even with 2 disks: In a 3D rendering, more structure begins to emerge: And here are the results for 3, 4 and 5 disks—with the “points of the ears” corresponding to states where all the disks are on a single peg: With 3 pegs, the shortest “solution to the puzzle”—of moving all disks from one peg to another—goes along the “side” of the multiway graph, and for n pegs is of length 2n – 1: With 4 pegs, there is no longer a unique “geodesic path”: (And the sequence of path lengths for successive numbers of pegs is: or a little below for a large number of pegs n.) What about branchial graphs? For the standard 3-disk 3-peg case we have where successive “time slices” are assumed to be obtained by looking at successive vertical levels in the rendering of the multiway graph above. For 4 disks one essentially gets “more of the same”: With 4 pegs things become slightly more complicated: And the trend continues for 5 pegs.
Games and Puzzles as Multicomputational Systems 26.12 Multicomputational Implications & Interpretation : We’ve now gone through many examples of games and puzzles. And in each case we’ve explored the multiway graphs that encapsulate the whole spectrum of their possible behavior. So what do we conclude? The most obvious point is that when games and puzzles seem to us difficult—and potentially “interesting”—it’s some kind of reflection of apparent complexity in the multiway graph. Or, put another way, it’s when we find the multiway graph somehow “difficult to decode” that we get a rich and engaging game or puzzle. In any particular instance of playing a game we’re basically following a specific path (that in analogy to physics we can call a “timelike path”) through the multiway graph (or “game graph”) for the game. And at some level we might just make the global statement that the game graph represents all such paths. But what the multicomputational paradigm suggests is that there are also more local statements that we can usefully make. In particular, at every step along a timelike path we can look “transversally” in the multiway graph, and see the “instantaneous branchial graph” that represents the “entanglement” of our path with “nearby paths”. Figuring out “what move to make next” is then in a sense about deciding in “what direction” in branchial space to go. And what makes a game difficult is that we can’t readily predict what happens as we “travel through branchial space”. There’s a certain analogy here to the concept of computational irreducibility. Going from one state to another along some timelike path, computational irreducibility implies that even though we may know the underlying rules, we can’t readily predict their consequences—because it can require an irreducible amount of computation to figure out what their consequences will be after many steps. Predicting “across branchial space” is a related, but slightly different phenomenon, that one can describe as “multicomputational irreducibility”. It’s not about the difficulty of working out a particular path of computation, but instead about the difficulty of seeing how many entangled paths interact. When one plays a game, it’s common to talk about “how many moves ahead one can see”. And in our terms here, this is basically about asking how “far out in branchial space” we can readily get. As computationally bounded entities, we have a certain “reach” in branchial space. And the game is “difficult for us” if that reach isn’t sufficient to be able to get to something like a “winning position”. There’s another point here, though. What counts as “winning” in a game is typically reaching some particular places or regions in the multiway graph. But the definition of these places or regions is typically something very computationally bounded (“just see if there’s a line of X’s”, etc.). It’s a certain “observation” of the system, that extracts just a particular (computationally bounded) sampling of the complete state. And then what’s key is that this sampling doesn’t manage to “decode the multicomputational irreducibility”. There’s an analogy here to thermodynamics. The fact that in thermodynamics we perceive “heat” and “entropy increase” is a consequence of the fact that our (coarse-grained) measurements can’t “decode” the computationally irreducible process that leads to the particular states generated in the system. Similarly, the fact we perceive it to be “hard to figure out how to win a game” is a consequence of the fact that our criterion for winning isn’t able to “look inside the playing of the game” and “decode what’s going on” to the point where it’s in effect just selecting one particular, straightforward path. Instead it’s a question of going through the multicomputationally irreducible process of playing the game, and in effect “seeing where it lands” relative to the observation of winning. There’s also an analogy here to quantum mechanics. Tracing through many possible paths of playing a game is like following many threads of history in quantum mechanics, and the criterion of winning is like a quantum measurement that selects certain threads. In our Physics Project we imagine that we as observers are extended in branchial space, “knitting together” different threads of history through our belief in our own single thread of experience. In games, the analog of our belief in a single thread of experience is presumably in effect that “all that matters is who wins or loses; it doesn’t matter how the game is played inside”. To make a closer analogy with quantum mechanics one can start thinking about combining different chunks of “multiway game play”, and trying to work out a calculus for how those chunks fit together. The games we’ve discussed here are all in a sense pure “games of skill”. But in games where there’s also an element of chance we can think of this as causing what is otherwise a single path in the multiway graph to “fuzz out” into a bundle of paths, and what is otherwise a single point in branchial space to become a whole extended region. In studying different specific games and puzzles, we’ve often had to look at rather simplified cases in order to get multiway graphs of manageable size. But if we look at very large multiway graphs, are there perhaps overall regularities that will emerge? Is there potentially some kind of “continuum limit” for game graphs? It’ll almost inevitably be the case that if we look in “enough detail” we’ll see all sorts of multicomputational irreducibility in action. But in our Physics Project—and indeed in the multicomputational paradigm in general—a key issue is that relevant observers don’t see that level of detail. And much like the emergence of thermodynamics or the gas laws from underlying molecular dynamics, the very existence of underlying computational irreducibility inevitably leads to simple laws for what the observer can perceive. So what is the analog of “the observer” for a game? For at least some purposes it can be thought of as basically the “win” criteria. So now the question arises: if we look only at these criteria, can we derive the analog of “laws of physics”, insensitive to all the multicomputationally irreducible details underneath? There’s much more to figure out about this, but perhaps one place to start is to look at the large-scale structure of branchial space—and the multiway graph—in various games. And one basic impression in many different games is that—while the character of branchial graphs may change between “different stages” in the game—across a single branchial graph there tends to be a certain uniformity. If one looks at the details there may be plenty of multicomputational irreducibility. But at some kind of “perceptible level” different parts of the graph may seem similar. And this suggests that the “local impression of the game” will tend to be similar at a particular stage even when quite different moves have been made, that take one to quite different parts of the “game space” defined by the branchial graph. But while there may be similarity between different parts of the branchial graph, what we’ve seen is that in some games and puzzles the branchial graph breaks up into multiple disconnected regions. And what this reflects is the presence of distinct “conserved sectors” in a game—regions of game space that players can get into, but are then stuck with (at least for a certain time), much as in spacetime event horizons can prevent transport between different regions of physical space. Another (related) effect that we notice in some games and puzzles but not others is large “holes” in the multiway graph: places where between two points in the graph there are multiple “distant” paths. When the multiway graph is densely connected, there’ll typically always be a way to “fix any mistake” by rerouting through nearby paths. But when there is a hole it is a sign that one can end up getting “committed” to one course of action rather than another, and it will be many steps before it’s possible to get to the same place as the other course of action would have reached. If we assume that at some level all we ultimately “observe” in the multiway graph is the kind of coarse-graining that corresponds to assessing winning or losing then inevitably we’ll be dealing with a distribution of possible paths. Without “holes” these paths can be close together, and may seem obviously similar. But when there’s a hole there can be different paths that are far apart. And the fact there can be distant paths that are “part of the same distribution” can then potentially be thought of as something like a quantum superposition effect. Are there analogs of general relativity and the path integral in games? To formulate this with clarity we’d have to define more carefully the character of “game space”. Presumably there’ll be the analog of a causal graph. And presumably there’ll also be an analog of energy in game space, associated with the “density of activity” at different places in game space. Then the analog of the phenomenon of gravity will be something like that the best game plays (i.e. the geodesic paths through the game graph) will tend to be deflected by the presence of high densities of activity. In other words, if there are lots of things to do when a game is in a certain state, good game plays will tend to be “pulled towards that state”. And at some level this isn’t surprising: when there’s high density of activity in the game graph, there will tend to be more options about what to do, so it’s more likely that one will be able to “do a good game play” if one goes through there. So far we didn’t explicitly talk about strategies for games. But in our multicomputational framework a strategy has a fairly definite interpretation: it is a place in rulial space, where in effect one’s assuming a certain set of rules about how to construct the multiway graph. In other words, given a strategy one is choosing some edges in the multiway graph (or some possible events in the associated multiway causal graph), and dropping others. In general it can be hard to talk about the “space of possible strategies”—because it’s like talking about the “space of possible programs”. But this is precisely what rulial space lets us talk about. What exact “geometry” the “space of strategies” has will depend on how we choose to coordinatize rulial space. But once again there will tend to be a certain level of coarse-graining achieved by looking only at the kinds of things one discusses in game theory—and at this level we can expect that all sorts of standard “structural” game-theoretic results will generically hold.
Games and Puzzles as Multicomputational Systems 26.13 Personal Notes : Even as a kid I was never particularly into playing games or doing puzzles. And maybe it’s a sign I was always a bit too much of a scientist. Because just picking specific moves always seemed to me too arbitrary. To get my interest I needed a bigger picture, more of a coherent intellectual story. But now, in a sense, that’s just what the multicomputational approach to games and puzzles that I discuss here is bringing to us. Yes, it’s very “humanizing” to be able think about making particular moves. But the multicomputational approach immediately gives one a coherent global view that, at least to me, is intellectually much more satisfying. The explorations I’ve discussed here can be thought of as originating from a single note in A New Kind of Science. In Chapter 5 of A New Kind of Science I had a section where I first introduced multiway systems. And as the very last note for that section I discussed “Game systems”: I did the research for this in the 1990s—and indeed I now find a notebook from 1998 about tic-tac-toe with some of the same results derived here together with a curious-looking graphical representation of the tic-tac-toe game graph: But back at that time I didn’t conclude much from the game graphs I generated; they just seemed large and complicated. Twenty years passed and I didn’t think much more about this. But then in 2017 my son Christopher was playing with a puzzle called Rush Hour: And perhaps in a sign of familial tendency he decided to construct its game graph—coming up with what to me seemed like a very surprising result: At the time I didn’t try to understand the structure one has here—but I still “filed this away” as evidence that game graphs can have “visible large-scale structure”. A couple of years later—in late 2019—our Physics Project was underway and we’d realized that there are deep relations between quantum mechanics and multiway graphs. Quantum mechanics had always seemed like something mysterious—the abstract result of pure mathematical formalism. But seeing the connection to multiway systems began to suggest that one might actually be able to “understand quantum mechanics” as something that could “mechanically arise” from some concrete underlying structure. I started to think about finding ways to explain quantum mechanics at an intuitive level. And for that I needed a familiar analogy: something everyday that one could connect to multiway systems. I immediately thought about games. And in September 2020 I decided to take a look at games to explore this analogy in more detail. I quickly analyzed games like tic-tac-toe and Nim—as well as simple sliding block puzzles and the Towers of Hanoi. But I wanted to explore more games and puzzles. And I had other projects to do, so the multicomputational analysis of games and puzzles got set aside. The Towers of Hanoi reappeared earlier this year, when I used it as an example of generating a proof-like multiway graph, in connection with my study of the physicalization of metamathematics. And finally, a few weeks ago I decided it was time to write down what I knew so far about games and puzzles—and produce what’s here.
Alien Intelligence and the Concept of Technology 27.1 The Nature of Alien Intelligence : “We’re going to launch lots of tiny spacecraft into interstellar space, have them discover alien intelligence, then bring back its technology to advance human technology by a million years”. I’ve heard some pretty wacky startup pitches over the years, but this might possibly be the all-time winner. But as I thought about it, I realized that beyond the “absurdly extreme moonshot” character of this pitch, there’s some science that I’ve done that makes it clear that it’s also fundamentally philosophically confused. The nature of the confusion is interesting, however, and untangling it will give us an opportunity to illuminate some deep features of both intelligence and technology—and in the end suggest a way to think about the long-term trajectory of the very concept of technology and its relation to our universe. Let’s start with a scenario. Let’s say one of the little spacecraft comes across a planet where it sees complicated swirling patterns: The spacecraft sends out a probe to “make contact”. The swirling pattern “responds” by changing slightly. The spacecraft analyzes the change, and sends out another probe. And pretty soon there’s a whole “conversation” going on between the spacecraft and the planet. But, you might say, that’s nothing like an “intelligence” there; there’s just a “pure physical system” that operates through physical laws. OK, but now let’s imagine the spacecraft has returned to Earth and is checking it out. It detects complicated patterns of radio signals. It sends out a radio signal of its own. Something on Earth responds. A “conversation” ensues. Maybe the spacecraft is “talking to” a cellphone tower, doing automated handshakes with it. Maybe it reached a ham radio operator, and is exchanging Morse code with them. Or maybe—in an ultimate version of code injection—there’s a computer that’s interpreting the spacecraft’s signals as a program, and is sending back the results of running the program. It all seems quite sophisticated—and at some level worthy of the technological civilization we’ve built up here on Earth. But let’s zoom out a bit. There’s something coming from the spacecraft, that’s causing something to happen on Earth, that’s causing something to be returned to the spacecraft. And ultimately whatever is happening on Earth must be a physical process of some kind—operating according to the laws of physics. So what’s the difference between this and those swirling “just physics” patterns that the spacecraft found on the other planet? Everything is ultimately “just physics” after all. OK, you might say, that’s surely true. But on Earth, even though we might have started from physics, we’ve somehow now “ascended” through chemistry and biology and technology to get to something that’s fundamentally more sophisticated. But here we run into an important—if at first surprising—piece of basic science: my Principle of Computational Equivalence. Let’s say we represent all those “physical processes” as computations (and our Physics Project implies that all of physics is indeed ultimately computational). Now we can compare the computations that correspond to the planet with the swirling patterns to the ones that correspond to our Earth with us humans in the loop. And what the Principle of Computational Equivalence tells us is that they’re ultimately equivalent. The computations associated with the swirling patterns are ultimately just as sophisticated as the ones we achieve with our brains and our technology here on Earth. It’s far from obvious that this would be true. But it’s something one discovers when one explores the computational universe of possible programs. One might think that simple programs would produce only simple behavior, and that somehow the behavior would get progressively more complex with more complicated programs. But that’s not what one finds. Instead, there’s increasing evidence that almost any program that doesn’t show obviously simple behavior can in fact show behavior that is as sophisticated as anything. It’s been known for about a century that there exist computation universal systems capable of being “programmed” to do essentially any computation. But what the Principle of Computational Equivalence says is that sophisticated computation is not only possible—even for simple programs—but is something that happens generically and ubiquitously. So what does this mean for our spacecraft? It means that what the spacecraft sees on Earth can be computationally no more sophisticated than what it sees on the planet with the swirling patterns. Yes, we consider there to be “intelligence” here on Earth. But what the Principle of Computational Equivalence tells us is that ultimately there’s nothing abstractly different going on from what’s going on in the swirling patterns. So if we characterize what’s going on here on Earth as an example of “intelligence” we really should say that those swirling patterns are also “examples of intelligence”. And, yes, it doesn’t seem much like human intelligence. But at an abstract computational level it’s really operating like intelligence—but to us humans it’s “alien intelligence”. There’s a common saying: “The weather has a mind of its own”. And what the Principle of Computational Equivalence tells us is that, yes, fluid dynamics in the atmosphere—and all the swirling patterns associated with it—are examples of computation that are just as sophisticated as those associated with human minds. But, OK, so there’s a sense in which the weather “has a mind of its own”. But it’s definitely not a “human-like mind”. Yes, the weather does sophisticated computations. But there’s no obvious way to attribute to those computations the purposes and intentions and other typical features of how we describe what goes on in a human mind. So if indeed we’re going to talk about the weather as being an intelligence, for us humans we have to consider it an “alien intelligence”. We started off talking about spacecraft going out into the cosmos to discover alien intelligence. But what the Principle Computational Equivalence is telling us is that actually there’s what we can think of as alien intelligence all around us. Yes, we humans have managed to get to the point where we do all sorts of sophisticated computations. But computations of just the same sophistication are being done in all sorts of systems that don’t have that whole human tower of biology and technology. For a long time it’s been a mystery why we’ve never detected alien intelligence out there in the cosmos. But actually I think there’s no lack of “alien intelligence”; indeed it’s all around us. But the point is that it really is alien. At an abstract computational level it’s like our intelligence. But in its details it’s not aligned with our intelligence. Abstractly it’s intelligence, but it’s not human-like intelligence. It’s alien intelligence.
Alien Intelligence and the Concept of Technology 27.2 The Role of Science and Technology : OK, so we can think of lots of systems as being examples of “alien intelligence”. But how does that alien intelligence connect to our human intelligence? Sometimes it’s close enough that we humans can immediately “anthropomorphize” the system to “understand what it’s doing in human terms”. But often we need to put effort into “making a bridge”. And in fact we can view that as being what science and technology are ultimately trying to do. Let’s say we’re looking at swirling patterns in a fluid. The fluid is doing what it does, in effect continually running a computation that generates its behavior. But how can we “align” that with what’s going on in our brains? That’s where science comes in. Because what science is trying to do is to extract some kind of “human-relatable narrative” from the actual behavior of a system out there in the world. Or in some sense it’s trying to provide a “channel” through which we can “communicate” with the “alien intelligence” that is embodied in what’s out there in the world. So what about technology? Fundamentally technology is about trying to take what exists out there in the world, and apply it to achieve human purposes. We have a fluid. Now we use it to create hydraulic technology that achieves some practical human purpose. We can see the history of technology as being a progressive effort to identify things out there in the world (metal ore, photoelectricity, liquid crystals, …) that can be sampled and fashioned in such a way as to achieve certain purposes we want. And insofar as we think of what’s out there in the world as being like alien intelligence, what technology is doing is finding ways to corral that intelligence into achieving human purposes. The truth is that in most of our technology today, we’re not letting that intelligence really do anything close to what it’s capable of. We’re keeping it tightly constrained to take only steps that we can readily understand and foresee. It’s a bit like having a horse with a harness that constrains it to just walk slowly in a straight line—even though without the harness the horse could gallop around and do all sorts of elaborate things, albeit things that we might not readily be able to understand or foresee. So let’s come back to the spacecraft. It’s reached a planet. And it’s interacting with what’s there. Perhaps there’s some weird electrical storm going on. And, yes, we can think of that as an example of alien intelligence. But if the mission of the spacecraft is to discover technology then what it needs to do is to figure out whether there’s some way to interact with the electrical storm so as to achieve some human purpose. The storm does what the storm does. But maybe by moving some piece of metal around in just the right way it’s possible to get the storm to charge a battery. Or, more elaborately, perhaps processes in the storm could be used like an analog computer, say to compute solutions to equations. And perhaps—having seen the storm on this planet—it’s even possible to “bottle it up” and replicate it, say in a piece of consumer electronics. One way to describe what’s going on is just to say rather prosaically that we discovered a phenomenon on the planet, that we were able to use for technology. But more colorfully we could say that we encountered an alien intelligence, we found a way to communicate with it, and then we “brought back” technology from it. The original startup pitch was about spacecraft getting technology by discovering alien intelligence out in the cosmos. But really the whole spacecraft thing is a distraction. Because actually—as we’ve discussed—there’s plenty we can describe as “alien intelligence” all around us, even right here on Earth. And the issue is in a sense just how to “communicate with it” and find ways to “harness it” for our technological purposes. We’ve learned in the past century that we can use electrons in semiconductors as a way to build computers. But what about other physical processes? Maybe flowing fluids, for example. Can we use that “alien intelligence” to make a new kind of computer? In the end, the point is that any technology is about finding and harnessing “alien intelligence”. That’s basically just what technology is, and always has been. The whole “alien intelligence” part of the story, though, is much more relevant when we’re thinking of technology that makes serious use of what we can identify as sophisticated computation. If we’re just using a system from nature for its physical mass, it doesn’t really feel as if we’re using its “intelligence”. But as soon as we try, for example, to base a general computer on it, it’s a quite different story. In getting technology from the universe we’re basically picking out certain aspects of what exists and choosing to apply these for our purposes. In doing science it seems like we have “less choice” about what aspects of the universe we deal with. After all, we might imagine that science is trying to give us a way to understand anything that’s out there in the universe. But in reality it’s much more like technology. The “scientific narratives” that we understand—at least at a given time in history—are ones that we’re in a sense “primed for”. Yes, something like fluid turbulence might give us “in-your-face” exposure to something computationally sophisticated that’s far from what we normally talk about. But what science mostly concentrates on is creating narratives that are aligned with our existing scientific understanding and discussions—much as technology is set up to be about things that are aligned with our existing human purposes.
Alien Intelligence and the Concept of Technology 27.3 Extracting Technology from the Ruliad : One might imagine that—wherever it ultimately comes from—technology must at least always in the end be based on the laws of physics. But what’s emerging from our Physics Project is that actually the story is considerably more complicated than that. It all begins with the ruliad: the object that represents the entangled limit of all possible computations. The ruliad is a unique, formally necessary object, that in a sense embodies all conceivable existence. And inevitably we are embedded within the ruliad, sampling certain aspects of it to form our perception of reality. In principle there are all sorts of kinds of observers of the ruliad, with all sorts of kinds of perceptions of reality. But the key point that has emerged as a foundation of our Physics Project is that “observers like us” have certain general characteristics—specifically that we assume that we are persistent through time, and also that we are computationally bounded—and from these characteristics alone, we can abstractly deduce from the structure of the ruliad that we must “experience” core standard laws of known physics. The ruliad in a sense contains all possible physicses. But it’s our particular kind of sampling of the ruliad that leads us to the particular laws of physics that we currently know. An “alien intelligence” might sample the ruliad quite differently, and thus in effect “experience” quite different laws of physics. Somewhere underneath everything we can think of there being a giant hypergraph of individual atoms of existence—but with the means of perception observers like us have, we inevitably “coarse grain” to the point where, for example, we experience this as continuous space. Another kind of observer, with different characteristics, might, for example, not do that coarse graining, might never experience continuous space, and might have a completely different perception of how the universe works. In some sense, therefore, physics is much more like technology than we might expect. There isn’t an “absolute physics”. There’s just the physics that we as observers extract from the ruliad. Much like there’s particular technology that we choose to build from the “raw material” that exists. Put another way, both physics and technology are ultimately things we “extract” from the ruliad, in effect by making certain choices. How we “extract” physics seems, however, much more constrained. For example, we as humans have only certain particular senses through which we are biologically set up to experience the world. Yet we have the feeling that in technology we can in effect “construct whatever we want”—although inevitably “what we want” is also still at least influenced by how we are biologically set up. We’re very used to the idea that over time technology progresses—as we invent more, and work out new ways to use our “raw material” to achieve human purposes. But physics as a science progresses too. And in a sense what’s happening there is that we’re expanding our character as observers to be able to perceive and experience more of “what’s going on”—ultimately in the ruliad. Part of that expansion is actually a matter of technology. We’re building telescopes and microscopes and amplifiers that allow us to extend our raw human senses to be sensitive to more things. But there’s also another part of the expansion that is in effect intellectual: we’re developing new conceptual frameworks that allow us to “corral” things we see happening in the world into forms that “fit narratives” we’ve constructed. And the important point here is that neither our technology nor our physics is fixed. They’re in a sense co-evolving—gradually allowing more and more of the ruliad to be pulled into our narratives and our purposes. Or, put another way, what we observe is gradually expanding to encompass more and more of the ruliad, and to be able to make use of more and more of it.
Alien Intelligence and the Concept of Technology 27.4 Reaching Out across Rulial Space : How are “different intelligences” manifest in the ruliad? We can imagine organizing the ruliad to be laid out in some form of rulial space. And from each point in rulial space one in effect gets a “different perspective” on the ruliad. And that’s at least the beginning of the story of how “different intelligences” exist and experience the ruliad. It’s similar to what happens with physical space: from different places in physical space one gets a different perspective on the universe. In physical space we have a concept of motion: that observers like us can move from one place in space to another while in effect maintaining our coherence and integrity. How does this work in rulial space? We can think of different points in rulial space as corresponding to different computations, with different rules. So rulial motion in effect corresponds to making a translation between one computation and another. At the outset, it’s not obvious this would even in principle be possible. But the Principle of Computational Equivalence implies that it ultimately will be. The computations at different points in rulial space will (almost always) be equivalent in their sophistication—and as is typical with universal computation—it’ll therefore in principle be possible to have an “interpretation process” that translates between them. But the big question is whether this can be achieved in practice. Just how far can a particular observer translate in rulial space while maintaining their coherence and integrity? The ruliad is a complex and (if sampled across slices in time) continually changing thing. But a critical feature is that there can be structures that have a certain persistence within it. In physical space these are things like particles (as well as black holes) that behave like “stable lumps of space”—or like stable lumps in certain projections of the ruliad. In rulial space there can presumably also be structures with a certain persistence: “particles” of rulial space. And these “particles” somehow correspond to features that “survive across different computational perspectives”—or in effect represent “robust concepts”. When we talk about “different intelligences” a very familiar example is different human minds. And in a sense we can think of different human minds as being laid out in rulial space—with each mind being at a different rulial position, and thus having a different computational rule by which it operates, and a different “experience of the ruliad”. So how can these minds “communicate”? Ultimately it is through “rulial motion”. But potentially the most robust form of rulial motion is through rulial particles—which we’ve identified above with the abstract idea of “robust concepts”. Put in a practical way: different (human) minds operate internally in different ways. But they can still “communicate” by exchanging something that in effect “survives translation” between one mind and another: rulial particles corresponding to robust concepts (say expressed in a language). But, OK, we can imagine rulial space with lots of human minds laid out at different places, with ones that communicate more easily closer together. So what about “alien intelligences”? Well, each one is somewhere in rulial space. But they may be far away from where our human minds are. We can imagine our rulial particles—or “robust concepts”—being able to reach a certain distance in rulial space. The human idea of “excitement” might for example be able to reach the place in rulial space where we’d find the minds of dogs. But what about the weather, for example? Well, as an alien intelligence, it’s presumably much further away in rulial space—and, anthropomorphize it as we might—it’s not clear what its notion of “excitement” would be. It’s an often-asked question why—with our spacecraft and radio telescopes and everything else—we haven’t ever run across what we consider to be “naturally occurring” alien intelligences. In the past we might have imagined that the answer is that there just isn’t anything like “intelligence” (outside of us humans) to be found in any part of the universe that we can probe. But the Principle of Computational Equivalence says that’s fundamentally not true, and that in fact “abstract intelligence” is thoroughly ubiquitous among systems with all but the most obviously simple behavior. So to “find” alien intelligence it’s not that we need a more powerful radio telescope (or a better spacecraft) that can reach further in physical space. Rather, the issue is to be able to reach far enough in rulial space. Or, put another way, even if we view the weather as “having a mind of its own”, the rulial distance between “its mind” and our human minds may be too great for us to be able to “understand” and “communicate with it”. So what will it take for us to “bridge this rulial gap”? At some level it’s just about building the right science and technology. We can think of science as being about defining a way to “translate” from the computational rules by which some particular system operates to the computational way our human minds operate. Or, in terms of rulial space, finding a way to “move” from the rulial position of the system to the rulial position of our minds—and translating from the way a system works to a “human narrative” that represents it. Centuries ago we might have just said “the planets do what they do”; maybe their motion in space is driven by an “alien intelligence” that we don’t understand. But then along came mathematical science and we were able to “translate” from the intrinsic computation done by the planets to a mathematical description that we internalized enough to consider it a human narrative that we understand. In some sense at any given time in intellectual history our minds “reach out a certain distance in rulial space”. We’ve developed conceptual frameworks that allow us to maintain a coherent understanding of a certain range of things—with that range growing as we invent new frameworks. At one time our “domain of understanding”—or the region of rulial space that we could reach—didn’t encompass the behavior of electricity. But our “intellectual expansion” in rulial space eventually reached this, and the result is that we can now use electricity as “raw material” from which to construct technology. One way we “expand our reach in rulial space” is in effect conceptual: by expanding what we understand. But another way is by being able to “sense” or “measure” more. When we invent radio—or, for that matter, gravitational wave detection—there are immediately new kinds of processes that we manage to “connect to human experience”. Or, put another way, there are more distant parts of the ruliad that we’re able to reach. More prosaically, we can say that if we want to be able to use something for technology, we’d better be able to detect that it’s there, and we’d better be able to understand it well enough that we can see how it could align with our human purposes. We can think of the ruliad as being full of alien intelligences—with plenty of capabilities to “mine”. But to be able to actually mine something for our technological purposes we have to be able to reach it across rulial space; we have to be able to connect it to us. So what does this mean for the original startup pitch? Yes, it’s a good idea to “mine alien intelligences” for technology. In fact, that’s basically where technology always comes from. But there’s no need to send out spacecraft, “discover” alien intelligence, and so on. There are “alien intelligences” all around us; the issue is just to reach them across rulial space, and be able to “communicate” with them. But what we’ve argued is that the process of progressively reaching out in rulial space is just the general process of progressively advancing science (and the technology on which it depends). So, yes, by all means explore more of what’s out there in the world, with more, different kinds of sensors and measurements. Then try to “understand” what you see enough to be able to tell how to align it with human purposes, and make technology out of it. But there’s no pressing need for interstellar spacecraft in this picture. It’s just a matter of doing more science to expand our domain in the ruliad, and mine more of rulial space.
Alien Intelligence and the Concept of Technology 27.5 The Evolution of Purpose and the Colonization of Rulial Space : We can think of technology as being about setting up things that exist in the world (or ultimately in the ruliad) to achieve human purposes. And we’ve talked about how the advance of science and technology allows us to progressively reach further in rulial space to get “raw material” for our technology. But we’ve said that technology is intended to “achieve human purposes”. So what might those purposes ultimately be? Our purposes have certainly evolved over the course of human history. In today’s world, we might view it as purposeful to walk on a treadmill, or to trade cryptocurrencies. But it would be challenging to explain the purpose of such things to someone from even a few hundred years ago. In a sense, purposes evolve as we build new conceptual frameworks, and as we set up technology that allows us to do new things. More abstractly, we might say that purposes are also something defined by places in rulial space. So when we talk about the evolution of purposes, what we’re really asking is where in rulial space our history and development has led us, and will lead us in the future. And certainly in the vastness of the whole ruliad, our existing human purposes occupy just an infinitesimally tiny part. Think, for example, of the natural world even as we are currently aware of it. The vast majority of things in it do not seem in any way aligned with our purposes—and we have not been able to mine them for technology. Historically, however, there’s been progressive expansion in the domain of our purposes. There was a time when we knew about magnetic rocks, but had no purpose for magnetism. But over the course of time, from compasses to actuators to memories, more and more human purposes have emerged that connect to the phenomenon of magnetism. And in a sense we can view the whole core trajectory of human progress as being about the expansion of the region of rulial space—and the ruliad—that represents our purposes. So how will this evolve? As I’ve discussed extensively before, there seem to be two central features that we as entities in the ruliad have. First, that we are computationally bounded. And second, that we believe we are persistent in time. Computational boundedness is essentially the statement that the region of rulial space that we occupy is limited. In some sense our minds can coherently span a certain region of rulial space, but it’s a bounded region. What about persistence in time? It means that even though we are always being reconstructed out of different atoms of existence (and different atoms of space), we conflate things to the point where we experience a single continuous thread of existence. Taken together, these features suggest a picture of us being a kind of “blob” that gradually moves around in rulial space. Does it matter that there isn’t just a single human mind? Well, yes. Without some kind of “observer” there’s no real way to even define what it means to have a “blob”. And in the end it’s a story of consistency of observers observing observers. But the result is that we can think of our whole collective “flotilla” of human purposes as being something localized that moves around in rulial space, expanding the region of the ruliad that it reaches. But just how far can this go? Imagine that at some time in the distant future we have successfully explored—and “colonized”—much of rulial space. To do this we’d certainly have to have broken out of the particular constraints of our biological construction, and made use of “additional raw material” in the ruliad. But what would it mean to be spread across a large swath of rulial space? Our very notion of existence seems to depend on localization in rulial space. The thing that we view as “us” is something particular and coherent. To be “bigger” in rulial space is to deny that particularity and coherence, and to become something generic that does not represent any kind of “specific entity that exists”. In a sense, it’s a pyrrhic view of the ultimate limit of our technological and other evolution. As we progress, we gradually “mine” more and more of the ruliad, pulling it into the domain of technology and of our “human” (or post-human) purposes. But in doing so, we eventually transcend the very characteristics that we identify with existence. In other words, if we go too far with our expansion in the ruliad, we simply cease to exist, at least in the sense that we currently define existence. Put another way, if we “absorb” more and more “alien intelligence” there’s eventually no longer any coherent “us”. Of course, the very notion of coherence is something we’re basically defining from our current human view of things. And no doubt there are other definitions that could be given. But they’re certainly far away from our current place in rulial space, and it’s not even clear they can be reached without some kind of “discontinuity of motion” that would in effect fundamentally break their connection to us as we are now.
Alien Intelligence and the Concept of Technology 27.6 Face to Face with Alien Intelligence, Out in Rulial Space : At a fundamental level, the ruliad is a purely computational object, that we can think of as being made of pure, abstract atoms of existence (or “emes”). When observers like us sample the ruliad we can attribute to it the characteristics that correspond to our perception of physical reality. And a notable feature of that sampling is that it supports the idea of pure motion in physical space. In other words, it allows for the possibility that structures can “maintain their perceived physical integrity” while being “re-formed” out of different atoms of space, which themselves are interpretations of the pure atoms of existence in the ruliad. But as soon as we start thinking about any kind of serious motion in rulial (rather than physical) space it no longer makes sense to talk about anything like “maintaining physical integrity”, not least because in different places in rulial space the very notion of physics changes. But wherever we are in the ruliad we can still think about what’s going on as computation. We might have some way of observing or sampling the ruliad that gives us some perception of reality—like physics, or mathematics. But if we “atomize” things down to the lowest level, we’ll always find raw computation. As “physical observers like us” we only have limited capabilities to probe or manipulate the raw ruliad and affect what we perceive as physical reality. We can move physical objects around, maintaining what we observe of their structure. In principle we could imagine deconstructing objects into individual atoms of existence, then recreating them “transporter style” somewhere else in physical space. But as of now, we don’t know how to do this, and most likely it’s not possible for observers like us—because it would require “outcomputing” computationally irreducible features of the structure of space down at the level of individual atoms of space, which is far from what computationally bounded observers like us can expect to do. But what about raw computation, of the kind that ultimately makes up the ruliad? There the story is different. Because we’re no longer constrained by our character as physical observers, so we’re free to in effect “make up any computation we want”. To explore the physical universe we need physical motion or something like it. And at least for observers like us the only way to achieve this seems to be to progressively move structures across physical space. But to find out what can happen in the computational universe we can effectively just write down any rule (i.e. any program) that appears “anywhere in the ruliad”, and run it. Of course, when we run a rule on a practical computer, it’s just an emulation of what’s happening in the raw ruliad. But it’s just an abstract rule—so although it will run astronomically slower, its ultimate behavior in our emulation will inevitably be identical to what it is when implemented in terms of individual atoms of existence in the “raw ruliad”. In principle we could take the same approach in emulating the elements that make up our physical reality. But observers like us are so big relative to the raw elements of the ruliad that we can’t expect our emulations to be at a scale where we can faithfully reproduce what we perceive. (Needless to say, in practice we can still get good approximations, and this is a particularly fertile application of our Physics Project.) But when we’re dealing with “raw computation” down at the lowest level of the raw ruliad, we can expect to faithfully emulate it. And so it is that we can just pick a cellular automaton or a Turing machine or some other kind of computational system—that in effect comes from anywhere in the ruliad—and emulate it to find out what it does. There’s no “object we have to move” to be able to “look at that part of the ruliad”. We’re emulating things down at the level of individual atoms of existence, and seeing what happens. We can think about our computational experiments as letting us “jump” to find out what it’s like anywhere in the ruliad. And as we “suddenly materialize” somewhere in the ruliad it’s as if we’re immediately “face to face” with whatever “alien intelligence” there is at that place in the ruliad. But what can “observers like us” expect to make of that alien intelligence? Well, to be able “communicate” or even “relate” we somehow have to be able to “bridge the gap in rulial space”. And since we just “jumped to a place in rulial space” we don’t immediately have any “progressive path” that “incrementally” takes us from our familiar position in rulial space to wherever the alien intelligence is. But what does that feel like in practice? The whole idea of ruliology is just to go anywhere we want in the computational universe or in the ruliad, and see what happens when we run the rules we find there. And it’s indeed routine to find that what they do seems quite “alien”. Still, they often have certain essential features that for example remind us of the natural world as we observe it. But our standard methods of science (and mathematics)—developed on the basis of being “observers like we are today”—don’t readily allow us to “understand” the behavior of these systems chosen in the course of doing ruliology. To us they usually just seem to be “showing computational irreducibility”, and behaving in ways that we can effectively get no handle on. But still, we can in a sense view these programs “out there in the computational universe” (and in effect strewn around the ruliad) as showing us what’s possible. They’re like alien intelligences that we know exist, but that we don’t yet understand, and don’t yet know how to harness or relate to. We can see them as some kind of beacons of possible technology of the future—of things that “exist in the ruliad”, but that we haven’t yet been able to connect to human purposes. But so how might we make this connection? Well, as it happens, I’ve devoted much of my life to what can be viewed as the construction of a systematic bridge between what’s “computationally possible” and what we humans think of as important. For that’s the story of what I call computational language—and indeed of the whole intellectual structure that is the Wolfram Language. There’s infinite potential content in the ruliad. But one can view the goal of the Wolfram Language as being to represent—in a way that’s optimized for us humans to understand—those parts that we humans consider important. The language lets us use the concepts of computation not only to crystallize our existing thinking, but also to expand what we can think about, in effect letting us reach out further in rulial space. Computational language is the general way that we “tame the ruliad”—extend the frontier of “human colonization” in the ruliad, and in the end “mine” more and more of the ruliad for “useful technology”. Just in terms of its practical place in the world today I’ve often said that the Wolfram Language is like an “artifact from the future”. But now we see a deep sense in which this is true. The raw ruliad is just “out there”, with “infinite potential”, but as something whose fundamental character has nothing to do with us humans. But what computational language is about is delivering what one can think of as the ultimate “meta-artifact”: something that progressively turns the raw ruliad into “human-recognizable technology”. Much of this progress involves the specific, systematic design of the Wolfram Language. But there are also forays that in effect jump further out into rulial space. For example, we’ve often enumerated large collections of simple programs, identifying ones that satisfy a certain criterion. And sometimes that feels a lot like “leveraging alien intelligence” without “understanding” it. The rule 30 cellular automaton, for example, is a good pseudorandom generator, even though we don’t really “understand” even fairly basic things about it. And, yes, computational language is what we need to concretely “state a criterion”, in effect expressing what we’re thinking about in computational terms—that we can use, for example, to let us explicitly search the ruliad for an “alien intelligence” that does what we want. What does it look like out in the “raw ruliad”? It’s easy to start just looking at simple programs, say picked at random. And, yes, they have all sorts of elaborate behavior: But what is this behavior “achieving”? Yes, it’s following the particular underlying rules that have been given. But we don’t have any immediate way to connect it to “human purposes”. And in general we can expect that to make that connection what’s needed is for those purposes themselves to “expand”. Maybe at some moment we call what’s produced “art”, and assign it some “aesthetic purpose”. Maybe at some point we see that it satisfies some engineering purpose that we’ve just realized we should care about. But in general, computational language is the way we can make the connection between “raw computational processes” out there in the ruliad, and our patterns of thinking about things. It’s the ultimate way for us to “communicate with alien intelligence”.
Alien Intelligence and the Concept of Technology 27.7 The Launch of a Rulial Space Program : We began with the far-out startup pitch of sending spacecraft to discover alien intelligence and bring its technology back to Earth. But what we’ve realized is that actually no spacecraft—of the ordinary kind—are needed. There’s “alien intelligence” to be found everywhere; you don’t have to travel to interstellar space to find it. But the challenge is to connect the “alien intelligence” to human purposes, and extract from it what we consider “useful technology”. Or, put another way, the issue is not about traversing physical space, but rather about traversing rulial space. With our spacecraft we humans have so far reached about a 20-trillionth of the way across the physical universe. But no doubt we’ve reached a far smaller fraction of the way across the ruliad. As our science, knowledge and technology increase, we gradually reach further into rulial space. But whether it’s our failure to communicate with cetaceans or our inability to make computers out of, say, fluids, it’s clear that by many measures the distance we’ve gone so far is not so large. In a sense the startup idea of “harnessing alien intelligence” is the meta-idea of all technology—that in our terms we can state as being to connect what’s “computationally possible” in the ruliad with purposes we humans want to achieve. And I’ve argued that the ultimate meta-technology for doing this is not spacecraft but computational language. Because computational language is what we need to make a bridge between what we care about, and “raw computation” out in the ruliad. It’s difficult to send physical spacecraft out into interstellar space. But it’s actually a lot easier to probe the much richer possibilities of the ruliad—because in a sense it’s straightforward to put a “rulial spacecraft” anywhere. We just have to pick a rule (or program), then see what the “world” it generates is. But the challenge is then in a sense one of interpretation. What is happening in that world? Can we relate it to things we care about? At the outset, all we’re likely to see at some “random place” in the ruliad is rampant computational irreducibility. But it’s a fundamental fact that wherever there’s computational irreducibility, there must also be slices of computational reducibility to be found. In the ordinary physical universe that we experience, those are basically our perceived laws of physics. But even in a random sample of the ruliad we can expect there’ll be computational reducibility to be found. It’ll typically be “alien stuff”, though. It might have the character of science, but it won’t be like our existing science. And most likely it won’t align with anything we currently think we care about. But that is the great challenge and promise of mounting a “rulial space program”. To be confronted not with what we might recognize as “new life and new civilizations”, but with things for which we have no description and no current way of thinking. Perhaps we might view it merely as humbling to encounter such things, and to realize how small a part of the ruliad we yet understand. But we can also view it as a beacon of where we could go. And we can view a whole “rulial space program” as a way of systematizing the ultimate project of exploring all formally possible processes. Or we could think about it not just as defining a single “startup opportunity”—but rather as defining the “meta-opportunity” of all possible technology startups….
A Candidate Geometrical Formalism for the Foundations of Mathematics and Physics: Formal Correspondences between Homotopy Type Theory and the Wolfram Model 28.1 : This bulletin is a writeup of work done in collaboration with Xerxes Arsiwalla and Stephen Wolfram, as publicly presented and discussed in livestreams here, here and here. This bulletin is intended to be a high-level survey of the effort so far; a more formal article, intended to give rigorous formulations and proofs of the various ideas discussed here, is currently in preparation for submission to an appropriate journal. Introduction The field of “metamathematics”—a term first popularized by David Hilbert in the context of Hilbert’s program to clarify the foundations of mathematics in the early 20th century—may ultimately be regarded as the study of “pre-mathematics”. In other words, while mathematics studies what the properties of particular mathematical structures (such as groups, rings, fields, topological spaces, etc.) may be, metamathematics studies what the properties of mathematics itself, when regarded as a mathematical structure in its own right, are. Metamathematics is thus “pre”-mathematics in the sense that it is the structure from which the structure of mathematics itself develops (much like “pre-geometry” in theoretical cosmology is regarded as the structure from which the structure of the universe itself develops). In much the same way, the Wolfram Physics Project may be thought of as being the study of “pre-physics”. The Wolfram model is not a model for fundamental physics in and of itself, but rather an infinite family of models (or, if you prefer, a “formalism”), within which concepts and theories of fundamental physics can be systematically investigated and validated. However, one of the most remarkable things that we have discovered over the year or so that we’ve been working on this project is that a shockingly high proportion of the known laws of physics (most notably, both general relativity and quantum mechanics) appear to be highly “generic” features of the formalism, i.e. they are exhibited by a wide class of models, as opposed to being features that are somehow peculiar to our specific universe, as one might otherwise expect. This leads us to the tantalizing possibility that, just as metamathematics aims to explain why mathematics has the structure that it does, the Wolfram model may serve to explain why physics has the structure that it does. The correspondence between the Wolfram model and foundational metamathematics goes far deeper than this, though. It does indeed appear that many of the recent innovations in the foundations of mathematics (including dependent type theory, homotopy type theory, the univalence axiom, Urs Schreiber’s cohesive geometry, etc.) have considerable overlaps with our recent insights into the foundations of physics (particularly surrounding concepts such as the topology and geometry of rulial space, foliations and fibrations of the rulial multiway system, completion procedures and the inevitability of quantum mechanics, etc.). Indeed, what I aim to sketch out in this bulletin is a potential scheme for a new, computable foundation of both mathematics and physics, which can be thought of as an abstract unification of homotopy type theory and the Wolfram Physics Project. I will discuss how the symbolic structure of the Wolfram Language’s automated theorem-proving framework reflects the Curry–Howard–(Lambek) correspondence in mathematical logic, and how the Wolfram model naturally generalizes this correspondence to the foundations of physics; I will provide a brief pedagogical introduction to homotopy type theory and the univalent foundations program, as well as to Michael Shulman’s cohesive variant of homotopy type theory and his synthetic differential geometry program; I will then discuss multiway systems as a candidate formalism for investigating ideas in homotopy type theory, with multiway rules corresponding to type constructors, completion rules corresponding to homotopies, etc.; next, I will discuss the significance of Grothendieck’s hypothesis for the origins of geometrical structure in both mathematics and physics, the implications it has for the interpretation of branchial graphs as projective Hilbert spaces, causal graphs as Lorentzian manifolds, etc., and will give informal justifications for our interpretation of the rulial multiway system as an infinity-topos endowed with a Grothendieck topology, as well as discuss the relationship between the univalence axiom and my previously developed “completion” interpretation of quantum mechanics; I will also introduce my new “relativistic” interpretation of the first incompleteness theorem, as a statement of the effective gauge dependence of semantic truth in mathematics, and its implications for the geometrization and computationalization of both proof theory and model theory, as well as the potential significance of the “generalized Einstein field equations” that act in the space of all possible mathematical proofs; I will then give some explicit examples of foliations and fibrations of rulial multiway systems for the special case of finitely presented algebraic structures; finally, I will discuss the geometrical and topological significance of fibrations in the rulial topos and their implications for the foundations of quantum mechanics (and, in particular, I will demonstrate a hitherto undiscovered relationship between the Wolfram model formalism and the dagger symmetric monoidal category formalism of categorical quantum mechanics), before ending with a brief philosophical note about how this new way of thinking about metamathematics and “metaphysics” may yield new insight into why both the laws of mathematics and the laws of physics (particularly those of quantum mechanics and general relativity) are actually true. The Curry–Howard Isomorphism and Its Physical Generalization Let me begin by describing the broad philosophical setup of this bulletin. In mathematical logic and theoretical computer science, there is a famous result named after Haskell Curry and William Alvin Howard (and deeply connected to the core ideas of intuitionistic logic and its operational interpretation) known as the “Curry–Howard isomorphism” or “Curry–Howard correspondence”. Loosely speaking, what it says is that all mathematical proofs can be interpreted as computer programs, and (somewhat more surprisingly) all computer programs can be interpreted as mathematical proofs. In other words, the core problems of proof theory in mathematical logic and programming language theory in theoretical computer science are actually identical. At some level, the Curry–Howard isomorphism provides the formal justification for an otherwise obvious intuition. In pure mathematics, one starts from a set of “axioms”, one applies a “proof” (in accordance with a set of “rules of inference”) and one obtains a “theorem”. The abstract notion of computation can thus be thought of as being an ultimately desiccated version of the concept of mathematical proof, in which one starts with some “input”, one applies a “computation” (in accordance with a set of “algorithmic rules”) and one obtains an “output”. In other words, one has the formal correspondence: <|"Axioms" -> "Input", "Proof" -> "Computation", Indeed, the symbolic nature of the Wolfram Language allows us to see this correspondence explicitly. If we prove a trivial mathematical theorem (such as the theorem that a==c, starting from the axioms a==b and b==c, i.e. a proof of the transitivity of the equality predicate), then we obtain a symbolic proof object: Running this piece of code allows us to determine that the corresponding theorem is actually true: As such, we can interpret the Curry–Howard isomorphism as being the statement that every ProofObject in the Wolfram Language has an associated proof function (which we can explicitly demonstrate, as shown previously), and moreover that every piece of symbolic Wolfram Language code has a corresponding interpretation as a ProofObject for some theorem. The latter direction is easy to see once one appreciates that all terminating Wolfram Language programs are ultimately reducible to finite sequences of transformation operations being applied to symbolic expressions, just like in a mathematical proof. To be a bit more rigorous, the Curry–Howard isomorphism involves invoking the so-called “propositions as types” interpretation of metamathematics; in other words, Curry–Howard states that a program that produces an output of a given data type can be interpreted as a proof that the corresponding type is inhabited, and vice versa. (Note that there is also a three-way “Curry–Howard–Lambek” correspondence employed within category theory, in which objects in a “Cartesian closed category” can be interpreted as propositions, and morphisms between those objects can then be interpreted as the corresponding proofs of implication between propositions. We will hint at some details of the Curry–Howard–Lambek correspondence later on in the bulletin when we discuss connections to category theory, but an exhaustive review of this relationship is beyond the scope of this article.) In the Wolfram Physics Project, and in the field of physics more generally, one also considers an ultimately desiccated notion of what a “physical system” truly is. More specifically, one imagines the system beginning in some “initial state”, undergoing “motion” (in accordance with some “laws of motion”), and ending up in some final state. This immediately suggests a further refinement of the Curry–Howard isomorphism, in which programs, proofs and physical systems are really the same kind of thing: <|"Initial State" -> "Input", "Motion" -> "Computation", Viewed in this way, the Church–Turing thesis is neither a definition, a theorem, nor a conjecture; rather it is a hypothesis of fundamental physics. Namely, what the Church–Turing thesis truly says in this context is that the set of partial functions that can be computed by a universal Turing machine is exactly the set of partial functions that is instantiated by the laws of physics. Or, more concretely, the set of possible motions of a (physically realized) universal Turing machine is in one-to-one correspondence with the set of possible motions of any physical system, anywhere in the universe. The rest of this bulletin will be dedicated to fleshing out the details and implications of this correspondence, both for the foundations of physics and for the foundations of mathematics. Homotopy Type Theory, Univalent Foundations and Synthetic Geometry Homotopy type theory is an augmentation of type theory (or, more specifically, of Per Martin-Löf’s intuitionistic type theory) with one key additional axiom, namely Vladimir Voevodsky’s axiom of univalence, whose significance we shall discuss later. The key philosophical idea underpinning homotopy type theory is a slight extension of the usual “propositions as types” interpretation of the Curry–Howard correspondence, in which “types” now correspond to topological spaces (homotopy spaces), “terms” of a given type correspond to points in those spaces, proofs of equivalence between terms of a given type correspond to paths connecting the associated points, proofs of equivalence between proofs correspond to (potentially higher) homotopies between the associated paths, etc. In other words, homotopy type theory is a way of endowing type theory (and the foundation of mathematics more generally) with a kind of inbuilt topological structure. We know from ordinary topology that a homotopy is just a continuous deformation between paths. More precisely, given a pair of continuous functions f and g from a topological space X to another topological space Y, one can define a homotopy as being a continuous function h from the product space of X with the (closed) unit interval to Y, where the unit interval can be thought of as “parameterizing” that homotopy. To see this explicitly, suppose that we define function f to just be an ordinary Sin function: In homotopy type theory, we would interpret this overall space as being a type, the endpoints of this path would be terms of that type and the path defined by the function f would correspond to the proof that those terms are equivalent. Now, in ordinary mathematics, we are used to the idea that a given theorem can have many (apparently distinct) proofs, such as in the case of the fundamental theorem of algebra, in which there is a great variety of possible proofs stemming from topology, complex analysis, abstract algebra, Riemannian geometry, etc. So suppose instead that we define g to be a slightly different function connecting the same two points: g[x_] := Sin[x]*Cos[x] Now we can interpret this path as being a different possible proof of the same proposition (namely the proposition that the terms corresponding to the endpoints of the path are equivalent). We can then define a smooth homotopy between these two paths as follows: Although this homotopy lives naturally within this higher-dimensional space, we can easily project it back down onto our original space: Finally, this homotopy can be interpreted itself as being a proof that the two original (different) proofs are actually equivalent (in the sense that they both constitute valid proofs of the same theorem). As we have now seen explicitly, this homotopy can itself be interpreted as a path in some higher-dimensional space, so we can also proceed to define homotopies between those paths, which would correspond to proofs of equivalence between proofs of equivalence of proofs etc. Thus, there is actually a whole infinite hierarchy containing all possible higher-order homotopies, corresponding to the infinite hierarchy of all possible higher-order proofs of equivalence, and this hierarchy will become relevant to our discussion soon, once we start considering its implications for the geometry and topology of rulial space. The effort to formalize mathematics in terms of homotopy type theory is commonly known as the “univalent foundations program”, in reference to the central role played by Voevodsky’s univalence axiom. One of the “big-picture” questions driving the advancement of univalent foundations (as well as many related areas, such as Michael Shulman’s formulations of “synthetic topology”, “cohesive geometry”, etc.) is a desire to better understand the role played by “space” in the foundations of mathematics. In some ways, it is a great mystery why so many different mathematical structures (such as groups, rings, fields, Boolean algebras, lattices, etc.) somehow also come naturally equipped with a corresponding notion of “space”. For instance, in the context of group theory, a group is usually defined purely algebraically (i.e. as a closed algebraic structure obeying the axioms of associativity, identity and inverse), and yet one nevertheless has Lie groups, topological groups, matrix groups, loop spaces, etc., within all of which there naturally appears some kind of corresponding spatial structure (such as a differentiable manifold in the case of a Lie group, or a topological space in the case of a topological group, etc.), which just seems to “come along for the ride”, in the sense that the spatial structure is somehow respected by all of the underlying algebraic operations, without ever being explicitly “told” to do so. We can make this idea manifest by considering the case of the Lie group SO(3). First, consider the set of all 3 × 3 orthogonal matrices of determinant 1; here, we shall look directly at a finite subset: Then, SO(3) can be defined purely syntactically in terms of a binary operation on this set of elements (in this case, corresponding to the operation of matrix multiplication) that obeys the axioms of associativity: identity: and inverse: However, because of the natural interpretation of 3 × 3 orthogonal matrices of determinant 1 as 3D rotation matrices, SO(3) comes naturally equipped with a spatial structure corresponding to the following differentiable manifold (and, hence, is indeed a Lie group): But why? The ZFC axioms of set theory are purely syntactical and algebraic: there is no inherent notion of “space” in ZFC. So why should objects (like groups) defined within ZFC in a purely syntactical and algebraic way induce a notion of space as if by magic? Homotopy type theory aims in part to address this mystery of the origin of space, by considering instead a foundation for mathematics that is explicitly spatial and topological at the outset, and in which the other aspect of mathematics (namely its syntactical structure) is somehow emergent. In the context of the Wolfram Physics Project, we found ourselves addressing a very similar question. Our models are defined purely combinatorially, in terms of replacement operations on set systems (or, equivalently, in terms of transformation rules on hypergraphs). And yet, the correspondence between our models and the known laws of physics depends upon there existing a reasonable interpretation of these combinatorial structures in terms of continuous spaces. My derivation of the Einstein field equations, for instance, depends upon spatial hypergraphs having a reasonable continuum limit in terms of Riemannian manifolds (and hence on them being a reasonable candidate for physical space): And, moreover, it depends upon causal graphs having a reasonable continuum limit in terms of Lorentzian manifolds (and hence on being a reasonable candidate for spacetime): And, at a more abstract and speculative level, some of my more recent conjectures regarding the quantum mechanical aspects of our formalism depend upon the continuum limits of branchial graphs in multiway systems being interpretable in terms of projective Hilbert spaces: Or multiway causal graphs as being akin to twistor correspondence spaces: So could these two questions be related? Could the explanations offered by homotopy type theory for the origins of spatial structure in mathematics also imply, after appropriate translation, equivalent explanations for the origins of spatial structure in physics? And, if so, what would the things that we’ve recently discovered in the course of the Wolfram Physics Project so far imply regarding the foundations of (meta)mathematics.
A Candidate Geometrical Formalism for the Foundations of Mathematics and Physics: Formal Correspondences between Homotopy Type Theory and the Wolfram Model 28.2 Multiway Systems as Models for Homotopy Type Theory : As discussed previously, the axioms of a mathematical theory can be interpreted as defining sets of transformation rules between symbolic expressions. For instance, in the axioms of group theory: The associativity axiom can be interpreted as the following pair of (delayed) pattern rules: And therefore a proof of a mathematical proposition can ultimately be represented as a multiway system. Indeed, at some level, this is precisely what a ProofObject is: When I first started developing FindEquationalProof back in 2017, I designed the ProofObject to be formatted a little differently to the way multiway systems are conventionally formatted. The green (downward-pointing) arrows are the axioms, the dark green diamond is the hypothesis and the red square is the conclusion (in this case, the conclusion being that the corresponding hypothesis is true). Each of the orange circles is a substitution lemma, and can be thought of as being an ordinary multiway state node: it’s an intermediate lemma that is deduced along the way by simply applying the axiomatic transformation rules described before, just like in a regular multiway system. However, each of the dark orange triangles is a critical pair lemma, which is an indication of a place where the multiway system bifurcates, and in which an additional lemma is required in order to prevent it from doing so (in other words, it corresponds to a place where a completion procedure has been applied). Looking at a specific example of such a lemma we see here that two rules, indicated by “Rule” and “MatchingRule” (derived from axiom 2 and axiom 1, respectively) both match a common subpattern, indicated by “Subpattern”, and produce (syntactically) inequivalent expressions, thus producing a bifurcation in the multiway system. However, since both elements in the bifurcation were derived from a common expression by valid application of the rewrite rules, this bifurcation itself constitutes a proof that the two elements are equivalent, and hence we can declare them to be equal (which is the statement of the critical pair lemma). This declaration of equivalence immediately forces the bifurcation to collapse, so we deduce that by adding a sufficient number of critical pair lemmas to the rewrite system, we can therefore force the multiway system to have only a single path (at least up to critical pair equivalence). This, incidentally, is identical to the process of adding critical pair completions so as to force causal invariance in the Wolfram model, which (as I first proposed back in 2019) is the formal basis of our current operational model for quantum measurement; this deep correspondence with the foundations of quantum mechanics will be explored in more detail later. Therefore, the proof graph may be thought of as being a kind of intermediate object between a single evolution thread and a whole multiway system; the substitution lemmas by themselves indicate the path of a single evolution thread, but the critical pair lemmas indicate the places where that evolution thread has been “plumbed into” a large, and otherwise bifurcating, multiway system. We can see this correspondence more directly by considering a simple string multiway system: We can clearly see that the strings “AA” and “ABBBABBB” are connected by a path in the multiway system—a fact that we can visualize more explicitly like so: In effect, we can interpret this path as being a proof of the proposition that “AA” → “ABBBABBB”, given the axiom “A” → “AB”. To make the correspondence manifest, with a small amount of additional code we can also represent this path directly as a ProofObject: Of course, a key idea in homotopy type theory is that we can consider the existence of multiple paths connecting the same pair of points, and hence multiple proofs of the same proposition. For instance, take: Homotopy type theory then tells us that we can represent the proof of equivalence of these proofs as a homotopy between the associated paths. Since an (invertible) homotopy is ultimately just a mapping from points on one path to corresponding points on the other path (and back again), this is easy to represent discretely as follows: However, this homotopy is kind of a “fake”, in the sense that it has been explicitly “grafted on” to the multiway system after the fact. The more “honest” way to enact a homotopy of this kind would be to define explicit multiway rules mapping from states along one path onto states along the other, thus yielding: This new multiway system is analogous to the higher-dimensional space in which we constructed a continuous homotopy earlier on in the bulletin; by “projecting down” onto the lower-dimensional multiway system, we are able to infer equivalence between the corresponding proofs. This process of adding invertible rules mapping from one point along a branch to another is precisely what a completion procedure on critical pairs does, as commonly enacted in automated theorem-proving systems, as well as within our present interpretation of quantum measurement. The correspondence among critical pair completion, quantum measurement and homotopies can thus be seen directly for the first time, although to make this correspondence mathematically rigorous requires first understanding its connection to the univalence axiom, as we shall see later. As Xerxes Arsiwalla pointed out, it is therefore appropriate to think of the multiway rules (like “A” → “AB”) as being analogous to type constructors in mathematical logic (i.e. rules for building new types), so as to allow the multiway systems themselves to be interpreted as inductive types, just as homotopy type theory requires. Moreover, these completion procedures described previously that allow us to define homotopies between paths are a kind of higher-order rule (i.e. they are rules for generating new rules, namely the completion rules), with the result being that the higher-dimensional multiway system that one obtains after applying such a completion procedure is the analog of a higher inductive type. Since there exists an infinite hierarchy of higher inductive types, it follows that there must exist a correspondingly infinite hierarchy of higher-order multiway systems, with the multiway system at level n being a partially “completed” version of the multiway system at level n – 1 (i.e. in which certain explicit homotopies have been defined between particular pairs of paths). So a natural question to ask is: what kind of mathematical structure would such a hierarchy represent.
A Candidate Geometrical Formalism for the Foundations of Mathematics and Physics: Formal Correspondences between Homotopy Type Theory and the Wolfram Model 28.3 Groupoids, Topoi and Grothendieck’s Hypothesis : In category theory, a “category” is just a collection of objects (which can be represented as nodes), along with a collection of “morphisms” between those objects (which can be represented as directed edges), and with the property that all morphisms are both associative and reflexive (i.e. there exists an identity morphism for each object). For instance, starting from a directed graph we can force every directed edge (i.e. every morphism) to be associative by computing the transitive closure: From here, we can then enforce reflexivity by adding a single self-loop (i.e. an identity morphism) around each vertex: Thus, this directed graph has a bona fide interpretation as a small category (“small” because the corresponding directed graph is finite). On the other hand, a “groupoid” is a special case of a category in which all morphisms are invertible (and hence are isomorphisms), which we can again demonstrate purely graph-theoretically by converting our directed graph into an undirected one: Or, to be even more explicit, we could add two-way directed edges to indicate the isomorphisms more directly: Groupoids are so named because they generalize the structure of ordinary algebraic groups; a group can essentially be thought of as being a special case of a groupoid that contains just a single object, and where the single invertible morphism corresponds to the (unary) group inverse operation. Closely related to groupoids are objects known as “topoi”, which can be thought of as being the abstract categorical generalization of point-set topology. A “topos” is a category that “behaves like” the category of sets (or, to be more precise, like the category of sheaves of sets on a given topological space) in some suitably well-defined sense. The crucial feature of a topos that relates it to a groupoid, however, is that topoi necessarily possess a notion of “localization”. In ordinary commutative algebra, rings are not required to possess multiplicative inverses for all elements, so one can “localize” the ring by introducing new denominators (such that the localization is like a restricted version of the field of fractions), thereby introducing multiplicative inverses for elements where they did not previously exist. Similarly, the localization of a category introduces some inverse morphisms where they did not previously exist, hence converting some collection of morphisms into isomorphisms, and thus causing the category to behave “locally” like a groupoid. All of our multiway systems are naturally equipped with a notion of localization; for instance, consider the string multiway system that we explored earlier: We can now select 10 edges at random and introduce their reverse directed edges, so as to simulate adding inverse morphisms for a collection of 10 random morphisms. Much like in the foliation example given previously, this method of localization is kind of a “fake”, since we are just artificially grafting these inverted edges onto a preexisting multiway graph. Instead, we can simply adjoin the reversed edges as a new set of multiway rules: Applying this localization procedure to the entire multiway rule, unsurprisingly, yields the following groupoid-like structure: Since, in the category of sets, all objects are sets and all morphisms are functions from sets to sets, and because for any function from set A to set B there can exist (at least locally) an inverse function from set B to set A, the category of sets is trivially localizable. It is in this sense that we can say that more general kinds of topoi “behave like” the category of sets. One can make this correspondence more mathematically rigorous by realizing (as noted by Xerxes) that, when interpreting multiway systems as inductive types and the associated multiway rules as type constructors, then so long as one includes a “subobject classifier” (namely a special object in the category that generalizes the notion of a subset identifier, where all subobjects of a given object in the category correspond to morphisms from the subobject onto the subobject classifier), and assuming that finite limits exist (where a limit is just a categorical construction that generalizes the universal constructions of products and pullbacks), the resulting multiway system that one obtains is precisely an elementary topos. We have furthermore made the conjecture (although we do not yet know how to prove it) that this subobject classifier is what endows our multiway systems with the concept of “foliation” that is so crucial for constructing branchial spaces, and for deducing both quantum mechanics and general relativity in the appropriate continuum limits, since, at least intuitively, each hypersurface in the foliation may be thought of as corresponding to a subobject in the corresponding category. We shall explore this conjecture more closely, and examine precisely what foliations of multiway systems mean from a metamathematical standpoint, within the next section. For the time being, however, it suffices to realize that such an elementary topos comes naturally equipped (thanks to various results in cohesive geometry) with a functor (that is, a map between categories) to the topos of sets, as well as a pair of adjoint functors (where adjunction refers here to a kind of “weak” equivalence relation between functors) admitting the discrete and indiscrete topologies, respectively. Free topoi possessing this particular collection of functors provide a means of formalizing the notion of a topological space. As discussed earlier, inducing a homotopy between two paths (or, equivalently, applying a completion procedure between multiway branches) can be thought of as introducing new, higher-order “rules of rules”, and hence has the effect of producing a higher-order object than the type (or multiway system) that one originally started with. This notion of “higher-order” objects can be made more precise using the language of category theory; if each path is interpreted as a morphism between objects (known as a “1-morphism”), then a homotopy can be interpreted as a morphism between 1-morphisms (known as a “2-morphism”), with the resultant structure being a “2-category”. Homotopies of homotopies can hence be interpreted as 3-morphisms within 3-categories, and so on, thus producing a whole infinite hierarchy of higher-order categories. The limit of this hierarchy is known as the “infinity-category”, which we can think of as being the structure obtained by inducing all possible homotopies (and, consequently, by applying all possible completion procedures) to a given multiway system. In fact, by applying all possible completion procedures, and therefore applying all possible rules for generating rules, etc., we will inevitably populate the space of all possible rules of a particular size, so the resulting structure can also be thought of as being a “rulial multiway system”, defined in the conventional sense. Moreover, since each completion procedure introduces new inverse morphisms where they didn’t previously exist, and hence also enacts a certain kind of localization, the limiting structure is actually an “infinity-groupoid”. An infinity-groupoid is a “quasi-category” (i.e. a weakening of the definition of an ordinary category in which the composition of two morphisms need not be uniquely defined, and therefore in which the equalities that normally appear in the axioms of identity and associativity are replaced by isomorphisms up to higher homotopy), in which all morphisms are necessarily isomorphisms. This realization is significant because of Grothendieck’s so-called “homotopy hypothesis”, which states that all infinity-groupoids can be interpreted as topological spaces (and consequently that all models of infinity-groupoids can be interpreted as homotopy types), which is ultimately one of the foundational assumptions of homotopy type theory. If Grothendieck’s hypothesis is true, it therefore provides rigorous justification for the claim that the rulial multiway system (which, as we have established, admits a natural interpretation as an infinity-groupoid) is itself a topological space, and hence is naturally endowed with a spatial structure. One immediate consequence of this is that, if we interpret the rulial multiway system as being a fibration, with the individual fibers corresponding to ordinary (non-rulial) multiway systems, and with spacetime causal graphs corresponding to the fibers of those multiway systems, etc., as we shall justify formally later, Grothendieck’s hypothesis effectively explains why our various combinatorial structures, such as multiway systems, branchial graphs, causal graphs, hypergraphs, etc., are additionally endowed with the spatial structures of things like projective Hilbert spaces and Lorentzian spacetimes: they are simply inheriting the spatial structure that is naturally endowed upon the rulial multiway system, as a consequence of the nature of these fibrations. We shall explore more precisely how this works momentarily. We have so far neglected to mention the role that Voevodsky’s univalence axiom plays within this whole discussion. In the foundations of mathematics, the notion of “extensionality” commonly refers to the criteria by which objects are deemed to be identical (e.g. the “axiom of extensionality” in axiomatic set theory states that two sets are identical if and only if they contain the same elements); extensionality is thus the logical analog of a “StateEquivalenceFunction” in the case of our multiway system formalism. In conventional mathematical logic, “propositional extensionality” asserts that a pair of propositions may be considered identical if and only if they logically imply each other. The univalence axiom is a grand generalization of the principle of propositional extensionality to what we may otherwise call “type extensionality”. it claims that two types can be considered identical if and only if they are equivalent (in the sense that they are connected by paths in the associated homotopy space). In fact, the statement of the axiom is a little bit more subtle than that (since it claims that the “if and only if” in the previous statement is itself a type-theoretic equivalence relation). But this captures the essential underlying idea. When one induces a homotopy between two independent paths in a multiway system (which we can interpret as being a critical pair completion in the context of quantum measurement, or a proof of equivalence between proofs in the context of homotopy type theory), we effectively treat the corresponding paths as being identical, in the sense that they proceed to evolve as an effective single path in the multiway system. For instance, consider the following minimal case of a multiway system containing a single bifurcation that does not resolve: These two paths currently act as totally separate and disconnected branches, but if we now perform a critical pair completion the resultant multiway system behaves as though there is only a single path, since now there are new rules permitting one to “jump” instantaneously from one branch to the other: We are justified in treating these two paths as identical precisely because they are equivalent (thanks to their common ancestry in the “S” state). In other words, the formal justification for our interpretation of quantum measurement in the context of the Wolfram model, just like the formal justification for the identification of homotopy-equivalent paths in homotopy type theory, comes directly from the univalence axiom. Or, to put it slightly more poetically, quantum measurement, at least within our formalism, is simply “applied univalence” in a very precise sense. We can think of “objective reality” in the context of mathematics as being effective causal invariance induced by the univalence axiom. We can also think of “objective reality” in the context of quantum mechanics as being effective causal invariance induced by the act of measurement. Both can be represented purely in terms of critical pair completions, and the correspondence between the two will be demonstrated in a very direct way later on.
A Candidate Geometrical Formalism for the Foundations of Mathematics and Physics: Formal Correspondences between Homotopy Type Theory and the Wolfram Model 28.4 Gauge-Dependence of Mathematical Truth and a “Relativistic” Interpretation of the Incompleteness Theorems : Kurt Gödel’s proof of the incompleteness theorems in 1931 established, via the ingenious technique of Gödel numbering, that Peano arithmetic (the standard axiom system for integer arithmetic) was capable of universal computation. Therefore, in particular, he demonstrated that Peano arithmetic could be set up so as to encode the statement “This statement is unprovable.” as a statement about ordinary natural numbers. Clearly, if this statement is true, then it is unprovable (so Peano arithmetic is incomplete), and if it is false, then it is provable (so Peano arithmetic is inconsistent). As such, one way to phrase the first incompleteness theorem is “If Peano arithmetic is consistent, then there will exist propositions that are independent of the Peano axioms”. In other words, there will exist propositions where neither the proposition nor its negation are (syntactically) provable using the axioms of Peano arithmetic in any finite time. Thus, the first incompleteness theorem is ultimately a proof-theoretic statement: it concerns the class of propositions that are syntactically provable, independent of any semantic interpretation of the terms. On the other hand, model theory studies what happens when one endows a mathematical theory with a concrete semantic interpretation of its propositional terms. For instance, returning to our previous example of the axioms of group theory, we have: Everything that we have discussed so far has concerned what can be proven purely on the basis of the syntax of these axioms, in which terms like a, b and c (for instance, in the case of the associativity axiom) can take the form of any arbitrary symbolic expression. However, if one now defines an explicit “domain of discourse” (e.g. a set over which one is quantifying, such that a, b and c now correspond to elements of that particular set), then one has thereby defined a model, with the resulting semantic interpretation now introducing an additional richness of structure that was absent in the “mere” proof theory. Incompleteness also admits a model-theoretic interpretation, in addition to a purely proof-theoretic one. For instance, Kurt Gödel and Paul Cohen (in 1940 and 1963, respectively) proved that the continuum hypothesis regarding the relative cardinalities of infinite sets is independent of the axioms of ZFC set theory. One way of understanding this result is that Gödel proved that there must exist models of ZFC in which the continuum hypothesis is semantically true (i.e. ZFC + CH is consistent if and only if ZFC is consistent), whereas Cohen proved, using his technique of set-theoretic forcing, that there must exist models of ZFC in which the continuum hypothesis is semantically false (i.e. ZFC + CH is inconsistent). Therefore, if ZFC is consistent, then the continuum hypothesis must be independent of the axioms, because any statement that is syntactically provable must be semantically true in all models, if the underlying system is consistent. Therefore, I believe that our formalism permits the following new “pseudo-relativistic” interpretation of the first incompleteness theorem: incompleteness is ultimately just a statement of gauge dependence in mathematics. In the conventional mathematical treatment of relativity, the conformal structure of spacetime induces a partial order (namely the “causal” partial order) on spacetime events. Since there are generically many different total orders that are consistent with a given partial order, we can infer that there are many possible gauge choices/reference frames/foliations of the spacetime that are consistent with the causal partial order generated by its conformal geometry. Thus, the requirement for Lorentz symmetry ultimately corresponds to the statement that the ordering of timelike-separated events (i.e. events whose ordering is specified by the partial order) is gauge invariant, but the ordering of spacelike-separated events is gauge dependent. Accordingly, my basic claim is that proof theory defines a partial order on the space of all possible mathematical propositions, and choices of models are coordinatizations of the space of mathematical proofs that are analogous to the choices of gauge that allow one to construct total orders that are consistent with this partial order. As such, there will exist propositions whose truth is gauge invariant (namely those that are syntactically provable, as defined by proof theory), and there will exist propositions whose truth is gauge dependent (namely those that are either semantically true or semantically false, but otherwise independent of the axioms, as defined by model theory). To see this explicitly, consider our usual string multiway system: A proposition such as “ABAB” → “ABBBBABB” is syntactically provable, since there exists a finite path (or a “chain”) through the partial order of the multiway system such that “ABBBBABB” is preceded by “ABAB”: On the other hand, a proposition such as “AABB” → “ABBA” is not syntactically provable, since the two expressions form an anti-chain, so their relative order is not specified by the partial order of the multiway system, and this therefore corresponds to an undecidable proposition (i.e. a proposition that is independent of the underlying axioms that define the multiway system): However, we know that there will exist models in which this proposition is semantically true, which we can see directly by making the following explicit gauge choice: Here, we can see directly that “ABBA” is preceded by “AABB” in the total order induced by the foliation, and so this model is clearly consistent with the proposition. However, consider an alternative gauge choice: We see now that that “AABB” is preceded by “ABBA” in this new total order, so the proposition “AABB” → “ABBA” is not consistent with the induced ordering, and therefore the corresponding model is inconsistent with the proposition. Note that these foliations all correspond to “flat” or “inertial” reference frames, which correspond to what we might call “free models” (i.e. models within which, when viewed as an algebraic structure equipped with finitary operations, the only nontrivial equations that hold between elements of the model are those defined by the axioms themselves, plus one additional relation defining the slope of the foliation, by analogy with “free groups”, “free algebras”, “free lattices”, etc.). Of course, in general, models can have arbitrary additional sets of constraints and equations, and hence will induce the analog of more arbitrary “non-inertial” reference frames, e.g: The precise connection between foliation choices and sets of relations in the presentation theory of algebraic structures will be outlined in the next section. For the time being, though, considering more arbitrary classes of possible foliations naturally presents the question of what concepts like “curvature” correspond to in the context of the space of all possible mathematical proofs. Considering, as we have previously, each possible proof as corresponding to a different geodesic within this space, we infer that the presence of a vanishing Ricci curvature (i.e. a space in which geodesics neither converge nor diverge) thus corresponds to the case in which all proofs consist of entirely linear sequences of substitution rule applications with no resultant bifurcation, and thus no need for critical pair lemmas, e.g.: One implication of the presence of this vanishing Ricci curvature is that making an infinitesimal perturbation to this geodesic (e.g. by “pulling back” one of the intermediate substitution lemmas so as to make it part of the axioms) will exert only a minimal effect on the length of the geodesic (e.g. shortening the proof length by just 1 step): Conversely, each critical pair lemma is an indication of the presence of an elementary non-holonomy in the associated manifold (since each critical pair describes an elementary triangle in the multiway system, and so the presence of an unresolved critical pair indicates a failure of parallel transport around the associated triangle to preserve geometrical information, since the two “endpoints” of the triangle end up at different points in the proof space). Since the Riemann curvature tensor measures the failure of commutativity of the covariant derivative operator on an arbitrary Riemannian manifold, we can therefore think of the critical pair lemma density in a particular “direction” as being the analog of the projection of the Riemann curvature tensor in proof space. For instance, the proof of commutativity of the “Wolfram axiom” for Boolean algebra contains a high density of critical pair lemmas, and therefore corresponds to a geodesic propagating through a space with a non-vanishing curvature: Accordingly, the analogous perturbation to this geodesic has a more dramatic effect on its length (e.g. shortening it by a total of 3 steps rather than just 1): In these proof graphs, each dashed line indicates where a previously proved result is being used in the proof of some future lemma, and so corresponds to the analog of a causal edge in an ordinary multiway system. Indeed, we can see the correspondence directly by first inspecting the sequence of causal edges as seen in an ordinary string multiway system and then comparing against the sequence of dashed edges in the associated proof object representation: Therefore, we may think of the presence of these dashed lines (and hence, of causal edges) as being an indication of the degree to which newly proved lemmas “build upon” previous results, which may, in turn, be interpreted as a crude measurement of the degree of mathematical abstraction used within the proof. In the conventional mathematical formulation of the Wolfram model, I also first proposed that the correct interpretation of the energy-momentum tensor is the flux of causal edges through a constant hypersurface in the causal graph (such that energy is flux through spacelike hypersurfaces, momentum is flux through timelike hypersurfaces, etc.). The immediate corollary of these realizations is that the abstract analog of the energy-momentum tensor in spacetime should be a kind of “abstraction tensor” in the space of mathematical proofs, measuring the density of these causal edges. Furthermore, we can infer that a generalization of the Einstein field equations should hold within this space of all possible mathematical proofs, and that these equations will assert that the level of abstraction of the proof (i.e. our abstraction of the energy-momentum tensor) should be directly related to the density of critical pair lemmas (i.e. our approximation to the abstraction of the Ricci curvature, which is, correspondingly, a proxy to the computational difficulty of generating the proof). Just as all relativistic observers with nonzero mass are required to lie strictly on the interior (and not on the boundary) of elementary light cones, so too are computationally bounded models required to lie strictly on the interior of elementary “proof cones”. For instance, all geodesics highlighted in red here constitute syntactically valid proofs of the proposition “AA” → ”ABBBBABBBB“ (since all are ultimately consistent with the induced partial order): However, two of these proofs lie strictly on the boundary of the associated “proof cone”, and hence correspond to the analog of lightlike, rather than ordinary timelike, paths: Therefore, although these proofs (here highlighted in yellow and purple) are syntactically correct, they cannot be witnessed by any computationally bounded (i.e. “subluminal”) model. Thus, we can conclude that non-constructive proofs correspond to the analog of lightlike paths in the associated proof space. One immediate and amusing consequence of this correspondence is that the analog of the Penrose–Hawking singularity theorems would imply that, in the presence of sufficient “abstraction density” in proof space, proofs will inevitably become non-constructive (i.e. the analog of a black hole event horizon). To be more precise, we can infer that certain axioms (such as the axiom of choice in ZFC set theory and the law of excluded middle in classical logic) that are known to permit non-constructive proofs of propositions can be thought of as inducing the formation of black holes in the space of mathematical proofs. On the surface of a black hole event horizon, there will exist pairs of points where the only geodesics connecting them correspond to lightlike paths. Similarly, in a system of mathematics based on non-constructive axioms, there will exist pairs of terms (i.e. propositions) where the only proofs connecting them are non-constructive, and so one would require effective hypercomputation (corresponding to a model “traveling” at the analog of the “speed of light”) in order to be able to witness them model-theoretically. Another rather interesting implication of this formalism is that there should exist model-theoretic analogs of logical speedup theorems, which correspond to the analog of Lorentz transformations in proof space. For instance, the famous Gödel speedup theorem asserts that, in formal axiomatic systems that are at least as rich as Peano arithmetic, there will exist propositions whose shortest proofs are of arbitrary length, but that such proofs can always be made shorter (i.e. “sped up”) by working in a stronger axiomatic system. We can easily see this by constructing explicitly the statement “This statement is unprovable using fewer than n symbols.”, which, via Gödel numbering, can be encoded as an elementary statement in Peano arithmetic. Clearly, if Peano arithmetic is consistent, then the shortest proof of this statement must contain at least n symbols, but it is nevertheless trivial to prove in the stronger axiomatic system PA + Con(PA) (i.e. the Peano axioms, plus an additional axiom asserting the consistency of the Peano axioms). Similarly, we see that we now have a model-theoretic version of the same idea; by shifting to a “boosted” foliation, we can reduce the computational effort necessary to verify a theorem (i.e. to confirm that a particular path is consistent with the total order induced by the model), which is the analog of relativistic time dilation, but the tradeoff is that more computational effort is required in order to verify the model (since there now exist more terms on each slice of the foliation, all of which must be confirmed as being valid anti-chains with respect to the partial order of the proof network, in order for the model to be verified as being consistent with the underlying axioms), which forms the analog of relativistic length contraction, as seen here: As such, the “invariant spacetime interval” now becomes the effective difference between the time complexity of verifying the theorem and the time complexity of verifying the associated model, such that, by making a parameterized change of model, there can be “no free lunch” (any computational saving made in one verification task is offset by a computational loss made in the other task). For instance, as mathematicians continue to increase the level of mathematical abstraction of the field by proving more theorems, a “generalized graduate student” who is prepared to accept the validity of those theorems on faith is able to make faster progress than they would have done if the abstraction level had been lower, but the tradeoff is that the foundations of mathematics become harder to formally verify as a result. In particular, we therefore conclude that non-constructive proofs are ones whose associated models would require infinite computational power to verify, as we conjectured previously. This entire correspondence rests upon an interpretation of “spacelike-separated” terms as being ones that are mutually independent of the underlying axioms (and therefore among which relations can be defined without contradicting the underlying theory). If we interpret the axioms as forming an orthonormal basis for the proof space, then these mutually independent terms become precisely the linearly independent vectors in that space (in other words, proof-theoretic independence becomes linear independence). We can see how this analogy works explicitly by considering the particular example of finite presentation theory on generic algebraic structures, as we do subsequently.
A Candidate Geometrical Formalism for the Foundations of Mathematics and Physics: Formal Correspondences between Homotopy Type Theory and the Wolfram Model 28.5 Foliation Choices for Finitely Presented Algebraic Structures : Consider the pair of highlighted terms in the following multiway proof network: Since they are not comparable within the partial order defined by the proof network (i.e. since they form an anti-chain), the associated proposition may be considered to be independent of the underlying axiom system, as discussed previously. If we add an additional proposition asserting the equivalence of these terms, then, in the absence of any further relations, we will obtain something akin to the following (inertial) foliation: In other words, we see here a flat foliation in which the proposition is semantically true, since “ABBA” immediately precedes “AABBB” in the total order induced by that foliation choice. The addition of this relation then implies further relations between terms, which is why the foliation lines are extrapolated out to cover the entirety of proof space; if further relations were to be explicitly enforced, then we would obtain a more arbitrary (non-flat, non-inertial) foliation choice, as we showed before. To see more directly how this works, consider the case of the finite presentation theory for simple algebraic structures. Note that we can represent generators for algebraic structures as unidirectional multiway rules like “A” → “AB”, and that the associativity of any such structure is enforced by the natural associativity of the concatenation operator for strings. Therefore, by examining the following multiway system generators = {A -> AA, A -> AB, B -> BA, B -> BB}; we see that we naturally obtain the finitely presented semigroup with two generators, “A” and “B”. One could argue that strings more typically form a monoid, since the empty string "" constitutes a natural choice of identity element, but let’s choose instead to specify the identity element more explicitly as “E”, thus yielding the following finitely presented monoid: generators = {"A" -> "AA", "A" -> "AB", "A" -> "AE", "B" -> "BA", identityAxiom = {"AE" -> "A", "EA" -> "A", "BE" -> "B", "EB" -> "B", Further specifying inverse elements “X” and “Y” for “A” and “B”, respectively, yields a finitely presented group: generators = {"A" -> "AA", "A" -> "AB", "A" -> "AE", "A" -> "AX", identityAxiom = {"AE" -> "A", "EA" -> "A", "BE" -> "B", "EB" -> "B", inverseAxiom = {"AX" -> "E", "XA" -> "E", "BY" -> "E", "YB" -> "E}; This multiway system consequently designates the free group on generators “A” and “B”—the group is free in the sense that it does not obey any additional relations beyond those necessitated by the group actions themselves, and its “freedom” can be seen directly from the fact that the associated network has four essentially independent “branches” (one for each of the elements “A”, “B”, “X” and “Y”). It is important to note that these multiway systems are not Cayley graphs, since they contain strictly more information: Cayley graphs solve the word problem implicitly (e.g. if “E” is the identity element, then there is no vertex “EAE” in a Cayley graph—there is only “A”), whereas in one of these multiway models there is an explicit path demonstrating the solution of the word problem directly, e.g. “EAE” → “AE” → “A” etc. Since each edge corresponds directly to the application of a group axiom, a generator or a relation, these multiway systems are thus much closer to the underlying structure of the group axioms than a Cayley graph otherwise would be. As an illustrative example, we could now force elements “A” and “B” to commute with one another by incorporating the additional relation “AB” == “BA”: commutativityRelation1 = {AB -> BA, BA -> AB}; Note that, within this scheme, generators are specified using unidirectional rules such as “A” → “AB”, whereas relations are specified using bidirectional rules such as “AB” ↔ “BA”. Unsurprisingly, the inclusion of such a relation has the effect of “binding” the two branches corresponding to elements “A” and “B” together, thus causing them to act as a single effective branch (since there is now a pair of rules that allows one to “jump” easily from one branch to another). In other words, the inclusion of this relation is equivalent to performing a critical pair completion on the associated branches, or, alternatively, to inducing a foliation on the multiway system (since the set of elements within which one is imposing equivalences in the context of a critical pair completion can equally well be thought of as being the set of elements within the level surface of some multiway foliation). Forcing commutativity of “X” and “Y” through the relation “XY”==“YX” has the same effect of binding the “X” and “Y” branches together: commutativityRelation2 = {XY -> YX, YX -> XY}; As one might reasonably expect, adding relations to enforce commutativity among all elements is equivalent to imposing an axiom of commutativity at a purely proof-theoretic level, and so the resulting multiway structure is isomorphic to that of an ordinary abelian group, in which all branches essentially collapse down to form a single branch: commutativityAxiom = {"AB" -> "BA", "AE" -> "EA", "AX" -> "XA", To take a slightly more subtle example, imagine instead imposing the relations “A”==“Y”, “B”==“X” (in other words, explicitly declaring that “A” is the inverse of “B”, and vice versa). The resultant multiway structure is inverseRelation = {A -> Y, Y -> A, B -> X, X -> B}; which we can see is ultimately just a more “decorated” version of the multiway structure for the free group generated by a single element “A”, with a single inverse “X”: generators = {"A" -> "AA", "A" -> "AE", "A" -> "AX", "E" -> "EA", identityAxiom = {AE -> A, EA -> A, XE -> X, EX -> X}; inverseAxiom = {AX -> E, XA -> E}; As such, a Knuth–Bendix completion applied to a multiway system corresponding to the free group on two generators can therefore be interpreted as applying all possible sets of relations generators = {"A" -> "AA", "A" -> "AB", "A" -> "AE", "A" -> "AX", identityAxiom = {"AE" -> "A", "EA" -> "A", "BE" -> "B", "EB" -> "B", inverseAxiom = {AX -> E, XA -> E, BY -> E, YB -> E}; and consequently generating the “maximally non-free” group structure: As an illustration of its “maximal non-freeness”, we can for instance see explicitly that this structure (which one can think of as being effectively the rulial space of all groups with two generators) exhibits, for instance, commutativity of elements “A” and “B”: And, being a rulial space, it follows that all other groups on two generators can be constructed via an appropriate fibration of this “maximally non-free” group structure, as we shall now try to demonstrate. Due to the correspondence between completion procedures in the context of imposing group relations and completion procedures in the context of performing quantum measurements, we can therefore think of the free group as being a “purely quantum mechanical” group structure, while the maximally non-free group is a “purely classical” group structure, with all other groups lying somewhere on the intermediate spectrum.
A Candidate Geometrical Formalism for the Foundations of Mathematics and Physics: Formal Correspondences between Homotopy Type Theory and the Wolfram Model 28.6 Fibrations of the Rulial Topos and the (Inevitable) Emergence of Geometry : I first developed the function MultiwayTuringMachine (as used in Stephen’s recent bulletin on exploring rulial space in the case of Turing machines) as a means of visualizing the evolution of arbitrary nondeterministic Turing machines using our multiway system framework; for instance, we can visualize the evolution of the 2-state, 2-color nondeterministic Turing machine constructed from deterministic Turing machine rules 100 and 150 as follows: After more evolution steps, we begin to see the following emergent structure in the states graph: One rather elegant ancillary benefit of the MultiwayTuringMachine functionality is its ability to produce visualizations of rulial space with ease, by simply constructing a multiway evolution from the application of all possible deterministic Turing machine rules of a particular size. For instance, we can visualize the rulial space of all 2-state, 2-color Turing machine rules ranging from 1 to 200 as follows: As discussed earlier, the rulial multiway system (which, within this new context, we may think of as being the structure that one obtains by starting from a particular multiway system and applying all possible completion rules, or equivalently by inducing all possible homotopies between paths), is naturally endowed with the structure of an infinity-topos, allowing us to localize by converting collections of morphisms into isomorphisms. We can do this because the rulial multiway system invokes all possible rules of a particular size, so if there exists a rule mapping state A to state B, then there will always necessarily exist another rule mapping state B back to state A etc. This realization makes it very natural to conjecture that the rulial topos can be “fibrated” in such a way that every particular multiway system corresponds to a particular “fiber” of rulial space in some fairly precise sense. For instance, the multiway system associated with the {100, 150} nondeterministic Turing machine shown previously would correspond to the fiber shown in red here: In ordinary topology, a “fibration” is a way of making precise the notion that it is possible to parameterize one topological space (known as a “fiber”) in terms of another topological space (known as a “base space”). In the more restricted “fiber bundle” construction, every fiber needs to be a copy of the exact same topological space (or, at the very least, the fibers must all be homeomorphic), but in the more general case of an arbitrary fibration, the only condition is that each pair of fibers be “homotopy-equivalent”, meaning that there must exist continuous maps from one fiber to the other such that the composition of those maps is homotopic to the identity map on the corresponding fiber. In particular, in order to demonstrate that the construction described is indeed a valid fibration in the topological sense, we must proceed to demonstrate that it satisfies the so-called “homotopy lifting property”. In an ordinary fiber bundle, the “connection” is a rule for “lifting” an elementary step in the base space up to a corresponding elementary step in the fiber; thus, in our context, where each particular multiway system is interpreted as a base space, and the subgraph of the overall rulial multiway system that corresponds to that particular multiway system is interpreted as the corresponding fiber, our “connection” is the rule that allows one to map edges in the ordinary multiway system to the corresponding edges in the rulial multiway system. The “homotopy lifting property” is thus the property that any homotopies in the base space will naturally be “lifted up” by the connection to homotopies in the corresponding fiber. To see how this works directly, consider the following pair of paths in our original (nondeterministic Turing machine) multiway system: We can now proceed to define a homotopy between these paths in the usual way: The homotopy lifting property will be satisfied (and, consequently, our interpretation of the ordinary multiway system as being the base space for an associated fiber in the rulial multiway system will be formally justified) if and only if for every such homotopy between pairs of paths the homotopy is “lifted” by the connection to form a valid homotopy in the rulial multiway system as well—a fact which we can now make manifest: This interpretation of ordinary multiway systems as being the fibers of some rulial topos, when combined with Grothendieck’s hypothesis, therefore explains not only why so many discretely defined mathematical structures (like groups, rings, algebras, lattices, etc.) come naturally associated with a notion of a continuous spatial structure (such a differentiable manifold for a Lie group, a topological space for a topological group, a collection of clopen subsets of a topological space for a Boolean algebra, etc.), as explained by Shulman’s synthetic topology and cohesive geometry programs. Indeed, in this context, it suffices to treat Grothendieck’s hypothesis less as a hypothesis and more as an abstract definition of what it means to be a topological space (i.e. anything with an infinity-groupoid structure). It also explains why our discretely defined physical structures (such as hypergraphs, causal graphs, multiway graphs, etc.) also come naturally associated with a continuous spatial structure (such as a Riemannian manifold for a hypergraph, a Lorentzian manifold for a causal graph, a projective Hilbert space for a branchial graph, [conjecturally] a twistor correspondence space for a multiway causal graph, etc.) At some ultimate level, all of these objects are simply inheriting their topology and geometry from that of the rulial multiway system, and its natural infinity-topos structure; the rulial multiway system endows its fibers (the ordinary multiway causal graphs) with the spatial structure of (conjecturally!) a twistor correspondence space. These multiway systems then endow their hypersurfaces (the branchial graphs) with the spatial structure of a projective Hilbert space, and also endow their fibrations (the ordinary spacetime causal graphs) with the spatial structure of a Lorentzian manifold; these spacetime causal graphs in turn endow their hypersurfaces (the spatial hypergraphs) with the spatial structure of a Riemannian manifold, etc. So does anything exciting happen when we choose to “descend down” from the geometry of the full rulial topos to the rather more mundane geometry of a particular multiway system (of the kind that we conventionally use in our descriptions of quantum mechanics)? Well, as it turns out, the answer seems to be yes.
A Candidate Geometrical Formalism for the Foundations of Mathematics and Physics: Formal Correspondences between Homotopy Type Theory and the Wolfram Model 28.7 Symmetric Monoidal Categories and the (Inevitable) Foundations of Quantum Mechanics : Categorical quantum mechanics, as pioneered by Samson Abramsky and Bob Coecke, aims to define a relatively weak set of axiomatic constraints (for instance, much weaker than the usual Hilbert space axioms used in the standard operator-theoretic formulations of quantum mechanics) that can nevertheless reproduce the salient features of key quantum phenomena; its basic setting is a so-called “dagger symmetric monoidal category”. A “monoidal category” is a category equipped with an additional associative “bifunctor” (i.e. a map from a pair of categories down to a single category), mapping the product of the category with itself onto itself, that can be interpreted as an additional tensor product structure on that category, along with an element that acts as both a left and right identity for this tensor product bifunctor (where associativity and identity are enforced only up to natural isomorphism). A “symmetric monoidal category” is hence a monoidal category in which the tensor product bifunctor is “as symmetric as possible”, in the sense that the tensor product of A and B is naturally isomorphic to the tensor product of B and A. Finally, a “dagger symmetric monoidal category” is a symmetric monoidal category equipped with an additional involutive “dagger structure”, whose significance we shall discuss momentarily. Arsiwalla subsequently observed that, with an appropriate interpretation of the tensor product type constructor, Abramsky and Coecke’s formulation of categorical quantum mechanics is precisely the formulation that we obtain upon foliating the rulial multiway system to obtain ordinary (quantum mechanical) multiway systems. More specifically, a particular multiway (nondeterministic) Turing machine, such as the one generated by deterministic 2-state, 2-color Turing machine rules 100, 150 and 200 will exhibit a particular branchial graph structure upon appropriate foliation into branchlike hypersurfaces, which in this specific case happens to correspond to the rather simple pattern of disconnected triangular graph components shown here: On the other hand, the rulial multiway system, such as the one generated by deterministic Turing machine rules 1 to 300 being naturally a much richer combinatorial structure, will in general produce a much more complicated branchial graph structure upon foliation into rulelike hypersurfaces: However, because the multiway graph itself corresponds to the base space for a particular fiber in the associated rulial multiway graph, we can consequently highlight where the triangles in the branchial graph structure for the ordinary multiway system have been “embedded” into the branchial graph structure for the overall rulial multiway system, as follows: So the obvious question now becomes: since the rulial multiway system is essentially constructed via a concatenation of all possible multiway rules, can we consequently interpret the rulial branchial graphs as corresponding to some generalized tensor product of all possible multiway branchial graphs? In other words, is a tensor product in branchial space simply a concatenation operation of rules in rulial space? More work is required in order to make this correspondence mathematically rigorous (as we shall demonstrate in our forthcoming paper), but I shall give here a plausibility argument instead. The tensor product operation is characterized by a universal property of being the maximally free bilinear operation; loosely speaking, it is the most general bilinear operation (i.e. the most general operation mapping a product of vector spaces to a vector space, in such a way as to be linear in each of its arguments) that “behaves like a multiplication operation should behave”. Take, as an illustrative example, the following pair of (isomorphic) multiway systems: Upon foliation, these yield the following pair of path-like branchial graphs, which we may interpret as corresponding to simple one-dimensional vector spaces: By concatenating the associated multiway rules together, we obtain a kind of “reduced” rulial multiway system with a somewhat more intricate structure yet one where the fibers associated with the original two multiway graphs are still easily discernible: Likewise, upon foliation of the reduced rulial multiway graph, the branchial graphs now form a two-dimensional vector space structure yet the original one-dimensional vector spaces involved in the tensor product can be identified as essentially forming the spanning set of this higher-dimensional space: Therefore, exactly as Arsiwalla had conjectured, the object obtained by foliation of the rulial multiway graph does indeed have a category-like structure, equipped with an additional “tensor product–like” bifunctor, whose associativity and commutativity (up to isomorphism) are guaranteed by the trivial associativity and commutativity of rule concatenation in multiway systems—or, in other words, we get exactly a symmetric monoidal category. This gives us a new, and much more eminently categorical, way of thinking about the formulation of quantum mechanics within our formalism, since now the branchial structure obtained through any foliation of the overall rulial multiway graph into ordinary multiway systems will necessarily correspond to a tensor product of (potentially finite-dimensional) Hilbert spaces, just as one would expect from the foundations of “ordinary” quantum mechanics. There is also much more to discuss here regarding this new interpretation of the tensor product operation in monoidal categories as being essentially a joining operation on type constructors, and the various algebraic and geometrical implications that it has. One immediate, and rather interesting, implication this new interpretation entails is that the fibrations we were constructing before, which essentially allowed us to “split” a rulial multiway system into its constituent ordinary multiway systems, can be thought of as corresponding to a kind of generalized partial trace operation. This, in turn, provides further formal justification for our present interpretation of branchial graphs as being effective “graphs of entanglement” between microstates of the universe (i.e. graphs in which each vertex corresponds to a microstate, and in which the natural combinatorial distance metric on the graph corresponds to a measure of entanglement entropy), because now we can see explicitly that if a pair of microstates are “entangled” (i.e. if they are connected by a branchial edge), then any fibration that attempts to separate them will necessarily lose the information associated with that branchial edge. This corresponds precisely to the statement in ordinary quantum mechanics that any attempt to partially trace out one subsystem from a pair of entangled subsystems will necessarily cause the state to lose purity etc. I mentioned previously that I would return to the remaining issue of the origins of the dagger structure in these models. The “dagger” in the dagger symmetric monoidal categories of categorical quantum mechanics is an additional structure that the monoidal category is equipped with that acts as an “involutive” functor (i.e. a functor that constitutes its own inverse operation), with the broad intuition being that the dagger operation corresponds to some appropriate generalization of the Hermitian adjoint operation from ordinary functional analysis, and hence can be used to define the appropriate generalizations of self-adjoint and unitary operators in categorical quantum mechanics. Of course, there are many possible choices of involutive functorial transformations that one could conceivably apply to a branchial graph (for instance, the reversal of a particular subset of directed branchial edges would be perhaps the most obvious example), but identifying which particular involutive transformation yields the most appropriate interpretation of the dagger structure within our models will require further work.
A Candidate Geometrical Formalism for the Foundations of Mathematics and Physics: Formal Correspondences between Homotopy Type Theory and the Wolfram Model 28.8 A Pseudo-Philosophical Remark about the Foundations of Physics : Indeed, the realizations discussed in this bulletin potentially give us a new insight into why the laws of both quantum mechanics and general relativity are actually true. As we have seen, the fact that the infinite limits of the combinatorial structures associated with causal graphs correspond to Lorentzian manifolds (for which, as I proved formally in the context of our models, the most general set of constraints needed to guarantee this limit correspond exactly to the Einstein field equations of ordinary general relativity) is guaranteed by Grothendieck’s hypothesis and the inheritance of spatial structure from fibrations of the rulial topos. But now, we see also that a symmetric monoidal category–like structure, and hence the entire formalism of categorical quantum mechanics, is also an inevitable feature arising from foliations of this exact same topos. And, as we have now established, the univalence axiom is precisely the assumption that allows us to transform these foliations into fibrations and back again (and hence, it is ultimately univalence that guarantees the logical consistency between quantum mechanics and general relativity in the context of our formalism, and demonstrates that both theories are really just different manifestations of the same fundamental principle of causal invariance). Thus, for the first time, we perhaps begin to glimpse not just what the laws of physics are, but also why the laws of physics (and, indeed, of mathematics) must be so.
A Candidate Geometrical Formalism for the Foundations of Mathematics and Physics: Formal Correspondences between Homotopy Type Theory and the Wolfram Model 28.9 Some Immediate Next Steps : Complete the writeup of the supplementary mathematical paper to this bulletin. Work to obtain a better understanding of the potential origins of the involutive dagger structure in foliations of the rulial multiway system, and its implications for the foundations of quantum mechanics. Develop efficient code for converting neatly between multiway systems and proof objects. Perform a systematic experimental study of mathematical proofs generated using FindEquationalProof, in an attempt to determine empirically the form of the generalized “Einstein field equations” in the space of possible mathematical proofs (i.e. the quantitative relationship between the density of critical pair lemmas and the density of causal edges, as projected in particular directions). Determine the nature and interpretation of topological obstructions in the space of all possible mathematical proofs (my personal conjecture: at least when viewed as a rulial space, these obstructions will correspond to the analog of NP-complete problems that prevent one from performing the analogs of polynomial-time reductions between models/complexity classes). Investigate hypergraphs as a potentially appropriate formalism for representing “thickened” branchial graphs (i.e. branchial graphs in which state ancestry goes back further than a single evolution edge), and the possibility of thus obtaining an abstract generalization of categorical quantum mechanics.
A Short Note on the Double-Slit Experiment and Other Quantum Interference Effects in the Wolfram Model 29.1 : This bulletin is a short note detailing how single-slit, double-slit and multi-slit photon diffraction and interference patterns can be successfully reproduced using the author’s own formulation of quantum mechanics in the Wolfram model. The author has benefited greatly from many fruitful conversations with Stephen Wolfram, as well as from the encouragement (and infectious enthusiasm) of Hatem Elshatlawy. Introduction When we announced the Wolfram Physics Project back in April, among the many launch documents that Stephen and I wrote was a paper in which I outlined a mathematical formulation of quantum mechanics in the Wolfram model in terms of multiway systems, path weights and completion procedures, and presented rigorous derivations of several key features of conventional quantum mechanical formalism, including the canonical commutation relations, the path integral and the Schrödinger equation. It even conjectured a precise algorithmic procedure by which one could describe the otherwise mysterious phenomenon of “wavefunction collapse” in the context of quantum measurement. It constituted a natural supplement to Stephen’s less mathematical (and correspondingly much more accessible) technical introduction to our quantum mechanical formalism. I outlined in great detail how quantum amplitudes would emerge as a consequence of path weights in the multiway evolution graph, with phase differences between pairs of paths thus corresponding to the ratios of branchlike- to spacelike-separated events, in such a way that applying an appropriate Knuth–Bendix completion procedure to the multiway system would force the analog of both constructive and destructive interference between different branches of history, exactly as conventional quantum mechanics predicts. Since many of the key “classic” phenomena of quantum mechanics, such as the diffraction and interference of photons passing through parallel slits, are ultimately just elementary corollaries of this derivation of the Schrödinger equation, I fully expected that very soon after the release of my paper, somebody out there would take the requisite couple of minutes to sit down and write the trivial piece of code needed to reproduce the famous double-slit experiment in the Wolfram model. Following Stephen’s earnest advice to me about how best to instigate a new research program (“You mustn’t pick all the low-hanging fruit yourself! Leave something for the new people to do!”), I had explicitly decided to leave this particularly juicy-looking piece of fruit unpicked, hoping that some young student would come along and be the first to see the interference fringes for themselves. Several months went by, and despite my continued encouragement to various people (including several students at our Summer School!), for whatever reason, nobody did the experiment. So, in the end, I decided that I would just do it myself over a spare weekend, and this short bulletin was the result. The first part of this bulletin demonstrates how, using only a simple string multiway system equipped with elementary path weights, one can easily reproduce the known phenomena of single-slit, double-slit and multi-slit photon interference, yielding intensity patterns that can be shown to converge to the results predicted analytically by the standard equations of optics/quantum mechanics, and indicates how these interference patterns connect to the geometry of branchial space (and hence to the geometry of the associated projective Hilbert space of the system). The second part then illustrates precisely why these interference patterns appear, as a straightforward consequence of the basic combinatorics of multiway systems and some elementary number theory; it demonstrates how the setup of the string multiway systems shown in the first part effectively encodes a position basis that maps points in branchial space onto corresponding points in physical space, it gives a minimal explicit example of pairs of interacting quantum oscillators to show directly how both constructive and destructive interference effects work within the multiway Wolfram model formalism, and it demonstrates precisely why these interference effects occur, as a consequence of some basic modular arithmetic and the combinatorial consequences of multiway completion rules. In effect, this bulletin may be considered to be a concrete computational instantiation of one of the somewhat more abstract (and correspondingly much more general) mathematical arguments presented within my previous quantum mechanics paper. The Big Result: Diffraction and Interference Patterns from Simple Multiway Systems In conventional optics, diffraction and interference patterns are governed (at least in the case of near-field Fresnel diffraction phenomena, which is the case that we consider in this bulletin) by the transcendental Fresnel integrals, which can in turn be approximated by products of Chebyshev polynomials (of the second kind) with the Sinc function. As such, we can reproduce the spatial intensity plots for the standard single-slit, double-slit and triple-slit interference fringes in the following straightforward way: In my previous paper on the quantum mechanical foundations of the Wolfram model, I provided a rigorous mathematical formulation of the magnitudes of quantum amplitudes (in terms of path weights in multiway systems), and of quantum phases (in terms of ratios of branchlike- to spacelike-separated events in the multiway causal graph). Thus, an obvious question to ask would be whether we can successfully reproduce these conventional interference patterns using pure string-based multiway systems? Rather gratifyingly, it turns out that the answer is yes: diffraction and interference phenomena essentially “fall out” of my multiway formulation of quantum mechanics in an exceptionally natural way, as we shall now see. Consider the following elementary string multiway system: It is possible to introduce so-called “path weights” for each vertex in this system, such that every vertex is weighted by the number of distinct evolution paths that lead to it, as follows: From here, we can now “normalize” these path weights by imposing the constraint that all path weights for state vertices produced at a given step in the multiway evolution must sum to 1, like so: And then, within my formulation of quantum mechanics, in which each global multiway state corresponds to a distinct eigenstate of the universe (i.e. as part of some generalized Hartle–Hawking wavefunction), these normalized state weights correspond neatly to the magnitudes (squared) of the quantum amplitudes for the associated eigenstates. Now let us construct a toy example of a photon diffraction experiment using an elementary string multiway system; in a very loose sense, we could use the character “X” to denote the “presence” of a photon, and the character “o” to denote the “absence” of a photon, within a given region of space (although precisely how to construct a mathematically consistent position basis from these substrings is a somewhat subtle problem, as will be discussed later in detail in the second part of this bulletin). From here, we can represent a simple approximation to the Huygens–Fresnel principle of wave propagation using the sorting rules “Xo” → “oX” and “oX” → “Xo”, corresponding to a photon scattering right and scattering left, respectively. If we use the initial condition “ooooooooXoooooooo” to denote a single photon entering through a single slit, we therefore obtain the following multiway evolution: We can see that there are exactly nine distinct states present at the final evolution step which means that we can extract the last nine vertex weights (which, due to the automatic string sorting performed by the MultiwaySystem function, have been naturally laid out from left to right in the appropriate “position basis”): Thanks to the symmetry of the string sort, we can actually just plot the last half (5) of the vertex weights, so as to show the intensity pattern on the right-hand side of the fictional experimental “screen” and then prepend a swapped version of the last half in order to reconstruct the pattern on the left-hand side: So is this a single-slit diffraction pattern? It’s a little hard to tell with so few states, but it certainly seems suggestive of one. Let’s try the same procedure again with a larger initial condition (“ooooooooooooooooXoooooooooooooooo”) and more evolution steps, leading to the following (somewhat larger) multiway evolution graph now containing 17 distinct states at its final evolution step: Once again, extracting the last half of the vertex weights thus allows us to reconstruct the complete spatial intensity pattern on both sides as: Compare now against the expected single-slit photon diffraction pattern, as predicted by conventional quantum mechanics: Our multiway approximation does indeed seem to be converging to the known analytical result! However, single-slit diffraction is a somewhat trivial phenomenon, since it lacks many of the destructive interference effects that make multi-slit diffraction phenomena so much more complex and interesting. In fact, its triviality can be witnessed explicitly by observing that its branchial graph (and, therefore, the associated projective Hilbert space that describes this particular quantum system) is strictly one-dimensional, reflecting the presence of only a single identifiable quantum subsystem: So what happens if we try a two-slit diffraction case? We can do this very easily by just adding a second “X” (corresponding to a second photon propagating through a second slit) to our previous initial condition, with a small separation between the first and second slits, e.g. “oooooooooXoooXooooooooo”, thus yielding the following slightly more complicated evolution graph: There now exist 35 distinct states at the final evolution step: So we extract the last half of the state weights and reconstruct the spatial intensity pattern in the usual way. Okay, now something pretty interesting is starting to happen! We see one large central maximum (as one might expect, given the previous case), but now the maximum has two immediate minima on either side of it, followed by smaller fringe maxima on either side of those etc. Let’s repeat this analysis by running the multiway system for a few more evolution steps, to see what this new pattern might be converging to: Now there are 75 distinct states at the final step: so by extracting the last half of the state weights: we can reconstruct the following higher-resolution spatial intensity pattern: Comparing once more against the expected double-slit photon diffraction pattern from conventional quantum mechanics we see astonishingly good convergence to the analytical result, with correct prediction of the relative positions of the minima and the fringe maxima, as well as of the relative intensities of the interference fringes (compared both to the central maximum and to each other) etc. Since there are two interacting quantum subsystems in this second case (one associated with each slit), the branchial graph, and hence the associated projective Hilbert space, is now two-dimensional where the “rounding off” at the corners of the branchial graph is a simple boundary effect associated with the finite nature of the string. Finally, just to confirm the robustness of these correspondences, let us consider the three-slit diffraction case, with an example initial state “oooooooXXXooooooo” yielding 162 distinct states at the final evolution step with corresponding weights for the right-hand side of the “screen”: This results in the final spatial intensity pattern: which again compares exceptionally favorably (particularly given the relatively small string size and low number of evolution steps) to the triple-slit diffraction pattern predicted from the usual Fresnel integral approximation with the associated branchial graph now corresponding, unsurprisingly, to a fully three-dimensional projective Hilbert space: Therefore it really does seem that, with these very simple string multiway system models of quantum mechanics, we’ve been able to capture both the key qualitative and quantitative aspects of the double-slit experiment, and of quantum interference phenomena more generally, exactly as my formulation originally predicted. But how does any of this actually work? What’s really going on underneath? Ultimately, it’s just a simple theorem of combinatorics (with a smattering of elementary number theory), but understanding intuitively why this theorem holds requires first delving into some of the gory details of position bases, completion procedures, induced causal invariance and the relationship between branchial geometry and quantum phase…. How It All Works: Completion Procedures, Phase Changes and the Combinatorics of Destructive Interference First of all, let me attempt to demonstrate explicitly how the construction of the string position basis works in the particular case of the double-slit setup I described in the last section. I should stress that the scheme I describe here for encoding the position basis in terms of substring positions is only one possible such method, and doubtless it can be improved in many ways. The essential idea here is to encode which “region” contains the photon using the sum of the “X” substring positions, and then to encode precisely where the photon is “within” that region using the difference of those same positions. For instance, consider the simple two-slit case described previously, defined by an initial condition “ooooooooXXoooooooo” with 28 distinct states at the final step: and consider, in particular, the right-hand side of our fictional “experimental apparatus”, characterized by the following 14 states (shown here in the standard order, as sorted internally by "StateWeights"): In our fictional experimental setup, there exist three distinct spatial regions: a central separator region, a region corresponding to the right-hand slit and a further right-hand separator region. Which of the three spatial regions the photon is contained within is then encoded by the sum of the string positions of the individual “X”s, i.e. where, in this particular case, the sum corresponds to either 21, 23 or 25 (ignoring the 13 and the 19 at the beginning of the following list, which correspond to boundary cutoff effects resulting from the finiteness of the string): The position of the photon within that region is then encoded by the difference in the positions of the two “X”s, which here correspond to either 1, 3, 5 or 7: Thanks to this elegant encoding of the position basis, it is therefore guaranteed that, by looking at the last half of the sequence of weights (as previously indicated), these weights will necessarily correspond to the intensities of incident photons at monotonically increasing positions on the fictional experimental “screen”, starting from the center and ending at the far right, exactly as required. Okay, so that takes care of the mapping between the positions of microstates in branchial space and the corresponding spatial positions of the photons on the “screen”, but how do the crucial concepts of quantum phase and interference make themselves manifest within this particular model of double-slit diffraction? To understand this, we begin by noticing that our simulated “Huygens–Fresnel” rules are trivially causal invariant: And, as detailed in my quantum mechanics paper, all causal invariant multiway systems may be considered to have been “derived” by an appropriate application of a Knuth–Bendix completion procedure from a previously “uncompleted” non-causal invariant system, by a process that is directly analogous to wavefunction collapse in the standard Copenhagen interpretation of quantum mechanics. For instance, starting from the following simple multiway system consisting of two independent evolution branches (which we can think of as corresponding to two independent paths of history in which the photon either goes through one slit or goes through the other, in the case of a minimal two-slit diffraction experiment) we can see that this also constitutes a minimal model for a non-causal invariant multiway system (in which there exists a single branch pair that is guaranteed never to reconverge): However, we can force these two non-intersecting branches to interact (and hence to collapse down to a single effective branch of history) by performing a Knuth–Bendix completion in the standard fashion thus obtaining the new evolution history: which is now trivially causal invariant: So how does a completion procedure achieve the destructive interference effects that are so crucial for reproducing the results of the double-slit experiment just shown? As I described in my quantum mechanics paper, the basic idea is to think of the phase difference between two paths in the multiway system as corresponding to the ratio of branchlike- to spacelike-separated events along those paths (or, strictly speaking, to twice the ArcTan of that ratio). Thus, if you have two paths along which every pair of events is purely spacelike separated (corresponding to a pair of isomorphic paths in the multiway system), then the phase difference between the two paths is exactly zero. This implies that the two paths merge and the corresponding state weights add, and so one ultimately achieves perfect constructive interference. If you have two paths along which every pair of events is purely branchlike separated (corresponding to a pair of non-intersecting paths in the multiway system, as seen in the first example), then the phase difference between the two paths is exactly π, meaning that they perfectly destructively interfere. But how does that destructive interference actually occur? Well, it’s entirely a consequence of the combinatorial structure of the completion procedure. Consider the element “X” within the pair of strings “Xo” and “oX”; in this particular case, the two “X”s are purely spacelike separated across the two states (since they occur in non-conflicting parts of the string), which means that the pair of states can be successfully completed to yield the common state “XX”, and no information is lost (i.e. one has perfect constructive interference). Consider now the element “X” within a pair of strings “Xo” and “XO”; in this new case, the two “X”s are purely branchlike separated across the two states (since they now occur in conflicting parts of the string, i.e. both at position 1), which means that the only consistent completion procedure for the two states will necessarily destroy any information about whether there was an “o” or an “O” at position 2. In other words, any completion procedure will necessarily yield a common state of the form "X"<>["O","o"], where the {...} here denotes an equivalence class of substrings, indicating that the information about whether the previous state contained an “o” or an “O” in at position 2 is now lost (i.e. one has perfect destructive interference). To see the connection among phase difference, destructive interference and completion procedures more explicitly, it is helpful to consider a minimal illustrative example of a quantum harmonic oscillator, simulated within a simple string multiway system, using a scheme proposed as part of Patrick Geraghty’s Summer School project. Using the pair of rules “Xo” → “oX” and “oY” → “Yo”, we can use the character “X” to simulate a particle traveling from left to right and the character “Y” to simulate the same particle traveling back from right to left. Then we can use the character “O” to denote the boundaries of some finite-sized region of configuration space, with the pair of rules “XO” → “YO” and “OY” → “OX” thus effectively implementing reflective boundary conditions for the oscillating particle. As a consequence, we can change the period of the oscillator by simply modifying the length of the initial string, as shown here (for the case of oscillators corresponding to periods of 4, 6, 8 and 10, respectively) with the associated evolution causal graphs: For instance, here is a multiway system consisting of two independent branches—one containing a period-4 oscillator and the other containing a period-6 oscillator: However, though it is no doubt useful for pedagogical purposes, in practice we don’t actually need all of the “internal structure” of a simulated particle bouncing between two simulated walls; we can just generate the exact same form of a cyclic multiway system using purely single-character rewrite cycles as follows: Consider first the simplest nontrivial case of a multiway system containing a pair of period-2 oscillators (the states graph, illustrating the actual cycle structure, is shown on the left, while the unmerged evolution graph is shown on the right): However, since each cycle ultimately corresponds to a single eigenstate of the associated quantum system, we can collapse each cycle down to occupy only a single state vertex in the multiway system (essentially by replacing a state with spatial periodicity 1 and temporal periodicity 2 by an equivalent state with spatial periodicity 2 and temporal periodicity 1), as follows: Since the periods are equal between the two branches, the two oscillators are exactly in phase and clearly, since the oscillators are non-interacting, the system is non-causal invariant: It is intuitively clear that a minimal completion (acting on single characters) would send “A” → “C” and “C” → “A”, as well as “B” → “D” and “D” → “B”. To see programmatically why this must be so, consider the set of canonical branch pairs for this particular system and note that the greatest common divisor of their lengths is 2 which means that we can simply construct the following direct mapping between the characters of the two strings exactly as expected. By introducing these new completion rules, we have effectively forced the two period-2 oscillators to interact, thus obtaining the following causal invariant multiway system: The constructive interference of the two branches is witnessed from the fact that the state weights here remain perfectly stable post-completion: Consider now a multiway system consisting of a period-2 oscillator and a period-3 oscillator, i.e. and collapse the eigenstates down to occupy single multiway states in the usual way: Since the periods of the two oscillators differ by a factor of 1/2, we can see that the two multiway branches are therefore exactly π radians out of phase and, since it has not been measured, the system is not yet causal invariant: Due to the mismatch between the periods of the two branches, it is no longer immediately clear what the minimal (single-character) completion procedure should be. Following the same basic algorithm we employed previously, begin by considering the set of canonical branch pairs and observe this time around that the greatest common divisor of their lengths is 1 (i.e. the lengths are coprime) so if we attempt to construct the same naive mapping between the characters of the two strings as before then we obtain only a partial set of completion rules since the substring “E” is still unaccounted for. Therefore, we must also incorporate an additional pair of rules allowing one to map from the end of the “AB” string (i.e. the empty string) to the end of the “CDE” string (i.e. the “E” substring), which we can do using elementary modular arithmetic. More precisely, the final 3 (i.e. the string length of “CDE”) modulo 2 (i.e. the string length of “AB”) characters of the “CDE” string must be mapped onto the empty string that terminates “AB”, and vice versa, i.e. to obtain the overall set of completion rules: thus yielding a causal invariant multiway system in the expected manner, by effectively forcing the two out-of-phase oscillators to interact: However, note that the state weights (post-completion), unlike in the earlier constructive case, are now unstable and that, in particular, the weights associated with states “AB” and “CDE” appear to be converging to zero, corresponding to the phenomenon of perfect destructive interference: We can validate this convergence empirically by simply running the multiway system for a couple more evolution steps: As a final illustrative step, let us consider a multiway system containing a period-2 oscillator and a period-4 oscillator, as shown here which, with the eigenstates collapsed down to occupy single multiway states, now looks like this: The periods of the two oscillators thus differ by exactly 2*π, so the two branches are now back in phase with each other and, as usual, the unmeasured system fails to be causal invariant: Thanks to the exact divisibility of the period of one oscillator by the period of the other, it is clear (by analogy with the earlier case of the two branches with equal periods) that a minimal single-character completion for this case would send “A” → “CD”, “B” → “EF”, in addition to “CD” → “A” and “EF” → “B”, which we can also confirm programmatically by simply computing the list of branch pairs: Since the greatest common divisor of their lengths is, again, 2 we are able to construct the following direct mapping between the characters so as to obtain: as we had anticipated, such that the (now causal invariant) multiway system for the pair of interacting oscillators, post-completion, takes the form: The new state weights remain stable, just as in the aforementioned constructive interference case such that, in particular, the weights associated with states “AB” and “CDEF” now converge to finite (nonzero) values, unlike in the destructive example previously shown: A more rigorous version of this argument using elementary combinatorial number theory is currently under preparation for submission as a short supplementary mathematical note to this bulletin.
Local Multiway Systems: A New Approach to Wolfram Model Evolution 30.1 Introduction : This note introduces local multiway systems and examines them in the context of the singleway and global multiway systems. By default, WolframModel computes only a single branch of the evolution. If there are multiple matches of the rules to the hypergraph, only one of these matches will be turned into an actualized event, and the other matches will be ignored. They will not appear in the evolution object. This, however, introduces a dependence on the evaluation order, which might not be desirable if one’s goal is to eliminate arbitrariness from the system. There are multiple possible resolutions to this problem. One is to only consider causal invariant rules, i.e. the rules with a property such that the result of the evolution does not depend on the event order. This is, however, quite limiting, as we will be ignoring the majority of the rules. Also, the idea of having multiple possible evolution paths is, in itself, interesting to investigate. Another approach is to consider the so-called multiway systems, which evaluate all possible ways to resolve such overlaps between matches. This is the approach that is discussed in this note. The original type of the multiway system that was first considered in the Wolfram Physics Project is what we will call the global multiway system, which will be discussed in more detail in the next section. Here we propose a new kind of multiway system, called a local multiway system. The prime difference is that it allows one to consider the branching of only parts of space (subgraphs of the state hypergraph) instead of the entire hypergraph all at once. In the subsequent sections, we will discuss the local multiway system in more detail and introduce the types of relationships possible between expressions produced during its evolution.
Local Multiway Systems: A New Approach to Wolfram Model Evolution 30.2 Global Multiway System : The most important thing to understand about the global multiway system is that it operates on entire states. The most fundamental object is the states graph, which has states at the vertices and updating events at the edges. All possible updating events are determined for each state, and edges leading to new states are created. This process is then repeated for the new states. Let’s consider an instance of the global multiway system (implemented in the MultiwaySystem resource function). Specifically, let’s start with a rule that moves a “particle” (a unary expression) along directed edges in a graph: If we run this system on a path graph with three vertices, we get a very simple behavior: Now, what happens if we split the path in this graph into two different branches? In this case, the rules will lead to non-deterministic behavior—multiple choices of substitutions are possible—so the system explores all possible choices at once: Now the states graph itself splits into two branches, mirroring precisely the input graph. One important feature of MultiwaySystem is that if it encounters multiple states that are isomorphic, it merges them into one. For example, if instead of the input graph in the example above we use the graph where the branches have the same length, the system will combine them, and we will not see the splitting behavior: This is why our original graph uses branches of different lengths ({{2, 3}} and {{2, 4}, {4, 5}}). But what if we start with particles on different branches and let them merge? Note that even at the first step, the system branches in two different states. However, there is no ambiguity. The two events there occur at entirely different places in space. Note also that some events are duplicated. For example, the two events highlighted red are the same event, as they both correspond to the particle on a more extended branch moving one step. So, based on the above, there are two issues with the global multiway system. First, there is an incredible amount of redundancy. In particular, if there is an event that happened somewhere in a faraway galaxy, it would mean the entire universe, including all of the expressions here on Earth, duplicates, which seems both strange and unnecessary, and is fundamentally non-local. In other words, there is exponentially more data in the global multiway system than is necessary to describe the state of the universe completely. Second, it is hard to distinguish between space evolving in disconnected regions in parallel (i.e. spacelike events) and multiway branching due to overlapping event inputs (i.e. branchlike events). In the global multiway system, these look identical. The only way to distinguish them is to examine their input and output states, which is a highly nontrivial problem, especially given that both states are canonicalized. To illustrate the point, here is an example of a branchlike- and a spacelike-separated pair of events, respectively.
Local Multiway Systems: A New Approach to Wolfram Model Evolution 30.3 Local Multiway System : Let us then consider a different approach. Suppose we have a system with overlapping pairs of rule matches. We still want to include all of those events in the multiway system. But instead of duplicating the entire state, we will do that locally; i.e. we will have multiple branches growing from that pair of events, and we will weave them into a single data structure describing the entire multiway system. The states themselves will then be reconstructed afterward in post-processing. This approach sounds complicated at first, but it is more straightforward than it appears. To understand how it would work, let’s think first about how one would implement a singleway Wolfram model evolution. Data Structure In our implementation, the data structure is simply a set of expressions, and each expression has information about events that have created and destroyed it: The causal graph, in particular, is the simplest one to reconstruct. Indeed, if each event is a vertex, and each expression is an edge going from its #CreatorEvent to its #DestroyerEvent, that would immediately give us a causal graph. Reconstructing states is a bit more complicated, but it can be done by selecting a foliation of a causal graph, i.e. a subset of events including all of their dependencies, and selecting all expressions that were created but not destroyed by one of those events.
Local Multiway Systems: A New Approach to Wolfram Model Evolution 30.4 Index of Matches : In addition to this data structure, we need to have an index of matches. It’s a set of all possible matches that can be made to the current state, and it is updated after each event is applied. It is initially created at the construction of the system by indexing the initial condition. Each event’s outputs are then indexed after its instantiation (potentially by matching those outputs with their neighbors). The matches that involve the input expressions are deleted from the index, and the expressions themselves are not allowed to be used in further matching. This deletion causes the system to be singleway, as once an expression is used in an event, it can never be matched again. It is also the reason there is only one destroyer event for each expression.
Local Multiway Systems: A New Approach to Wolfram Model Evolution 30.5 Match-All Local Multiway System : However, imagine that instead of deleting all matches involving the input expressions, we will only remove the instantiated match. With only that change, we will evolve the system precisely the same way we used to. Note, in this case, we will automatically get a multiway system—in fact, the match-all version of it. It is called match-all because it will match not only the spacelike sets of expressions but also branchlike and even timelike ones. Evolution To understand what it means, let’s try some examples. Even the most trivial rules become too complicated quite quickly in the match-all system, so let’s use the pattern rules with globally named atoms here. We will be using the "ExpressionsEventsGraph" property of the WolframModel, which will allow us to see both expressions and events on the same graph. Let’s then take a look at the following system: In this case we have two rules, {{1, 2}} → {{2, 3}} and {{1, 2}, {2, 3}} → {{1, 2, 3}}. Note that here 1, 2 and 3 are not patterns but labeled vertices. We have started with an initial condition, which is a single expression {1, 2}. We have obtained two events. The first event replaced {1, 2} with {2, 3}. That is entirely normal and would happen in a singleway system as well. However, the singleway system would terminate immediately after that, as there is only a single expression {2, 3} left now, {1, 2} has been deleted and the second rule requires both {1, 2} and {2, 3} as inputs. In other words, {1, 2} and {2, 3} are timelike expressions, and our singleway WolframModel only matches spacelike expressions. However, the match-all multiway system will proceed, as both {1, 2} and {2, 3} are now in the system, and it does not care that they are timelike. Hence, the second event is instantiated as well, {{1, 2}, {2, 3}} → {{1, 2, 3}}. Note that at the end of this evolution, all three expressions {1, 2}, {2, 3} and {1, 2, 3} are open for further matching, and the only reason further matching does not occur is because both possible exact matches have already been instantiated. If, however, we add another rule, {{1, 2}, {1, 2, 3}} → {{2, 3}}, the system will keep evolving indefinitely, as {2, 3} created after applying the new rule is not the same {2, 3} as was created by the first rule. Therefore, it will be matched again by the second rule. After that, the second and third rules will keep “oscillating”, supplying inputs to one another: The match-all system will match branchlike events as well, as can be seen in the following example: Further note the obvious feature of the match-all system: it produces expressions and events that would occur in neither the singleway WolframModel nor the global MultiwaySystem. As such, it is a form of “interference” between branches and might allow branches to merge and interact.
Local Multiway Systems: A New Approach to Wolfram Model Evolution 30.6 Reconstruction : Reconstructing causal graphs in this system is straightforward as well. In fact, all one needs to do is to allow multiple destroyer events: For example, the pair of systems demonstrated above where the separation was hard to distinguish in the global multiway system now looks like this: Note that the vertex {2, 3} in the first example has an out-degree of 2, which indicates the multiway branching. Also note that the second example’s events are entirely disconnected, as there are no causal connections between them. In addition, there are only two events instead of four, as the local multiway system does not duplicate identical events. The reconstruction of spatial states is more complicated. However, it is still relatively straightforward to understand how a local patch of space would look. Indeed, what is space? Ultimately, it’s a collection of expressions that are all pairwise spacelike separated with one another. And indeed, we can create such patches in a match-all system just like in any other system. However, we need to discuss in more detail what it means for the two events or expressions to be spacelike, branchlike or timelike.
Local Multiway Systems: A New Approach to Wolfram Model Evolution 30.7 Expressions Separation : Suppose we want to have more control over the local multiway system’s matching behavior and reproduce the global multiway system’s evolution. In that case, we need a way to detect whether a pair of expressions is: Spacelike — the expressions were produced (directly or indirectly) by a single event Branchlike — the expressions were made (directly or indirectly) by multiple events that matched the same expression Timelike — the expressions are causally related: one produced or was produced by another And in simple systems, it is straightforward to understand what the separation is, i.e. in the following three systems, the expressions {2, 3} and {3, 4} are spacelike, branchlike and timelike, respectively: In the first example, a single event produces two expressions, {2, 3} and {3, 4}. This corresponds to a spacelike separation, as both of these expressions can appear simultaneously in a singleway system. In the second example, there are two possibilities: the first or the second rule can be matched to the initial expression {1, 2}. Hence, the expressions produced from that, {2, 3} and {3, 4}, are branchlike separated. Finally, in the third example, evolution is linear. The first event and the expression {2, 3} are the prerequisites for the second event and the expression {3, 4}. Hence, these expressions are timelike separated. So, if the expressions are immediate neighbors in the causal graph, as in the examples above, the separation is not hard to determine. However, what if the expressions are further back in history? For example, what about the expressions {4, 5} and {5, 6} here? In this example, the expression {1, 2} first branches into two expressions {2, 3} and {3, 4}. They are then merged by the third event, which creates two expressions, {4, 5} and {5, 6}. In that case, the final expressions {4, 5} and {5, 6} would still be spacelike separated, as they were produced from a single event after the branches have already merged. But what about something like this? What is the separation between the expressions {v, f, 1} and {v, f, 2}? On one hand, they are spacelike separated, because one of their common ancestors is the event {{v, 1}} → {{v, 1, 1}, {v, 1, 2}}. But on the other hand, they are branchlike separated, as they have an expression {v, 2} as another common ancestor. In other words, these expressions are mixed spacelike and branchlike separated. And if we had an event matching both {v, f, 1} and {v, f, 2} as inputs, it would simultaneously merge different pieces of space and two different branches. It seems, however, that the branchial separation should take precedence. That is because if we only allow events to match spacelike-separated expressions, we expect the evolution to be equivalent to the global multiway system evolution. In this case, we have to define the separation above as branchlike, as the two final expressions can never simultaneously appear in the same singleway system. Thus, to determine the separation between two expressions, A and B, in an expressions-events graph: Compute the past causal cones of both A and B. Compute the intersection between the causal cones. Take all vertices with out-degree zero (the future boundary of the intersection):   — If the boundary contains either A or B, they are timelike separated.   — If any vertices on the boundary are expression-vertices, they are branchlike separated.   — Otherwise, all vertices on the boundary are event-vertices, and they are spacelike separated.
Local Multiway Systems: A New Approach to Wolfram Model Evolution 30.8 Spacelike-Only Local Multiway System : Now that we understand how to precisely define separations between expressions, we can make a spacelike-only multiway system, i.e. a system that would only match pairwise spacelike groups of expressions, similar to what the global MultiwaySystem does. Indeed, all we need for that case is a verification that an event we are about to instantiate does not merge branches. The only modification we need to do compared to the match-all system is adding matches to the index only if their inputs are pairwise spacelike separated. The expressions we will get then will be exactly the same as in the global MultiwaySystem. It is interesting now to consider the same example of two particles starting on two different branches that caused lots of redundancy in the global MultiwaySystem. As a reminder, the system evaluates the propagation of two “particles” along a directed graph: Instead of a mesh of redundant events that the global multiway system produced, we now only have two places where the events merge: It is perhaps easier to see in a different layout: If we look closely at the events near the merge points, we can see that some redundancy remains. However, it is no longer due to the spacelike-separated events, but rather due to the “background” expressions being rewritten during evolution. And as a result, if a particle follows another particle on the same “track”, the expressions it’s going through are different (even though they involve the same vertices), hence the duplication. Note that all branches are entirely independent in this kind of evolution, and they simply reproduce all possible evolution orders in singleway systems.
Local Multiway Systems: A New Approach to Wolfram Model Evolution 30.9 Future Research : Some further improvements and features can be implemented and investigated. Another direction to consider is the deduplication of expressions. The global MultiwaySystem canonicalizes and deduplicates every state it encounters; thus, for example, the final state {1, 2, 3, 4} here only appears once: The current implementation of the local multiway system does not do that, however: It would be interesting to introduce isomorphism testing to the local multiway system, as it will allow for a much better understanding of how branches combine at the local level. This isomorphism testing can be done by starting with a subset of spacelike-separated expressions. We can then consider a pair of singleway evolutions starting from these expressions (events in them will be subsets of the entire multiway evolution). Let’s then consider the final states of these spacelike evolutions. Suppose they are identical up to the renaming of new atoms. In that case, we can deduplicate the resulting expressions so that the two branches corresponding to singleway evolutions end at the same set of expressions, even if the intermediate steps are different. In the example above, we can consider two evolutions starting from {{1}} and ending at {{1, 4}, {1, 5}} and {{1, 2}, {1, 3}}, respectively. Note that if we rename {4 → 2, 5 → 3}, the final states become identical. Hence, we can deduplicate them, merging future evolutions (since they now start from the same set of expressions). We then get: However, looking at even this simple example, we can determine that the algorithm described above is not quite right; e.g. consider the last two expressions, {1, 2, 3, 4} and {1, 3, 2, 4}. On the one hand, if we start all the way from {{1}} and consider two possible evolutions ending with {{1, 2, 3, 4}} and {{1, 3, 2, 4}}, they should be deduplicated, as they are the same up to the renaming 2 ↔ 3. However, if one was to deduplicate them, one of the events corresponding to rule 4 will be incorrectly instantiated, as it will match the inputs in the order {{1, 3}, {1, 2}} but will name the outputs as if they were matched in the order {{1, 2}, {1, 3}}.
Confluence and Causal Invariance 31.1 Introduction : There are claims made in the Wolfram Physics Project about the equivalence of confluence and causal invariance. This bulletin demonstrates that some of these claims are not correct. For example, consider the glossary on the project’s website, which as of October 30, 2020, says: Causal Invariance: A property of multiway graphs whereby all possible paths yield the isomorphic causal graphs. When causal invariance exists, every branch in the multiway system must eventually merge. Causal invariance is a core property associated with relativistic invariance, quantum objectivity, etc. In the theory of term rewriting, a closely related property is confluence. In a terminating system, causal invariance implies that whatever path is taken, the “answer” will always be the same. Confluence: A simplified form of causal invariance considered in term rewriting systems such as ones that reach fixed points. However, the glossary does not explicitly define confluence, so we are going to use the standard definition from the theory of rewriting systems: A state a is deemed confluent if, for all pairs of states b, c that can be reached from a, there exists a state d that can be reached from both b and c. If every state in the system is confluent, the system itself is confluent. We can summarize the statements above with the following definitions: A Wolfram model evolution is called causal invariant if and only if the causal graphs for singleway evolutions with any possible event ordering functions are isomorphic. Note that the definition above is only meaningful for terminating systems (i.e. the systems that always reach a "FixedPoint", a state during the evolution where no more matches can be made to its expressions). We can then define confluence as: A Wolfram model evolution is called confluent if and only if any pair of partial singleway evolutions starting from a particular state can be continued in such a way as to reach isomorphic final states. In what follows, we will demonstrate that causal invariance is not equivalent to confluence, neither of them implies the other, and the two statements made above are false: When causal invariance exists, every branch in the multiway system must eventually merge. In a terminating system, causal invariance implies that whatever path is taken, the “answer” will always be the same. We will not make any comments in this note about the physics claims made above. Before we get to specific examples, it’s essential to understand the fundamental difference between these two properties. Causal invariance has to do with symmetries between evolution branches of expressions-events graphs. It requires that, even though the branches operate on different expressions, they have the same causal structure: On the other hand, confluence has to do with the symmetries between expressions’ contents. It requires that particular states from different branches are isomorphic as hypergraphs, regardless of the causal structures that lead to them: Consider the following system: This system is confluent, as any partial evolution, if continued, will always terminate at the final state isomorphic to {{1, 2}, {2, 3}, {1, 3}, {3}}: However, this system is not causal invariant. We can generate two non-isomorphic causal graphs by using different event ordering functions, which contradicts the definition above: Therefore, confluence does not imply causal invariance. Note that the "CausalInvariantQ" property of MultiwaySystem checks for confluence despite its name: Consider the following causal invariant system: To see that it’s causal invariant, note that the multiway system in this case only has two events: These two events correspond to two different singleway evolutions terminating at states {{1}, {1}} and {{1}, {2}}, respectively: These evolutions yield isomorphic causal graphs, which are composed of a single vertex with no edges, implying that this system is causal invariant by definition: It is not, however, confluent, because the final states in these two evolutions are not isomorphic, and the evolutions terminate after these states are reached: Therefore, causal invariance does not imply confluence. Note that the "CausalInvariantQ" property of MultiwaySystem returns False in this case.
Confluence and Causal Invariance 31.2 Future Research : It would be interesting to investigate the systems exhibiting these properties in different combinations. For example, systems that don’t exhibit multiway branching at all do satisfy both of these conditions. One can also consider systems in which causal-graphs isomorphism is equivalent to states isomorphism by construction—for example, classical sequential growth models of causal sets. It might be possible to generalize this to include more classes of systems. We could also enumerate (#57) simple rules and determine how many of them exhibit just one of these properties, both or neither. We will also need to generalize the definition of causal invariance to non-terminating Wolfram models (#487). It will be interesting to investigate other similar properties. For example, one can investigate a stronger version of confluence: Consider any infinite singleway evolution of a system. Consider another finite partial singleway evolution. If any such finite evolution can be continued in such a way as to reach one of the states from the infinite evolution, we define the system as “super”-confluent (#478).
Multicomputational Irreducibility 32.1 : Multicomputation is the cornerstone of much of the basic science that my teammates and I are doing with Stephen Wolfram. We see an opportunity to metamodel many areas of applied science using the multicomputational paradigm. In fact, the range of opportunities that we envisage is so wide that we are launching the Wolfram Institute in order to expand the effort beyond Wolfram Research. It’s a momentous period. But because multicomputation is still new, I feel a responsibility to help communicate in greater detail the aspects of multicomputation that I personally find to be compelling. And I have great reference material for doing so, because introducing a new paradigm of science is precisely what Stephen began to do in the 1980s with the computational paradigm. And I still refer to those works from the 1980s today because they cover then-novel concepts that have come to serve as guiding principles for the work that we do now. And a key concept, which distinguishes those papers from other theoretical literature on the study of computability, is that of computational irreducibility. So, now that we are developing a new paradigm that builds upon the one that Stephen pioneered decades ago, it seems appropriate to consider irreducibility in the multicomputational context.
Multicomputational Irreducibility 32.2 A Review of Computational Irreducibility : Computational irreducibility is the property of computational processes whose behaviors resist prediction. Although popularized through A New Kind of Science, the property was previously subject to considerable study in the 1980s, with the results published in several papers. Such results were surprising because they contradicted algorithmic information theory (AIT), which was in turn influenced by earlier works in information theory and computability theory. The key presumption of AIT, which Stephen’s findings contradicted, was that one could assign a value to the complexity of an algorithm by measuring its description length, where algorithms with shorter rules were presumed to produce outputs with greater regularity and less randomness. The finding that computations with simple rules could be autoplectic—that is, increasing in complexity without external influence—was surprising, and its implications extended beyond the theory of computation to the general study of physical systems. It cast doubt on a paradigmatic principle of modern science, namely that one, in the mathematical tradition, could design parsimonious models with few parameters that make the world tractable enough for humans to gain predictive power over it.
Multicomputational Irreducibility 32.3 Multicomputational Irreducibility : In what follows, we too will study the behavior of simple computational rules. However, our focus will not be placed on individual rules; rather, we will examine multicomputations that involve either multiple rules that are computed together or rules that admit multiple pattern matchings. In either case, multicomputations differ from their single-way predecessors in that they have parallel evaluation fronts. Let’s consider the case of a multicomputation that admits two different Turing machine (TM) rules: Let’s first run each TM rule separately for 25 steps: Looking at the individual TM plots, we see that the behavior shown in either case is predictable. Next, let’s contrast the behavior of the individual rules with the behavior of a multiway Turing machine (MTM) that accepts both rules, which we’ll run for 10 steps: Intriguingly, the multicomputational behavior is more difficult to anticipate. We can predict how each rule behaves individually, but we cannot as readily predict the “interactions” between those rules. Thus, it appears as though multicomputations too can be either predictable or unpredictable, depending on the convergence and divergence of their parallel evaluation fronts. We will refer to the property of unpredictability for multicomputations as multicomputational irreducibility. Such a property might seem obvious, but it opens upon a trove of new questions about the behavioral study of computational rules (or, as we like to call it, ruliology) and its application to real-world systems (metamodeling).
Multicomputational Irreducibility 32.4 A Pure n-Machine Definition : Let us try to produce a slightly more formal definition of multicomputational irreducibility. In order to do so, we must think about an idealized machine, one that differs from those that we might usually encounter. The machines that we typically find in theoretical computer science are 1-machines. Graph-theoretically, we model their discrete computations as 0-spaces (vertices) and the overall program that they execute as a 1-space (a path). For a 1-machine, the initial condition is a vertex, as is the final state, and every state in between. A 2-machine, on the other hand, is a machine that computes over paths (1-spaces), with the overall computation forming a sheetlike 2-path (which, prima facie, is not unlike the terms of higher identity types constructed in univalent foundations). Together, one can use 2-machines, 1-machines and higher n-machines to determine the path deformations, boundaries and geni of multicomputations, as one does in generalized homology with CW-complexes, homotopies and functors. And one can think of the “laps” performed by such machines over multicomputational graphs as playing a role similar to that of homotopy groups. But a key point here—as shall be explored later—is that such concepts are relevant to the experimental study of multicomputational irreducibility. For instance, a maximally confluent multicomputation consists of highly interdeformable paths; such a multicomputation is also easily predictable and thus multicomputationally reducible. As the multicomputation runs, we know that the paths are, and will continue to be, interdeformable. On the other hand, with multicomputations for which 2-machine laps are non-Abelian (such that as the multicomputation proceeds, one cannot always “go back and forth” between paths), irreducibility is more likely. Because the paths are not all interdeformable, we do not know whether or not they will converge as the multicomputation continues to run. Another elementary example of reducible multicomputations is the class of fully ramified multicomputations that do not exhibit any confluence (and are thus entirely refractory to 2-machines); we can safely anticipate that confluence will never occur. As of now, 2-machines and higher n-machines are idealizations. No known computational technologies can implement 2-machine capabilities. Nonetheless, 2-machines are useful for positing theoretical definitions. In particular, multicomputation can be defined with reference to such machines: a computation with multiple evaluation fronts is multicomputationally irreducible if its corresponding 2-machine computation is undecidable. Put informally, if as a multicomputation proceeds it is unclear if and how one could compute “horizontally” (pathwise) from one evaluation front to another, then the multicomputation is itself irreducible. But, on the other hand, if one always knows that all paths are interdeformable, then the corresponding 2-machine computation is decidable (positively so), as is the case if we know that the multicomputation is fully ramified and never converges (negatively so). Alternatively, as Stephen once put it during a conversation, a multicomputation is irreducible if, in order to know what paths a multicomputation gives, one must simply compute all paths. However, we do have a way to study multicomputational irreducibility even without 2-machines: branchial space. By extracting from a multicomputation its branchial graph, we can examine the relations between paths, and are thus effectively performing a “2-machine branchial reduction,” or a conversion of an (infeasible) 2-machine computability problem into something that our Physics Project makes tractable. (And Jonathan Gorard’s branchial Turing machine offers a nice connection to the TM and MTM examples given previously.) Approximation of Multicomputational Irreducibility Using a Branchial Lyapunov Exponent Branchial reductions of multicomputations provide a practical way to approximate irreducibility. In order to show how approximations as such can be done, let’s begin with some simple examples. Consider the numerical multicomputation . As we shall see, it is multicomputationally reducible, as its paths are all interdeformable. Here is the multicomputation, which we run for six steps: And here are the branchial graphs generated at successive steps: Evidently, the branchial graphs do not evolve whatsoever, and neither do their corresponding distance matrices (given as array plots here): And we can predict (correctly) that such will continue to be the case, indefinitely, for all future steps. Graphs for which all paths are interdeformable are multicomputationally reducible, as discussed previously. Next, consider a multicomputation that is entirely ramified: The corresponding branchial graphs consist of disconnected graph segments, each containing two vertices: This behavior is also predictable. At step t, the number of segments in the branchial space is , where . A general remark: if one can readily provide an equation that predicts future branchial behavior, it is clear that one has found a case of multicomputational reducibility. Here’s another view of the same branchial evolution: an array plot of the graph distance matrices for each step. In this case, we see that the distance-1 branchial relation (i.e. the segment) is “propagated” consistently: One initial test that Stephen performed when studying randomness and autoplectic behavior among cellular automata is, among other tests, the study of their Lyapunov exponents. In the case of elementary cellular automata, one can measure Lyapunov exponents by calculating the slopes of a computation. In order to study multicomputational irreducibility, we can introduce a branchial analog of the Lyapunov exponent, denoted by λB. Lyapunov exponents measure the “drift” of initial conditions in dynamical systems. When we perform a branchial reduction of a 2-computation, the initial condition is the branchial graph at step 1, which in this case is a single branchial edge with two vertices (i.e. the segment). And, as we can see in this case, the initial distance does not “drift” as we iterate the ramified multicomputation given previously: the one-unit distance segment is simply “propagated” along the matrix diagonal. As a result, the array plot appears “linear” in that the same branchial distance is “passed” from one branchial pair to another. If, however, the branchial graph assumed a more elaborate, connected graphical configuration, then the branchial evolution would “drift” from the initial condition, with matrix entries appearing farther from the diagonal (as we’ll see shortly). We can also describe λB more formally. Consider branchial graph distance matrix entries for a distance matrix D = r · c. Now, consider only those entries for which the distance matrix value d is 1 (which constitute the entries of the adjacency matrix): Row and column positions of each entry are obtained via a position query : And yields the positions nearest to the diagonal We measure λBi by taking, for each i: In the case of our ramified multicomputation, for each node in the branchial graph, λB = 0: And in general, “λB plot flatness” is a visual heuristic for multicomputational reducibility. In the case of the ramified multicomputation, the result is not surprising. We can see that confluence will never happen; thus, the multicomputation is reducible. Next, let us consider multicomputations with nontrivial confluence, such as this one, which we run for seven steps: We see that the branchial distance matrices exhibit slightly more “complex” behavior and cannot be as easily predicted: Consider now the following multicomputation: Its branchial graphs are nontrivial as are their corresponding distance matrices: Notice here that the original condition, the branchial segment, is not simply “passed” to other segments. With time, as the branchial graph takes shape, there are many branchial vertices that are one branchial unit of distance away from others in a large connected component. Thus, the initial condition is effectively “diffused” throughout branchial space, rather than just being “propagated” at each step in segmentary form. Here are the λBi values for the branchial graph obtained after 10 steps: As one can see, this plot is far from being flat. One should keep in mind that our distance matrices can be permuted, and that the distance matrices that we obtain by default in the Wolfram Language correspond to a default assignment of vertex numberings. But it so happens that the matrices that we obtain by default make it particularly easy to study the predictability of branchial behavior. Nik Murzin has suggested that an ensemble-like λB measure could be obtained by considering all possible matrix permutations. Such an approach will be subject to further study.
Multicomputational Irreducibility 32.5 Foliation as Multicomputational Choice : Multicomputational research differs from computational research in a number of ways. One key difference between the two is that we enjoy the prerogative of choice when doing multicomputational research; there are different options available to us that aren’t available when we study single-way computations. When one runs a single computation—and an irreducible one in particular—one has little choice other than to run it and study its behavior. But in the multicomputational case, one is able to make choices in how one studies behavior. This is the case because, when one computes multiple evaluation fronts, one can “synchronize” or “coordinate” multicomputational states in different ways. And yes—we actually do have a choice in this matter. One might presume that, when we run multiple rules, it is simply the case that all states “from step 1” of each computation are synchronized, all states that result “from step 2” are synchronized and so on. But, as will be shown, one has a choice in the “simultaneity orientation” that dictates which vertices in the multicomputational graph “occur together” at each step. The researcher makes such choices by selecting a foliation. Originally, the foliation was proposed as a theoretical concept and computational technique for the Physics Project. But we have since come to understand that foliation choices can be made for all multicomputations. Curiously, with multicomputation, we are making the study of computational behavior even more “complex” than before by building systems with many rules (or rules that can be evaluated in different ways); nevertheless, by doing so, we reintroduce a prerogative of choice into our computational methodology that we do not have when we study rules individually. And we anticipate that choice as such affords tremendous metamodeling advantages. For instance, we think that multicomputation makes it possible to capture the aggregate behavior of systems, so long as one selects the right foliation. And we believe that observation and the general transduction of all systems can be metamodeled this way. But why is choice of foliation important? As we will see, λB approximations for the same multicomputation differ by foliation. For instance, let’s revisit the multicomputation , this time experimenting with possible foliations. Nine such foliations are given here (using Nik’s ever-helpful GraphFoliations function): Each foliation includes 8–9 foliation slices (which, in the original Physics Project metamodel, are hypersurfaces). For the first foliation shown, the final slice includes vertices 14, 16, 17 and 18, whereas the final slice in the last foliation contains only vertex 14. In order to better understand the difference between these foliations, we can plot the “cardinality” of each foliation slice (i.e. the number of vertices in each) for 500 foliations of the same multicomputation: We might notice that, even though we take 500 different foliations, we don’t see in the previous plot what appear to be 500 distinct “trajectories.” But we should be able to partially disaggregate the trajectories because there are many foliation slices that share a common cardinality value but include different states. As a proxy measure, we can summate the states in each foliation slice (because, if the numbers differ, then their sums often will too): Given here is a histogram showing the foliation slice cardinality variance and the kurtosis: It appears as though the most “probable” foliation choices do not minimize cardinality. Thus, foliations are not “all the same,” and if one wants slices that minimize volatility (i.e. variance) or surprisal (i.e. kurtosis), then one must select an “improbable” foliation, which requires careful choice. As we shall see, careful choice of foliation can mitigate multicomputational irreducibility by allowing one to “observe” the multicomputational evolution without “taking in too much at once.” Let’s consider two random foliations for the rule , run for seven steps: And here are array plots for the graph distance matrices for the respective branchial graphs (computed up to 11 steps): Let’s compare their respective λBi values: Overall, foliation 2 exhibits “tamer” branchial Lyapunov exponent values. And looking at the previous array plots, we see that the evolution of the multicomputation with foliation 2 is more “gradual” than that of foliation 1. An Interlude: On Non-Archimedean Reductions and Rulial Primes Understanding branchial reductions might require honing some unfamiliar modeling sensibilities. This is because branchial distance is non-Archimedean, unlike most graph distances. In a conventional graph, we straightforwardly measure the distance between vertices in terms of the edges that join them. Such a measure of graph distance obeys the Archimedean property in that if the distance d from vertex c to vertex a is greater than the distance d from vertex b to vertex a, it is still the case, for some n, that n×d (b, a) > d (c, a). Put differently, one can multiply n by the distance from b to a and obtain a distance greater than that from c to vertex a. Consider the distance function computed as a density plot: Here, d is Archimedean. But branchial distance measures something different. It measures the shared graph ancestry of vertices that may not be joined by edges in the original graph at all; thus, it is non-Archimedean. A common non-Archimedean system in mathematics is that of the p-adic numbers. The p-adic distance dp(x, y) between x and y, for some prime p, is p exponentiated by the reciprocal of the greatest power of p that divides the absolute value of the difference between x and y: Being non-Archimedean, p-adic distance can be difficult to visualize. And p-adic space itself is totally disconnected. But nonetheless, for expository purposes, we can construct a “Euclidean-interpolated” contour space of p-adic distances for –10 ≤ x ≤ 10, –10 ≤ y 10, p = 2, 3, 5, 7, 11, 13. Here, we plot the contours over different surfaces in order to provide a cursory glimpse of the four-dimensional contours: Branchial distance and p-adic distance share some similarities. When we measure branchial distance, we are effectively asking, “For any two vertices in a graph, how many steps backward, going from vertices to the prior vertices that feed into those vertices, must we take until we find a common vertex?” Similarly, p-adic distance concerns relationships between numbers in a way that corresponds to prime factorization. In neither case is one asking, “How far in some direction must one travel directly to reach there from here?” Rather, in both cases, one is asking, “How far removed are these two things from one another in terms of common feeders?” However, the affinities between branchial and p-adic distance become somewhat more concrete when the feeders themselves embody some quality of primality, and primality as such need not be limited to the prime numbers. For instance, consider the case of metamathematical space. One important finding that Stephen made in A New Kind of Science is that, when one lists all possible theorems that can be proved from the axioms of Boolean algebra, it so happens that those that are “named” and are subject to human study are precisely those theorems that “cannot be derived from preceding theorems.” In the following, I plot graphically the dependencies between 60 theorems of Boolean algebra (the same that Stephen considers in the section on empirical metamathematics in the metamathematics piece): Here, each of the named theorems is an attractor, a common ancestor without its own respective ancestor. I propose that ancestorless theorems in metamathematics are a particular case of rulial primes, objects in entailment fabrics (coarsened slices of the Ruliad that serve as reference frames for observers) that are not constructed from the application of a rule to another object. Of course, all objects are constructed from lower-level computations, with the ultimate primordia of the Ruliad being emes. But the point is that multicomputation gives rise to aggregate properties that can be captured by observers (or instruments that transduce values from systems), by merit of interactions between the different evaluation fronts in the multicomputation. And it is in the aggregate context that rulial primes, untethered from lower-level computational dynamics, arise. In the case of Boolean algebra, the theorems that share a rulial prime as a common “sink” are connected in branchial space, such that the branchial distance between each is a non-Archimedean distance taken with respect to a rulial prime (which is not too different, conceptually, from p-adic distance). Thus, a branchial 2-machine reduction might not simply be an expedient gadget; it might be important in its own right. It might help us to identify particular bulk objects that “stand out” in systems. ϱ-Varieties: A Multicomputational Response to Arithmetic and Algebraic Geometry Multicomputation is a paradigmatic successor to computation, with computation itself already being a successor to the mathematical paradigm. However, it is perfectly possible for multicomputation to “reach backward” two paradigms and metamodel mathematics, even in exotic ways. But multicomputation should also allow us to compute certain objects of mathematics that are not in the canon of the mathematical paradigm, precisely because they involve multicomputational irreducibility and thus cannot be easily studied without some experimental, generative procedure. For a numerical multicomputation, confluence occurs when different rules yield the same value; that is, their outputs agree. And these outputs are just values that satisfy multiple rules. But the idea that we can study values that satisfy multiple formal specifications is not new. In areas of mathematics such as algebraic geometry, we study common values that satisfy systems of polynomial equations (known as solutions or “roots”) as spaces, known as algebraic varieties. A well-known example of an algebraic variety is an elliptic curve, consisting of solutions to the equation y2 = x3 + a x + b. The following are examples of this particular algebraic variety with values for a and b toggled: And in arithmetic geometry, one seeks to answer questions such as the number of solutions admitted by a variety. Arithmetic geometry is sometimes described as the study of the “complexity” of varieties, though perhaps it doesn’t capture as much complexity as it could. In the case of multicomputation, however, we don’t study static equations. We study systems composed of rules, such as , which can be iterated for an indefinite number of steps. Were we to write such rules as equations, we would use recursive functions; in this case, , where for some initial condition . And, for many multicomputations, one can extract from the overall multicomputation the graph of its confluent values, which I will call a rulial variety (shortened as ϱ-variety). For something generative like a ϱ-variety, it is much more interesting to study the behavior with which confluent values appear than the total number of “solutions” (which, in arithmetic geometry, we would approximate using some height function). And this behavior can be multicomputationally irreducible. Consider the multicomputation . In the following, we begin with initial condition 2 and multicompute for two steps: And here, we multicompute for eight steps: There are vertices in these graphs with indegree three (maximal indegree); that is, there are values that satisfy all three rules. Thus we can, in turn, extract from this eight-step multicomputation a ϱ-variety, which is a “subgraph” composed of all vertices with maximal indegree. Indegree must be maximal, for vertices with less than maximal indegree are not numerical values that satisfy all rules. The ϱ-variety for the eight-step multicomputation is shown here: This ϱ-variety possesses notable properties. For instance, it appears to be the case that, if we compute the eight-step ϱ-variety for the rule with any two initial conditions, the resulting ϱ-varieties are isomorphic to one another. Here are the eight-step ϱ-varieties for initial conditions four and five: They look isomorphic. And indeed, we can prove such to be the case: And this suggests that, for the rule , one could take the ϱ-variety for all initial conditions and obtain a holochaotic moduli space composed of initial conditions in an isomorphism class (with “holochaotic” being a portmanteau of χάος and ὅλος to connote “possessing all initial states”). And, in principle, one can generate multicomputations that begin with different initial conditions, which makes it easy to study chaos. We can also continue to yield sub-ϱ-varieties, or subgraphs of ϱ-varieties that are in turn their own respective ϱ-varieties. And we do so by forming a subgraph of vertices that themselves have maximal indegree in the ϱ-variety itself. For the multicomputation , we can yield two further sub-ϱ-varieties, the last of which is a minimal sub-ϱ-variety: Finally, consider the original ϱ-variety and its sub-ϱ-varieties together: As one can see, the branchial graphs for the ϱ-variety and the first sub-ϱ-variety consist of paths that are all interdeformable (with the minimal sub-ϱ-variety possessing only one branchial path): Thus, it appears to be the case that the branchial 2-machine reductions are decidable. Now, let us once again examine the nettlesome multicomputation . Here are the multicomputations for initial conditions 2, 3 and 4: Next, we can compute their respective ϱ-varieties: These are clearly not isomorphic: And the distance matrices for their respective branchial graphs differ considerably (chaotically) according to choice of initial condition. Here are array plots for graph distance matrices for 15 different initial conditions (ranging from 3 to 45, increasing by increments of three): We can compare initial conditions by measuring the entropies of the graph distance matrices corresponding to their multicomputations. The lower the entropy, the more uniform the values of the matrix. Here, we plot the entropy over initial conditions 2 ≤ ≤ 101: Disregarding the cases of small-valued initial conditions, the plot seems to exhibit random walk–like behavior, suggesting that for the rule , one branchial graph for a given initial condition tells us little about the corresponding branchial graph for another. (Note that here we are studying initial conditions for the rule itself, rather than the initial branchial conditions, which we study when we measure λB.) There is much more to be explored with respect to ϱ-varieties. I introduce them here to reinforce the idea that the behavior of interacting computational rules (such as satisfaction of common values) can be studied behaviorally (i.e. ruliologically) rather than in limit cases (as arithmetic geometry does when estimating the number of solutions for systems of polynomial equations). And, what is more, there are many questions of multicomputational irreducibility (as well as chaotic and holochaotic behavior) that can be examined when studying ϱ-varieties and sub-ϱ-varieties. Some Next Steps: Metamodeling Physics For those interested in straightforward, initial projects on multicomputational irreducibility, the Wolfram Physics Project presents a few opportunities. It is suspected that multicomputational irreducibility is an important concept for metamodeling quantum interference. (In fact, it was during a conversation with Stephen on the topic that I first proposed the concept of multicomputational irreducibility.) There also exists a Registry of Notable Universes on the Physics Project website, in which the graph distance matrices for Wolfram models are already computed. Thus, it should not be too difficult to identify examples that, at least after a certain number of steps, exhibit irreducibility.
Multicomputational Irreducibility 32.6 Conclusion : Multicomputation is just beginning. And it appears as though there are countless open questions that we can consider. A careful reader might note that this bulletin introduces several new concepts and raises many open questions, with the findings provided being specific to selected case studies and lacking in generality. This piece is intended to serve as an invitation, with many ideas and provocations introduced with brevity. As the popularity of this paradigm grows, the open questions raised and case studies suggested in early multicomputational works can serve as guideposts for those who are interested and wish to find a way to make helpful contributions. The computational paradigm of basic science was motivated by the finding that many computations (which are the most fundamental models of discrete processes that follow rules) are unpredictable and autoplectic in their behavior. Multicomputation is an exciting new paradigm because, thanks to the prerogative of choice afforded by foliations, it appears as though what we had previously thought to be physical limits to the understandability of algorithmic behavior can in fact be “negotiated.” As we proceed from ruliologically studying individual processes to systems of interacting processes, we expect to develop a general theory and metamodeling practice with which we can extract bulk, aggregate properties from multicomputations. Such a goal might sound ambitious, but our quotidian experience as humans suggests overwhelmingly that, despite the dizzying behavior of low-level processes, we, as observers of the world, enjoy a phenomenology with nice “physical UX” and an “interface of macros.” Thus, the foliation can be thought of as the abstract equivalent of a user interface in the pure sciences. And UIs have been incredibly important in that they have made it possible for more people to technologically harness computational power. And in the case of multicomputation, foliations should allow us to scientifically interface between low-level and high-level computational behaviors (including high-level behaviors corresponding to things in the world that we care about), effectively fashioning a bridge from pure computation to the applied sciences. And we will be pursuing many projects in applied multicomputation at the Wolfram Institute for this very reason. Lastly, multicomputation provides a way to understand the construction and exploration of the Ruliad; it is constructed theoretically by running all possible computations at once, and we explore it by taking foliations over “slices” of the Ruliad and transporting such foliations across rulial space. More will be written about the exploration of the Ruliad—a paramount scientific imperative of our time, also to be pioneered by the Wolfram Institute—on another occasion. A Note of Appreciation I would like to thank Stephen Wolfram for taking an interest in my ideas on multicomputational irreducibility and providing helpful advice on how to communicate my ideas effectively to others; to Nik Murzin for his identification of errors and points in need of greater clarification; to Hatem Elshatlawy for his suggestions regarding descriptions of foundational concepts; and to Xerxes Arsiwalla for encouraging me to think about long-term directions in which I can take the study of multicomputational irreducibility.</.
Computational Foundations for the Second Law of Thermodynamics 33.1 The Mystery of the Second Law : Entropy increases. Mechanical work irreversibly turns into heat. The Second Law of thermodynamics is considered one of the great general principles of physical science. But 150 years after it was first introduced, there’s still something deeply mysterious about the Second Law. It almost seems like it’s going to be “provably true”. But one never quite gets there; it always seems to need something extra. Sometimes textbooks will gloss over everything; sometimes they’ll give some kind of “common-sense-but-outside-of-physics argument”. But the mystery of the Second Law has never gone away. Why does the Second Law work? And does it even in fact always work, or is it actually sometimes violated? What does it really depend on? What would be needed to “prove it”? For me personally the quest to understand the Second Law has been no less than a 50-year story. But back in the 1980s, as I began to explore the computational universe of simple programs, I discovered a fundamental phenomenon that was immediately reminiscent of the Second Law. And in the 1990s I started to map out just how this phenomenon might finally be able to demystify the Second Law. But it is only now—with ideas that have emerged from our Physics Project—that I think I can pull all the pieces together and finally be able to construct a proper framework to explain why—and to what extent—the Second Law is true. In its usual conception, the Second Law is a law of thermodynamics, concerned with the dynamics of heat. But it turns out that there’s a vast generalization of it that’s possible. And in fact my key realization is that the Second Law is ultimately just a manifestation of the very same core computational phenomenon that is at the heart of our Physics Project and indeed the whole conception of science that is emerging from our study of the ruliad and the multicomputational paradigm. It’s all a story of the interplay between underlying computational irreducibility and our nature as computationally bounded observers. Other observers—or even our own future technology—might see things differently. But at least for us now the ubiquity of computational irreducibility leads inexorably to the generation of behavior that we—with our computationally bounded nature—will read as “random”. We might start from something highly ordered (like gas molecules all in the corner of a box) but soon—at least as far as we’re concerned—it will typically seem to “randomize”, just as the Second Law implies. In the twentieth century there emerged three great physical theories: general relativity, quantum mechanics and statistical mechanics, with the Second Law being the defining phenomenon of statistical mechanics. But while there was a sense that statistical mechanics (and in particular the Second Law) should somehow be “formally derivable”, general relativity and quantum mechanics seemed quite different. But our Physics Project has changed that picture. And the remarkable thing is that it now seems as if all three of general relativity, quantum mechanics and statistical mechanics are actually derivable, and from the same ultimate foundation: the interplay between computational irreducibility and the computational boundedness of observers like us. The case of statistical mechanics and the Second Law is in some ways simpler than the other two because in statistical mechanics it’s realistic to separate the observer from the system they’re observing, while in general relativity and quantum mechanics it’s essential that the observer be an integral part of the system. It also helps that phenomena about things like molecules in statistical mechanics are much more familiar to us today than those about atoms of space or branches of multiway systems. And by studying the Second Law we’ll be able to develop intuition that we can use elsewhere, say in discussing “molecular” vs. “fluid” levels of description in my recent exploration of the physicalization of the foundations of metamathematics.
Computational Foundations for the Second Law of Thermodynamics 33.2 The Core Phenomenon of the Second Law : The earliest statements of the Second Law were things like: “Heat doesn’t flow from a colder body to a hotter one” or “You can’t systematically purely convert heat to mechanical work”. Later on there came the somewhat more abstract statement “Entropy tends to increase”. But in the end, all these statements boil down to the same idea: that somehow things always tend to get progressively “more random”. What may start in an orderly state will—according to the Second Law—inexorably “degrade” to a “randomized” state. But how general is this phenomenon? Does it just apply to heat and temperature and molecules and things? Or is it something that applies across a whole range of kinds of systems? The answer, I believe, is that underneath the Second Law there’s a very general phenomenon that’s extremely robust. And that has the potential to apply to pretty much any kind of system one can imagine. Here’s a longtime favorite example of mine: the rule 30 cellular automaton: Start from a simple “orderly” state, here containing just a single non-white cell. Then apply the rule over and over again. The pattern that emerges has some definite, visible structure. But many aspects of it “seem random”. Just as in the Second Law, even starting from something “orderly”, one ends up getting something “random”. But is it “really random”? It’s completely determined by the initial condition and rule, and you can always recompute it. But the subtle yet critical point is that if you’re just given the output, it can still “seem random” in the sense that no known methods operating purely on this output can find regularities in it. It’s reminiscent of the situation with something like the digits of π. There’s a fairly simple algorithm for generating these digits. Yet once generated, the digits on their own seem for practical purposes random. In studying physical systems there’s a long history of assuming that whenever randomness is seen, it somehow comes from outside the system. Maybe it’s the effect of “thermal noise” or “perturbations” acting on the system. Maybe it’s chaos-theory-style “excavation” of higher-order digits supplied through real-number initial conditions. But the surprising discovery I made in the 1980s by looking at things like rule 30 is that actually no such “external source” is needed: instead, it’s perfectly possible for randomness to be generated intrinsically within a system just through the process of applying definite underlying rules. How can one understand this? The key is to think in computational terms. And ultimately the source of the phenomenon is the interplay between the computational process associated with the actual evolution of the system and the computational processes that our perception of the output of that evolution brings to bear. We might have thought if a system had a simple underlying rule—like rule 30—then it’d always be straightforward to predict what the system will do. Of course, we could in principle always just run the rule and see what happens. But the question is whether we can expect to “jump ahead” and “find the outcome”, with much less computational effort than the actual evolution of the system involves. And an important conclusion of a lot of science I did in the 1980s and 1990s is that for many systems—presumably including rule 30—it’s simply not possible to “jump ahead”. And instead the evolution of the system is what I call computationally irreducible—so that it takes an irreducible amount of computational effort to find out what the system does. Ultimately this is a consequence of what I call the Principle of Computational Equivalence, which states that above some low threshold, systems always end up being equivalent in the sophistication of the computations they perform. And this is why even our brains and our most sophisticated methods of scientific analysis can’t “computationally outrun” even something like rule 30, so that we must consider it computationally irreducible. So how does this relate to the Second Law? It’s what makes it possible for a system like rule 30 to operate according to a simple underlying rule, yet to intrinsically generate what seems like random behavior. If we could do all the necessary computationally irreducible work then we could in principle “see through” to the simple rules underneath. But the key point (emphasized by our Physics Project) is that observers like us are computationally bounded in our capabilities. And this means that we’re not able to “see through the computational irreducibility”—with the result that the behavior we see “looks random to us”. And in thermodynamics that “random-looking” behavior is what we associate with heat. And the Second Law assertion that energy associated with systematic mechanical work tends to “degrade into heat” then corresponds to the fact that when there’s computational irreducibility the behavior that’s generated is something we can’t readily “computationally see through”—so that it appears random to us.
Computational Foundations for the Second Law of Thermodynamics 33.3 The Road from Ordinary Thermodynamics : Systems like rule 30 make the phenomenon of intrinsic randomness generation particularly clear. But how do such systems relate to the ones that thermodynamics usually studies? The original formulation of the Second Law involved gases, and the vast majority of its applications even today still concern things like gases. At a basic level, a typical gas consists of a collection of discrete molecules that interact through collisions. And as an idealization of this, we can consider hard spheres that move according to the standard laws of mechanics and undergo perfectly elastic collisions with each other, and with the walls of a container. Here’s an example of a sequence of snapshots from a simulation of such a system, done in 2D: We begin with an organized “flotilla” of “molecules”, systematically going in a particular direction (and not touching, to avoid a “Newton’s Cradle” many-collisions-at-a-time effect). But after these molecules collide with a wall, they quickly start to move in what seem like much more random ways. The original systematic motion is like what happens when one is “doing mechanical work”, say moving a solid object. But what we see is that—just as the Second Law implies—this motion is quickly “degraded” into disordered and seemingly random “heat-like” microscopic motion. Here’s a “spacetime” view of the behavior: Looking from far away, with each molecule’s spacetime trajectory shown as a slightly transparent tube, we get: There’s already some qualitative similarity with the rule 30 behavior we saw above. But there are many detailed differences. And one of the most obvious is that while rule 30 just has a discrete collection of cells, the spheres in the hard-sphere gas can be at any position. And, what’s more, the precise details of their positions can have an increasingly large effect. If two elastic spheres collide perfectly head on, they’ll bounce back the way they came. But as soon as they’re even slightly off center they’ll bounce back at a different angle, and if they do this repeatedly even the tiniest initial off-centeredness will be arbitrarily amplified: And, yes, this chaos-theory-like phenomenon makes it very difficult even to do an accurate simulation on a computer with limited numerical precision. But does it actually matter to the core phenomenon of randomization that’s central to the Second Law? To begin testing this, let’s consider not hard spheres but instead hard squares (where we assume that the squares always stay in the same orientation, and ignore the mechanical torques that would lead to spinning). If we set up the same kind of “flotilla” as before, with the edges of the squares aligned with the walls of the box, then things are symmetrical enough that we don’t see any randomization—and in fact the only nontrivial thing that happens is a little Newton’s-Cradling when the “caravan” of squares hits a wall: Viewed in “spacetime” we can see the “flotilla” is just bouncing unchanged off the walls: But remove even a tiny bit of the symmetry—here by roughly doubling the “masses” of some of the squares and “riffling” their positions (which also avoids singular multi-square collisions)—and we get: In “spacetime” this becomes or “from the side”: So despite the lack of chaos-theory-like amplification behavior (or any associated loss of numerical precision in our simulations), there’s still rapid “degradation” to a certain apparent randomness. So how much further can we go? In the hard-square gas, the squares can still be at any location, and be moving at any speed in any direction. As a simpler system (that I happened to first investigate a version of nearly 50 years ago), let’s consider a discrete grid in which idealized molecules have discrete directions and are either present or not on each edge: The system operates in discrete steps, with the molecules at each step moving or “scattering” according to the rules (up to rotations) and interacting with the “walls” according to: Running this starting with a “flotilla” we get on successive steps: Or, sampling every 10 steps: In “spacetime” this becomes (with the arrows tipped to trace out “worldlines”): or “from the side”: And again we see at least a certain level of “randomization”. With this model we’re getting quite close to the setup of something like rule 30. And reformulating this same model we can get even closer. Instead of having “particles” with explicit “velocity directions”, consider just having a grid in which an alternating pattern of 2×2 blocks are updated at each step according to and the “wall” rules as well as the “rotations” of all these rules. With this “block cellular automaton” setup, “isolated particles” move according to the  rule like the pieces on a checkerboard: A “flotilla” of particles—like equal-mass hard squares—has rather simple behavior in the “square enclosure”: In “spacetime” this is just: But if we add even a single fixed (“single-cell-of-wall”) “obstruction cell” (here at the very center of the box, so preserving reflection symmetry) the behavior is quite different: In “spacetime” this becomes (with the “obstruction cell” shown in gray) or “from the side” (with the “obstruction” sometimes getting obscured by cells in front): As it turns out, the block cellular automaton model we’re using here is actually functionally identical to the “discrete velocity molecules” model we used above, as the correspondence of their rules indicates: And seeing this correspondence one gets the idea of considering a “rotated container”—which no longer gives simple behavior even without any kind of “interior fixed obstruction cell”: Here’s the corresponding “spacetime” view and here’s what it looks like “from the side”: Here’s a larger version of the same setup (though no longer with exact symmetry) sampled every 50 steps: And, yes, it’s increasingly looking as if there’s intrinsic randomness generation going on, much like in rule 30. But if we go a little further the correspondence becomes even clearer. The systems we’ve been looking at so far have all been in 2D. But what if—like in rule 30—we consider 1D? It turns out we can set up very much the same kind of “gas-like” block cellular automata. Though with blocks of size 2 and two possible values for each cell, there’s only one viable rule where in effect the only nontrivial transformation is: (In 1D we can also make things simpler by not using explicit “walls”, but instead just wrapping the array of cells around cyclically.) Here’s then what happens with this rule with a few possible initial states: And what we see is that in all cases the “particles” effectively just “pass through each other” without really “interacting”. But we can make there be something closer to “real interactions” by introducing another color, and adding a transformation which effectively introduces a “time delay” to each “crossover” of particles (as an alternative, one can also stay with 2 colors, and use size-3 blocks): And with this “delayed particle” rule (that, as it happens, I first studied in 1986) we get: With sufficiently simple initial conditions this still gives simple behavior, such as: But as soon as one reaches the 121st initial condition () one sees (As we’ll discuss below, in a finite-size region of the kind we’re using, it’s inevitable that the pattern eventually repeats, though in the particular case shown it takes 7022 steps.) Here’s a slightly larger example, in which there’s clearer “progressive degradation” of the initial condition to apparent randomness: We’ve come quite far from our original hard-sphere “realistic gas molecules”. But there’s even further to go. With hard spheres there’s built-in conservation of energy, momentum and number of particles. And we don’t specifically have these things anymore. But the rule we’re using still does have conservation of the number of non-white cells. Dropping this requirement, we can have rules like which gradually “fill in with particles”: What happens if we just let this “expand into a vacuum”, without any “walls”? The behavior is complex. And as is typical when there’s computational irreducibility, it’s at first hard to know what will happen in the end. For this particular initial condition everything becomes essentially periodic (with period 70) after 979 steps: But with a slightly different initial condition, it seems to have a good chance of growing forever: With slightly different rules (that here happen not be left-right symmetric) we start seeing rapid “expansion into the vacuum”—basically just like rule 30: The whole setup here is very close to what it is for rule 30. But there’s one more feature we’ve carried over here from our hard-sphere gas and other models. Just like in standard classical mechanics, every part of the underlying rule is reversible, in the sense that if the rule says that block u goes to block v it also says that block v goes to block u. Rules like remove this restriction but produce behavior that’s qualitatively no different from the reversible rules above. But now we’ve got to systems that are basically set up just like rule 30. (They happen to be block cellular automata rather than ordinary ones, but that really doesn’t matter.) And, needless to say, being set up like rule 30 it shows the same kind of intrinsic randomness generation that we see in a system like rule 30. We started here from a “physically realistic” hard-sphere gas model—which we’ve kept on simplifying and idealizing. And what we’ve found is that through all this simplification and idealization, the same core phenomenon has remained: that even starting from “simple” or “ordered” initial conditions, complex and “apparently random” behavior is somehow generated, just like it is in typical Second Law behavior. At the outset we might have assumed that to get this kind of “Second Law behavior” would need at least quite a few features of physics. But what we’ve discovered is that this isn’t the case. And instead we’ve got evidence that the core phenomenon is much more robust and in a sense purely computational. Indeed, it seems that as soon as there’s computational irreducibility in a system, it’s basically inevitable that we’ll see the phenomenon. And since from the Principle of Computational Equivalence we expect that computational irreducibility is ubiquitous, the core phenomenon of the Second Law will in the end be ubiquitous across a vast range of systems, from things like hard-sphere gases to things like rule 30. 
Computational Foundations for the Second Law of Thermodynamics 33.4 Reversibility, Irreversibility and Equilibrium : Our typical everyday experience shows a certain fundamental irreversibility. An egg can readily be scrambled. But you can’t easily reverse that: it can’t readily be unscrambled. And indeed this kind of one-way transition from order to disorder—but not back—is what the Second Law is all about. But there’s immediately something mysterious about this. Yes, there’s irreversibility at the level of things like eggs. But if we drill down to the level of atoms, the physics we know says there’s basically perfect reversibility. So where is the irreversibility coming from? This is a core (and often confused) question about the Second Law, and in seeing how it resolves we will end up face to face with fundamental issues about the character of observers and their relationship to computational irreducibility. A “particle cellular automaton” like the one from the previous section has transformations that “go both ways”, making its rule perfectly reversible. Yet we saw above that if we start from a “simple initial condition” and then just run the rule, it will “produce increasing randomness”. But what if we reverse the rule, and run it backwards? Well, since the rule is reversible, the same thing must happen: we must get increasing randomness. But how can it be that “randomness increases” both going forward in time and going backward? Here’s a picture that shows what’s going on: In the middle the system takes on a “simple state”. But going either forward or backward it “randomizes”. The second half of the evolution we can interpret as typical Second-Law-style “degradation to randomness”. But what about the first half? Something unexpected is happening here. From what seems like a “rather random” initial state, the system appears to be “spontaneously organizing itself” to produce—at least temporarily—a simple and “orderly” state. An initial “scrambled” state is spontaneously becoming “unscrambled”. In the setup of ordinary thermodynamics, this would be a kind of “anti-thermodynamic” behavior in which what seems like “random heat” is spontaneously generating “organized mechanical work”. So why isn’t this what we see happening all the time? Microscopic reversibility guarantees that in principle it’s possible. But what leads to the observed Second Law is that in practice we just don’t normally end up setting up the kind of initial states that give “anti-thermodynamic” behavior. We’ll be talking at length below about why this is. But the basic point is that to do so requires more computational sophistication than we as computationally bounded observers can muster. If the evolution of the system is computationally irreducible, then in effect we have to invert all of that computationally irreducible work to find the initial state to use, and that’s not something that we—as computationally bounded observers—can do. But before we talk more about this, let’s explore some of the consequences of the basic setup we have here. The most obvious aspect of the “simple state” in the middle of the picture above is that it involves a big blob of “adjacent particles”. So now here’s a plot of the “size of the biggest blob that’s present” as a function of time starting from the “simple state”: The plot indicates that—as the picture above indicates—the “specialness” of the initial state quickly “decays” to a “typical state” in which there aren’t any large blobs present. And if we were watching the system at the beginning of this plot, we’d be able to “use the Second Law” to identify a definite “arrow of time”: later times are the ones where the states are “more disordered” in the sense that they only have smaller blobs. There are many subtleties to all of this. We know that if we set up an “appropriately special” initial state we can get anti-thermodynamic behavior. And indeed for the whole picture above—with its “special initial state”—the plot of blob size vs. time looks like this, with a symmetrical peak “developing” in the middle: We’ve “made this happen” by setting up “special initial conditions”. But can it happen “naturally”? To some extent, yes. Even away from the peak, we can see there are always little fluctuations: blobs being formed and destroyed as part of the evolution of the system. And if we wait long enough we may see a fairly large blob. Like here’s one that forms (and decays) after about 245,400 steps: The actual structure this corresponds to is pretty unremarkable: But, OK, away from the “special state”, what we see is a kind of “uniform randomness”, in which, for example, there’s no obvious distinction between forward and backward in time. In thermodynamic terms, we’d describe this as having “reached equilibrium”—a situation in which there’s no longer “obvious change”. To be fair, even in “equilibrium”, there will always be “fluctuations”. But for example in the system we’re looking at here, “fluctuations” corresponding to progressively larger blobs tend to occur exponentially less frequently. So it’s reasonable to think of there being an “equilibrium state” with certain unchanging “typical properties”. And, what’s more, that state is the basic outcome from any initial condition. Whatever special characteristics might have been present in the initial state will tend to be degraded away, leaving only the generic “equilibrium state”. One might think that the possibility of such an “equilibrium state” showing “typical behavior” would be a specific feature of microscopically reversible systems. But this isn’t the case. And much as the core phenomenon of the Second Law is actually something computational that’s deeper and more general than the specifics of particular physical systems, so also this is true of the core phenomenon of equilibrium. And indeed the presence of what we might call “computational equilibrium” turns out to be directly connected to the overall phenomenon of computational irreducibility. Let’s look again at rule 30. We start it off with different initial states, but in each case it quickly evolves to look basically the same: Yes, the details of the patterns that emerge depend on the initial conditions. But the point is that the overall form of what’s produced is always the same: the system has reached a kind of “computational equilibrium” whose overall features are independent of where it came from. Later, we’ll see that the rapid emergence of “computational equilibrium” is characteristic of what I long ago identified as “class 3 systems”—and it’s quite ubiquitous to systems with a wide range of underlying rules, microscopically reversible or not. That’s not to say that microscopic reversibility is irrelevant to “Second-Law-like” behavior. In what I called class 1 and class 2 systems the force of irreversibility in the underlying rules is strong enough that it overcomes computational irreducibility, and the systems ultimately evolve not to a “computational equilibrium” that looks random but rather to a definite, predictable end state: How common is microscopic reversibility? In some types of rules it’s basically always there, by construction. But in other cases microscopically reversible rules represent just a subset of possible rules of a given type. For example, for block cellular automata with k colors and blocks of size b, there are altogether (kb)kb possible rules, of which kb! are reversible (i.e. of all mappings between possible blocks, only those that are permutations correspond to reversible rules). Among reversible rules, some—like the particle cellular automaton rule above—are “self-inverses”, in the sense that the forward and backward versions of the rule are the same. But a rule like this is still reversible and there’s still a straightforward backward rule, but it’s not exactly the same as the forward rule: Using the backward rule, we can again construct an initial state whose forward evolution seems “anti-thermodynamic”, but the detailed behavior of the whole system isn’t perfectly symmetric between forward and backward in time: Basic mechanics—like for our hard-sphere gas—is reversible and “self-inverse”. But it’s known that in particle physics there are small deviations from time reversal invariance, so that the rules are not precisely self-inverse—though they are still reversible in the sense that there’s always both a unique successor and a unique predecessor to every state (and indeed in our Physics Project such reversibility is probably guaranteed to exist in the laws of physics assumed by any observer who “believes they are persistent in time”). For block cellular automata it’s very easy to determine from the underlying rule whether the system is reversible (just look to see if the rule serves only to permute the blocks). But for something like an ordinary cellular automaton it’s more difficult to determine reversibility from the rule (and above one dimension the question of reversibility can actually be undecidable). Among the 256 2-color nearest-neighbor rules there are only 6 reversible examples, and they are all trivial. Among the 134,217,728 3-color nearest-neighbor rules, 1800 are reversible. Of the 82 of these rules that are self-inverse, all are trivial. But when the inverse rules are different, the behavior can be nontrivial: Note that unlike with block cellular automata the inverse rule often involves a larger neighborhood than the forward rule. (So, for example, here 396 rules have r = 1 inverses, 612 have r = 2, 648 have r = 3 and 144 have r = 4.) A notable variant on ordinary cellular automata are “second-order” ones, in which the value of a cell depends on its value two steps in the past: With this approach, one can construct reversible second-order variants of all 256 “elementary cellular automata”: Note that such second-order rules are equivalent to 4-color first-order nearest-neighbor rules:
Computational Foundations for the Second Law of Thermodynamics 33.5 Ergodicity and Global Behavior : Whenever there’s a system with deterministic rules and a finite total number of states, it’s inevitable that the evolution of the system will eventually repeat. Sometimes the repetition period—or “recurrence time”—will be fairly short and sometimes it’s much longer: In general we can make a state transition graph that shows how each possible state of the system transitions to another under the rules. For a reversible system this graph consists purely of cycles in which each state has a unique successor and a unique predecessor. For a size-4 version of the system we’re studying here, there are a total of 2 x 34 = 162 possible states (the factor 2 comes from the even/odd “phases” of the block cellular automaton)—and the state transition graph for this system is: For a non-reversible system—like rule 30—the state transition graph (here shown for sizes 4 and 8) also includes “transient trees” of states that can be visited only once, on the way to a cycle: In the past one of the key ideas for the origin of Second-Law-like behavior was ergodicity. And in the discrete-state systems we’re discussing here the definition of perfect ergodicity is quite straightforward: ergodicity just implies that the state transition graph must consist not of many cycles, but instead purely of one big cycle—so that whatever state one starts from, one’s always guaranteed to eventually visit every possible other state. But why is this relevant to the Second Law? Well, we’ve said that the Second Law is about “degradation” from “special states” to “typical states”. And if one’s going to “do the ergodic thing” of visiting all possible states, then inevitably most of the states we’ll at least eventually pass through will be “typical”. But on its own, this definitely isn’t enough to explain “Second-Law behavior” in practice. In an example like the following, one sees rapid “degradation” of a simple initial state to something “random” and “typical”: But of the 2 x 380 ≈ 1038 possible states that this system would eventually visit if it were ergodic, there are still a huge number that we wouldn’t consider “typical” or “random”. For example, just knowing that the system is eventually ergodic doesn’t tell one that it wouldn’t start off by painstakingly “counting down” like this, “keeping the action” in a tightly organized region: So somehow there’s more than ergodicity that’s needed to explain the “degradation to randomness” associated with “typical Second-Law behavior”. And, yes, in the end it’s going to be a computational story, connected to computational irreducibility and its relationship to observers like us. But before we get there, let’s talk some more about “global structure”, as captured by things like state transition diagrams. Consider again the size-4 case above. The rules are such that they conserve the number of “particles” (i.e. non-white cells). And this means that the states of the system necessarily break into separate “sectors” for different particle numbers. But even with a fixed number of particles, there are typical quite a few distinct cycles: The system we’re using here is too small for us to be able to convincingly identify “simple” versus “typical” or “random” states, though for example we can see that only a few of the cycles have the simplifying feature of left-right symmetry. Going to size 6 one begins to get a sense that there are some “always simple” cycles, as well as others that involve “more typical” states: At size 10 the state transition graph for “4-particle” states has the form and the longer cycles are: It’s notable that most of the longest (“closest-to-ergodicity”) cycles look rather “simple and deliberate” all the way through. The “more typical and random” behavior seems to be reserved here for shorter cycles. But in studying “Second Law behavior” what we’re mostly interested in is what happens from initially orderly states. Here’s an example of the results for progressively larger “blobs” in a system of size 30: To get some sense of how the “degradation to randomness” proceeds, we can plot how the maximum blob size evolves in each case: For some of the initial conditions one sees “thermodynamic-like” behavior, though quite often it’s overwhelmed by “freezing”, fluctuations, recurrences, etc. In all cases the evolution must eventually repeat, but the “recurrence times” vary widely (the longest—for a width-13 initial blob—being 861,930): Let’s look at what happens in these recurrences, using as an example a width-17 initial blob—whose evolution begins: As the picture suggests, the initial “large blob” quickly gets at least somewhat degraded, though there continue to be definite fluctuations visible: If one keeps going long enough, one reaches the recurrence time, which in this case is 155,150 steps. Looking at the maximum blob size through a “whole cycle” one sees many fluctuations: Most are small—as illustrated here with ordinary and logarithmic histograms: But some are large. And for example at half the full recurrence time there is a fluctuation that involves an “emergent blob” as wide as in the initial condition—that altogether lasts around 280 steps: There are also “runner-up” fluctuations with various forms—that reach “blob width 15” and occur more or less equally spaced throughout the cycle: It’s notable that clear Second-Law-like behavior occurs even in a size-30 system. But if we go, say, to a size-80 system it becomes even more obvious and one sees rapid and systematic evolution towards an “equilibrium state” with fairly small fluctuations: It’s worth mentioning again that the idea of “reaching equilibrium” doesn’t depend on the particulars of the rule we’re using—and in fact it can happen more rapidly in other reversible block cellular automata where there are no “particle conservation laws” to slow things down: In such rules there also tend to be fewer, longer cycles in the state transition graph, as this comparison for size 6 with the “delayed particle” rule suggests: But it’s important to realize that the “approach to equilibrium” is its own—computational—phenomenon, not directly related to long cycles and concepts like ergodicity. And indeed, as we mentioned above, it also doesn’t depend on built-in reversibility in the rules, so one sees it even in something like rule 30:
Computational Foundations for the Second Law of Thermodynamics 33.6 How Random Does It Get? : At an everyday level, the core manifestation of the Second Law is the tendency of things to “degrade” to randomness. But just how random is the randomness? One might think that anything that is made by a simple-to-describe algorithm—like the pattern of rule 30 or the digits of π—shouldn’t really be considered “random”. But for the purpose of understanding our experience of the world what matters is not what is “happening underneath” but instead what our perception of it is. So the question becomes: when we see something produced, say by rule 30 or by π, can we recognize regularities in it or not? And in practice what the Second Law asserts is that systems will tend to go from states where we can recognize regularities to ones where we cannot. And the point is that this phenomenon is something ubiquitous and fundamental, arising from core computational ideas, in particular computational irreducibility. But what does it mean to “recognize regularities”? In essence it’s all about seeing if we can find succinct ways to summarize what we see—or at least the aspects of what we see that we care about. In other words, what we’re interested in is finding some kind of compressed representation of things. And what the Second Law is ultimately about is saying that even if compression works at first, it won’t tend to keep doing so. As a very simple example, let’s consider doing compression by essentially “representing our data as a sequence of blobs”—or, more precisely, using run-length encoding to represent sequences of 0s and 1s in terms of lengths of successive runs of identical values. For example, given the data we split into runs of identical values then as a “compressed representation” just give the length of each run which we can finally encode as a sequence of binary numbers with base-3 delimiters: “Transforming” our “particle cellular automaton” in this way we get: The “simple” initial conditions here are successfully compressed, but the later “random” states are not. Starting from a random initial condition, we don’t see any significant compression at all: What about other methods of compression? A standard approach involves looking at blocks of successive values on a given step, and asking about the relative frequencies with which different possible blocks occur. But for the particular rule we are discussing here, there’s immediately an issue. The rule conserves the total number of non-white cells—so at least for size-1 blocks the frequency of such blocks will always be what it was for the initial conditions. What about for larger blocks? This gives the evolution of relative frequencies of size-2 blocks starting from the simple initial condition above: Arranging for exactly half the cells to be non-white, the frequencies of size-2 block converge towards equality: In general, the presence of unequal frequencies for different blocks allows the possibility of compression: much like in Morse code, one just has to use shorter codewords for more frequent blocks. How much compression is ultimately possible in this way can be found by computing -Σpi log pi for the probabilities pi of all blocks of a given length, which we see quickly converge to constant “equilibrium” values: In the end we know that the initial conditions were “simple” and “special”. But the issue is whether whatever method we use for compression or for recognizing regularities is able to pick up on this. Or whether somehow the evolution of the system has sufficiently “encoded” the information about the initial condition that it’s no longer detectable. Clearly if our “method of compression” involved explicitly running the evolution of the system backwards, then it’d be possible to pick out the special features of the initial conditions. But explicitly running the evolution of the system requires doing lots of computational work. So in a sense the question is whether there’s a shortcut. And, yes, one can try all sorts of methods from statistics, machine learning, cryptography and so on. But so far as one can tell, none of them make any significant progress: the “encoding” associated with the evolution of the system seems to just be too strong to “break”. Ultimately it’s hard to know for sure that there’s no scheme that can work. But any scheme must correspond to running some program. So a way to get a bit more evidence is just to enumerate “possible compression programs” and see what they do. In particular, we can for example enumerate simple cellular automata, and see whether when run they produce “obviously different” results. Here’s what happens for a collection of different cellular automata when they are applied to a “simple initial condition”, to states obtained after 20 and 200 steps of evolution according to the particle cellular automaton rule and to an independently random state: And, yes, in many cases the simple initial condition leads to “obviously different behavior”. But there’s nothing obviously different about the behavior obtained in the last two cases. Or, in other words, at least programs based on these simple cellular automata don’t seem to be able to “decode” the different origins of the second and third cases shown here. What does all this mean? The fundamental point is that there seems to be enough computational irreducibility in the evolution of the system that no computationally bounded observer can “see through it”. And so—at least as far as a computationally bounded observer is concerned—“specialness” in the initial conditions is quickly “degraded” to an “equilibrium” state that “seems random”. Or, in other words, the computational process of evolution inevitably seems to lead to the core phenomenon of the Second Law.
Computational Foundations for the Second Law of Thermodynamics 33.7 The Concept of Entropy : “Entropy increases” is a common statement of the Second Law. But what does this mean, especially in our computational context? The answer is somewhat subtle, and understanding it will put us right back into questions of the interplay between computational irreducibility and the computational boundedness of observers. When it was first introduced in the 1860s, entropy was thought of very much like energy, and was computed from ratios of heat content to temperature. But soon—particularly through work on gases by Boltzmann—there arose a quite different way of computing (and thinking about) entropy: in terms of the log of the number of possible states of a system. Later we’ll discuss the correspondence between these different ideas of entropy. But for now let’s consider what I view as the more fundamental definition based on counting states. In the early days of entropy, when one imagined that—like in the cases of the hard-sphere gas—the parameters of the system were continuous, it could be mathematically complex to tease out any kind of discrete “counting of states”. But from what we’ve discussed here, it’s clear that the core phenomenon of the Second Law doesn’t depend on the presence of continuous parameters, and in something like a cellular automaton it’s basically straightforward to count discrete states. But now we have get more careful about our definition of entropy. Given any particular initial state, a deterministic system will always evolve through a series of individual states—so that there’s always only one possible state for the system, which means the entropy will always be exactly zero. (This is a lot muddier and more complicated when continuous parameters are considered, but in the end the conclusion is the same.) So how do we get a more useful definition of entropy? The key idea is to think not about individual states of a system but instead about collections of states that we somehow consider “equivalent”. In a typical case we might imagine that we can’t measure all the detailed positions of molecules in a gas, so we look just at “coarse-grained” states in which we consider, say, only the number of molecules in particular overall bins or blocks. The entropy can be thought of as counting the number of possible microscopic states of the system that are consistent with some overall constraint—like a certain number of particles in each bin. If the constraint talks specifically about the position of every particle, there’ll only be one microscopic state consistent with the constraints, and the entropy will be zero. But if the constraint is looser, there’ll often be many possible microscopic states consistent with it, and the entropy we define will be nonzero. Let’s look at this in the context of our particle cellular automaton. Here’s a particular evolution, starting from a specific microscopic state, together with a sequence of “coarse grainings” of this evolution in which we keep track only of “overall particle density” in progressively larger blocks: The very first “coarse graining” here is particularly trivial: all it’s doing is to say whether a “particle is present” or not in each cell—or, in other words, it’s showing every particle but ignoring whether it’s “light” or “dark”. But in making this and the other coarse-grained pictures we’re always starting from the single “underlying microscopic evolution” that’s shown and just “adding coarse graining after the fact”. But what if we assume that all we ever know about the system is a coarse-grained version? Say we look at the “particle-or-not” case. At a coarse-grained level the initial condition just says there are 6 particles present. But it doesn’t say if each particle is light or dark, and actually there are 26 = 64 possible microscopic configurations. And the point is that each of these microscopic configurations has its own evolution: But now we can consider coarse graining things. All 64 initial conditions are—by construction—equivalent under particle-or-not coarse graining: But after just one step of evolution, different initial “microstates” can lead to different coarse-grained evolutions: In other words, a single coarse-grained initial condition “spreads out” after just one step to several coarse-grained states: After another step, a larger number of coarse-grained states are possible: And in general the number of distinct coarse-grained states that can be reached grows fairly rapidly at first, though soon saturates, showing just fluctuations thereafter: But the coarse-grained entropy is basically just proportional to the log of this quantity, so it too will show rapid growth at first, eventually leveling off at an “equilibrium” value. The framework of our Physics Project makes it natural to think of coarse-grained evolution as a multicomputational process—in which a given coarse-grained state has not just a single successor, but in general multiple possible successors. For the case we’re considering here, the multiway graph representing all possible evolution paths is then: The branching here reflects a spreading out in coarse-grained state space, and an increase in coarse-grained entropy. If we continue longer—so that the system begins to “approach equilibrium”—we’ll start to see some merging as well as a less “time-oriented” graph layout makes clear: But the important point is that in its “approach to equilibrium” the system in effect rapidly “spreads out” in coarse-grained state space. Or, in other words, the number of possible states of the system consistent with a particular coarse-grained initial condition increases, corresponding to an increase in what one can consider to be the entropy of the system. There are many possible ways to set up what we might view as “coarse graining”. An example of another possibility is to focus on the values of a particular block of cells, and then to ignore the values of all other cells. But it typically doesn’t take long for the effects of other cells to “seep into” the block we’re looking at: So what is the bigger picture? The basic point is that insofar as the evolution of each individual microscopic state “leads to randomness”, it’ll tend to end up in a different “coarse-grained bin”. And the result is that even if one starts with a tightly defined coarse-grained description, it’ll inevitably tend to “spread out”, thereby encompassing more states and increasing the entropy. In a sense, looking at entropy and coarse graining is just a less direct way to detect that a system tends to “produce effective randomness”. And while it may have seemed like a convenient formalism when one was, for example, trying to tease things out from systems with continuous variables, it now feels like a rather indirect way to get at the core phenomenon of the Second Law. It’s useful to understand a few more connections, however. Let’s say one’s trying to work out the average value of something (say particle density) in a system. What do we mean by “average”? One possibility is that we take an “ensemble” of possible states of the system, then find the average across these. But another possibility is that we instead look at the average across successive states in the evolution of the system. The “ergodic hypothesis” is that the ensemble average will be the same as the time average. One way this would—at least eventually—be guaranteed is if the evolution of the system is ergodic, in the sense that it eventually visits all possible states. But as we saw above, this isn’t something that’s particularly plausible for most systems. But it also isn’t necessary. Because so long as the evolution of the system is “effectively random” enough, it’ll quickly “sample typical states”, and give essentially the same averages as one would get from sampling all possible states, but without having to laboriously visit all these states. How does one tie all this down with rigorous, mathematical-style proofs? Well, it’s difficult. And in a first approximation not much progress has been made on this for more than a century. But having seen that the core phenomenon of the Second Law can be reduced to an essentially purely computational statement, we’re now in a position to examine this in a different—and I think ultimately very clarifying—way.
Computational Foundations for the Second Law of Thermodynamics 33.8 Why the Second Law Works : At its core the Second Law is essentially the statement that “things tend to get more random”. And in a sense the ultimate driver of this is the surprising phenomenon of computational irreducibility I identified in the 1980s—and the remarkable fact that even from simple initial conditions simple computational rules can generate behavior of great complexity. But there are definitely additional nuances to the story. For example, we’ve seen that—particularly in a reversible system—it’s always in principle possible to set up initial conditions that will evolve to “magically produce” whatever “simple” configuration we want. And when we say that we generate “apparently random” states, our “analyzer of randomness” can’t go in and invert the computational process that generated the states. Similarly, when we talk about coarse-grained entropy and its increase, we’re assuming that we’re not inventing some elaborate coarse-graining procedure that’s specially set up to pick out collections of states with “special” behavior. But there’s really just one principle that governs all these things: that whatever method we have to prepare or analyze states of a system is somehow computationally bounded. This isn’t as such a statement of physics. Rather, it’s a general statement about observers, or, more specifically, observers like us. We could imagine some very detailed model for an observer, or for the experimental apparatus they use. But the key point is that the details don’t matter. Really all that matters is that the observer is computationally bounded. And it’s then the basic computational mismatch between the observer and the computational irreducibility of the underlying system that leads us to “experience” the Second Law. At a theoretical level we can imagine an “alien observer”—or even an observer with technology from our own future—that would not have the same computational limitations. But the point is that insofar as we are interested in explaining our own current experience, and our own current scientific observations, what matters is the way we as observers are now, with all our computational boundedness. And it’s then the interplay between this computational boundedness, and the phenomenon of computational irreducibility, that leads to our basic experience of the Second Law. At some level the Second Law is a story of the emergence of complexity. But it’s also a story of the emergence of simplicity. For the very statement that things go to a “completely random equilibrium” implies great simplification. Yes, if an observer could look at all the details they would see great complexity. But the point is that a computationally bounded observer necessarily can’t look at those details, and instead the features they identify have a certain simplicity. And so it is, for example, that even though in a gas there are complicated underlying molecular motions, it’s still true that at an overall level a computationally bounded observer can meaningfully discuss the gas—and make predictions about its behavior—purely in terms of things like pressure and temperature that don’t probe the underlying details of molecular motions. In the past one might have thought that anything like the Second Law must somehow be specific to systems made from things like interacting particles. But in fact the core phenomenon of the Second Law is much more general, and in a sense purely computational, depending only on the basic computational phenomenon of computational irreducibility, together with the fundamental computational boundedness of observers like us. And given this generality it’s perhaps not surprising that the core phenomenon appears far beyond where anything like the Second Law has normally been considered. In particular, in our Physics Project it now emerges as fundamental to the structure of space itself—as well as to the phenomenon of quantum mechanics. For in our Physics Project we imagine that at the lowest level everything in our universe can be represented by some essentially computational structure, conveniently described as a hypergraph whose nodes are abstract “atoms of space”. This structure evolves by following rules, whose operation will typically show all sorts of computational irreducibility. But now the question is how observers like us will perceive all this. And the point is that through our limitations we inevitably come to various “aggregate” conclusions about what’s going on. It’s very much like with the gas laws and their broad applicability to systems involving different kinds of molecules. Except that now the emergent laws are about spacetime and correspond to the equations of General Relativity. But the basic intellectual structure is the same. Except that in the case of spacetime, there’s an additional complication. In thermodynamics, we can imagine that there’s a system we’re studying, and the observer is outside it, “looking in”. But when we’re thinking about spacetime, the observer is necessarily embedded within it. And it turns out that there’s then one additional feature of observers like us that’s important. Beyond the statement that we’re computationally bounded, it’s also important that we assume that we’re persistent in time. Yes, we’re made of different atoms of space at different moments. But somehow we assume that we have a coherent thread of experience. And this is crucial in deriving our familiar laws of physics. We’ll talk more about it later, but in our Physics Project the same underlying setup is also what leads to the laws of quantum mechanics. Of course, quantum mechanics is notable for the apparent randomness associated with observations made in it. And what we’ll see later is that in the end the same core phenomenon responsible for randomness in the Second Law also appears to be what’s responsible for randomness in quantum mechanics. The interplay between computational irreducibility and computational limitations of observers turns out to be a central phenomenon throughout the multicomputational paradigm and its many emerging applications. It’s core to the fact that observers can experience computationally reducible laws in all sorts of samplings of the ruliad. And in a sense all of this strengthens the story of the origins of the Second Law. Because it shows that what might have seemed like arbitrary features of observers are actually deep and general, transcending a vast range of areas and applications. But even given the robustness of features of observers, we can still ask about the origins of the whole computational phenomenon that leads to the Second Law. Ultimately it begins with the Principle of Computational Equivalence, which asserts that systems whose behavior is not obviously simple will tend to be equivalent in their computational sophistication. The Principle of Computational Equivalence has many implications. One of them is computational irreducibility, associated with the fact that “analyzers” or “predictors” of a system cannot be expected to have any greater computational sophistication than the system itself, and so are reduced to just tracing each step in the evolution of a system to find out what it does. Another implication of the Principle of Computational Equivalence is the ubiquity of computation universality. And this is something we can expect to see “underneath” the Second Law. Because we can expect that systems like the particle cellular automaton—or, for that matter, the hard-sphere gas—will be provably capable of universal computation. Already it’s easy to see that simple logic gates can be constructed from configurations of particles, but a full demonstration of computation universality will be considerably more elaborate. And while it’d be nice to have such a demonstration, there’s still more that’s needed to establish full computational irreducibility of the kind the Principle of Computational Equivalence implies. As we’ve seen, there are a variety of “indicators” of the operation of the Second Law. Some are based on looking for randomness or compression in individual states. Others are based on computing coarse grainings and entropy measures. But with the computational interpretation of the Second Law we can expect to translate such indicators into questions in areas like computational complexity theory. At some level we can think of the Second Law as being a consequence of the dynamics of a system so “encrypting” the initial conditions of a system that no computations available to an “observer” can feasibly “decrypt” it. And indeed as soon as one looks at “inverting” coarse-grained results one is immediately faced with fairly classic NP problems from computational complexity theory. (Establishing NP completeness in a particular case remains challenging, just like establishing computation universality.) 
Computational Foundations for the Second Law of Thermodynamics 33.9 Textbook Thermodynamics : In our discussion here, we’ve treated the Second Law of thermodynamics primarily as an abstract computational phenomenon. But when thermodynamics was historically first being developed, the computational paradigm was still far in the future, and the only way to identify something like the Second Law was through its manifestations in terms of physical concepts like heat and temperature. The First Law of thermodynamics asserted that heat was a form of energy, and that overall energy was conserved. The Second Law then tried to characterize the nature of the energy associated with heat. And a core idea was that this energy was somehow incoherently spread among a large number of separate microscopic components. But ultimately thermodynamics was always a story of energy. But is energy really a core feature of thermodynamics or is it merely “scaffolding” relevant for its historical development and early practical applications? In the hard-sphere gas example that we started from above, there’s a pretty clear notion of energy. But quite soon we largely abstracted energy away. Though in our particle cellular automaton we do still have something somewhat analogous to energy conservation: we have conservation of the number of non-white cells. In a traditional physical system like a gas, temperature gives the average energy per degree of freedom. But in something like our particle cellular automaton, we’re effectively assuming that all particles always have the same energy—so there is for example no way to “change the temperature”. Or, put another way, what we might consider as the energy of the system is basically just given by the number of particles in the system. Does this simplification affect the core phenomenon of the Second Law? No. That’s something much stronger, and quite independent of these details. But in the effort to make contact with recognizable “textbook thermodynamics”, it’s useful to consider how we’d add in ideas like heat and temperature. In our discussion of the Second Law, we’ve identified entropy with the log of the number states consistent with a constraint. But more traditional thermodynamics involves formulas like dS = dQ/T. And it’s not hard to see at least roughly where this formula comes from. Q gives total heat content, or “total heat energy” (not worrying about what this is measured relative to, which is what makes it dQ rather than Q). T gives average energy per “degree of freedom” (or, roughly, particle). And this means that Q/T effectively measures something like the “number of particles”. But at least in a system like a particle cellular automaton, the number of possible complete configurations is exponential in the number of particles, making its logarithm, the entropy S, roughly proportional to the number of particles, and thus to Q/T. That anything like this argument works depends, though, on being able to discuss things “statistically”, which in turn depends on the core phenomenon of the Second Law: the tendency of things to evolve to uniform (“equilibrium”) randomness. When the Second Law was first introduced, there were several formulations given, all initially referencing energy. One formulation stated that “heat does not spontaneously go from a colder body to a hotter”. And even in our particle cellular automaton we can see a fairly direct version of this. Our proxy for “temperature” is density of particles. And what we observe is that an initial region of higher density tends to “diffuse” out: Another formulation of the Second Law talks about the impossibility of systematically “turning heat into mechanical work”. At a computational level, the analog of “mechanical work” is systematic, predictable behavior. So what this is saying is again that systems tend to generate randomness, and to “remove predictability”. In a sense this is a direct reflection of computational irreducibility. To get something that one can “harness as mechanical work” one needs something that one can readily predict. But the whole point is that the presence of computational irreducibility makes prediction take an irreducible amount of computational work—that is beyond the capabilities of an “observer like us”. Closely related is the statement that it’s not possible to make a perpetual motion machine (“of the second kind”, i.e. violating the Second Law), that continually “makes systematic motion” from “heat”. In our computational setting this would be like extracting a systematic, predictable sequence of bits from our particle cellular automaton, or from something like rule 30. And, yes, if we had a device that could for example systematically predict rule 30, then it would be straightforward, say, “just to pick out black cells”, and effectively to derive a predictable sequence. But computational irreducibility implies that we won’t be able to do this, without effectively just directly reproducing what rule 30 does, which an “observer like us” doesn’t have the computational capability to do. Much of the textbook discussion of thermodynamics is centered around the assumption of “equilibrium”—or something infinitesimally close to it—in which one assumes that a system behaves “uniformly and randomly”. Indeed, the Zeroth Law of thermodynamics is essentially the statement that “statistically unique” equilibrium can be achieved, which in terms of energy becomes a statement that there is a unique notion of temperature. Once one has the idea of “equilibrium”, one can then start to think of its properties as purely being functions of certain parameters—and this opens up all sorts of calculus-based mathematical opportunities. That anything like this makes sense depends, however, yet again on “perfect randomness as far as the observer is concerned”. Because if the observer could notice a difference between different configurations, it wouldn’t be possible to treat all of them as just being “in the equilibrium state”. Needless to say, while the intuition of all this is made rather clear by our computational view, there are details to be filled in when it comes to any particular mathematical formulation of features of thermodynamics. As one example, let’s consider a core result of traditional thermodynamics: the Maxwell–Boltzmann exponential distribution of energies for individual particles or other degrees of freedom. To set up a discussion of this, we need to have a system where there can be many possible microscopic amounts of energy, say, associated with some kind of idealized particles. Then we imagine that in “collisions” between such particles energy is exchanged, but the total is always conserved. And the question is how energy will eventually be distributed among the particles. As a first example, let’s imagine that we have a collection of particles which evolve in a series of steps, and that at each step particles are paired up at random to “collide”. And, further, let’s assume that the effect of the collision is to randomly redistribute energy between the particles, say with a uniform distribution. We can represent this process using a token-event graph, where the events (indicated here in yellow) are the collisions, and the tokens (indicated here in red) represent states of particles at each step. The energy of the particles is indicated here by the size of the “token dots”: Continuing this a few more steps we get: At the beginning we started with all particles having equal energies. But after a number of steps the particles have a distribution of energies—and the distribution turns out to be accurately exponential, just like the standard Maxwell–Boltzmann distribution: If we look at the distribution on successive steps we see rapid evolution to the exponential form: Why we end up with an exponential is not hard to see. In the limit of enough particles and enough collisions, one can imagine approximating everything purely in terms of probabilities (as one does in deriving Boltzmann transport equations, basic SIR models in epidemiology, etc.) Then if the probability for a particle to have energy E is ƒ(E), in every collision once the system has “reached equilibrium” one must have ƒ(E1)ƒ(E2) = ƒ(E3)ƒ(E4) where E1 + E2 = E3 + E4—and the only solution to this is ƒ(E) ∼ e-β E. In the example we’ve just given, there’s in effect “immediate mixing” between all particles. But what if we set things up more like in a cellular automaton—with particles only colliding with their local neighbors in space? As an example, let’s say we have our particles arranged on a line, with alternating pairs colliding at each step in analogy to a block cellular automaton (the long-range connections represent wraparound of our lattice): In the picture above we’ve assumed that in each collision energy is randomly redistributed between the particles. And with this assumption it turns out that we again rapidly evolve to an exponential energy distribution: But now that we have a spatial structure, we can display what’s going on in more of a cellular automaton style—where here we’re showing results for 3 different sequences of random energy exchanges: And once again, if we run long enough, we eventually get an exponential energy distribution for the particles. But note that the setup here is very different from something like rule 30—because we’re continuously injecting randomness from the outside into the system. And as a minimal way to avoid this, consider a model where at each collision the particles get fixed fractions (1 – α)/2 and (1 + α)/2 of the total energy. Starting with all particles having equal energies, the results are quite trivial—basically just reflecting the successive pairings of particles: Here’s what happens with energy concentrated into a few particles and with random initial energies: And in all cases the system eventually evolves to a “pure checkerboard” in which the only particle energies are (1 – α)/2 and (1 + α)/2. (For α = 0 the system corresponds to a discrete version of the diffusion equation.) But if we look at the structure of the system, we can think of it as a continuous block cellular automaton. And as with other cellular automata, there are lots of possible rules that don’t lead to such simple behavior. In fact, all we need do is allow α to depend on the energies E1 and E2 of colliding pairs of particles (or, here, the values of cells in each block). As an example, let’s take α(E1, E2) = ±FractionalPart[κ E], where E is the total energy of the pair, and the + is used when E1 > E2: And with this setup we once again often see “rule-30-like behavior” in which effectively quite random behavior is generated even without any explicit injection of randomness from outside (the lower panels start at step 1000): The underlying construction of the rule ensures that total energy is conserved. But what we see is that the evolution of the system distributes it across many elements. And at least if we use random initial conditions we eventually in all cases see an exponential distribution of energy values (with simple initial conditions it can be more complicated): The evolution towards this is very much the same as in the systems above. In a sense it depends only on having a suitably randomized energy-conserving collision process, and it takes only a few steps to go from a uniform initial distribution energy to an accurately exponential one: So how does this all work in a “physically realistic” hard-sphere gas? Once again we can create a token-event graph, where the events are collisions, and the tokens correspond to periods of free motion of particles. For a simple 1D “Newton’s cradle” configuration, there is an obvious correspondence between the evolution in “spacetime”, and the token-event graph: But we can do exactly the same thing for a 2D configuration. Indicating the energies of particles by the sizes of tokens we get (excluding wall collisions, which don’t affect particle energy) where the “filmstrip” at the side gives snapshots of the evolution of the system. (Note that in this system, unlike the ones above, there aren’t definite “steps” of evolution; the collisions just happen “asynchronously” at times determined by the dynamics.) In the initial condition we’re using here, all particles have the same energy. But when we run the system we find that the energy distribution for the particles rapidly evolves to the standard exponential form (though note that here successive panels are “snapshots”, not “steps”): And because we’re dealing with “actual particles”, we can look not only at their energies, but also at their speeds (related simply by E = 1/2 m v2). When we look at the distribution of speeds generated by the evolution, we find that it has the classic Maxwellian form: And it’s this kind of final or “equilibrium” result that’s what’s mainly discussed in typical textbooks of thermodynamics. Such books also tend to talk about things like tradeoffs between energy and entropy, and define things like the (Helmholtz) free energy F = U – T S (where U is internal energy, T is temperature and S is entropy) that are used in answering questions like whether particular chemical reactions will occur under certain conditions. But given our discussion of energy here, and our earlier discussion of entropy, it’s at first quite unclear how these quantities might relate, and how they can trade off against each other, say in the formula for free energy. But in some sense what connects energy to the standard definition of entropy in terms of the logarithm of the number of states is the Maxwell–Boltzmann distribution, with its exponential form. In the usual physical setup, the Maxwell–Boltzmann distribution is basically e(–E/kT), where T is the temperature, and kT is the average energy. But now imagine we’re trying to figure out whether some process—say a chemical reaction—will happen. If there’s an energy barrier, say associated with an energy difference Δ, then according to the Maxwell–Boltzmann distribution there’ll be a probability proportional to e(–Δ/kT) for molecules to have a high enough energy to surmount that barrier. But the next question is how many configurations of molecules there are in which molecules will “try to surmount the barrier”. And that’s where the entropy comes in. Because if the number of possible configurations is Ω, the entropy S is given by k log Ω, so that in terms of S, Ω = e(S/k). But now the “average number of molecules which will surmount the barrier” is roughly given by e(S/k) e(–Δ/kT), so that in the end the exponent is proportional to Δ – T S, which has the form of the free energy U – T S. This argument is quite rough, but it captures the essence of what’s going on. And at first it might seem like a remarkable coincidence that there’s a logarithm in the definition of entropy that just “conveniently fits together” like this with the exponential in the Maxwell–Boltzmann distribution. But it’s actually not a coincidence at all. The point is that what’s really fundamental is the concept of counting the number of possible states of a system. But typically this number is extremely large. And we need some way to “tame” it. We could in principle use some slow-growing function other than log to do this. But if we use log (as in the standard definition of entropy) we precisely get the tradeoff with energy in the Maxwell–Boltzmann distribution. There is also another convenient feature of using log. If two systems are independent, one with Ω1 states, and the other with Ω2 states, then a system that combines these (without interaction) will have Ω1, Ω2 states. And if S = k log Ω, then this means that the entropy of the combined state will just be the sum S1 + S2 of the entropies of the individual states. But is this fact actually “fundamentally independent” of the exponential character of the Maxwell–Boltzmann distribution? Well, no. Or at least it comes from the same mathematical idea. Because it’s the fact that in equilibrium the probability ƒ(E) is supposed to satisfy ƒ(E1)ƒ(E2) = ƒ(E3)ƒ(E4) when E1 + E2 = E3 + E4 that makes ƒ(E) have its exponential form. In other words, both stories are about exponentials being able to connect additive combination of one quantity with multiplicative combination of another. Having said all this, though, it’s important to understand that you don’t need energy to talk about entropy. The concept of entropy, as we’ve discussed, is ultimately a computational concept, quite independent of physical notions like energy. In many textbook treatments of thermodynamics, energy and entropy are in some sense put on a similar footing. The First Law is about energy. The Second Law is about entropy. But what we’ve seen here is that energy is really a concept at a different level from entropy: it’s something one gets to “layer on” in discussing physical systems, but it’s not a necessary part of the “computational essence” of how things work. (As an extra wrinkle, in the case of our Physics Project—as to some extent in traditional general relativity and quantum mechanics—there are some fundamental connections between energy and entropy. In particular—related to what we’ll discuss below—the number of possible discrete configurations of spacetime is inevitably related to the “density” of events, which defines energy.)
Computational Foundations for the Second Law of Thermodynamics 33.10 Towards a Formal Proof of the Second Law : It would be nice to be able to say, for example, that “using computation theory, we can prove the Second Law”. But it isn’t as simple as that. Not least because, as we’ve seen, the validity of the Second Law depends on things like what “observers like us” are capable of. But we can, for example, formulate what the outline of a proof of the Second Law could be like, though to give a full formal proof we’d have to introduce a variety of “axioms” (essentially about observers) that don’t have immediate foundations in existing areas of mathematics, physics or computation theory. The basic idea is that one imagines a state S of a system (which could just be a sequence of values for cells in something like a cellular automaton). One considers an “observer function” Θ which, when applied to the state S, gives a “summary” of S. (A very simple example would be the run-length encoding that we used above.) Now we imagine some “evolution function” Ξ that is applied to S. The basic claim of the Second Law is that the “sizes” normally satisfy the inequality Θ[Ξ[S]] ≥ Θ[S], or in other words, that “compression by the observer” is less effective after the evolution of system, in effect because the state of the system has “become more random”, as our informal statement of the Second Law suggests. What are the possible forms of Θ and Ξ? It’s slightly easier to talk about Ξ, because we imagine that this is basically any not-obviously-trivial computation, run for an increasing number of steps. It could be repeated application of a cellular automaton rule, or a Turing machine, or any other computational system. We might represent an individual step by an operator ξ, and say that in effect Ξ = ξt. We can always construct ξt by explicitly applying ξ successively t times. But the question of computational irreducibility is whether there’s a shortcut way to get to the same result. And given any specific representation of ξt (say, rather prosaically, as a Boolean circuit), we can ask how the size of that representation grows with t. With the current state of computation theory, it’s exceptionally difficult to get definitive general results about minimal sizes of ξt, though in sufficiently small cases it’s possible to determine this “experimentally”, essentially by exhaustive search. But there’s an increasing amount of at least circumstantial evidence that for many kinds of systems, one can’t do much better than explicitly constructing ξt, as the phenomenon of computational irreducibility suggests. (One can imagine “toy models”, in which ξ corresponds to some very simple computational process—like a finite automaton—but while this likely allows one to prove things, it’s not at all clear how useful or representative any of the results will be.) OK, so what about the “observer function” Θ? For this we need some kind of “observer theory”, that characterizes what observers—or, at least “observers like us”—can do, in the same kind of way that standard computation theory characterizes what computational systems can do. There are clearly some features Θ must have. For example, it can’t involve unbounded amounts of computation. But realistically there’s more than that. Somehow the role of observers is to take all the details that might exist in the “outside world”, and reduce or compress these to some “smaller” representation that can “fit in the mind of the observer”, and allow the observer to “make decisions” that abstract from the details of the outside world whatever specifics the observer “cares about”. And—like a construction such as a Turing machine—one must in the end have some way of building up “possible observers” from something like basic primitives. Needless to say, even given primitives—or an axiomatic foundation—for Ξ and Θ, things are not straightforward. For example, it’s basically inevitable that many specific questions one might ask will turn out to be formally undecidable. And we can’t expect (particularly as we’ll see later) that we’ll be able to show that the Second Law is “just true”. It’ll be a statement that necessarily involves qualifiers like “typically”. And if we ask to characterize “typically” in terms, say, of “probabilities”, we’ll be stuck in a kind of recursive situation of having to define probability measures in terms of the very same constructs we’re starting from. But despite these difficulties in making what one might characterize as general abstract statements, what our computational formulation achieves is to provide a clear intuitive guide to the origin of the Second Law. And from this we can in particular construct an infinite range of specific computational experiments that illustrate the core phenomenon of the Second Law, and give us more and more understanding of how the Second Law works, and where it conceptually comes from.
Computational Foundations for the Second Law of Thermodynamics 33.11 Maxwell’s Demon and the Character of Observers : Even in the very early years of the formulation of the Second Law, James Clerk Maxwell already brought up an objection to its general applicability, and to the idea that systems “always become more random”. He imagined that a box containing gas molecules had a barrier in the middle with a small door controlled by a “demon” who could decide on a molecule-by-molecule basis which molecules to let through in each direction. Maxwell suggested that such a demon should readily be able to “sort” molecules, thereby reversing any “randomness” that might be developing. As a very simple example, imagine that at the center of our particle cellular automaton we insert a barrier that lets particles pass from left to right but not the reverse. (We also add “reflective walls” on the two ends, rather than having cyclic boundary conditions.) Unsurprisingly, after a short while, all the particles have collected on one side of the barrier, rather than “coming to equilibrium” in a “uniform random distribution” across the system: Over the past century and a half (and even very recently) a whole variety of mechanical ratchets, molecular switches, electrical diodes, noise-reducing signal processors and other devices have been suggested as at least conceptually practical implementations of Maxwell’s demon. Meanwhile, all kinds of objections to their successful operation have been raised. “The demon can’t be made small enough”; “The demon will heat up and stop working”; “The demon will need to reset its memory, so has to be fundamentally irreversible”; “The demon will inevitably randomize things when it tries to sense molecules”; etc. So what’s true? It depends on what we assume about the demon—and in particular to what extent we suppose that the demon needs to be following the same underlying laws as the system it’s operating on. As a somewhat extreme example, let’s imagine trying to “make a demon out of gas molecules”. Here’s an attempt at a simple model of this in our particle cellular automaton: For a while we successfully maintain a “barrier”. But eventually the barrier succumbs to the same “degradation” processes as everything else, and melts away. Can we do better? Let’s imagine that “inside the barrier” (AKA “demon”) there’s “machinery” that whenever the barrier is “buffeted” in a given way “puts up the right kind of armor” to “protect it” from that kind of buffeting. Assuming our underlying system is for example computation universal, we should at some level be able to “implement any computation we want”. (What needs to be done is quite analogous to cellular automata that successfully erase up to finite levels of “noise”.) But there’s a problem. In order to “protect the barrier” we have to be able to “predict” how it will be “attacked”. Or, in other words, our barrier (or demon) will have to be able to systematically determine what the outside system is going to do before it does it. But if the behavior of the outside system is computationally irreducible this won’t in general be possible. So in the end the criterion for a demon like this to be impossible is essentially the same as the criterion for Second Law behavior to occur in the first place: that the system we’re looking at is computationally irreducible. There’s a bit more to say about this, though. We’ve been talking about a demon that’s trying to “achieve something fairly simple”, like maintaining a barrier or a “one-way membrane”. But what if we’re more flexible in what we consider the objective of the demon to be? And even if the demon can’t achieve our original “simple objective” might there at least be some kind of “useful sorting” that it can do? Well, that depends on what we imagine constitutes “useful sorting”. The system is always following its rules to do something. But probably it’s not something we consider “useful sorting”. But what would count as “useful sorting”? Presumably it’s got to be something that an observer will “notice”, and more than that, it should be something that has “done some of the job of decision making” ahead of the observer. In principle a sufficiently powerful observer might be able to “look inside the gas” and see what the results of some elaborate sorting procedure would be. But the point is for the demon to just make the sorting happen, so the job of the observer becomes essentially trivial. But all of this then comes back to the question of what kind of thing an observer might want to observe. In general one would like to be able to characterize this by having an “observer theory” that provides a metatheory of possible observers in something like the kind of way that computation theory and ideas like Turing machines provide a metatheory of possible computational systems. So what really is an observer, or at least an observer like us? The most crucial feature seems to be that the observer is always ultimately some kind of “finite mind” that takes all the complexity of the world and extracts from it just certain “summary features” that are relevant to the “decisions” it has to make. (Another crucial feature seems to be that the observer can consistently view themselves as being “persistent”.) But we don’t have to go all the way to a sophisticated “mind” to see this picture in operation. Because it’s already what’s going on not only in something like perception but also in essentially anything we’d usually call “measurement”. For example, imagine we have a gas containing lots of molecules. A standard measurement might be to find the pressure of the gas. And in doing such a measurement, what’s happening is that we’re reducing the information about all the detailed motions of individual molecules, and just summarizing it by a single aggregate number that is the pressure. How do we achieve this? We might have a piston connected to the box of gas. And each time a molecule hits the piston it’ll push it a little. But the point is that in the end the piston moves only as a whole. And the effects of all the individual molecules are aggregated into that overall motion. At a microscopic level, any actual physical piston is presumably also made out of molecules. But unlike the molecules in the gas, these molecules are tightly bound together to make the piston solid. Every time a gas molecule hits the surface of the piston, it’ll transfer some momentum to a molecule in the piston, and there’ll be some kind of tiny deformation wave that goes through the piston. To get a “definitive pressure measurement”—based on definitive motion of the piston as a whole—that deformation wave will somehow have to disappear. And in making a theory of the “piston as observer” we’ll typically ignore the physical details, and idealize things by saying that the piston moves only as a whole. But ultimately if we were to just look at the system “dispassionately”, without knowing the “intent” of the piston, we’d just see a bunch of molecules in the gas, and a bunch of molecules in the piston. So how would we tell that the piston is “acting as an observer”? In some ways it’s a rather circular story. If we assume that there’s a particular kind of thing an observer wants to measure, then we can potentially identify parts of a system that “achieve that measurement”. But in the abstract we don’t know what an observer “wants to measure”. We’ll always see one part of a system affecting another. But is it “achieving measurement” or not? To resolve this, we have to have some kind of metatheory of the observer: we have to be able to say what kinds of things we’re going to count as observers and what not. And ultimately that’s something that must inevitably devolve to a rather human question. Because in the end what we care about is what we humans sense about the world, which is what, for example, we try to construct science about. We could talk very specifically about the sensory apparatus that we humans have—or that we’ve built with technology. But the essence of observer theory should presumably be some kind of generalization of that. Something that recognizes fundamental features—like computational boundedness—of us as entities, but that does not depend on the fact that we happen to use sight rather than smell as our most important sense. The situation is a bit like the early development of computation theory. Something like a Turing machine was intended to define a mechanism that roughly mirrored the computational capabilities of the human mind, but that also provided a “reasonable generalization” that covered, for example, machines one could imagine building. Of course, in that particular case the definition that was developed proved extremely useful, being, it seems, of just the right generality to cover computations that can occur in our universe—but not beyond. And one might hope that in the future observer theory would identify a similarly useful definition for what a “reasonable observer” can be. And given such a definition, we will, for example, be in position to further tighten up our characterization of what the Second Law might say. It may be worth commenting that in thinking about an observer as being an “entity like us” one of the immediate attributes we might seek is that the observer should have some kind of “inner experience”. But if we’re just looking at the pattern of molecules in a system, how do we tell where there’s an “inner experience” happening? From the outside, we presumably ultimately can’t. And it’s really only possible when we’re “on the inside”. We might have scientific criteria that tell us whether something can reasonably support an inner experience. But to know if there actually is an inner experience “going on” we basically have to be experiencing it. We can’t make a “first-principles” objective theory; we just have to posit that such-and-such part of the system is representing our subjective experience. Of course, that doesn’t mean that there can’t still be very general conclusions to be drawn. Because it can still be—as it is in our Physics Project and in thinking about the ruliad—that it takes knowing only rather basic features of “observers like us” to be able to make very general statements about things like the effective laws we will experience.
Computational Foundations for the Second Law of Thermodynamics 33.12 The Heat Death of the Universe : It didn’t take long after the Second Law was first proposed for people to start talking about its implications for the long-term evolution of the universe. If “randomness” (for example as characterized by entropy) always increases, doesn’t that mean that the universe must eventually evolve to a state of “equilibrium randomness”, in which all the rich structures we now see have decayed into “random heat”? There are several issues here. But the most obvious has to do with what observer one imagines will be experiencing that future state of the universe. After all, if the underlying rules which govern the universe are reversible, then in principle it will always be possible to go back from that future “random heat” and reconstruct from it all the rich structures that have existed in the history of the universe. But the point of the Second Law as we’ve discussed it is that at least for computationally bounded observers like us that won’t be possible. The past will always in principle be determinable from the future, but it will take irreducibly much computation to do so—and vastly more than observers like us can muster. And along the same lines, if observers like us examine the future state of the universe we won’t be able to see that there’s anything special about it. Even though it came from the “special state” that is the current state of our universe, we won’t be able to tell it from a “typical” state, and we’ll just consider it “random”. But what if the observers evolve with the evolution of the universe? Yes, to us today that future configuration of particles may just “look random”. But in actuality, it has rich computational content that there’s no reason to assume a future observer will not find in some way or another significant. Indeed, in a sense the longer the universe has been around, the larger the amount of irreducible computation it will have done. And, yes, observers like us today might not care about most of what comes out of that computation. But in principle there are features of it that could be mined to inform the “experience” of future observers. At a practical level, our basic human senses pick out certain features on certain scales. But as technology progresses, it gives us ways to pick out much more, on much finer scales. A century ago we couldn’t realistically pick out individual atoms or individual photons; now we routinely can. And what seemed like “random noise” just a few decades ago is now often known to have specific, detailed structure. There is, however, a complex tradeoff. A crucial feature of observers like us is that there is a certain coherence to our experience; we sample little enough about the world that we’re able to turn it into a coherent thread of experience. But the more an observer samples, the more difficult this will become. So, yes, a future observer with vastly more advanced technology might successfully be able to sample lots of details of the future universe. But to do that, the observer will have to lose some of their own coherence, and ultimately we won’t even be able to identify that future observer as “coherently existing” at all. The usual “heat death of the universe” refers to the fate of matter and other particles in the universe. But what about things like gravity and the structure of spacetime? In traditional physics, that’s been a fairly separate question. But in our Physics Project everything is ultimately described in terms of a single abstract structure that represents both space and everything in it. And we can expect that the evolution of this whole structure then corresponds to a computationally irreducible process. The basic setup is at its core just like what we’ve seen in our general discussion of the Second Law. But here we’re operating at the lowest level of the universe, so the irreducible progression of computation can be thought of as representing the fundamental inexorable passage of time. As time moves forward, therefore, we can generally expect “more randomness” in the lowest-level structure of the universe. But what will observers perceive? There’s considerable trickiness here—particularly in connection with quantum mechanics—that we’ll discuss later. In essence, the point is that there are many paths of history for the universe, that branch and merge—and observers sample certain collections of paths. And for example on some paths the computations may simply halt, with no further rules applying—so that in effect “time stops”, at least for observers on those paths. It’s a phenomenon that can be identified with spacetime singularities, and with what happens inside (at least certain) black holes. So does this mean that the universe might “just stop”, in effect ending with a collection of black holes? It’s more complicated than that. Because there are always other paths for observers to follow. Some correspond to different quantum possibilities. But ultimately what we imagine is that our perception of the universe is a sampling from the whole ruliad—the limiting entangled structure formed by running all abstractly possible computations forever. And it’s a feature of the construction of the ruliad that it’s infinite. Individual paths in it can halt, but the whole ruliad goes on forever. So what does this mean about the ultimate fate of the universe? Much like the situation with heat death, specific observers may conclude that “nothing interesting is happening anymore”. But something always will be happening, and in fact that something will represent the accumulation of larger and larger amounts of irreducible computation. It won’t be possible for an observer to encompass all this while still themselves “remaining coherent”. But as we’ll discuss later there will inexorably be pockets of computational reducibility for which coherent observers can exist, although what those observers will perceive is likely to be utterly incoherent with anything that we as observers now perceive. The universe does not fundamentally just “descend into randomness”. And indeed all the things that exist in our universe today will ultimately be encoded in some way forever in the detailed structure that develops. But what the core phenomenon of the Second Law suggests is that at least many aspects of that encoding will not be accessible to observers like us. The future of the universe will transcend what we so far “appreciate”, and will require a redefinition of what we consider meaningful. But it should not be “taken for dead” or dismissed as being just “random heat”. It’s just that to find what we consider interesting, we may in effect have to migrate across the ruliad.
Computational Foundations for the Second Law of Thermodynamics 33.13 Traces of Initial Conditions : The Second Law gives us the expectation that so long as we start from “reasonable” initial conditions, we should always evolve to some kind of “uniformly random” configuration that we can view as a “unique equilibrium state” that’s “lost any meaningful memory” of the initial conditions. But now that we’ve got ways to explore the Second Law in specific, simple computational systems, we can explicitly study the extent to which this expectation is upheld. And what we’ll find is that even though as a general matter it is, there can still be exceptions in which traces of initial conditions can be preserved at least long into the evolution. Let’s look again at our “particle cellular automaton” system. We saw above that the evolution of an initial “blob” (here of size 17 in a system with 30 cells) leads to configurations that typically look quite random: But what about other initial conditions? Here are some samples of what happens: In some cases we again get what appears to be quite random behavior. But in other cases the behavior looks much more structured. Sometimes this is just because there’s a short recurrence time: And indeed the overall distribution of recurrence times falls off in a first approximation exponentially (though with a definite tail): But the distribution is quite broad—with a mean of more than 50,000 steps. (The 17-particle initial blob gives a recurrence time of 155,150 steps.) So what happens with “typical” initial conditions that don’t give short recurrences? Here’s an example: What’s notable here is that unlike for the case of the “simple blob”, there seem to be identifiable traces of the initial conditions that persist for a long time. So what’s going on—and how does it relate to the Second Law? Given the basic rules for the particle cellular automaton we immediately know that at least a couple of aspects of the initial conditions will persist forever. In particular, the rules conserve the total number of “particles” (i.e. non-white cells) so that: In addition, the number of light or dark cells can change only by increments of 2, and therefore their total number must remain either always even or always odd—and combined with overall particle conservation this then implies that: What about other conservation laws? We can formulate the conservation of total particle number as saying that the number of instances of “length-1 blocks” with weights specified as follows is always constant: Then we can go on and ask about conservation laws associated with longer blocks. For blocks of length 2, there are no new nontrivial conservation laws, though for example the weighted combination of blocks is nominally “conserved”—but only because it is 0 for any possible configuration. But in addition to such global conservation laws, there are also more local kinds of regularities. For example, a single “light particle” on its own just stays fixed, and a pair of light particles can always trap a single dark particle between them: For any separation of light particles, it turns out to always be possible to trap any number of dark particles: But not every initial configuration of dark particles gets trapped. With separation s and d dark particles, there are a total of Binomial[s,d] possible initial configurations. For d = 2, a fraction (s – 3)/(s – 1) of these get trapped while the rest do not. For d = 3, the fraction becomes (s – 3)(s – 4)/(s(s – 1)) and for d = 4 it is (s – 4)(s – 5)/(s(s – 1)). (For larger d, the trapping fraction continues to be a rational function of s, but the polynomials involved rapidly become more complicated.) For sufficiently large separation s the trapping fraction always goes to 1—though does so more slowly as d increases: What’s basically going on is that a single dark particle always just “bounces off” a light particle: But a pair of dark particles can “go through” the light particle, shifting it slightly: Different things happen with different configurations of dark particles: And with more complicated “barriers” the behavior can depend in detail on precise phase and separation relationships: But the basic point is that—although there are various ways they can be modified or destroyed—“light particle walls” can persist for a least a long time. And the result is that if such walls happen to occur in an initial condition they can at least significantly slow down “degradation to randomness”. For example, this shows evolution over the course of 200,000 steps from a particular initial condition, sampled every 20,000 steps—and even over all these steps we see that there’s definite “wall structure” that survives: Let’s look at a simpler case: a single light particle surrounded by a few dark particles: If we plot the position of the light particle we see that for thousands of steps it just jiggles around but if one runs it long enough it shows systematic motion at a rate of about 1 position every 1300 steps, wrapping around the cyclic boundary conditions, and eventually returning to its starting point—at the recurrence time of 46,836 steps: What does all this mean? Essentially the point is that even though something like our particle cellular automaton exhibits computational irreducibility and often generates “featureless” apparent randomness, a system like this is also capable of exhibiting computational reducibility in which traces of the initial conditions can persist, and there isn’t just “generic randomness generation”. Computational irreducibility is a powerful force. But, as we’ll discuss below, its very presence implies that there must inevitably also be “pockets” of computational reducibility. And once again (as we’ll discuss below) it’s a question of the observer how obvious or not these pockets may be in a particular case, and whether—say for observers like us—they affect what we perceive in terms of the operation of the Second Law. It’s worth commenting that such issues are not just a feature of systems like our particle cellular automaton. And indeed they’ve appeared—stretching all the way back to the 1950s—pretty much whenever detailed simulations have been done of systems that one might expect would show “Second Law” behavior. The story is typically that, yes, there is apparent randomness generated (though it’s often barely studied as such), just as the Second Law would suggest. But then there’s a big surprise of some kind of unexpected regularity. In arrays of nonlinear springs, there were solitons. In hard-sphere gases, there were “long-time tails”—in which correlations in the motion of spheres were seen to decay not exponentially in time, but rather like power laws. The phenomenon of long-time tails is actually visible in the cellular automaton “approximation” to hard-sphere gases that we studied above. And its interpretation is a good example of how computational reducibility manifests itself. At a small scale, the motion of our idealized molecules shows computational irreducibility and randomness. But on a larger scale, it’s more like “collective hydrodynamics”, with fluid mechanics effects like vortices. And it’s these much-simpler-to-describe computationally reducible effects that lead to the “unexpected regularities” associated with long-time tails. 
Computational Foundations for the Second Law of Thermodynamics 33.14 When the Second Law Works, and When It Doesn’t : At its core, the Second Law is about evolution from orderly “simple” initial conditions to apparent randomness. And, yes, this is a phenomenon we can certainly see happen in things like hard-sphere gases in which we’re in effect emulating the motion of physical gas molecules. But what about systems with other underlying rules? Because we’re explicitly doing everything computationally, we’re in a position to just enumerate possible rules (i.e. possible programs) and see what they do. As an example, here are the distinct patterns produced by all 288 3-color reversible block cellular automata that don’t change the all-white state (but don’t necessarily conserve “particle number”): As is typical to see in the computational universe of simple programs, there’s quite a diversity of behavior. Often we see it “doing the Second Law thing” and “decaying” to apparent randomness although sometimes taking a while to do so: But there are also cases where the behavior just stays simple forever as well as other cases where it takes a fairly long time before it’s clear what’s going to happen: In many ways, the most surprising thing here is that such simple rules can generate randomness. And as we’ve discussed, that’s in the end what leads to the Second Law. But what about rules that don’t generate randomness, and just produce simple behavior? Well, in these cases the Second Law doesn’t apply. In standard physics, the Second Law is often applied to gases—and indeed this was its very first application area. But to a solid whose atoms have stayed in more or less fixed positions for a billion years, it really doesn’t usefully apply. And the same is true, say, for a line of masses connected by perfect springs, with perfect linear behavior. There’s been a quite pervasive assumption that the Second Law is somehow always universally valid. But it’s simply not true. The validity of the Second Law is associated with the phenomenon of computational irreducibility. And, yes, this phenomenon is quite ubiquitous. But there are definitely systems and situations in which it does not occur. And those will not show “Second Law” behavior. There are plenty of complicated “marginal” cases, however. For example, for a given rule (like the 3 shown here), some initial conditions may not lead to randomness and “Second Law behavior”, while others do: And as is so often the case in the computational universe there are phenomena one never expects, like the strange “shock-front-like” behavior of the third rule, which produces randomness, but only on a scale determined by the region it’s in: It’s worth mentioning that while restricting to a finite region often yields behavior that more obviously resembles a “box of gas molecules”, the general phenomenon of randomness generation also occurs in infinite regions. And indeed we already know this from the classic example of rule 30. But here it is in a reversible block cellular automaton: In some simple cases the behavior just repeats, but in other cases it’s nested albeit sometimes in rather complicated ways:
Computational Foundations for the Second Law of Thermodynamics 33.15 The Second Law and Order in the Universe : Having identified the computational nature of the core phenomenon of the Second Law we can start to understand in full generality just what the range of this phenomenon is. But what about the ordinary Second Law as it might be applied to familiar physical situations? Does the ubiquity of computational irreducibility imply that ultimately absolutely everything must “degrade to randomness”? We saw in the previous section that there are underlying rules for which this clearly doesn’t happen. But what about with typical “real-world” systems involving molecules? We’ve seen lots of examples of idealized hard-sphere gases in which we observe randomization. But—as we’ve mentioned several times—even when there’s computational irreducibility, there are always pockets of computational reducibility to be found. And for example the fact that simple overall gas laws like PV = constant apply to our hard-sphere gas can be viewed as an example of computational reducibility. And as another example, consider a hard-sphere gas in which vortex-like circulation has been set up. To get a sense of what happens we can just look at our simple discrete model. At a microscopic level there’s clearly lots of apparent randomness, and it’s hard to see what’s globally going on: But if we coarse grain the system by 3×3 blocks of cells with “average velocities” we see that there’s a fairly persistent hydrodynamic-like vortex that can be identified: Microscopically, there’s computational irreducibility and apparent randomness. But macroscopically the particular form of coarse-grained measurement we’re using picks out a pocket of reducibility—and we see overall behavior whose obvious features don’t show “Second-Law-style” randomness. And in practice this is how much of the “order” we see in the universe seems to work. At a small scale there’s all sorts of computational irreducibility and randomness. But on a larger scale there are features that we as observers notice that tap into pockets of reducibility, and that show the kind of order that we can describe, for example, with simple mathematical laws. There’s an extreme version of this in our Physics Project, where the underlying structure of space—like the underlying structure of something like a gas—is full of computational irreducibility, but where there are certain overall features that observers like us notice, and that show computational reducibility. One example involves the large-scale structure of spacetime, as described by general relativity. Another involves the identification of particles that can be considered to “move without change” through the system. One might have thought—as people often have—that the Second Law would imply a degradation of every feature of a system to uniform randomness. But that’s just not how computational irreducibility works. Because whenever there’s computational irreducibility, there are also inevitably an infinite number of pockets of computational reducibility. (If there weren’t, that very fact could be used to “reduce the irreducibility”.) And what that means is that when there’s irreducibility and Second-Law-like randomization, there’ll also always be orderly laws to be found. But which of those laws will be evident—or relevant—to a particular observer depends on the nature of that observer. The Second Law is ultimately a story of the mismatch between the computational irreducibility of underlying systems, and the computational boundedness of observers like us. But the point is that if there’s a pocket of computational reducibility that happens to be “a fit” for us as observers, then despite our computational limitations, we’ll be perfectly able to recognize the orderliness that’s associated with it—and we won’t think that the system we’re looking at has just “degraded to randomness”. So what this means is that there’s ultimately no conflict between the existence of order in the universe, and the operation of the Second Law. Yes, there’s an “ocean of randomness” generated by computational irreducibility. But there’s also inevitably order that lives in pockets of reducibility. And the question is just whether a particular observer “notices” a given pocket of reducibility, or whether they only “see” the “background” of computational irreducibility. In the “hydrodynamics” example above, the “observer” picks out a “slice” of behavior by looking at aggregated local averages. But another way for an observer to pick out a “slice” of behavior is just to look only at a specific region in a system. And in that case one can observe simpler behavior because in effect “the complexity has radiated away”. For example, here are reversible cellular automata where a random initial block is “simplified” by “radiating its information out”: If one picked up all those “pieces of radiation” one would be able—with appropriate computational effort—to reconstruct all the randomness in the initial condition. But if we as observers just “ignore the radiation to infinity” then we’ll again conclude that the system has evolved to a simpler state—against the “Second-Law trend” of increasing randomness.
Computational Foundations for the Second Law of Thermodynamics 33.16 Class 4 and the Mechanoidal Phase : When I first studied cellular automata back in the 1980s, I identified four basic classes of behavior that are seen when starting from generic initial conditions—as exemplified by: Class 1 essentially always evolves to the same final “fixed-point” state, immediately destroying information about its initial state. Class 2, however, works a bit like solid matter, essentially just maintaining whatever configuration it was started in. Class 3 works more like a gas or a liquid, continually “mixing things up” in a way that looks quite random. But class 4 does something more complicated. In class 3 there aren’t significant identifiable persistent structures, and everything always seems to quickly get randomized. But the distinguishing feature of class 4 is the presence of identifiable persistent structures, whose interactions effectively define the activity of the system. So how do these types of behavior relate to the Second Law? Class 1 involves intrinsic irreversibility, and so doesn’t immediately connect to standard Second Law behavior. Class 2 is basically too static to follow the Second Law. But class 3 shows quintessential Second Law behavior, with rapid evolution to “typical random states”. And it’s class 3 that captures the kind of behavior that’s seen in typical Second Law systems, like gases. But what about class 4? Well, it’s a more complicated story. The “level of activity” in class 4—while above class 2—is in a sense below class 3. But unlike in class 3, where there is typically “too much activity” to “see what’s going on”, class 4 often gives one the idea that it’s operating in a “more potentially understandable” way. There are many different detailed kinds of behavior that appear in class 4 systems. But here are a few examples in reversible block cellular automata: Looking at the first rule, it’s easy to identify some simple persistent structures, some stationary, some moving: But even with this rule, many other things can happen too and in the end the whole behavior of the system is built up from combinations and interactions of structures like these. The second rule above behaves in an immediately more elaborate way. Here it is starting from a random initial condition: Starting just from  one gets: Sometimes the behavior seems simpler though even in the last case here, there is elaborate “number-theoretical” behavior that seems to never quite become either periodic or nested: We can think of any cellular automaton—or any system based on rules—as “doing a computation” when it evolves. Class 1 and 2 systems basically behave in computationally simple ways. But as soon as we reach class 3 we’re dealing with computational irreducibility, and with a “density of computation” that lets us decode almost nothing about what comes out, with the result that what we see we can basically describe only as “apparently random”. Class 4 no doubt has the same ultimate computational irreducibility—and the same ultimate computational capabilities—as class 3. But now the computation is “less dense”, and seemingly more accessible to human interpretation. In class 3 it’s difficult to imagine making any kind of “symbolic summary” of what’s going on. But in class 4, we see definite structures whose behavior we can imagine being able to describe in a symbolic way, building up what we can think of as a “human-accessible narrative” in which we talk about “structure X collides with structure Y to produce structure Z” and so on. And indeed if we look at the picture above, it’s not too difficult to imagine that it might correspond to the execution trace of a computation we might do. And more than that, given the “identifiable components” that arise in class 4 systems, one can imagine assembling these to explicitly set up particular computations one wants to do. In a class 3 system “randomness” always just “spurts out”, and one has very little ability to “meaningfully control” what happens. But in a class 4 system, one can potentially do what amounts to traditional engineering or programming to set up an arrangement of identifiable component “primitives” that achieves some particular purpose one has chosen. And indeed in a case like the rule 110 cellular automaton we know that it’s possible to perform any computation in this way, proving that the system is capable of universal computation, and providing a piece of evidence for the phenomenon of computational irreducibility. No doubt rule 30 is also computation universal. But the point is that with our current ways of analyzing things, class 3 systems like this don’t make this something we can readily recognize. Like so many other things we’re discussing, this is basically again a story of observers and their capabilities. If observers like us—with our computational boundedness—are going to be able to “get things into our minds” we seem to need to break them down to the point where they can be described in terms of modest numbers of types of somewhat-independent parts. And that’s what the “decomposition into identifiable structures” that we observe in class 4 systems gives us the opportunity to do. What about class 3? Notwithstanding things like our discussion of traces of initial conditions above, our current powers of perception just don’t seem to let us “understand what’s going on” to the point where we can say much more than there’s apparent randomness. And of course it’s this very point that we’re arguing is the basis for the Second Law. Could there be observers who could “decode class 3 systems”? In principle, absolutely yes. And even if the observers—like us—are computationally bounded, we can expect that there will be at least some pockets of computational reducibility that could be found that would allow progress to be made. But as of now—with the methods of perception and analysis currently at our disposal—there’s something very different for us about class 3 and class 4. Class 3 shows quintessential “apparently random” behavior, like molecules in a gas. Class 4 shows behavior that looks more like the “insides of a machine” that could have been “intentionally engineered for a purpose”. Having a system that is like this “in bulk” is not something familiar, say from physics. There are solids, and liquids, and gases, whose components have different general organizational characteristics. But what we see in class 4 is something yet different—and quite unfamiliar. Like solids, liquids and gases, it’s something that can exist “in bulk”, with any number of components. We can think of it as a “phase” of a system. But it’s a new type of phase, that we might call a “mechanoidal phase”. How do we recognize this phase? Again, it’s a question of the observer. Something like a solid phase is easy for observers like us to recognize. But even the distinction between a liquid and a gas can be more difficult to recognize. And to recognize the mechanoidal phase we basically have to be asking something like “Is this a computation we recognize?” How does all this relate to the Second Law? Class 3 systems—like gases—immediately show typical “Second Law” behavior, characterized by randomness, entropy increase, equilibrium, and so on. But class 4 systems work differently. They have new characteristics that don’t fit neatly into the rubric of the Second Law. No doubt one day we will have theories of the mechanoidal phase just like today we have theories of gases, of liquids and of solids. Likely those theories will have to get more sophisticated in characterizing the observer, and in describing what kinds of coarse graining can reasonably be done. Presumably there will be some kind of analog of the Second Law that leverages the difference between the capabilities and features of the observer and the system they’re observing. But in the mechanoidal phase there is in a sense less distance between the mechanism of the system and the mechanism of the observer, so we probably can’t expect a statement as ultimately simple and clear-cut as the usual Second Law. 
Computational Foundations for the Second Law of Thermodynamics 33.17 The Mechanoidal Phase and Bulk Molecular Biology : The Second Law has long had an uneasy relationship with biology. “Physical” systems like gases readily show the “decay” to randomness expected from the Second Law. But living systems instead somehow seem to maintain all sorts of elaborate organization that doesn’t immediately “decay to randomness”— and indeed actually seems able to grow just through “processes of biology”. It’s easy to point to the continual absorption of energy and material by living systems—as well as their eventual death and decay—as reasons why such systems might still at least nominally follow the Second Law. But even if at some level this works, it’s not particularly useful in letting us talk about the actual significant “bulk” features of living systems—in the kind of way that the Second Law routinely lets us make “bulk” statements about things like gases. So how might we begin to describe living systems “in bulk”? I suspect a key is to think of them as being in large part in what we’re here calling the mechanoidal phase. If one looks inside a living organism at a molecular scale, there are some parts that can reasonably be described as solid, liquid or gas. But what molecular biology has increasingly shown is that there’s often much more elaborate molecular-scale organization than exist in those phases—and moreover that at least at some level this organization seems “describable” and “machine-like”, with molecules and collections of molecules that we can say have “particular functions”, often being “carefully” and actively transported by things like the cytoskeleton. In any given organism, there are for example specific proteins defined by the genomics of the organism, that behave in specific ways. But one suspects that there’s also a higher-level or “bulk” description that allows one to make at least some kinds of general statements. There are already some known general principles in biology—like the concept of natural selection, or the self-replicating digital character of genetic information—that let one come to various conclusions independent of microscopic details. And, yes, in some situations the Second Law provides certain kinds of statements about biology. But I suspect that there are much more powerful and significant principles to be discovered, that in fact have the potential to unlock a whole new level of global understanding of biological systems and processes. It’s perhaps worth mentioning an analogy in technology. In a microprocessor what we can think of as the “working fluid” is essentially a gas of electrons. At some level the Second Law has things to say about this gas of electrons, for example describing scattering processes that lead to electrical resistance. But the vast majority of what matters in the behavior of this particular gas of electrons is defined not by things like this, but by the elaborate pattern of wires and switches that exist in the microprocessor, and that guide the motion of the electrons. In living systems one sometimes also cares about the transport of electrons—though more often it’s atoms and ions and molecules. And living systems often seem to provide what one can think of as a close analog of wires for transporting such things. But what is the arrangement of these “wires”? Ultimately it’ll be defined by the application of rules derived from things like the genome of the organism. Sometimes the results will for example be analogous to crystalline or amorphous solids. But in other cases one suspects that it’ll be better described by something like the mechanoidal phase. Quite possibly this may also provide a good bulk description of technological systems like microprocessors or large software codebases. And potentially then one might be able to have high-level laws—analogous to the Second Law—that would make high-level statements about these technological systems. It’s worth mentioning that a key feature of the mechanoidal phase is that detailed dynamics—and the causal relations it defines—matter. In something like a gas it’s perfectly fine for most purposes to assume “molecular chaos”, and to say that molecules are arbitrarily mixed. But the mechanoidal phase depends on the “detailed choreography” of elements. It’s still a “bulk phase” with arbitrarily many elements. But things like the detailed history of interactions of each individual element matter. In thinking about typical chemistry—say in a liquid or gas phase—one’s usually just concerned with overall concentrations of different kinds of molecules. In effect one assumes that the “Second Law has acted”, and that everything is “mixed randomly” and the causal histories of molecules don’t matter. But it’s increasingly clear that this picture isn’t correct for molecular biology, with all its detailed molecular-scale structures and mechanisms. And instead it seems more promising to model what’s there as being in the mechanoidal phase. So how does this relate to the Second Law? As we’ve discussed, the Second Law is ultimately a reflection of the interplay between underlying computational irreducibility and the limited computational capabilities of observers like us. But within computational irreducibility there are inevitably always “pockets” of computational reducibility—which the observer may or may not care about, or be able to leverage. In the mechanoidal phase there is ultimately computational irreducibility. But a defining feature of this phase is the presence of “local computational reducibility” visible in the existence of identifiable localized structures. Or, in other words, even to observers like us, it’s clear that the mechanoidal phase isn’t “uniformly computationally irreducible”. But just what general statements can be made about it will depend—potentially in some detail—on the characteristics of the observer. We’ve managed to get a long way in discussing the Second Law—and even more so in doing our Physics Project—by making only very basic assumptions about observers. But to be able to make general statements about the mechanoidal phase—and living systems—we’re likely to have to say more about observers. If one’s presented with a lump of biological tissue one might at first just describe it as some kind of gel. But we know there’s much more to it. And the question is what features we can perceive. Right now we can see with microscopes all kinds of elaborate spatial structures. Perhaps in the future there’ll be technology that also lets us systematically detect dynamic and causal structures. And it’ll be the interplay of what we perceive with what’s computationally going on underneath that’ll define what general laws we will be able to see emerge. We already know we won’t just get the ordinary Second Law. But just what we will get isn’t clear. But somehow—perhaps in several variants associated with different kinds of observers—what we’ll get will be something like “general laws of biology”, much like in our Physics Project we get general laws of spacetime and of quantum mechanics, and in our analysis of metamathematics we get “general laws of mathematics”.
Computational Foundations for the Second Law of Thermodynamics 33.18 The Thermodynamics of Spacetime : Traditional twentieth-century physics treats spacetime a bit like a continuous fluid, with its characteristics being defined by the continuum equations of General Relativity. Attempts to align this with quantum field theory led to the idea of attributing an entropy to black holes, in essence to represent the number of quantum states “hidden” by the event horizon of the black hole. But in our Physics Project there is a much more direct way of thinking about spacetime in what amount to thermodynamic terms. A key idea of our Physics Project is that there’s something “below” the “fluid” representation of spacetime—and in particular that space is ultimately made of discrete elements, whose relations (which can conveniently be represented by a hypergraph) ultimately define everything about the structure of space. This structure evolves according to rules that are somewhat analogous to those for block cellular automata, except that now one is doing replacements not for blocks of cell values, but instead for local pieces of the hypergraph. So what happens in a system like this? Sometimes the behavior is simple. But very often—much like in many cellular automata—there is great complexity in the structure that develops even from simple initial conditions: It’s again a story of computational irreducibility, and of the generation of apparent randomness. The notion of “randomness” is a bit less straightforward for hypergraphs than for arrays of cell values. But what ultimately matters is what “observers like us” perceive in the system. A typical approach is to look at geodesic balls that encompass all elements within a certain graph distance of a given element—and then to study the effective geometry that emerges in the large-scale limit. It’s then a bit like seeing fluid dynamics emerge from small-scale molecular dynamics, except that here (after navigating many technical issues) it’s the Einstein equations of General Relativity that emerge. But the fact that this can work relies on something analogous to the Second Law. It has to be the case that the evolution of the hypergraph leads at least locally to something that can be viewed as “uniformly random”, and on which statistical averages can be done. In effect, the microscopic structure of spacetime is reaching some kind of “equilibrium state”, whose detailed internal configuration “seems random”—but which has definite “bulk” properties that are perceived by observers like us, and give us the impression of continuous spacetime. As we’ve discussed above, the phenomenon of computational irreducibility means that apparent randomness can arise completely deterministically just by following simple rules from simple initial conditions. And this is presumably what basically happens in the evolution and “formation” of spacetime. (There are some additional complications associated with multicomputation that we’ll discuss at least to some extent later.) But just like for the systems like gases that we’ve discussed above, we can now start talking directly about things like entropy for spacetime. As “large-scale observers” of spacetime we’re always effectively doing coarse graining. So now we can ask how many microscopic configurations of spacetime (or space) are consistent with whatever result we get from that coarse graining. As a toy example, consider just enumerating all possible graphs (say up to a given size), then asking which of them have a certain pattern of volumes for geodesic balls (i.e. a certain sequence of numbers of distinct nodes within a given graph distance of a particular node). The “coarse-grained entropy” is simply determined by the number of graphs in which the geodesic ball volumes start in the same way. Here are all trivalent graphs (with up to 24 nodes) that have various such geodesic ball “signatures” (most, but not all, turn out to be vertex transitive; these graphs were found by filtering a total of 125,816,453 possibilities): We can think of the different numbers of graphs in each case as representing different entropies for a tiny fragment of space constrained to have a given “coarse-grained” structure. At the graph sizes we’re dealing with here, we’re very far from having a good approximation to continuum space. But assume we could look at much larger graphs. Then we might ask how the entropy varies with “limiting geodesic ball signature”—which in the continuum limit is determined by dimension, curvature, etc. For a general “disembodied lump of spacetime” this is all somewhat hard to define, particularly because it depends greatly on issues of “gauge” or of how the spacetime is foliated into spacelike slices. But event horizons, being in a sense much more global, don’t have such issues, and so we can expect to have fairly invariant definitions of spacetime entropy in this case. And the expectation would then be that for example the entropy we would compute would agree with the “standard” entropy computed for example by analyzing quantum fields or strings near a black hole. But with the setup we have here we should also be able to ask more general questions about spacetime entropy—for example seeing how it varies with features of arbitrary gravitational fields. In most situations the spacetime entropy associated with any spacetime configuration that we can successfully identify at our coarse-grained level will be very large. But if we could ever find a case where it is instead small, this would be somewhere we could expect to start seeing a breakdown of the continuum “equilibrium” structure of spacetime, and where evidence of discreteness should start to show up. We’ve so far mostly been discussing hypergraphs that represent instantaneous states of space. But in talking about spacetime we really need to consider causal graphs that map out the causal relationships between updating events in the hypergraph, and that represent the structure of spacetime. And once again, such graphs can show apparent randomness associated with computational irreducibility. One can make causal graphs for all sorts of systems. Here is one for a “Newton’s cradle” configuration of an (effectively 1D) hard-sphere gas, in which events are collisions between spheres, and two events are causally connected if a sphere goes from one to the other: And here is an example for a 2D hard-sphere case, with the causal graph now reflecting the generation of apparently random behavior: Similar to this, we can make a causal graph for our particle cellular automaton, in which we consider it an event whenever a block changes (but ignore “no-change updates”): For spacetime, features of the causal graph have some definite interpretations. We define the reference frame we’re using by specifying a foliation of the causal graph. And one of the results of our Physics Project is then that the flux of causal edges through the spacelike hypersurfaces our foliation defines can be interpreted directly as the density of physical energy. (The flux through timelike hypersurfaces gives momentum.) One can make a surprisingly close analogy to causal graphs for hard-sphere gases—except that in a hard-sphere gas the causal edges correspond to actual, nonrelativistic motion of idealized molecules, while in our model of spacetime the causal edges are abstract connections that are in effect always lightlike (i.e. they correspond to motion at the speed of light). In both cases, reducing the number of events is like reducing some version of temperature—and if one approaches no-event “absolute zero” both the gas and spacetime will lose their cohesion, and no longer allow propagation of effects from one part of the system to another. If one increases density in the hard-sphere gas one will eventually form something like a solid, and in this case there will be a regular arrangement of both spheres and the causal edges. In spacetime something similar may happen in connection with event horizons—which may behave like an “ordered phase” with causal edges aligned. What happens if one combines thinking about spacetime and thinking about matter? A long-unresolved issue concerns systems with many gravitationally attracting bodies—say a “gas” of stars or galaxies. While the molecules in an ordinary gas might evolve to an apparently random configuration in a standard “Second Law way”, gravitationally attracting bodies tend to clump together to make what seem like “progressively simpler” configurations. It could be that this is a case where the standard Second Law just doesn’t apply, but there’s long been a suspicion that the Second Law can somehow be “saved” by appropriately associating an entropy with the structure of spacetime. In our Physics Project, as we’ve discussed, there’s always entropy associated with our coarse-grained perception of spacetime. And it’s conceivable that, at least in terms of overall counting of states, increased “organization” of matter could be more than balanced by enlargement in the number of available states for spacetime. We’ve discussed at length above the idea that “Second Law behavior” is the result of us as observers (and preparers of initial states) being “computationally weak” relative to the computational irreducibility of the underlying dynamics of systems. And we can expect that very much the same thing will happen for spacetime. But what if we could make a Maxwell’s demon for spacetime? What would this mean? One rather bizarre possibility is that it could allow faster-than-light “travel”. Here’s a rough analogy. Gas molecules—say in air in a room—move at roughly the speed of sound. But they’re always colliding with other molecules, and getting their directions randomized. But what if we had a Maxwell’s-demon-like device that could tell us at every collision which molecule to ride on? With an appropriate choice for the sequence of molecules we could then potentially “surf” across the room at roughly the speed of sound. Of course, to have the device work it’d have to overcome the computational irreducibility of the basic dynamics of the gas. In spacetime, the causal graph gives us a map of what event can affect what other event. And insofar as we just treat spacetime as “being in uniform equilibrium” there’ll be a simple correspondence between “causal distance” and what we consider distance in physical space. But if we look down at the level of individual causal edges it’ll be more complicated. And in general we could imagine that an appropriate “demon” could predict the microscopic causal structure of spacetime, and carefully pick causal edges that could “line up” to “go further in space” than the “equilibrium expectation”. Of course, even if this worked, there’s still the question of what could be “transported” through such a “tunnel”—and for example even a particle (like an electron) presumably involves a vast number of causal edges, that one wouldn’t be able to systematically organize to fit through the tunnel. But it’s interesting to realize that in our Physics Project the idea that “nothing can go faster than light” becomes something very much analogous to the Second Law: not a fundamental statement about underlying rules, but rather a statement about our interaction with them, and our capabilities as observers. So if there’s something like the Second Law that leads to the structure of spacetime as we typically perceive it, what can be said about typical issues in thermodynamics in connection with spacetime? For example, what’s the story with perpetual motion machines in spacetime? Even before talking about the Second Law, there are already issues with the First Law of Thermodynamics—because in a cosmological setting there isn’t local conservation of energy as such, and for example the expansion of the universe can transfer energy to things. But what about the Second Law question of “getting mechanical work from heat”? Presumably the analog of “mechanical work” is a gravitational field that is “sufficiently organized” that observers like us can readily detect it, say by seeing it pull objects in definite directions. And presumably a perpetual motion machine based on violating the Second Law would then have to take the heat-like randomness in “ordinary spacetime” and somehow organize it into a systematic and measurable gravitational field. Or, in other words, “perpetual motion” would somehow have to involve a gravitational field “spontaneously being generated” from the microscopic structure of spacetime. Just like in ordinary thermodynamics, the impossibility of doing this involves an interplay between the observer and the underlying system. And conceivably it might be possible that there could be an observer who can measure specific features of spacetime that correspond to some slice of computational reducibility in the underlying dynamics—say some weird configuration of “spontaneous motion” of objects. But absent this, a “Second-Law-violating” perpetual motion machine will be impossible.
Computational Foundations for the Second Law of Thermodynamics 33.19 Quantum Mechanics : Like statistical mechanics (and thermodynamics), quantum mechanics is usually thought of as a statistical theory. But whereas the statistical character of statistical mechanics one imagines to come from a definite, knowable “mechanism underneath”, the statistical character of quantum mechanics has usually just been treated as a formal, underivable “fact of physics”. In our Physics Project, however, the story is different, and there’s a whole lower-level structure—ultimately rooted in the ruliad—from which quantum mechanics and its statistical character appears to be derived. And, as we’ll discuss, that derivation in the end has close connections both to what we’ve said about the standard Second Law, and to what we’ve said about the thermodynamics of spacetime. In our Physics Project the starting point for quantum mechanics is the unavoidable fact that when one’s applying rules to transform hypergraphs, there’s typically more than one rewrite that can be done to any given hypergraph. And the result of this is that there are many different possible “paths of history” for the universe. As a simple analog, consider rewriting not hypergraphs but strings. And doing this, we get for example: This is a deterministic representation of all possible “paths of history”, but in a sense it’s very wasteful, among other things because it includes multiple copies of identical strings (like BBBB). If we merge such identical copies, we get what we call a multiway graph, that contains both branchings and mergings: In the “innards” of quantum mechanics one can imagine that all these paths are being followed. So how is it that we as observers perceive definite things to happen in the world? Ultimately it’s a story of coarse graining, and of us conflating different paths in the multiway graph. But there’s a wrinkle here. In statistical mechanics we imagine that we can observe from outside the system, implementing our coarse graining by sampling particular features of the system. But in quantum mechanics we imagine that the multiway system describes the whole universe, including us. So then we have the peculiar situation that just as the universe is branching and merging, so too are our brains. And ultimately what we observe is therefore the result of a branching brain perceiving a branching universe. But given all those branches, can we just decide to conflate them into a single thread of experience? In a sense this is a typical question of coarse graining and of what we can consistently equivalence together. But there’s something a bit different here because without the “coarse graining” we can’t talk at all about “what happened”, only about what might be happening. Put another way, we’re now fundamentally dealing not with computation (like in a cellular automaton) but with multicomputation. And in multicomputation, there are always two fundamental kinds of operations: the generation of new states from old, and the equivalencing of states, effectively by the observer. In ordinary computation, there can be computational irreducibility in the process of generating a thread of successive states. In multicomputation, there can be multicomputational irreducibility in which in a sense all computations in the multiway system have to be done in order even to determine a single equivalenced result. Or, put another way, you can’t shortcut following all the paths of history. If you try to equivalence at the beginning, the equivalence class you’ve built will inevitably be “shredded” by the evolution, forcing you to follow each path separately. It’s worth commenting that just as in classical mechanics, the “underlying dynamics” in our description of quantum mechanics are reversible. In the original unmerged evolution tree above, we could just reverse each rule and from any point uniquely construct a “backwards tree”. But once we start merging and equivalencing, there isn’t the same kind of “direct reversibility”—though we can still count possible paths to determine that we preserve “total probability”. In ordinary computational systems, computational irreducibility implies that even from simple initial conditions we can get behavior that “seems random” with respect to most computationally bounded observations. And something directly analogous happens in multicomputational systems. From simple initial conditions, we generate collections of paths of history that “seem random” with respect to computationally bounded equivalencing operations, or, in other words, to observers who do computationally bounded coarse graining of different paths of history. When we look at the graphs we’ve drawn representing the evolution of a multiway system, we can think of there being a time direction that goes down the page, following the arrows that point from states to their successors. But across the page, in the transverse direction, we can think of there as being a space in which different paths of history are laid—what we call “branchial space”. A typical way to start constructing branchial space is to take slices across the multiway graph, then to form a branchial graph in which two states are joined if they have a common ancestor on the step before (which means we can consider them “entangled”): Although the details remain to be clarified, it seems as if in the standard formalism of quantum mechanics, distance in branchial space corresponds essentially to quantum phase, so that, for example, particles whose phases would make them show destructive interference will be at “opposite ends” of branchial space. So how do observers relate to branchial space? Basically what an observer is doing is to coarse grain in branchial space, equivalencing certain paths of history. And just as we have a certain extent in physical space, which determines our coarse graining of gases, and—at a much smaller scale—of the structure of spacetime, so also we have an extent in branchial space that determines our coarse graining across branches of history. But this is where multicomputational irreducibility and the analog of the Second Law are crucial. Because just as we imagine that gases—and spacetime—achieve a certain kind of “unique random equilibrium” that leads us to be able to make consistent measurements of them, so also we can imagine that in quantum mechanics there is in effect a “branchial space equilibrium” that is achieved. Think of a box of gas in equilibrium. Put two pistons on different sides of the box. So long as they don’t perturb the gas too much, they’ll both record the same pressure. And in our Physics Project it’s the same story with observers and quantum mechanics. Most of the time there’ll be enough effective randomness generated by the multicomputationally irreducible evolution of the system (which is completely deterministic at the level of the multiway graph) that a computationally bounded observer will always see the same “equilibrium values”. A central feature of quantum mechanics is that by making sufficiently careful measurements one can see what appear to be random results. But where does that randomness come from? In the usual formalism for quantum mechanics, the idea of purely probabilistic results is just burnt into the formal structure. But in our Physics Project, the apparent randomness one sees has a definite, “mechanistic” origin. And it’s basically the same as the origin of randomness for the standard Second Law, except that now we’re dealing with multicomputational rather than pure computational irreducibility. By the way, the “Bell’s inequality” statement that quantum mechanics cannot be based on “mechanistic randomness” unless it comes from a nonlocal theory remains true in our Physics Project. But in the Physics Project we have an immediate ubiquitous source of “nonlocality”: the equivalencing or coarse graining “across” branchial space done by observers. (We’re not discussing the role of physical space here. But suffice it to say that instead of having each node of the multiway graph represent a complete state of the universe, we can make an extended multiway graph in which different spatial elements—like different paths of history—are separated, with their “causal entanglements” then defining the actual structure of space, in a spatial analog of the branchial graph.) As we’ve already noted, the complete multiway graph is entirely deterministic. And indeed if we have a complete branchial slice of the graph, this can be used to determine the whole future of the graph (the analog of “unitary evolution” in the standard formalism of quantum mechanics). But if we equivalence states—corresponding to “doing a measurement”—then we won’t have enough information to uniquely determine the future of the system, at least when it comes to what we consider to be quantum effects. At the outset, we might have thought that statistical mechanics, spacetime mechanics and quantum mechanics were all very different theories. But what our Physics Project suggests is that in fact they are all based on a common, fundamentally computational phenomenon. So what about other ideas associated with the standard Second Law? How do they work in the quantum case? Entropy, for example, now just becomes a measure of the number of possible configurations of a branchial graph consistent with a certain coarse-grained measurement. Two independent systems will have disconnected branchial graphs. But as soon as the systems interact, their branchial graphs will connect, and the number of possible graph configurations will change, leading to an “entanglement entropy”. One question about the quantum analog of the Second Law is what might correspond to “mechanical work”. There may very well be highly structured branchial graphs—conceivably associated with things like coherent states—but it isn’t yet clear how they work and whether existing kinds of measurements can readily detect them. But one can expect that multicomputational irreducibility will tend to produce branchial graphs that can’t be “decoded” by most computationally bounded measurements—so that, for example, “quantum perpetual motion”, in which “branchial organization” is spontaneously produced, can’t happen. And in the end randomness in quantum measurements is happening for essentially the same basic reason we’d see randomness if we looked at small numbers of molecules in a gas: it’s not that there’s anything fundamentally not deterministic underneath, it’s just there’s a computational process that’s making things too complicated for us to “decode”, at least as observers with bounded computational capabilities. In the case of the gas, though, we’re sampling molecules at different places in physical space. But in quantum mechanics we’re doing the slightly more abstract thing of sampling states of the system at different places in branchial space. But the same fundamental randomization is happening, though now through multicomputational irreducibility operating in branchial space.
Computational Foundations for the Second Law of Thermodynamics 33.20 The Future of the Second Law : The original formulation of the Second Law a century and a half ago—before even the existence of molecules was established—was an impressive achievement. And one might assume that over the course of 150 years—with all the mathematics and physics that’s been done—a complete foundational understanding of the Second Law would long ago have been developed. But in fact it has not. And from what we’ve discussed here we can now see why. It’s because the Second Law is ultimately a computational phenomenon, and to understand it requires an understanding of the computational paradigm that’s only very recently emerged. Once one starts doing actual computational experiments in the computational universe (as I already did in the early 1980s) the core phenomenon of the Second Law is surprisingly obvious—even if it violates one’s traditional intuition about how things should work. But in the end, as we have discussed here, the Second Law is a reflection of a very general, if deeply computational, idea: an interplay between computational irreducibility and the computational limitations of observers like us. The Principle of Computational Equivalence tells us that computational irreducibility is inevitable. But the limitation of observers is something different: it’s a kind of epiprinciple of science that’s in effect a formalization of our human experience and our way of doing science. Can we tighten up the formulation of all this? Undoubtedly. We have various standard models of the computational process—like Turing machines and cellular automata. We still need to develop an “observer theory” that provides standard models for what observers like us can do. And the more we can develop such a theory, the more we can expect to make explicit proofs of specific statements about the Second Law. Ultimately these proofs will have solid foundations in the Principle of Computational Equivalence (although there remains much to formalize there too), but will rely on models for what “observers like us” can be like. So how general do we expect the Second Law to be in the end? In the past couple of sections we’ve seen that the core of the Second Law extends to spacetime and to quantum mechanics. But even when it comes to the standard subject matter of statistical mechanics, we expect limitations and exceptions to the Second Law. Computational irreducibility and the Principle of Computational Equivalence are very general, but not very specific. They talk about the overall computational sophistication of systems and processes. But they do not say that there are no simplifying features. And indeed we expect that in any system that shows computational irreducibility, there will always be arbitrarily many “slices of computational reducibility” that can be found. The question then is whether those slices of reducibility will be what an observer can perceive, or will care about. If they are, then one won’t see Second Law behavior. If they’re not, one will just see “generic computational irreducibility” and Second Law behavior. How can one find the slices of reducibility? Well, in general that’s irreducibly hard. Every slice of reducibility is in a sense a new scientific or mathematical principle. And the computational irreducibility involved in finding such reducible slices basically speaks to the ultimately unbounded character of the scientific and mathematical enterprise. But once again, even though there might be an infinite number of slices of reducibility, we still have to ask which ones matter to us as observers. The answer could be one thing for studying gases, and another, for example, for studying molecular biology, or social dynamics. The question of whether we’ll see “Second Law behavior” then boils to whether whatever we’re studying turns out to be something that doesn’t simplify, and ends up showing computational irreducibility. If we have a sufficiently small system—with few enough components—then the computational irreducibility may not be “strong enough” to stop us from “going beyond the Second Law”, and for example constructing a successful Maxwell’s demon. And indeed as computer and sensor technology improve, it’s becoming increasingly feasible to do measurement and set up control systems that effectively avoid the Second Law in particular, small systems. But in general the future of the Second Law and its applicability is really all about how the capabilities of observers develop. What will future technology, and future paradigms, do to our ability to pick away at computational irreducibility? In the context of the ruliad, we are currently localized in rulial space based on our existing capabilities. But as we develop further we are in effect “colonizing” rulial space. And a system that may look random—and may seem to follow the Second Law—from one place in rulial space may be “revealed as simple” from another. There is an issue, though. Because the more we as observers spread out in rulial space, the less coherent our experience will become. In effect we’ll be following a larger bundle of threads in rulial space, which makes who “we” are less definite. And in the limit we’ll presumably be able to encompass all slices of computational reducibility, but at the cost of having our experience “incoherently spread” across all of them. It’s in the end some kind of tradeoff. Either we can have a coherent thread of experience, in which case we’ll conclude that the world produces apparent randomness, as the Second Law suggests. Or we can develop to the point where we’ve “spread our experience” and no longer have coherence as observers, but can recognize enough regularities that the Second Law potentially seems irrelevant. But as of now, the Second Law is still very much with us, even if we are beginning to see some of its limitations. And with our computational paradigm we are finally in a position to see its foundations, and understand how it ultimately works.
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.1 I’ve been trying to understand the Second Law now for a bit more than 50 years : It all started when I was 12 years old. Building on an earlier interest in space and spacecraft, I’d gotten very interested in physics, and was trying to read everything I could about it. There were several shelves of physics books at the local bookstore. But what I coveted most was the largest physics book collection there: a series of five plushly illustrated college textbooks. And as a kind of graduation gift when I finished (British) elementary school in June 1972 I arranged to get those books. And here they are, still on my bookshelf today, just a little faded, more than half a century later: For a while the first book in the series was my favorite. Then the third. The second. The fourth. The fifth one at first seemed quite mysterious—and somehow more abstract in its goals than the others: What story was the filmstrip on its cover telling? For a couple of months I didn’t look seriously at the book. And I spent much of the summer of 1972 writing my own (unseen by anyone else for 30+ years) Concise Directory of Physics that included a rather stiff page about energy, mentioning entropy—along with the heat death of the universe. But one afternoon late that summer I decided I should really find out what that mysterious fifth book was all about. Memory being what it is I remember that—very unusually for me—I took the book to read sitting on the grass under some trees. And, yes, my archives almost let me check my recollection: in the distance, there’s the spot, except in 1967 the trees are significantly smaller, and in 1985 they’re bigger: Of course, by 1972 I was a little bigger than in 1967—and here I am a little later, complete with a book called Planets and Life on the ground, along with a tube of (British) Smarties, and, yes, a pocket protector (but, hey, those were actual ink pens): But back to the mysterious green book. It wasn’t like anything I’d seen before. It was full of pictures like the one on the cover. And it seemed to be saying that—just by looking at those pictures and thinking—one could figure out fundamental things about physics. The other books I’d read had all basically said “physics works like this”. But here was a book saying “you can figure out how physics has to work”. Back then I definitely hadn’t internalized it, but I think what was so exciting that day was that I got a first taste of the idea that one didn’t have to be told how the world works; one could just figure it out: I didn’t yet understand quite a bit of the math in the book. But it didn’t seem so relevant to the core phenomenon the book was apparently talking about: the tendency of things to become more random. I remember wondering how this related to stars being organized into galaxies. Why might that be different? The book didn’t seem to say, though I thought maybe somewhere it was buried in the math. But soon the summer was over, and I was at a new school, mostly away from my books, and doing things like diligently learning more Latin and Greek. But whenever I could I was learning more about physics—and particularly about the hot area of the time: particle physics. The pions. The kaons. The lambda hyperon. They all became my personal friends. During the school vacations I would excitedly bicycle the few miles to the nearby university library to check out the latest journals and the latest news about particle physics. The school I was at (Eton) had five centuries of history, and I think at first I assumed no particular bridge to the future. But it wasn’t long before I started hearing mentions that somewhere at the school there was a computer. I’d seen a computer in real life only once—when I was 10 years old, and from a distance. But now, tucked away at the edge of the school, above a bicycle repair shed, there was an island of modernity, a “computer room” with a glass partition separating off a loudly humming desk-sized piece of electronics that I could actually touch and use: an Elliott 903C computer with 8 kilowords of 18-bit ferrite core memory (acquired by the school in 1970 for £12,000, or about about $300k today): At first it was such an unfamiliar novelty that I was satisfied writing little programs to do things like compute primes, print curious patterns on the teleprinter, and play tunes with the built-in diagnostic tone generator. But it wasn’t long before I set my sights on the goal of using the computer to reproduce that interesting picture on the book cover. I programmed in assembler, with my programs on paper tape. The computer had just 16 machine instructions, which included arithmetic ones, but only for integers. So how was I going to simulate colliding “molecules” with that? Somewhat sheepishly, I decided to put everything on a grid, with everything represented by discrete elements. There was a convention for people to name their programs starting with their own first initial. So I called the program SPART, for “Stephen’s Particle Program”. (Thinking about it today, maybe that name reflected some aspiration of relating this to particle physics.) It was the most complicated program I had ever written. And it was hard to test, because, after all, I didn’t really know what to expect it to do. Over the course of several months, it went through many versions. Rather often the program would just mysteriously crash before producing any output (and, yes, there weren’t real debugging tools yet). But eventually I got it to systematically produce output. But to my disappointment the output never looked much like the book cover. I didn’t know why, but I assumed it was because I was simplifying things too much, putting everything on a grid, etc. A decade later I realized that in writing my program I’d actually ended up inventing a form of 2D cellular automaton. And I now rather suspect that this cellular automaton—like rule 30—was actually intrinsically generating randomness, and in some sense showing what I now understand to be the core phenomenon of the Second Law. But at the time I absolutely wasn’t ready for this, and instead I just assumed that what I was seeing was something wrong and irrelevant. (In past years, I had suspected that what went wrong had to do with details of particle behavior on square—as opposed to other—grids. But I now suspect it was instead that the system was in a sense generating too much randomness, making the intended “molecular dynamics” unrecognizable.) I’d love to “bring SPART back to life”, but I don’t seem to have a copy anymore, and I’m pretty sure the printouts I got as output back in 1973 seemed so “wrong” I didn’t keep them. I do still have quite a few paper tapes from around that time, but as of now I’m not sure what’s on them—not least because I wrote my own “advanced” paper-tape loader, which used what I later learned were error-correcting codes to try to avoid problems with pieces of “confetti” getting stuck in the holes that had been punched in the tape:
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.2 Becoming a Physicist : I don’t know what would have happened if I’d thought my program was more successful in reproducing “Second Law” behavior back in 1973 when I was 13 years old. But as it was, in the summer of 1973 I was away from “my” computer, and spending all my time on particle physics. And between that summer and early 1974 I wrote a book-length summary of what I called “The Physics of Subatomic Particles”: I don’t think I’d looked at this in any detail in 48 years. But reading it now I am a bit shocked to find history and explanations that I think are often better than I would immediately give today—even if they do bear definite signs of coming from a British early teenager writing “scientific prose”. Did I talk about statistical mechanics and the Second Law? Not directly, though there’s a curious passage where I speculate about the possibility of antimatter galaxies, and their (rather un-Second-Law-like) segregation from ordinary, matter galaxies: By the next summer I was writing the 230-page, much more technical “Introduction to the Weak Interaction”. Lots of quantum mechanics and quantum field theory. No statistical mechanics. The closest it gets is a chapter on CP violation (AKA time-reversal violation)—a longtime favorite topic of mine—but from a very particle-physics point of view. By the next year I was publishing papers about particle physics, with no statistical mechanics in sight—though in a picture of me (as a “lanky youth”) from that time, the Statistical Physics book is right there on my shelf, albeit surrounded by particle physics books: But despite my focus on particle physics, I still kept thinking about statistical mechanics and the Second Law, and particularly its implications for the large-scale structure of the universe, and things like the possibility of matter-antimatter separation. And in early 1977, now 17 years old, and (briefly) a college student in Oxford, my archives record that I gave a talk to the newly formed (and short-lived) Oxford Natural Science Club entitled “Whither Physics” in which I talked about “large, small, many” as the main frontiers of physics, and presented the visual with a dash of “unsolved purple” impinging on statistical mechanics, particularly in connection with non-equilibrium situations. Meanwhile, looking at my archives today, I find some “back of the envelope” equilibrium statistical mechanics from that time (though I have no idea now what this was about): But then, in the fall of 1977 I ended up for the first time really needing to use statistical mechanics “in production”. I had gotten interested in what would later become a hot area: the intersection between particle physics and the early universe. One of my interests was neutrino background radiation (the neutrino analog of the cosmic microwave background); another was early-universe production of stable charged particles heavier than the proton. And it turned out that to study these I needed all three of cosmology, particle physics, and statistical mechanics: In the couple of years that followed, I worked on all sorts of topics in particle physics and in cosmology. Quite often ideas from statistical mechanics would show up, like when I worked on the hadronization of quarks and gluons, or when I worked on phase transitions in the early universe. But it wasn’t until 1979 that the Second Law made its first explicit appearance by name in my published work. I was studying how there could be a net excess of matter over antimatter throughout the universe (yes, I’d by then given up on the idea of matter-antimatter separation). It was a subtle story of quantum field theory, time reversal violation, General Relativity—and non-equilibrium statistical mechanics. And in the paper we wrote we included a detailed appendix about Boltzmann’s H theorem and the Second Law—and the generalization we needed for relativistic quantum time-reversal-violating systems in an expanding universe: All this got me thinking again about the foundations of the Second Law. The physicists I was around mostly weren’t too interested in such topics—though Richard Feynman was something of an exception. And indeed when I did my PhD thesis defense in November 1979 it ended up devolving into a spirited multi-hour debate with Feynman about the Second Law. He maintained that the Second Law must ultimately cause everything to randomize, and that the order we see in the universe today must be some kind of temporary fluctuation. I took the point of view that there was something else going on, perhaps related to gravity. Today I would have more strongly made the rather Feynmanesque point that if you have a theory that says everything we observe today is an exception to your theory, then the theory you have isn’t terribly useful.
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.3 Statistical Mechanics and Simple Programs : Back in 1973 I never really managed to do much science on the very first computer I used. But by 1976 I had access to much bigger and faster computers (as well as to the ARPANET—forerunner of the internet). And soon I was routinely using computers as powerful tools for physics, and particularly for symbolic manipulation. But by late 1979 I had basically outgrown the software systems that existed, and within weeks of getting my PhD I embarked on the project of building my own computational system. It’s a story I’ve told elsewhere, but one of the important elements for our purposes here is that in designing the system I called SMP (for “Symbolic Manipulation Program”) I ended up digging deeply into the foundations of computation, and its connections to areas like mathematical logic. But even as I was developing the critical-to-Wolfram-Language-to-this-day paradigm of basing everything on transformations for symbolic expressions, as well as leading the software engineering to actually build SMP, I was also continuing to think about physics and its foundations. There was often something of a statistical mechanics orientation to what I did. I worked on cosmology where even the collection of possible particle species had to be treated statistically. I worked on the quantum field theory of the vacuum—or effectively the “bulk properties of quantized fields”. I worked on what amounts to the statistical mechanics of cosmological strings. And I started working on the quantum-field-theory-meets-statistical-mechanics problem of “relativistic matter” (where my unfinished notes contain questions like “Does causality forbid relativistic solids?”): But hovering around all of this was my old interest in the Second Law, and in the seemingly opposing phenomenon of the spontaneous emergence of complex structure. SMP Version 1.0 was ready in mid-1981. And that fall, as a way to focus my efforts, I taught a “Topics in Theoretical Physics” course at Caltech (supposedly for graduate students but actually almost as many professors came too) on what, for want of a better name, I called “non-equilibrium statistical mechanics”. My notes for the first lecture dived right in: Echoing what I’d seen on that book cover back in 1972 I talked about the example of the expansion of a gas, noting that even in this case “Many features [are] still far from understood”: I talked about the Boltzmann transport equation and its elaboration in the BBGKY hierarchy, and explored what might be needed to extend it to things like self-gravitating systems. And then—in what must have been a very overstuffed first lecture—I launched into a discussion of “Possible origins of irreversibility”. I began by talking about things like ergodicity, but soon made it clear that this didn’t go the distance, and there was much more to understand—saying that “with a bit of luck” the material in my later lectures might help: I continued by noting that some systems can “develop order and considerable organization”—which non-equilibrium statistical mechanics should be able to explain: I then went quite “cosmological”: The first candidate explanation I listed was the fluctuation argument Feynman had tried to use: I discussed the possibility of fundamental microscopic irreversibility—say associated with time-reversal violation in gravity—but largely dismissed this. I talked about the possibility that the universe could have started in a special state in which “the matter is in thermal equilibrium, but the gravitational field is not.” And finally I gave what the 22-year-old me thought at the time was the most plausible explanation: All of this was in a sense rooted in a traditional mathematical physics style of thinking. But the second lecture gave a hint of a quite different approach: In my first lecture, I had summarized my plans for subsequent lectures: But discovery intervened. People had discussed reaction-diffusion patterns as examples of structure being formed “away from equilibrium”. But I was interested in more dramatic examples, like galaxies, or snowflakes, or turbulent flow patterns, or forms of biological organisms. What kinds of models could realistically be made for these? I started from neural networks, self-gravitating gases and spin systems, and just kept on simplifying and simplifying. It was rather like language design, of the kind I’d done for SMP. What were the simplest primitives from which I could build up what I wanted? Before long I came up with what I’d soon learn could be called one-dimensional cellular automata. And immediately I started running them on a computer to see what they did: And, yes, they were “organizing themselves”—even from random initial conditions—to make all sorts of structures. By December I was beginning to frame how I would write about what was going on: And by May 1982 I had written my first long paper about cellular automata (published in 1983): The Second Law featured prominently, even in the first sentence: I made quite a lot out of the fundamentally irreversible character of most cellular automaton rules, pretty much assuming that this was the fundamental origin of their ability to “generate complex structures”—as the opening transparencies of two talks I gave at the time suggested: It wasn’t that I didn’t know there could be reversible cellular automata. And a footnote in my paper even records the fact these can generate nested patterns with a certain fractal dimension—as computed in a charmingly manual way on a couple of pages I now find in my archives: But somehow I hadn’t quite freed myself from the assumption that microscopic irreversibility was what was “causing” structures to be formed. And this was related to another important—and ultimately incorrect—assumption: that all the structure I was seeing was somehow the result of the “filtering” random initial conditions. Right there in my paper is a picture of rule 30 starting from a single cell: And, yes, the printout from which that was made is still in my archives, if now a little worse for wear: Of course, it probably didn’t help that with my “display” consisting of an array of printed characters I couldn’t see too much of the pattern—though my archives do contain a long “imitation-high-resolution” printout of the conveniently narrow, and ultimately nested, pattern from rule 225: But I think the more important point was that I just didn’t have the necessary conceptual framework to absorb what I was seeing in rule 30—and I wasn’t ready for the intuitional shock that it takes only simple rules with simple initial conditions to produce highly complex behavior. My motivation for studying the behavior of cellular automata had come from statistical mechanics. But I soon realized that I could discuss cellular automata without any of the “baggage” of statistical mechanics, or the Second Law. And indeed even as I was finishing my long statistical-mechanics-themed paper on cellular automata, I was also writing a short paper that described cellular automata essentially as purely computational systems (even though I still used the term “mathematical models”) without talking about any kind of Second Law connections: Through much of 1982 I was alternating between science, technology and the startup of my first company. I left Caltech in October 1982, and after stops at Los Alamos and Bell Labs, started working at the Institute for Advanced Study in Princeton in January 1983, equipped with a newly obtained Sun workstation computer whose (“one megapixel”) bitmap display let me begin to see in more detail how cellular automata behave: It had very much the flavor of classic observational science—looking not at something like mollusc shells, but instead at images on a screen—and writing down what I saw in a “lab notebook”: What did all those rules do? Could I somehow find a way to classify their behavior? Mostly I was looking at random initial conditions. But in a near miss of the rule 30 phenomenon I wrote in my lab notebook: “In irregular cases, appears that patterns starting from small initial states are not self-similar (e.g. code 10)”. I even looked again at asymmetric “elementary” rules (of which rule 30 is an example)—but only from random initial conditions (though noting the presence of “class 4” rules, which would include rule 110): My technology stack at the time consisted of printing screen dumps of cellular automaton behavior then using repeated photocopying to shrink them—and finally cutting out the images and assembling arrays of them using Scotch tape: And looking at these arrays I was indeed able to make an empirical classification, identifying initially five—but in the end four—basic classes of behavior. And although I sometimes made analogies with solids, liquids and gases—and used the mathematical concept of entropy—I was now mostly moving away from thinking in terms of statistical mechanics, and was instead using methods from areas like dynamical systems theory, and computation theory: Even so, when I summarized the significance of investigating the computational characteristics of cellular automata, I reached back to statistical mechanics, suggesting that much as information theory provided a mathematical basis for equilibrium statistical mechanics, so similarly computation theory might provide a foundation for non-equilibrium statistical mechanics:
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.4 Computational Irreducibility and Rule 30 : My experiments had shown that cellular automata could “spontaneously produce structure” even from randomness. And I had been able to characterize and measure various features of this structure, notably using ideas like entropy. But could I get a more complete picture of what cellular automata could make? I turned to formal language theory, and started to work out the “grammar of possible states”. And, yes, a quarter century before Graph in Wolfram Language, laying out complicated finite state machines wasn’t easy: But by November 1983 I was writing about “self-organization as a computational process”: The introduction to my paper again led with the Second Law, though now talked about the idea that computation theory might be what could characterize non-equilibrium and self-organizing phenomena: The concept of equilibrium in statistical mechanics makes it natural to ask what will happen in a system after an infinite time. But computation theory tells one that the answer to that question can be non-computable or undecidable. I talked about this in my paper, but then ended by discussing the ultimately much richer finite case, and suggesting (with a reference to NP completeness) that it might be common for there to be no computational shortcut to cellular automaton evolution. And rather presciently, I made the statement that “One may speculate that [this phenomenon] is widespread in physical systems” so that “the consequences of their evolution could not be predicted, but could effectively be found only by direct simulation or observation.”: These were the beginnings of powerful ideas, but I was still tying them to somewhat technical things like ensembles of all possible states. But in early 1984, that began to change. In January I’d been asked to write an article for the then-top popular science magazine Scientific American on the subject of “Computers in Science and Mathematics”. I wrote about the general idea of computer experiments and simulation. I wrote about SMP. I wrote about cellular automata. But then I wanted to bring it all together. And that was when I came up with the term “computational irreducibility”. By May 26, the concept was pretty clearly laid out in my draft text: But just a few days later something big happened. On June 1 I left Princeton for a trip to Europe. And in order to “have something interesting to look at on the plane” I decided to print out pictures of some cellular automata I hadn’t bothered to look at much before. The first one was rule 30: And it was then that it all clicked. The complexity I’d been seeing in cellular automata wasn’t the result of some kind of “self-organization” or “filtering” of random initial conditions. Instead, here was an example where it was very obviously being “generated intrinsically” just by the process of evolution of the cellular automaton. This was computational irreducibility up close. No need to think about ensembles of states or statistical mechanics. No need to think about elaborate programming of a universal computer. From just a single black cell rule 30 could produce immense complexity, and showed what seemed very likely to be clear computational irreducibility. Why hadn’t I figured out before that something like this could happen? After all, I’d even generated a small picture of rule 30 more than two years earlier. But at the time I didn’t have a conceptual framework that made me pay attention to it. And a small picture like that just didn’t have the same in-your-face “complexity from nothing” character as my larger picture of rule 30. Of course, as is typical in the history of ideas, there’s more to the story. One of the key things that had originally let me start “scientifically investigating” cellular automata is that out of all the infinite number of possible constructible rules, I’d picked a modest number on which I could do exhaustive experiments. I’d started by considering only “elementary” cellular automata, in one dimension, with k = 2 colors, and with rules of range r = 1. There are 256 such “elementary rules”. But many of them had what seemed to me “distracting” features—like backgrounds alternating between black and white on successive steps, or patterns that systematically shifted to the left or right. And to get rid of these “distractions” I decided to focus on what I (somewhat foolishly in retrospect) called “legal rules”: the 32 rules that leave blank states blank, and are left-right symmetric. When one uses random initial conditions, the legal rules do seem—at least in small pictures—to capture the most obvious behaviors one sees across all the elementary rules. But it turns out that’s not true when one looks at simple initial conditions. Among the “legal” rules, the most complicated behavior one sees with simple initial conditions is nesting. But even though I concentrated on “legal” rules, I still included in my first major paper on cellular automata pictures of a few “illegal” rules starting from simple initial conditions—including rule 30. And what’s more, in a section entitled “Extensions”, I discussed cellular automata with more than 2 colors, and showed—though without comment—the pictures: These were low-resolution pictures, and I think I imagined that if one ran them further, the behavior would somehow resolve into something simple. But by early 1983, I had some clues that this wouldn’t happen. Because by then I was generating fairly high-resolution pictures—including ones of the k = 2, r = 2 totalistic rule with code 10 starting from a simple initial condition: In early drafts of my 1983 paper on “Universality and Complexity in Cellular Automata” I noted the generation of “irregularity”, and speculated that it might be associated with class 4 behavior. But later I just stated as an observation without “cause” that some rules—like code 10—generate “irregular patterns”. I elaborated a little, but in a very “statistical mechanics” kind of way, not getting the main point: In September 1983 I did a little better: But in the end it wasn’t until June 1, 1984, that I really grokked what was going on. And a little over a week later I was in a scenic area of northern Sweden at a posh “Nobel Symposium” conference on “The Physics of Chaos and Related Problems”—talking for the first time about the phenomenon I’d seen in rule 30 and code 10. And from June 15 there’s a transcript of a discussion session where I bring up the never-before-mentioned-in-public concept of computational irreducibility—and, unsurprisingly, leave the other participants (who were basically all traditional mathematically oriented physicists) at best slightly bemused: I think I was still a bit prejudiced against rule 30 and code 10 as specific rules: I didn’t like the asymmetry of rule 30, and I didn’t like the rapid growth of code 10. (Rule 73—while symmetric—I also didn’t like because of its alternating background.) But having now grokked the rule 30 phenomenon I knew it also happened in “more aesthetic” “legal” rules with more than 2 colors. And while even 3 colors led to a rather large total space of rules, it was easy to generate examples of the phenomenon there. A few days later I was back in the US, working on finishing my article for Scientific American. A photographer came to help get pictures from the color display I now had: And, yes, those pictures included multicolor rules that showed the rule 30 phenomenon: The caption I wrote commented: “Even in this case the patterns generated can be complex, and they sometimes appear quite random. The complex patterns formed in such physical processes as the flow of a turbulent fluid may well arise from the same mechanism.” The article went on to describe computational irreducibility and its implications in quite a lot of detail— illustrating it rather nicely with a diagram, and commenting that “It seems likely that many physical and mathematical systems for which no simple description is now known are in fact computationally irreducible”: I also included an example—that would show up almost unchanged in A New Kind of Science nearly 20 years later—indicating how computational irreducibility could lead to undecidability (back in 1984 the picture was made by stitching together many screen photographs, yes, with strange artifacts from long-exposure photography of CRTs): In a rather newspaper-production-like experience, I spent the evening of July 18 at the offices of Scientific American in New York City putting finishing touches to the article, which at the end of the night—with minutes to spare—was dispatched for final layout and printing. But already by that time, I was talking about computational irreducibility and the rule 30 phenomenon all over the place. In July I finished “Twenty Problems in the Theory of Cellular Automata” for the proceedings of the Swedish conference, including what would become a rather standard kind of picture: Problem 15 talks specifically about rule 30, and already asks exactly what would—35 years later—become Problem #2 in my 2019 Rule 30 Prizes while Problem 18 asks the (still largely unresolved) question of what the ultimate frequency of computational irreducibility is: Very late in putting together the Scientific American article I’d added to the caption of the picture showing rule-30-like behavior the statement “Complex patterns generated by cellular automata can also serve as a source of effectively random numbers, and they can be applied to encrypt messages by converting a text into an apparently random form.” I’d realized both that cellular automata could act as good random generators (we used rule 30 as the default in Wolfram Language for more than 25 years), and that their evolution could effectively encrypt things, much as I’d later describe the Second Law as being about “encrypting” initial conditions to produce effective irreversibility. Back in 1984 it was a surprising claim that something as simple and “science-oriented” as a cellular automaton could be useful for encryption. Because at the time practical encryption was basically always done by what at least seemed like arbitrary and complicated engineering solutions, whose security relied on details or explanations that were often considered military or commercial secrets. I’m not sure when I first became aware of cryptography. But back in 1973 when I first had access to a computer there were a couple of kids (as well as a teacher who’d been a friend of Alan Turing’s) who were programming Enigma-like encryption systems (perhaps fueled by what were then still officially just rumors of World War II goings-on at Bletchley Park). And by 1980 I knew enough about encryption that I made a point of encrypting the source code of SMP (using a modified version of the Unix crypt program). (As it happens, we lost the password, and it was only in 2015 that we got access to the source again.) My archives record a curious interaction about encryption in May 1982—right around when I’d first run (though didn’t appreciate) rule 30. A rather colorful physicist I knew named Brosl Hasslacher (who we’ll encounter again later) was trying to start a curiously modern-sounding company named Quantum Encryption Devices (or QED for short)—that was actually trying to market a quite hacky and definitively classical (multiple-shift-register-based) encryption system, ultimately to some rather shady customers (and, yes, the “expected” funding did not materialize): But it was 1984 before I made a connection between encryption and cellular automata. And the first thing I imagined was giving input as the initial condition of the cellular automaton, then running the cellular automaton rule to produce “encrypted output”. The most straightforward way to make encryption was then to have the cellular automaton rule be reversible, and to run the inverse rule to do the decryption. I’d already done a little bit of investigation of reversible rules, but this led to a big search for reversible rules—which would later come in handy for thinking about microscopically reversible processes and thermodynamics. Just down the hall from me at the Institute for Advanced Study was a distinguished mathematician named John Milnor, who got very interested in what I was doing with cellular automata. My archives contain all sorts of notes from Jack, like: There’s even a reversible (“one-to-one”) rule, with nice, minimal BASIC code, along with lots of “real math”: But by the spring of 1984 Jack and I were talking a lot about encryption in cellular automata—and we even began to draft a paper about it complete with outlines of how encryption schemes could work: The core of our approach involved reversible rules, and so we did all sorts of searches to find these (and by 1984 Jack was—like me—writing C code): I wondered how random the output from cellular automata was, and I asked people I knew at Bell Labs about randomness testing (and, yes, email headers haven’t changed much in four decades, though then I was swolf@ias.uucp; research!ken was Ken Thompson of Unix fame): But then came my internalization of the rule 30 phenomenon, which led to a rather different way of thinking about encryption with cellular automata. Before, we’d basically been assuming that the cellular automaton rule was the encryption key. But rule 30 suggested one could instead have a fixed rule, and have the initial condition define the key. And this is what led me to more physics-oriented thinking about cryptography—and to what I said in Scientific American. In July I was making “encryption-friendly” pictures of rule 30: But what Jack and I were most interested in was doing something more “cryptographically sophisticated”, and in particular inventing a practical public-key cryptosystem based on cellular automata. Pretty much the only public-key cryptosystems known then (or even now) are based on number theory. But we thought maybe one could use something like products of rules instead of products of numbers. Or maybe one didn’t need exact invertibility. Or something. But by the late summer of 1984, things weren’t looking good: And eventually we decided we just couldn’t figure it out. And it’s basically still not been figured out (and maybe it’s actually impossible). But even though we don’t know how to make a public-key cryptosystem with cellular automata, the whole idea of encrypting initial data and turning it into effective randomness is a crucial part of the whole story of the computational foundations of thermodynamics as I think I now understand them.
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.5 Where Does Randomness Come From? : Right from when I first formulated it, I thought computational irreducibility was an important idea. And in the late summer of 1984 I decided I’d better write a paper specifically about it. The result was: It was a pithy paper, arranged to fit in the 4-page limit of Physical Review Letters, with a rather clear description of computational irreducibility and its immediate implications (as well as the relation between physics and computation, which it footnoted as a “physical form of the Church–Turing thesis”). It illustrated computational reducibility and irreducibility in a single picture, here in its original Scotch-taped form: The paper contains all sorts of interesting tidbits, like this run of footnotes: In the paper itself I didn’t mention the Second Law, but in my archives I find some notes I made in preparing the paper, about candidate irreducible or undecidable problems (with many still unexplored) which include “Will a hard sphere gas started from a particular state ever exhibit some specific anti-thermodynamic behaviour?” In November 1984 the then-editor of Physics Today asked if I’d write something for them. I never did, but my archives include a summary of a possible article—which among other things promises to use computational ideas to explain “why the Second Law of thermodynamics holds so widely”: So by November 1984 I was already aware of the connection between computational irreducibility and the Second Law (and also I didn’t believe that the Second Law would necessarily always hold). And my notes—perhaps from a little later—make it clear that actually I was thinking about the Second Law along pretty much the same lines as I do now, except that back then I didn’t yet understand the fundamental significance of the observer: And spelunking now in my old filesystem (retrieved from a 9-track backup tape) I find from November 17, 1984 (at 2:42am), troff source for a putative paper (which, yes, we even now run through troff): This is all that’s in my filesystem. So, yes, in effect, I’m finally (more or less) finishing this 38 years later. But in 1984 one of the hot—if not new—ideas of the time was “chaos theory”, which talked about how “randomness” could “deterministically arise” from progressive “excavation” of higher and higher-order digits in the initial conditions for a system. But having seen rule 30 this whole phenomenon of what was often (misleadingly) called “deterministic chaos” seemed to me at best like a sideshow—and definitely not the main effect leading to most randomness seen in physical systems. I began to draft a paper about this including for the first time an anchor picture of rule 30 intrinsically generating randomness—to be contrasted with pictures of randomness being generated (still in cellular automata) from sensitive dependence on random initial conditions: It was a bit of a challenge to find an appropriate publishing venue for what amounted to a rather “interdisciplinary” piece of physics-meets-math-meets-computation. But Physical Review Letters seemed like the best bet, so on November 19, 1984, I submitted a version of the paper there, shortened to fit in its 4-page limit. A couple of months later the journal said it was having trouble finding appropriate reviewers. I revised the paper a bit (in retrospect I think not improving it), then on February 1, 1985, sent it in again, with the new title “Origins of Randomness in Physical Systems”: On March 8 the journal responded, with two reports from reviewers. One of the reviewers completely missed the point (yes, a risk in writing shift-the-paradigm papers). The other sent a very constructive two-page report: I didn’t know it then, but later I found out that Bob Kraichnan had spent much of his life working on fluid turbulence (as well as that he was a very independent and think-for-oneself physicist who’d been one of Einstein’s last assistants at the Institute for Advanced Study). Looking at his report now it’s a little charming to see his statement that “no one who has looked much at turbulent flows can easily doubt [that they intrinsically generate randomness]” (as opposed to getting randomness from noise, initial conditions, etc.). Even decades later, very few people seem to understand this. There were several exchanges with the journal, leaving it controversial whether they would publish the paper. But then in May I visited Los Alamos, and Bob Kraichnan invited me to lunch. He’d also invited a then-young physicist from Los Alamos who I’d known fairly well a few years earlier—and who’d once paid me the unintended compliment that it wasn’t fair for me to work on science because I was “too efficient”. (He told me he’d “intended to work on cellular automata”, but before he’d gotten around to it, I’d basically figured everything out.) Now he was riding the chaos theory bandwagon hard, and insofar as my paper threatened that, he wanted to do anything he could to kill the paper. I hadn’t seen this kind of “paradigm attack” before. Back when I’d been doing particle physics, it had been a hot and cutthroat area, and I’d had papers plagiarized, sometimes even egregiously. But there wasn’t really any “paradigm divergence”. And cellular automata—being quite far from the fray—were something I could just peacefully work on, without anyone really paying much attention to whatever paradigm I might be developing. At lunch I was treated to a lecture about why what I was doing was nonsense, or even if it wasn’t, I shouldn’t talk about it, at least now. Eventually I got a chance to respond, I thought rather effectively—causing my “opponent” to leave in a huff, with the parting line “If you publish the paper, I’ll ruin your career”. It was a strange thing to say, given that in the pecking order of physics, he was quite junior to me. (A decade and half later there were nevertheless a couple of “incidents”.) Bob Kraichnan turned to me, cracked a wry smile and said “OK, I’ll go right now and tell [the journal] to publish your paper”: Kraichnan was quite right that the paper was much too short for what it was trying to say, and in the end it took a long book—namely A New Kind of Science—to explain things more clearly. But the paper was where a high-resolution picture of rule 30 first appeared in print. And it was the place where I first tried to explain the distinction between “randomness that’s just transcribed from elsewhere” and the fundamental phenomenon one sees in rule 30 where randomness is intrinsically generated by computational processes within a system. I wanted words to describe these two different cases. And reaching back to my years of learning ancient Greek in school I invented the terms “homoplectic” and “autoplectic”, with the noun “autoplectism” to describe what rule 30 does. In retrospect, I think these terms are perhaps “too Greek” (or too “medical sounding”), and I’ve tended to just talk about “intrinsic randomness generation” instead of autoplectism. (Originally, I’d wanted to avoid the term “intrinsic” to prevent confusion with randomness that’s baked into the rules of a system.) The paper (as Bob Kraichnan pointed out) talks about many things. And at the end, having talked about fluid turbulence, there’s a final sentence—about the Second Law: In my archives, I find other mentions of the Second Law too. Like an April 1985 proto-paper that was never completed but included the statement: My main reason for working on cellular automata was to use them as idealized models for systems in nature, and as a window into foundational issues. But being quite involved in the computer industry, I couldn’t help wondering whether they might be directly useful for practical computation. And I talked about the possibility of building a “metachip” in which—instead of having predefined “meaningful” opcodes like in an ordinary microprocessor—everything would be built up “purely in software” from an underlying universal cellular automaton rule. And various people and companies started sending me possible designs: But in 1984 I got involved in being a consultant to an MIT-spinoff startup called Thinking Machines Corporation that was trying to build a massively parallel “Connection Machine” computer with 65536 processors. The company had aspirations around AI (hence the name, which I’d actually been involved in suggesting), but their machine could also be put to work simulating cellular automata, like rule 30. In June 1985, hot off my work on the origins of randomness, I went to spend some of the summer at Thinking Machines, and decided it was time to do whatever analysis—or, as I would call it now, ruliology—I could on rule 30. My filesystem from 1985 records that it was fast work. On June 24 I printed a somewhat-higher-resolution image of rule 30 (my login was “swolf” back then, so that’s how my printer output was labeled): By July 2 a prototype Connection Machine had generated 2000 steps of rule 30 evolution: With a large-format printer normally used to print integrated circuit layouts I got an even larger “piece of rule 30”—that I laid out on the floor for analysis, for example trying to measure (with meter rules, etc.) the slope of the border between regularity and irregularity in the pattern. Richard Feynman was also a consultant at Thinking Machines, and we often timed our visits to coincide: Feynman and I had talked about randomness quite a bit over the years, most recently in connection with the challenges of making a “quantum randomness chip” as a minimal example of quantum computing. Feynman at first didn’t believe that rule 30 could really be “producing randomness”, and that there must be some way to “crack” it. He tried, both by hand and with a computer, particularly using statistical mechanics methods to try to compute the slope of the border between regularity and irregularity: But in the end, he gave up, telling me “OK, Wolfram, I think you’re on to something”. Meanwhile, I was throwing all the methods I knew at rule 30. Combinatorics. Dynamical systems theory. Logic minimization. Statistical analysis. Computational complexity theory. Number theory. And I was pulling in all sorts of hardware and software too. The Connection Machine. A Cray supercomputer. A now-long-extinct Celerity C1200 (which successfully computed a length-40,114,679,273 repetition period). A LISP machine for graph layout. A circuit-design logic minimization program. As well as my own SMP system. (The Wolfram Language was still a few years in the future.) But by July 21, there it was: a 50-page “ruliological profile” of rule 30, in a sense showing what one could of the “anatomy” of its randomness: A month later I attended in quick succession a conference in California about cryptography, and one in Japan about fluid turbulence—with these two fields now firmly connected through what I’d discovered.
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.6 Hydrodynamics, and a Turbulent Tale : Back from when I first saw it at the age of 14 it was always my favorite page in The Feynman Lectures on Physics. But how did the phenomenon of turbulence that it showed happen, and what really was it? In late 1984, the first version of the Connection Machine was nearing completion, and there was a question of what could be done with it. I agreed to analyze its potential uses in scientific computation, and in my resulting (never ultimately completed) report the very first section was about fluid turbulence (others sections were about quantum field theory, n-body problems, number theory, etc.): The traditional computational approach to studying fluids was to start from known continuum fluid equations, then to try to construct approximations to these suitable for numerical computation. But that wasn’t going to work well for the Connection Machine. Because in optimizing for parallelism, its individual processors were quite simple, and weren’t set up to do fast (e.g. floating-point) numerical computation. I’d been saying for years that cellular automata should be relevant to fluid turbulence. And my recent study of the origins of randomness made me all the more convinced that they would for example be able to capture the fundamental randomness associated with turbulence (which I explained as being a bit like encryption): I sent a letter to Feynman expressing my enthusiasm: I had been invited to a conference in Japan that summer on “High Reynolds Number Flow Computation” (i.e. computing turbulent fluid flow), and on May 4 I sent an abstract which explained a little more of my approach: My basic idea was to start not from continuum equations, but instead from a cellular automaton idealization of molecular dynamics. It was the same kind of underlying model as I’d tried to set up in my SPART program in 1973. But now instead of using it to study thermodynamic phenomena and the microscopic motions associated with heat, my idea was to use it to study the kind of visible motion that occurs in fluid dynamics—and in particular to see whether it could explain the apparent randomness of fluid turbulence. I knew from the beginning that I needed to rely on “Second Law behavior” in the underlying cellular automaton—because that’s what would lead to the randomness necessary to “wash out” the simple idealizations I was using in the cellular automaton, and allow standard continuum fluid behavior to emerge. And so it was that I embarked on the project of understanding not only thermodynamics, but also hydrodynamics and fluid turbulence, with cellular automata—on the Connection Machine. I’ve had the experience many times in my life of entering a field and bringing in new tools and new ideas. Back in 1985 I’d already done that several times, and it had always been a pretty much uniformly positive experience. But, sadly, with fluid turbulence, it was to be, at best, a turbulent experience. The idea that cellular automata might be useful in studying fluid turbulence definitely wasn’t obvious. The year before, for example, at the Nobel Symposium conference in Sweden, a French physicist named Uriel Frisch had been summarizing the state of turbulence research. Fittingly for the topic of turbulence, he and I first met after a rather bumpy helicopter ride to a conference event—where Frisch told me in no uncertain terms that cellular automata would never be relevant to turbulence, and talked about how turbulence was better thought of as being associated (a bit like in the mathematical theory of phase transitions) with “singularities getting close to the real line”. (Strangely, I just now looked at Frisch’s paper in the proceedings of the conference: “Ou en est la Turbulence Developpée?” [roughly: “Fully Developed Turbulence: Where Do We Stand?”], and was surprised to discover that its last paragraph actually mentions cellular automata, and its acknowledgements thank me for conversations—even though the paper says it was received June 11, 1984, a couple of days before I had met Frisch. And, yes, this is the kind of thing that makes accurately reconstructing history hard.) Los Alamos had always been a hotbed of computational fluid dynamics (not least because of its importance in simulating nuclear explosions)—and in fact of computing in general—and, starting in the late fall of 1984, on my visits there I talked to many people about using cellular automata to do fluid dynamics on the Connection Machine. Meanwhile, Brosl Hasslacher (mentioned above in connection with his 1982 encryption startup) had—after a rather itinerant career as a physicist—landed at Los Alamos. And in fact I had been asked by the Los Alamos management for a letter about him in December 1984 (yes, even though he was 18 years older than me), and ended what I wrote with: “He has considerable ability in identifying promising areas of research. I think he would be a significant addition to the staff at Los Alamos.” Well, in early 1985 Brosl identified cellular automaton fluid dynamics as a promising area, and started energetically talking to me about it. Meanwhile, the Connection Machine was just starting to work, and a young software engineer named Jim Salem was assigned to help me get cellular automaton fluid dynamics running on it. I didn’t know it at the time, but Brosl—ever the opportunist—had also made contact with Uriel Frisch, and now I find the curious document in French dated May 10, 1985, with the translated title “A New Concept for Supercomputers: Cellular Automata”, laying out a grand international multiyear plan, and referencing the (so far as I know, nonexistent) B. Hasslacher and U. Frisch (1985), “The Cellular Automaton Turbulence Machine”, Los Alamos: I visited Los Alamos again in May, but for much of the summer I was at Thinking Machines, and on July 18 Uriel Frisch came to visit there, along with a French physicist named Yves Pomeau, who had done some nice work in the 1970s on applying methods of traditional statistical mechanics to “lattice gases”. But what about realistic fluid dynamics, and turbulence? I wasn’t sure how easy it would be to “build up from the (idealized) molecules” to get to pictures of recognizable fluid flows. But we were starting to have some success in generating at least basic results. It wasn’t clear how seriously anyone else was taking this (especially given that at the time I hadn’t seen the material Frisch had already written), but insofar as anything was “going on”, it seemed to be a perfectly collegial interaction—where perhaps Los Alamos or the French government or both would buy a Connection Machine computer. But meanwhile, on the technical side, it had become clear that the most obvious square-lattice model (that Pomeau had used in the 1970s, and that was basically what my SPART program from 1973 was supposed to implement) was fine for diffusion processes, but couldn’t really represent proper fluid flow. When I first started working on cellular automata in 1981 the minimal 1D case in which I was most interested had barely been studied, but there had been quite a bit of work done in previous decades on the 2D case. By the 1980s, however, it had mostly petered out—with the exception of a group at MIT led by Ed Fredkin, who had long had the belief that one might in effect be able to “construct all of physics” using cellular automata. Tom Toffoli and Norm Margolus, who were working with him, had built a hardware 2D cellular automaton simulator—that I happened to photograph in 1982 when visiting Fredkin’s island in the Caribbean: But while “all of physics” was elusive (and our Physics Project suggests that a cellular automaton with a rigid lattice is not the right place to start), there’d been success in making for example an idealized gas, using essentially a block cellular automaton on a square grid. But mostly the cellular automaton machine was used in a maddeningly “Look at this cool thing!” mode, often accompanied by rapid physical rewiring. In early 1984 I visited MIT to use the machine to try to do what amounted to natural science, systematically studying 2D cellular automata. The result was a paper (with Norman Packard) on 2D cellular automata. We restricted ourselves to square grids, though mentioned hexagonal ones, and my article in Scientific American in late 1984 opened with a full-page hexagonal cellular automaton simulation of a snowflake made by Packard (and later in 1984 turned into one of a set of cellular automaton cards for sale): In any case, in the summer of 1985, with square lattices not doing what was needed, it was time to try hexagonal ones. I think Yves Pomeau already had a theoretical argument for this, but as far as I was concerned, it was (at least at first) just a “next thing to try”. Programming the Connection Machine was at that time a rather laborious process (which, almost unprecedentedly for me, I wasn’t doing myself), and mapping a hexagonal grid onto its basically square architecture was a little fiddly, as my notes record: Meanwhile, at Los Alamos, I’d introduced a young and very computer-savvy protege of mine named Tsutomu Shimomura (who had a habit of getting himself into computer security scrapes, though would later become famous for taking down a well-known hacker) to Brosl Hasslacher, and now Tsutomu jumped into writing optimized code to implement hexagonal cellular automata on a Cray supercomputer. In my archives I now find a draft paper from September 7 that starts with a nice (if not entirely correct) discussion of what amounts to computational irreducibility, and then continues by giving theoretical symmetry-based arguments that a hexagonal cellular automaton should be able to reproduce fluid mechanics: Near the end, the draft says (misspelling Tsutomu Shimomura’s name): Meanwhile, we (as well as everyone else) were starting to get results that looked at least suggestive: By November 15 I had drafted a paper that included some more detailed pictures and that at the end (I thought, graciously) thanked Frisch, Hasslacher, Pomeau and Shimomura for “discussions and for sharing their unpublished results with us”, which by that point included a bunch of suggestive, if not obviously correct, pictures of fluid-flow-like behavior. To me, what was important about our paper is that, after all these years, it filled in with more detail just how computational systems like cellular automata could lead to Second-Law-style thermodynamic behavior, and it “proved” the physicality of what was going on by showing easy-to-recognize fluid-dynamics-like behavior. Just four days later, though, there was a big surprise. The Washington Post ran a front-page story—alongside the day’s characteristic-Cold-War-era geopolitical news—about the “Hasslacher–Frisch model”, and about how it might be judged so important that it “should be classified to keep it out of Soviet hands”: At that point, things went crazy. There was talk of Nobel Prizes (I wasn’t buying it). There were official complaints from the French embassy about French scientists not being adequately recognized. There was upset at Thinking Machines for not even being mentioned. And, yes, as the originator of the idea, I was miffed that nobody seemed to have even suggested contacting me—even if I did view the rather breathless and “geopolitical” tenor of the article as being pretty far from immediate reality. At the time, everyone involved denied having been responsible for the appearance of the article. But years later it emerged that the source was a certain John Gage, former political operative and longtime marketing operative at Sun Microsystems, who I’d known since 1982, and had at some point introduced to Brosl Hasslacher. Apparently he’d called around various government contacts to help encourage open (international) sharing of scientific code, quoting this as a test case. But as it was, the article had pretty much exactly the opposite effect, with everyone now out for themselves. In Princeton, I’d interacted with Steve Orszag, whose funding for his new (traditional) computational fluid dynamics company, Nektonics, now seemed at risk, and who pulled me into an emergency effort to prove that cellular automaton fluid dynamics couldn’t be competitive. (The paper he wrote about this seemed interesting, but I demurred on being a coauthor.) Meanwhile, Thinking Machines wanted to file a patent as quickly as possible. Any possibility of the French government getting a Connection Machine evaporated and soon Brosl Hasslacher was claiming that “the French are faking their data”. And then there was the matter of the various academic papers. I had been sent the Frisch–Hasslacher–Pomeau paper to review, and checking my 1985 calendar for my whereabouts I must have received it the very day I finished my paper. I told the journal they should publish the paper, suggesting some changes to avoid naivete about computing and computer technology, but not mentioning its very thin recognition of my work. Our paper, on the other hand, triggered a rather indecorous competitive response, with two “anonymous reviewers” claiming that the paper said nothing more than its “reference 5” (the Frisch–Hasslacher–Pomeau paper). I patiently pointed out that that wasn’t the case, not least because our paper had actual simulations, but also that actually I happened to have “been there first” with the overall idea. The journal solicited other opinions, which were mostly supportive. But in the end a certain Leo Kadanoff swooped in to block it, only to publish his own a few months later. It felt corrupt, and distasteful. I was at that point a successful and increasingly established academic. And some of the people involved were even longtime friends. So was this kind of thing what I had to look forward to in a life in academia? That didn’t seem attractive, or necessary. And it was what began the process that led me, a year and a half later, to finally choose to leave academia behind, never to return. Still, despite the “turbulence”—and in the midst of other activities—I continued to work hard on cellular automaton fluids, and by January 1986 I had the first version of a long (and, I thought, rather good) paper on their basic theory (that was finished and published later that year): As it turns out, the methods I used in that paper provide some important seeds for our Physics Project, and even in recent times I’ve often found myself referring to the paper, complete with its SMP open-code appendix: But in addition to developing the theory, I was also getting simulations done on the Connection Machine, and getting actual experimental data (particularly on flow past cylinders) to compare them to. By February 1986, we had quite a few results: But by this point there was a quite industrial effort, particularly in France, that was churning out papers on cellular automaton fluids at a high rate. I’d called my theory paper “Cellular Automaton Fluid 1: Basic Theory”. But was it really worth finishing part 2? There was a veritable army of perfectly good physicists “competing” with me. And, I thought, “I have other things to do. Just let them do this. This doesn’t need me”. And so it was that in the middle of 1986 I stopped working on cellular automaton fluids. And, yes, that freed me up to work on lots of other interesting things. But even though methods derived from cellular automaton fluids have become widely used in practical fluid dynamics computations, the key basic science that I thought could be addressed with cellular automaton fluids—about things like the origin of randomness in turbulence—has still, even to this day, not really been further explored.
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.7 Getting to the Continuum : In June 1986 I was about to launch both a research center (the Center for Complex Systems Research at the University of Illinois) and a journal (Complex Systems)—and I was also organizing a conference called CA ’86 (which was held at MIT). The core of the conference was poster presentations, and a few days before the conference was to start I decided I should find a “nice little project” that I could quickly turn into a poster. In studying cellular automaton fluids I had found that cellular automata with rules based on idealized physical molecular dynamics could on a large scale approximate the continuum behavior of fluids. But what if one just started from continuum behavior? Could one derive underlying rules that would reproduce it? Or perhaps even find the minimal such rules? By mid-1985 I felt I’d made decent progress on the science of cellular automata. But what about their engineering? What about constructing cellular automata with particular behavior? In May 1985 I had given a conference talk about “Cellular Automaton Engineering”, which turned into a paper about “Approaches to Complexity Engineering”—that in effect tried to set up “trainable cellular automata” in what might still be a powerful simple-programs-meet-machine-learning scheme that deserves to be explored: But so it was that a few days before the CA ’86 conference I decided to try to find a minimal “cellular automaton approximation” to a simple continuum process: diffusion in one dimension. I explained and described as my objective: I used block cellular automata, and tried to find rules that were reversible and also conserved something that could serve as “microscopic density” or “particle number”. I quickly determined that there were no such rules with 2 colors and blocks of sizes 2 or 3 that achieved any kind of randomization. To go to 3 colors, I used SMP to generate candidate rules where for example the function Apper can be literally be translated into Wolfram Language as or, more idiomatically, just then did what I have done so many times and just printed out pictures of their behavior: Some clearly did not show randomization, but a couple did. And soon I was studying what I called the “winning rule”, which—like rule 30—went from simple initial conditions to apparent randomness: I analyzed what the rule was “microscopically doing” and explored its longer-time behavior: Then I did things like analyze its cycle structure in a finite-size region by running C programs I’d basically already developed back in 1982 (though now they were modified to automatically generate troff code for typesetting): And, like rule 30, the “winning rule” that I found back in June 1986 has stayed with me, essentially as a minimal example of reversible, number-conserving randomness. It appeared in A New Kind of Science, and it appears now in my recent work on the Second Law—and, of course, the patterns it makes are always the same: Back in 1986 I wanted to know just how efficiently a simple rule like this could reproduce continuum behavior. And in a portent of observer theory my notes from the time talk about “optimal coarse graining, where the 2nd law is ‘most true’”, then go on to compare the distributed character of the cellular automaton with traditional “collect information into numerical value” finite-difference approximations: In a talk I gave I summarized my understanding: The phenomenon of randomization is generic in computational systems (witness rule 30, the “winning rule”, etc.) This leads to the genericity of thermodynamics. And this in turn leads to the genericity of continuum behavior, with diffusion and fluid behavior being two examples. It would take another 34 years, but these basic ideas would eventually be what underlies our Physics Project, and our understanding of the emergence of things like spacetime. As well as now being crucial to our whole understanding of the Second Law.
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.8 The Second Law in A New Kind of Science : By the end of 1986 I had begun the development of Mathematica, and what would become the Wolfram Language, and for most of the next five years I was submerged in technology development. But in 1991 I started to use the technology I now had, and began the project that became A New Kind of Science. Much of the first couple of years was spent exploring the computational universe of simple programs, and discovering that the phenomena I’d discovered in cellular automata were actually much more general. And it was seeing that generality that led me to the Principle of Computational Equivalence. In formulating the concept of computational irreducibility I’d in effect been thinking about trying to “reduce” the behavior of systems using an external as-powerful-as-possible universal computer. But now I’d realized I should just be thinking about all systems as somehow computationally equivalent. And in doing that I was pulling the conception of the “observer” and their computational ability closer to the systems they were observing. But the further development of that idea would have to wait nearly three more decades, until the arrival of our Physics Project. In A New Kind of Science, Chapter 7 on “Mechanisms in Programs and Nature” describes the concept of intrinsic randomness generation, and how it’s distinguished from other sources of randomness. Chapter 8 on “Implications for Everyday Systems” then has a section on fluid flow, where I describe the idea that randomness in turbulence could be intrinsically generated, making it, for example, repeatable, rather than inevitably different every time an experiment is run. And then there’s Chapter 9, entitled “Fundamental Physics”. The majority of the chapter—and its “most famous” part—is the presentation of the direct precursor to our Physics Project, including the concept of graph-rewriting-based computational models for the lowest-level structure of spacetime and the universe. But there’s an earlier part of Chapter 9 as well, and it’s about the Second Law. There’s a precursor about “The Notion of Reversibility”, and then we’re on to a section about “Irreversibility and the Second Law of Thermodynamics”, followed by “Conserved Quantities and Continuum Phenomena”, which is where the “winning rule” I discovered in 1996 appears again: My records show I wrote all of this—and generated all the pictures—between May 2 and July 11, 1995. I felt I already had a pretty good grasp of how the Second Law worked, and just needed to write it down. My emphasis was on explaining how a microscopically reversible rule—through its intrinsic ability to generate randomness—could lead to what appears to be irreversible behavior. Mostly I used reversible 1D cellular automata as my examples, showing for example randomization both forwards and backwards in time: I soon got to the nub of the issue with irreversibility and the Second Law: I talked about how “typical textbook thermodynamics” involves a bunch of details about energy and motion, and to get closer to this I showed a simple example of an “ideal gas” 2D cellular automaton: But despite my early exposure to hard-sphere gases, I never went as far as to use them as examples in A New Kind of Science. We did actually take some photographs of the mechanics of real-life billiards: But cellular automata always seemed like a much clearer way to understand what was going on, free from issues like numerical precision, or their physical analogs. And by looking at cellular automata I felt as if I could really see down the foundations of the Second Law, and why it was true. And mostly it was a story of computational irreducibility, and intrinsic randomness generation. But then there was rule 37R. I’ve often said that in studying the computational universe we have to remember that the “computational animals” are at least as smart as we are—and they’re always up to tricks we don’t expect. And so it is with rule 37R. In 1986 I’d published a book of cellular automaton papers, and as an appendix I’d included lots of tables of properties of cellular automata. Almost all the tables were about the ordinary elementary cellular automata. But as a kind of “throwaway” at the very end I gave a table of the behavior of the 256 second-order reversible versions of the elementary rules, including 37R starting both from completely random initial conditions and from single black cells: So far, nothing remarkable. And years go by. But then—apparently in the middle of working on the 2D systems section of A New Kind of Science—at 4:38am on February 21, 1994 (according to my filesystem records), I generate pictures of all the reversible elementary rules again, but now from initial conditions that are slightly more complicated than a single black cell. Opening the notebook from that time (and, yes, Wolfram Language and our notebook format have been stable enough that 28 years later that still works) it shows up tiny on a modern screen, but there it is: rule 37R doing something “interesting”: Clearly I noticed it. Because by 4:47am I’ve generated lots of pictures of rule 37R, like this one evolving from a block of 21 black cells, and showing only every other step and by 4:54am I’ve got things like: My guess is that I was looking for class 4 behavior in reversible cellular automata. And with rule 37R I’d found it. And at the time I moved on to other things. (On March 1, 1994, I slipped on some ice and broke my ankle, and was largely out of action for several weeks.) And that takes us back to May 1995, when I was working on writing about the Second Law. My filesystem records that I did quite a few more experiments on rule 37R then, looking at different initial conditions, and running it as long as I could, to see if its strange neither-simple-nor-randomizing—and not very Second-Law-like—behavior would somehow “resolve”. Up to that moment, for nearly a quarter of a century, I had always fundamentally believed in the Second Law. Yes, I thought there might be exceptions with things like self-gravitating systems. But I’d always assumed that—perhaps with some pathological exceptions—the Second Law was something quite universal, whose origins I could even now understand through computational irreducibility. But seeing rule 37R this suddenly didn’t seem right. In A New Kind of Science I included a long run of rule 37R (here colorized to emphasize the structure) then explained: How could one describe what was happening in rule 37R? I discussed the idea that it was effectively forming “membranes” which could slowly move, but keep things “modular” and organized inside. I summarized at the time, tagging it as “something I wanted to explore in more detail one day”: Rounding out the rest of A New Kind of Science takes another seven years of intense work. But finally in May 2002 it was published. The book talked about many things. And even within Chapter 9 my discussion of the Second Law was overshadowed by the outline I gave of an approach to finding a truly fundamental theory of physics—and of the ideas that evolved into our Physics Project.
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.9 The Physics Project—and the Second Law Again : After A New Kind of Science was finished I spent many years working mainly on technology—building Wolfram|Alpha, launching the Wolfram Language and so on. But “follow up on Chapter 9” was always on my longterm to-do list. The biggest—and most difficult—part of that had to do with fundamental physics. But I still had a great intellectual attachment to the Second Law, and I always wanted to use what I’d then understood about the computational paradigm to “tighten up” and “round out” the Second Law. I’d mention it to people from time to time. Usually the response was the same: “Wasn’t the Second Law understood a century ago? What more is there to say?” Then I’d explain, and it’d be like “Oh, yes, that is interesting”. But somehow it always seemed like people felt the Second Law was “old news”, and that whatever I might do would just be “dotting an i or crossing a t”. And in the end my Second Law project never quite made it onto my active list, despite the fact that it was something I always wanted to do. Occasionally I would write about my ideas for finding a fundamental theory of physics. And, implicitly I’d rely on the understanding I’d developed of the foundations and generalization of the Second Law. In 2015, for example, celebrating the centenary of General Relativity, I wrote about what spacetime might really be like “underneath” and how a perceived spacetime continuum might emerge from discrete underlying structure like fluid behavior emerges from molecular dynamics—in effect through the operation of a generalized Second Law: It was 17 years after the publication of A New Kind of Science that (as I’ve described elsewhere) circumstances finally aligned to embark on what became our Physics Project. And after all those years, the idea of computational irreducibility—and its immediate implications for the Second Law—had come to seem so obvious to me (and to the young physicists with whom I worked) that they could just be taken for granted as conceptual building blocks in constructing the tower of ideas we needed. One of the surprising and dramatic implications of our Physics Project is that General Relativity and quantum mechanics are in a sense both manifestations of the same fundamental phenomenon—but played out respectively in physical space and in branchial space. But what really is this phenomenon? What became clear is that ultimately it’s all about the interplay between underlying computational irreducibility and our nature as observers. It’s a concept that had its origins in my thinking about the Second Law. Because even in 1984 I’d understood that the Second Law is about our inability to “decode” underlying computationally irreducible behavior. In A New Kind of Science I’d devoted Chapter 10 to “Processes of Perception and Analysis”, and I’d recognized that we should view such processes—like any processes in nature or elsewhere—as being fundamentally computational. But I still thought of processes of perception and analysis as being separated from—and in some sense “outside”—actual processes we might be studying. But in our Physics Project we’re studying the whole universe, so inevitably we as observers are “inside” and part of the system. And what then became clear is the emergence of things like General Relativity and quantum mechanics depends on certain characteristics of us as observers. “Alien observers” might perceive quite different laws of physics (or no systematic laws at all). But for “observers like us”, who are computationally bounded and believe we are persistent in time, General Relativity and quantum mechanics are inevitable. In a sense, therefore, General Relativity and quantum mechanics become “abstractly derivable” given our nature as observers. And the remarkable thing is that at some level the story is exactly the same with the Second Law. To me it’s a surprising and deeply beautiful scientific unification: that all three of the great foundational theories of physics—General Relativity, quantum mechanics and statistical mechanics—are in effect manifestations of the same core phenomenon: an interplay between computational irreducibility and our nature as observers. Back in the 1970s I had no inkling of all this. And even when I chose to combine my discussions of the Second Law and of my approach to a fundamental theory of physics into a single chapter of A New Kind of Science, I didn’t know how deeply these would be connected. It’s been a long and winding path, that’s needed to pass through many different pieces of science and technology. But in the end the feeling I had when I first studied that book cover when I was 12 years old that “this was something fundamental” has played out on a scale almost incomprehensibly beyond what I had ever imagined.
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.10 Discovering Class 4 : Most of my journey with the Second Law has had to do with understanding origins of randomness, and their relation to “typical Second-Law behavior”. But there’s another piece—still incompletely worked out—which has to do with surprises like rule 37R, and, more generally, with large-scale versions of class 4 behavior, or what I’ve begun to call the “mechanoidal phase”. I first identified class 4 behavior as part of my systematic exploration of 1D cellular automata at the beginning of 1983—with the “code 20” k = 2, r = 2 totalistic rule being my first clear example: Very soon my searches had identified a whole variety of localized structures in this rule: At the time, the most significant attribute of class 4 cellular automata as far as I was concerned was that they seemed likely to be computation universal—and potentially provably so. But from the beginning I was also interested in what their “thermodynamics” might be. If you start them off from random initial conditions, will their patterns die out, or will some arrangement of localized structures persist, and perhaps even grow? In most cellular automata—and indeed most systems with local rules—one expects that at least their statistical properties will somehow stabilize when one goes to the limit of infinite size. But, I asked, does that infinite-size limit even “exist” for class 4 systems—or if you progressively increase the size, will the results you get keep on jumping around forever, perhaps as you succeed in sampling progressively more exotic structures? A paper I wrote in September 1983 talks about the idea that in a sufficiently large class 4 cellular automaton one would eventually get self-reproducing structures, which would end up “taking over everything”: The idea that one might be able to see “biology-like” self-reproduction in cellular automata has a long history. Indeed, one of the multiple ways that cellular automata were invented (and the one that led to their name) was through John von Neumann’s 1952 effort to construct a complicated cellular automaton in which there could be a complicated configuration capable of self-reproduction. But could self-reproducing structures ever “occur naturally” in cellular automata? Without the benefit of intuition from things like rule 30, von Neumann assumed that something like self-reproduction would need an incredibly complicated setup, as it seems to have, for example, in biology. But having seen rule 30—and more so class 4 cellular automata—it didn’t seem so implausible to me that even with very simple underlying rules, there could be fairly simple configurations that would show phenomena like self-reproduction. But for such a configuration to “occur naturally” in a random initial condition might require a system with exponentially many cells. And I wondered if in the oceans of the early Earth there might have been only “just enough” molecules for something like a self-reproducing lifeform to occur. Back in 1983 I already had pretty efficient code for searching for structures in class 4 cellular automata. But even running for days at a time, I never found anything more complicated than purely periodic (if sometimes moving) structures. And in March 1985, following an article about my work in Scientific American, I appealed to the public to find “interesting structures”—like “glider guns” that would “shoot out” moving structures: As it happened, right before I made my “public appeal”, a student at Princeton working with a professor I knew had sent me a glider gun he’d found the k = 2, r = 3 totalistic code 88 rule: At the time, though, with computer displays only large enough to see behavior like I wasn’t convinced this was an “ordinary class 4 rule”—even though now, with the benefit of higher display resolution, it seems more convincing: The “public appeal” generated a lot of interesting feedback—but no glider guns or other exotic structures in the rules I considered “obviously class 4”. And it wasn’t until after I started working on A New Kind of Science that I got back to the question. But then, on the evening of December 31, 1991, using exactly the same code as in 1983, but now with faster computers, there it was: in an ordinary class 4 rule (k = 3, r = 1 code 1329), after finding several localized structures, there was one that grew without bound (albeit not in the most obvious “glider gun” way): But that wasn’t all. Exemplifying the principle that in the computational universe there are always surprises, searching a little further revealed yet other unexpected structures: Every few years something else would come up with class 4 rules. In 1994, lots of work on rule 110. In 1995, the surprise of rule 37R. In 1998 efforts to find analogs of particles that might carry over to my graph-based model of space. After A New Kind of Science was published in 2002, we started our annual Wolfram Summer School (at first called the NKS Summer School)—and in 2010 our High School Summer Camp. Some years we asked students to pick their “favorite cellular automaton”. Often they were class 4: And occasionally someone would do a project to explore the world of some particular class 4 rule. But beyond those specifics—and statements about computation universality—it’s never been clear quite what one could say about class 4. Back in 1984 in the series of cellular automaton postcards I’d produced, there were a couple of class 4 examples: And even then the typical response to these images was that they looked “organic”—like the kind of thing living organisms might produce. A decade later—for A New Kind of Science—I studied “organic forms” quite a bit, trying to understand how organisms get their overall shapes, and surface patterns. Mostly that didn’t end up being a story of class 4 behavior, though. Since the early 1980s I’ve been interested in molecular computing, and in how computation might be done at the level of molecules. My discoveries in A New Kind of Science (and specifically the Principle of Computational Equivalence) convinced me that it should be possible to get even fairly simple collections of molecules to “do arbitrary computations” or even build more or less arbitrary structures (in a more general and streamlined way than happens with the whole protein synthesis structure in biology). And over the years, I sometimes thought about trying to do practical work in this area. But it didn’t feel as if the ambient technology was quite ready. So I never jumped in. Meanwhile, I’d long understood the basic correspondence between multiway systems and patterns of possible pathways for chemical reactions. And after our Physics Project was announced in 2020 and we began to develop the general multicomputational paradigm, I immediately considered molecular computing a potential application. But just what might the “choreography” of molecules be like? What causal relationships might there be, for example, between different interactions of the same molecule? That’s not something ordinary chemistry—dealing for example with liquid-phase reactions—tends to consider important. But what I increasingly started to wonder is whether in molecular biology it might actually be crucial. And even in the 20 years since A New Kind of Science was published, it’s become increasingly clear that in molecular biology things are extremely “orchestrated”. It’s not about molecules randomly moving around, like in a liquid. It’s about molecules being carefully channeled and actively transported from one “event” to another. Class 3 cellular automata seem to be good “metamodels” for things like liquids, and readily give Second-Law-like behavior. But what about the kind of situation that seems to exist in molecular biology? It’s something I’ve been thinking about only recently, but I think this is a place where class 4 cellular automata can contribute. I’ve started calling the “bulk limit” of class 4 systems the “mechanoidal phase”. It’s a place where the ordinary Second Law doesn’t seem to apply. Four decades ago when I was trying to understand how structure could arise “in violation of the Second Law” I didn’t yet even know about computational irreducibility. But now we’ve come a lot further, in particular with the development of the multicomputational paradigm, and the recognition of the importance of the characteristics of the observer in defining what perceived overall laws there will be. It’s an inevitable feature of computational irreducibility that there will always be an infinite sequence of new challenges for science, and new pieces of computational reducibility to be found. So, now, yes, a challenge is to understand the mechanoidal phase. And with all the tools and ideas we’ve developed, I’m hoping the process will happen more than it has for the ordinary Second Law.
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.11 The End of a 50-Year Journey : I began my quest to understand the Second Law a bit more than 50 years ago. And—even though there’s certainly more to say and figure out—it’s very satisfying now to be able to bring a certain amount of closure to what has been the single longest-running piece of intellectual “unfinished business” in my life. It’s been an interesting journey—that’s very much relied on, and at times helped drive, the tower of science and technology that I’ve spent my life building. There are many things that might not have happened as they did. And in the end it’s been a story of longterm intellectual tenacity—stretching across much of my life so far. For a long time I’ve kept (automatically when possible) quite extensive archives. And now these archives allow one to reconstruct in almost unprecedented detail my journey with the Second Law. One sees the gradual formation of intellectual frameworks over the course of years, then the occasional discovery or realization that allows one to take the next step in what is sometimes mere days. There’s a curious interweaving of computational and essentially philosophical methodologies—with an occasional dash of mathematics. Sometimes there’s general intuition that’s significantly ahead of specific results. But more often there’s a surprise computational discovery that seeds the development of new intuition. And, yes, it’s a little embarrassing how often I managed to generate in a computer experiment something that I completely failed to interpret or even notice at first because I didn’t have the right intellectual framework or intuition. And in the end, there’s an air of computational irreducibility to the whole process: there really wasn’t a way to shortcut the intellectual development; one just had to live it. Already in the 1990s I had taken things a fair distance, and I had even written a little about what I had figured out. But for years it hung out there as one of a small collection of unfinished projects: to finally round out the intellectual story of the Second Law, and to write down an exposition of it. But the arrival of our Physics Project just over two years ago brought both a cascade of new ideas, and for me personally a sense that even things that had been out there for a long time could in fact be brought to closure. And so it is that I’ve returned to the quest I began when I was 12 years old—but now with five decades of new tools and new ideas. The wonder and magic of the Second Law is still there. But now I’m able to see it in a much broader context, and to realize that it’s not just a law about thermodynamics and heat, but instead a window into a very general computational phenomenon. None of this I could know when I was 12 years old. But somehow the quest I was drawn to all those years ago has turned out to be deeply aligned with the whole arc of intellectual development that I have followed in my life. And no doubt it’s no coincidence. But for now I’m just grateful to have had the quest to understand Second Law as one of my guiding forces through so much of my life, and now to realize that my quest was part of something so broad and so deep.
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.12 Appendix: The Backstory of the Book Cover That Started It All : What is the backstory of the book cover that launched my long journey with the Second Law? The book was published in 1965, and inside its front flap we find: On page 7 we then find: In 2001—as I was putting the finishing touches to the historical notes for A New Kind of Science—I tracked down Berni Alder (who died in 2020 at the age of 94) to ask him the origin of the pictures. It turned out to be a complex story, reaching back to the earliest serious uses of computers for basic science, and even beyond. The book had been born out the sense of urgency around science education in the US that followed the launch of Sputnik by the Soviet Union—with a group of professors from Berkeley and Harvard believing that the teaching of freshman college physics was in need of modernization, and that they should write a series of textbooks to enable this. (It was also the time of the “new math”, and a host of other STEM-related educational initiatives.) Fred Reif (who died at the age of 92 in 2019) was asked to write the statistical physics volume. As he explained in the preface to the book ending with: Well, it’s taken me 50 years to get to the point where I think I really understand the Second Law that is at the center of the book. And in 2001 I was able to tell Fred Reif that, yes, his book had indeed been useful. He said he was pleased to learn that, adding “It is all too rare that one’s educational efforts seem to bear some fruit.” He explained to me that when he was writing the book he thought that “the basic ideas of irreversibility and fluctuations might be very vividly illustrated by the behavior of a gas of particles spreading through a box”. He added: “It then occurred to me that Berni Alder might actually show this by a computer generated film since he had worked on molecular dynamics simulations and had also good computer facilities available to him. I was able to enlist Berni’s interest in this project, with the results shown in my book.” The acknowledgements in the book report: Berni Alder and and Fred Reif did indeed create a “film loop”, which “could be bought separately from the book and viewed in the physics lab”, as Alder told me, adding that “I understand the students liked it very much, but the venture was not a commercial success.” Still, he sent me a copy of a videotape version: The film (which has no sound) begins: Soon it’s showing an actual process of “coming to equilibrium”: “However”, as Alder explained it to me, “if a large number of particles are put in the corner and the velocities of all the particles are reversed after a certain time, the audience laughs or is supposed to after all the particles return to their original positions.” (One suspects that particularly in the 1960s this might have been reminiscent of various cartoon-film gags.) OK, so how were the pictures (and the film) made? It was done in 1964 at what’s now Lawrence Livermore Lab (that had been created in 1952 as a spinoff of the Berkeley Radiation Lab, which had initiated some key pieces for the Manhattan Project) on a computer called the LARC (“Livermore Advanced Research Computer”), first made in 1960, that was probably the most advanced scientific computer of the time. Alder explained to me, however: “We could not run the problem much longer than about 10 collision times with 64 bits [sic] arithmetic before the round-off error prevented the particles from returning.” Why did they start the particles off in a somewhat random configuration? (The randomness, Alder told me, had been created by a middle-square random number generator.) Apparently if they’d been in a regular array—which would have made the whole process of randomization much easier to see—the roundoff errors would have been too obvious. (And it’s issues like this that made it so hard to recognize the rule 30 phenomenon in systems based on real numbers—and without the idea of just studying simple programs not tied to traditional equation-based formulations of physics.) The actual code for the molecular dynamics simulation was written in assembler and run by Mary Ann Mansigh (Karlsen), who had a degree in math and chemistry and worked as a programmer at Livermore from 1955 until the 1980s, much of the time specifically with Alder. Here she is at the console of the LARC (yes, computers had built-in desks in those days): The program that was used was called STEP, and the original version of it had actually been written (by a certain Norm Hardy, who ended up having a long Silicon Valley career) to run on a previous generation of computer. (A still-earlier program was called APE, for “Approach to Equilibrium”.) But it was only with the LARC—and STEP—that things were fast enough to run substantial simulations, at the rate of about 200,000 collisions per hour (the simulation for the book cover involved 40 particles and about 500 collisions). At the time of the book STEP used an n2 algorithm where all pairs of particles were tested for collisions; later a neighborhood-based linked list method was used. The standard method of getting output from a computer back in 1964—and basically until the 1980s—was to print characters on paper. But the LARC could also drive an oscilloscope, and it was with this that the graphics for the book were created (capturing them from the oscilloscope screen with a Polaroid instant camera). But why was Berni Alder studying molecular dynamics and “hard sphere gases” in the first place? Well, that’s another long story. But ultimately it was driven by the effort to develop a microscopic theory of liquids. The notion that gases might consist of discrete molecules in motion had arisen in the 1700s (and even to some extent in antiquity), but it was only in the mid-1800s that serious development of the “kinetic theory” idea began. Pretty immediately it was clear how to derive the ideal gas law P V = R T for essentially non-interacting molecules. But what analog of this “equation of state” might apply to gases with significant interactions between molecules, or, for that matter, liquids? In 1873 Johannes Diderik van der Waals proposed, on essentially empirical grounds, the formula (P + a/V2)(V–b) = RT—where the parameter b represented “excluded volume” taken up by molecules, that were implicitly being viewed as hard spheres. But could such a formula be derived—like the ideal gas law—from a microscopic kinetic theory of molecules? At the time, nobody really knew how to start, and the problem languished for more than half a century. (It’s worth pointing out, by the way, that the idea of modeling gases, as opposed to liquids, as collections of hard spheres was extensively pursued in the mid-1800s, notably by Maxwell and Boltzmann—though with their traditional mathematical analysis methods, they were limited to studying average properties of what amount to dilute gases.) Meanwhile, there was increasing interest in the microscopic structure of liquids, particularly among chemists concerned for example with how chemical solutions might work. And at the end of the 1920s the technique of x-ray diffraction, which had originally been used to study the microscopic structure of crystals, was applied to liquids—allowing in particular the experimental determination of the radial distribution function (or pair correlation function) g(r), which gives the probability to find another molecule a distance r from a given one. But how might this radial distribution function be computed? By the mid-1930s there were several proposals based on looking at the statistics of random assemblies of hard spheres: Some tried to get results by mathematical methods; others did physical experiments with ball bearings and gelatin balls, getting at least rough agreement with actual experiments on liquids: But then in 1939 a physical chemist named John Kirkwood gave an actual probabilistic derivation (using a variety of simplifying assumptions) that fairly closely reproduced the radial distribution function: But what about just computing from first principles, on the basis of the mechanics of colliding molecules? Back in 1872 Ludwig Boltzmann had proposed a statistical equation (the “Boltzmann transport equation”) for the behavior of collections of molecules, that was based on the approximation of independent probabilities for individual molecules. By the 1940s the independence assumption had been overcome, but at the cost of introducing an infinite hierarchy of equations (the “BBGKY hierarchy”, where the “K” stood for Kirkwood). And although the full equations were intractable, approximations were suggested that—while themselves mathematically sophisticated—seemed as if they should, at least in principle, be applicable to liquids. Meanwhile, in 1948, Berni Alder, fresh from a master’s degree in chemical engineering, and already interested in liquids, went to Caltech to work on a PhD with John Kirkwood—who suggested that he look at a couple of approximations to the BBGKY hierarchy for the case of hard spheres. This led to some nasty integro-differential equations which couldn’t be solved by analytical techniques. Caltech didn’t yet have a computer in the modern sense, but in 1949 they acquired an IBM 604 Electronic Calculating Punch, which could be wired to do calculations with input and output specified on punched cards—and it was on this machine that Alder got the calculations he needed done (the paper records that “[this] … was calculated … with the use of IBM equipment and the file of punched cards of sin(ut) employed in these laboratories for electron diffraction calculation”): Our story now moves to Los Alamos, where in 1947 Stan Ulam had suggested the Monte Carlo method as a way to study neutron diffusion. In 1949 the method was implemented on the ENIAC computer. And in 1952 Los Alamos got its own MANIAC computer. Meanwhile, there was significant interest at Los Alamos in computing equations of state for matter, especially in extreme conditions such as those in a nuclear explosion. And by 1953 the idea had arisen of using the Monte Carlo method to do this. The concept was to take a collection of hard spheres (or actually 2D disks), and move them randomly in a series of steps with the constraint that they could not overlap—then look at the statistics of the resulting “equilibrium” configurations. This was done on the MANIAC, with the resulting paper now giving “Monte Carlo results” for things like the radial distribution function: Kirkwood and Alder had been continuing their BBGKY hierarchy work, now using more realistic Lennard-Jones forces between molecules. But by 1954 Alder was also using the Monte Carlo method, implementing it partly (rather painfully) on the IBM Electronic Calculating Punch, and partly on the Manchester Mark II computer in the UK (whose documentation had been written by Alan Turing): In 1955 Alder started working full-time at Livermore, recruited by Edward Teller. Another Livermore recruit—fresh from a physics PhD—was Thomas Wainwright. And soon Alder and Wainwright came up with an alternative to the Monte Carlo method—that would eventually give the book cover pictures: just explicitly compute the dynamics of colliding hard spheres, with the expectation that after enough collisions the system would come to equilibrium and allow things like equations of state to be obtained. In 1953 Livermore had obtained its first computer: a Remington Rand Univac I. And it was on this computer that Alder and Wainwright did a first proof of concept of their method, tracing 100 hard spheres with collisions computed at the rate of about 100 per hour. Then in 1955 Livermore got IBM 704 computers, which, with their hardware floating-point capabilities, were able to compute about 2000 collisions per hour. Alder and Wainwright reported their first results at a statistical mechanics conference in Brussels in August 1956 (organized by Ilya Prigogine). The published version appeared in 1958: It gives evidence—that they tagged as “provisional”—for the emergence of a Maxwell–Boltzmann velocity distribution “after the system reached equilibrium” as well as things like the radial distribution function—and the equation of state: It was notable that there seemed to be a discrepancy between the results for the equation of state computed by explicit molecular dynamics and by the Monte Carlo method. And what is more, there seemed to be evidence of some kind of discontinuous phase-transition-like behavior as the density of spheres changed (an effect which Kirkwood had predicted in 1949). Given the small system sizes and short runtimes it was all a bit muddy. But by August 1957 Alder and Wainwright announced that they’d found a phase transition, presumably between a high-density phase where the spheres were packed together like in a crystalline solid, and a low-density phase, where they were able to more freely “wander around” like in a liquid or gas. Meanwhile, the group at Los Alamos had redone their Monte Carlo calculations, and they too now claimed a phase transition. Their papers were published back to back: But at this point no actual pictures of molecular trajectories had yet been published, or, I believe, made. All there was were traditional plots of aggregated quantities. And in 1958, these plots made their first appearance in a textbook. Tucked into Appendix C of Elementary Statistical Physics by Berkeley physics professor Charles Kittel (who would later be chairman of the group developing the Berkeley Physics Course book series) were two rather confusing plots about the approach to the Maxwell–Boltzmann distribution taken from a pre-publication version of Alder and Wainwright’s paper: Alder and Wainwright’s phase transition result had created enough of a stir that they were asked to write a Scientific American article about it. And in that article—entitled “Molecular Motions”, from October 1959—there were finally pictures of actual trajectories, with their caption explaining that the “paths of particles … appear as bright lines on the face of a cathode-ray tube hooked to a computer” (the paths are of the centers of the colliding disks): A technical article published at the same time gave a diagram of the logic for the dynamical computation: Then in 1960 Livermore (after various delays) took delivery of the LARC computer—arguably the first scientific supercomputer—which allowed molecular dynamics computations to be done perhaps 20 times times faster. A 1962 picture shows Berni Alder (left) and Thomas Wainwright (right) looking at outputs from the LARC with Mary Ann Mansigh (yes, in those days it was typical for male physicists to wear ties): And in 1964, the pictures for the Statistical Physics book (and film loop) got made, with Mary Ann Mansigh painstakingly constructing images of disks on the oscilloscope display. Work on molecular dynamics continued, though to do it required the most powerful computers, so for many years it was pretty much restricted to places like Livermore. And in 1967, Alder and Wainwright made another discovery about hard spheres. Even in their first paper about molecular dynamics they’d plotted the velocity autocorrelation function, and noted that it decayed roughly exponentially with time. But by 1967 they had much more precise data, and realized that there was a deviation from exponential decay: a definite “long-time tail”. And soon they had figured out that this power-law tail was basically the result of a continuum hydrodynamic effect (essentially a vortex) operating even on the scale of a few molecules. (And—though it didn’t occur to me at the time—this should have suggested that even with fairly small numbers of cells cellular automaton fluid simulations had a good chance of giving recognizable hydrodynamic results.) It’s never been entirely easy to do molecular dynamics, even with hard spheres, not least because in standard computations one’s inevitably confronted with things like numerical roundoff errors. And no doubt this is why some of the obvious foundational questions about the Second Law weren’t really explored there, and why intrinsic randomness generation and the rule 30 phenomenon weren’t identified. Incidentally, even before molecular dynamics emerged, there was already one computer study of what could potentially have been Second Law behavior. Visiting Los Alamos in the early 1950s Enrico Fermi had gotten interested in using computers for physics, and wondered what would happen if one simulated the motion of an array of masses with nonlinear springs between them. The results of running this on the MANIAC computer were reported in 1955 (after Fermi had died) and it was noted that there wasn’t just exponential approach to equilibrium, but instead something more complicated (later connected to solitons). Strangely, though, instead of plotting actual particle trajectories, what were given were mode energies—but these still exhibited what, if it hadn’t been obscured by continuum issues, might have been recognized as something like the rule 30 phenomenon: But I knew none of this history when I saw the Statistical Physics book cover in 1972. And indeed, for all I knew, it could have been a “standard statistical physics cover picture”. I didn’t know it was the first of its kind—and a leading-edge example of the use of computers for basic science, accessible only with the most powerful computers of the time. Of course, had I known those things, I probably wouldn’t have tried to reproduce the picture myself and I wouldn’t have had that early experience in trying to use a computer to do science. (Curiously enough, looking at the numbers now, I realize that the base speed of the LARC was only 20x the Elliott 903C, though with floating point, etc.—a factor that pales in comparison with the 500x speedup in computers in the 40 years since I started working on cellular automata.) But now I know the history of that book cover, and where it came from. And what I only just discovered now is that actually there’s a bigger circle than I knew. Because the path from Berni Alder to that book cover to my work on cellular automaton fluids came full circle—when in 1988 Alder wrote a paper based on cellular automaton fluids (though through the vicissitudes of academic behavior I don’t think he knew these had been my idea—and now it’s too late to tell him his role in seeding them):
A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics 34.13 Notes & Thanks : There are many people who’ve contributed to the 50-year journey I’ve described here. Some I’ve already mentioned by name, but others not—including many who doubtless wouldn’t even be aware that they contributed. The longtime store clerk at Blackwell’s bookstore who in 1972 sold college physics books to a 12-year old without batting an eye. (I learned his name—Keith Clack—30 years later when he organized a book signing for A New Kind of Science at Blackwell’s.) John Helliwell and Lawrence Wickens who in 1977 invited me to give the first talk where I explicitly discussed the foundations of the Second Law. Douglas Abraham who in 1977 taught a course on mathematical statistical mechanics that I attended. Paul Davies who wrote a book on The Physics of Time Asymmetry that I read around that time. Rocky Kolb who in 1979 and 1980 worked with me on cosmology that used statistical mechanics. The students (including professors like Steve Frautschi and David Politzer) who attended my 1981 class at Caltech about “nonequilibrium statistical mechanics”. David Pines and Elliott Lieb who in 1983 were responsible for publishing my breakout paper on “Statistical Mechanics of Cellular Automata”. Charles Bennett (curiously, a student of Berni Alder’s) with whom in the early 1980s I discussed applying computation theory (notably the ideas of Greg Chaitin) to physics. Brian Hayes who commissioned my 1984 Scientific American article, and Peter Brown who edited it. Danny Hillis and Sheryl Handler who in 1984 got me involved with Thinking Machines. Jim Salem and Bruce Nemnich (Walker) who worked on fluid dynamics on the Connection Machine with me. Then—36 years later—Jonathan Gorard and Max Piskunov, who catalyzed the doing of our Physics Project. In the last 50 years, there’ve been surprisingly few people with whom I’ve directly discussed the foundations of the Second Law. Perhaps one reason is that back when I was a “professional physicist” statistical mechanics as a whole wasn’t a prominent area. But, more important, as I’ve described elsewhere, for more than a century most physicists have effectively assumed that the foundations of the Second Law are a solved (or at least merely pedantic) problem. Probably the single person with whom I had the most discussions about the foundations of the Second Law is Richard Feynman. But there are others with whom at one time or another I’ve discussed related issues, including: Bruce Boghosian, Richard Crandall, Roger Dashen, Mitchell Feigenbaum, Nigel Goldenfeld, Theodore Gray, Bill Hayes, Joel Lebowitz, David Levermore, Ed Lorenz, John Maddox, Roger Penrose, Ilya Prigogine, Rudy Rucker, David Ruelle, Rob Shaw, Yakov Sinai, Michael Trott, Léon van Hove and Larry Yaffe. (There are also many others with whom I’ve discussed general issues about origins of randomness.) Finally, one technical note about the presentation here: in an effort to maintain a clearer timeline, I’ve typically shown the earliest drafts or preprint versions of papers that I have. Their final published versions (if indeed they were ever published) appeared anything from weeks to years later, sometimes with changes.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.1 The Basic Arc of the Story : As I’ve explained elsewhere, I think I now finally understand the Second Law of thermodynamics. But it’s a new understanding, and to get to it I’ve had to overcome a certain amount of conventional wisdom about the Second Law that I at least have long taken for granted. And to check myself I’ve been keen to know just where this conventional wisdom came from, how it’s been validated, and what might have made it go astray. And from this I’ve been led into a rather detailed examination of the origins and history of thermodynamics. All in all, it’s a fascinating story, that both explains what’s been believed about thermodynamics, and provides some powerful examples of the complicated dynamics of the development and acceptance of ideas. The basic concept of the Second Law was first formulated in the 1850s, and rather rapidly took on something close to its modern form. It began partly as an empirical law, and partly as something abstractly constructed on the basis of the idea of molecules, that nobody at the time knew for sure existed. But by the end of the 1800s, with the existence of molecules increasingly firmly established, the Second Law began to often be treated as an almost-mathematically-proven necessary law of physics. There were still mathematical loose ends, as well as issues such as its application to living systems and to systems involving gravity. But the almost-universal conventional wisdom became that the Second Law must always hold, and if it didn’t seem to in a particular case, then that must just be because there was something one didn’t yet understand about that case. There was also a sense that regardless of its foundations, the Second Law was successfully used in practice. And indeed particularly in chemistry and engineering it’s often been in the background, justifying all the computations routinely done using entropy. But despite its ubiquitous appearance in textbooks, when it comes to foundational questions, there’s always been a certain air of mystery around the Second Law. Though after 150 years there’s typically an assumption that “somehow it must all have been worked out”. I myself have been interested in the Second Law now for a little more than 50 years, and over that time I’ve had a growing awareness that actually, no, it hasn’t all been worked out. Which is why, now, it’s wonderful to see the computational paradigm—and ideas from our Physics Project—after all these years be able to provide solid foundations for understanding the Second Law, as well as seeing its limitations. And from the vantage point of the understanding we now have, we can go back and realize that there were precursors of it even from long ago. In some ways it’s all an inspiring tale—of how there were scientists with ideas ahead of their time, blocked only by the lack of a conceptual framework that would take another century to develop. But in other ways it’s also a cautionary tale, of how the forces of “conventional wisdom” can blind people to unanswered questions and—over a surprisingly long time—inhibit the development of new ideas. But, first and foremost, the story of the Second Law is the story of a great intellectual achievement of the mid-19th century. It’s exciting now, of course, to be able to use the latest 21st-century ideas to take another step. But to appreciate how this fits in with what’s already known we have to go back and study the history of what originally led to the Second Law, and how what emerged as conventional wisdom about it took shape.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.2 What Is Heat? : Once it became clear what heat is, it actually didn’t take long for the Second Law to be formulated. But for centuries—and indeed until the mid-1800s—there was all sorts of confusion about the nature of heat. That there’s a distinction between hot and cold is a matter of basic human perception. And seeing fire one might imagine it as a disembodied form of heat. In ancient Greek times Heraclitus (~500 BC) talked about everything somehow being “made of fire”, and also somehow being intrinsically “in motion”. Democritus (~460–~370 BC) and the Epicureans had the important idea (that also arose independently in other cultures) that everything might be made of large numbers of a few types of tiny discrete atoms. They imagined these atoms moving around in the “void” of space. And when it came to heat, they seem to have correctly associated it with the motion of atoms—though they imagined it came from particular spherical “fire” atoms that could slide more quickly between other atoms, and they also thought that souls were the ultimate sources of motion and heat (at least in warm-blooded animals?), and were made of fire atoms. And for two thousand years that’s pretty much where things stood. And indeed in 1623 Galileo (1564–1642) (in his book The Assayer, about weighing competing world theories) was still saying: Those materials which produce heat in us and make us feel warmth, which are known by the general name of “fire,” would then be a multitude of minute particles having certain shapes and moving with certain velocities. Meeting with our bodies, they penetrate by means of their extreme subtlety, and their touch as felt by us when they pass through our substance is the sensation we call “heat.” He goes on: Since the presence of fire-corpuscles alone does not suffice to excite heat, but their motion is needed also, it seems to me that one may very reasonably say that motion is the cause of heat… But I hold it to be silly to accept that proposition in the ordinary way, as if a stone or piece of iron or a stick must heat up when moved. The rubbing together and friction of two hard bodies, either by resolving their parts into very subtle flying particles or by opening an exit for the tiny fire-corpuscles within, ultimately sets these in motion; and when they meet our bodies and penetrate them, our conscious mind feels those pleasant or unpleasant sensations which we have named heat… And although he can tell there’s something different about it, he thinks of heat as effectively being associated with a substance or material: The tenuous material which produces heat is even more subtle than that which causes odor, for the latter cannot leak through a glass container, whereas the material of heat makes its way through any substance. In 1620, Francis Bacon (1561–1626) (in his “update on Aristotle”, The New Organon) says, a little more abstractly, if obscurely—and without any reference to atoms or substances: [It is not] that heat generates motion or that motion generates heat (though both are true in certain cases), but that heat itself, its essence and quiddity, is motion and nothing else. But real progress in understanding the nature of heat had to wait for more understanding about the nature of gases, with air being the prime example. (It was actually only in the 1640s that any kind of general notion of gas began to emerge—with the word “gas” being invented by the “anti-Galen” physician Jan Baptista van Helmont (1580–1644), as a Dutch rendering of the Greek word “chaos”, that meant essentially “void”, or primordial formlessness.) Ever since antiquity there’d been Aristotle-style explanations like “nature abhors a vacuum” about what nature “wants to do”. But by the mid-1600s the idea was emerging that there could be more explicit and mechanical explanations for phenomena in the natural world. And in 1660 Robert Boyle (1627–1691)—now thoroughly committed to the experimental approach to science—published New Experiments Physico-mechanicall, Touching the Spring of the Air and its Effects in which he argued that air has an intrinsic pressure associated with it, which pushes it to fill spaces, and for which he effectively found Boyle’s Law PV = constant. But what was air actually made of? Boyle had two basic hypotheses that he explained in rather flowery terms: His first hypothesis was that air might be like a “fleece of wool” made of “aerial corpuscles” (gases were later often called “aeriform fluids”) with a “power or principle of self-dilatation” that resulted from there being “hairs” or “little springs” between these corpuscles. But he had a second hypothesis too—based, he said, on the ideas of “that most ingenious gentleman, Monsieur Descartes”: that instead air consists of “flexible particles” that are “so whirled around” that “each corpuscle endeavors to beat off all others”. In this second hypothesis, Boyle’s “spring of the air” was effectively the result of particles bouncing off each other. And, as it happens, in 1668 there was quite an effort to understand the “laws of impact” (that would for example be applicable to balls in games like croquet and billiards, that had existed since at least the 1300s, and were becoming popular), with John Wallis (1616–1703), Christopher Wren (1632–1723) and Christiaan Huygens (1629–1695) all contributing, and Huygens producing diagrams like: But while some understanding developed of what amount to impacts between pairs of hard spheres, there wasn’t the mathematical methodology—or probably the idea—to apply this to large collections of spheres. Meanwhile, in his 1687 Principia Mathematica, Isaac Newton (1642–1727), wanting to analyze the properties of self-gravitating spheres of fluid, discussed the idea that fluids could in effect be made up of arrays of particles held apart by repulsive forces, as in Boyle’s first hypothesis. Newton had of course had great success with his 1/r2 universal attractive force for gravity. But now he noted (writing originally in Latin) that with a 1/r repulsive force between particles in a fluid, he could essentially reproduce Boyle’s law: Newton discussed questions like whether one particle would “shield” others from the force, but then concluded: But whether elastic fluids do really consist of particles so repelling each other, is a physical question. We have here demonstrated mathematically the property of fluids consisting of particles of this kind, that hence philosophers may take occasion to discuss that question. Well, in fact, particularly given Newton’s authority, for well over a century people pretty much just assumed that this was how gases worked. There was one major exception, however, in 1738, when—as part of his eclectic mathematical career spanning probability theory, elasticity theory, biostatistics, economics and more—Daniel Bernoulli (1700–1782) published his book on hydrodynamics. Mostly he discusses incompressible fluids and their flow, but in one section he considers “elastic fluids”—and along with a whole variety of experimental results about atmospheric pressure in different places—draws the picture and says Let the space ECDF contain very small particles in rapid motion; as they strike against the piston EF and hold it up by their impact, they constitute an elastic fluid which expands as the weight P is removed or reduced; but if P is increased it becomes denser and presses on the horizontal case CD just as if it were endowed with no elastic property. Then—in a direct and clear anticipation of the kinetic theory of heat—he goes on: The pressure of the air is increased not only by reduction in volume but also by rise in temperature. As it is well known that heat is intensified as the internal motion of the particles increases, it follows that any increase in the pressure of air that has not changed its volume indicates more intense motion of its particles, which is in agreement with our hypothesis… But at the time, and in fact for more than a century thereafter, this wasn’t followed up. A large part of the reason seems to have been that people just assumed that heat ultimately had to have some kind of material existence; to think that it was merely a manifestation of microscopic motion was too abstract an idea. And then there was the observation of “radiant heat” (i.e. infrared radiation)—that seemed like it could only work by explicitly transferring some kind of “heat material” from one body to another. But what was this “heat material”? It was thought of as a fluid—called caloric—that could suffuse matter, and for example flow from a hotter body to a colder. And in an echo of Democritus, it was often assumed that caloric consisted of particles that could slide between ordinary particles of matter. There was some thought that it might be related to the concept of phlogiston from the mid-1600s, that was effectively a chemical substance, for example participating in chemical reactions or being generated in combustion (through the “principle of fire”). But the more mainstream view was that there were caloric particles that would collect around ordinary particles of matter (often called “molecules”, after the use of that term by Descartes (1596–1650) in 1620), generating a repulsive force that would for example expand gases—and that in various circumstances these caloric particles would move around, corresponding to the transfer of heat. To us today it might seem hacky and implausible (perhaps a little like dark matter, cosmological inflation, etc.), but the caloric theory lasted for more than two hundred years and managed to explain plenty of phenomena—and indeed was certainly going strong in 1825 when Laplace wrote his A Treatise of Celestial Mechanics, which included a successful computation of properties of gases like the speed of sound and the ratio of specific heats, on the basis of a somewhat elaborated and mathematicized version of caloric theory (that by then included the concept of “caloric rays” associated with radiant heat). But even though it wasn’t understood what heat ultimately was, one could still measure its attributes. Already in antiquity there were devices that made use of heat to produce pressure or mechanical motion. And by the beginning of the 1600s—catalyzed by Galileo’s development of the thermoscope (in which heated liquid could be seen to expand up a tube)—the idea quickly caught on of making thermometers, and of quantitatively measuring temperature. And given a measurement of temperature, one could correlate it with effects one saw. So, for example, in the late 1700s the French balloonist Jacques Charles (1746–1823) noted the linear increase of volume of a gas with temperature. Meanwhile, at the beginning of the 1800s Joseph Fourier (1768–1830) (science advisor to Napoleon) developed what became his 1822 Analytical Theory of Heat, and in it he begins by noting that: Heat, like gravity, penetrates every substance of the universe, its rays occupy all parts of space. The object of our work is to set forth the mathematical laws which this element obeys. The theory of heat will hereafter form one of the most important branches of general physics. Later he describes what he calls the “Principle of the Communication of Heat”. He refers to “molecules”—though basically just to indicate a small amount of substance—and says When two molecules of the same solid are extremely near and at unequal temperatures, the most heated molecule communicates to that which is less heated a quantity of heat exactly expressed by the product of the duration of the instant, of the extremely small difference of the temperatures, and of certain function of the distance of the molecules. then goes on to develop what’s now called the heat equation and all sorts of mathematics around it, all the while effectively adopting a caloric theory of heat. (And, yes, if you think of heat as a fluid it does lead you to describe its “motion” in terms of differential equations just like Fourier did. Though it’s then ironic that Bernoulli, even though he studied hydrodynamics, seemed to have a less “fluid-based” view of heat.)
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.3 Heat Engines and the Beginnings of Thermodynamics : At the beginning of the 1800s the Industrial Revolution was in full swing—driven in no small part by the availability of increasingly efficient steam engines. There had been precursors of steam engines even in antiquity, but it was only in 1712 that the first practical steam engine was developed. And after James Watt (1736–1819) produced a much more efficient version in 1776, the adoption of steam engines began to take off. Over the years that followed there were all sorts of engineering innovations that increased the efficiency of steam engines. But it wasn’t clear how far it could go—and whether for example there was a limit to how much mechanical work could ever, even in principle, be derived from a given amount of heat. And it was the investigation of this question—in the hands of a young French engineer named Sadi Carnot (1796–1832)—that began the development of an abstract basic science of thermodynamics, and to the Second Law. The story really begins with Sadi Carnot’s father, Lazare Carnot (1753–1823), who was trained as an engineer but ascended to the highest levels of French politics, and was involved with both the French Revolution and Napoleon. Particularly in years when he was out of political favor, Lazare Carnot worked on mathematics and mathematical engineering. His first significant work—in 1778—was entitled Memoir on the Theory of Machines. The mathematical and geometrical science of mechanics was by then fairly well developed; Lazare Carnot’s objective was to understand its consequences for actual engineering machines, and to somehow abstract general principles from the mechanical details of the operation of those machines. In 1803 (alongside works on the geometrical theory of fortifications) he published his Fundamental Principles of [Mechanical] Equilibrium and Movement, which argued for what was at one time called (in a strange foreshadowing of reversible thermodynamic processes) “Carnot’s Principle”: that useful work in a machine will be maximized if accelerations and shocks of moving parts are minimized—and that a machine with perpetual motion is impossible. Sadi Carnot was born in 1796, and was largely educated by his father until he went to college in 1812. It’s notable that during the years when Sadi Carnot was a kid, one of his father’s activities was to give opinions on a whole range of inventions—including many steam engines and their generalizations. Lazare Carnot died in 1823. Sadi Carnot was by that point a well-educated but professionally undistinguished French military engineer. But in 1824, at the age of 28, he produced his one published work, Reflections on the Motive Power of Fire, and on Machines to Develop That Power (where by “fire” he meant what we would call heat): The style and approach of the younger Carnot’s work is quite similar to his father’s. But the subject matter turned out to be more fruitful. The book begins: Everyone knows that heat can produce motion. That it possesses vast motive-power none can doubt, in these days when the steam-engine is everywhere so well known… The study of these engines is of the greatest interest, their importance is enormous, their use is continually increasing, and they seem destined to produce a great revolution in the civilized world. Already the steam-engine works our mines, impels our ships, excavates our ports and our rivers, forges iron, fashions wood, grinds grain, spins and weaves our cloths, transports the heaviest burdens, etc. It appears that it must some day serve as a universal motor, and be substituted for animal power, water-falls, and air currents. … Notwithstanding the work of all kinds done by steam-engines, notwithstanding the satisfactory condition to which they have been brought to-day, their theory is very little understood, and the attempts to improve them are still directed almost by chance. … The question has often been raised whether the motive power of heat is unbounded, whether the possible improvements in steam-engines have an assignable limit, a limit which the nature of things will not allow to be passed by any means whatever; or whether, on the contrary, these improvements may be carried on indefinitely. We propose now to submit these questions to a deliberate examination. Carnot operated very much within the framework of caloric theory, and indeed his ideas were crucially based on the concept that one could think about “heat itself” (which for him was caloric fluid), independent of the material substance (like steam) that was hot. But—like his father’s efforts with mechanical machines—his goal was to develop an abstract “metamodel” of something like a steam engine, crucially assuming that the generation of unbounded heat or mechanical work (i.e. perpetual motion) in the closed cycle of the operation of the machine was impossible, and noting (again with a reflection of his father’s work) that the system would necessarily maximize efficiency if it operated reversibly. And he then argued that: The production of motive power is then due in steam-engines not to an actual consumption of caloric, but to its transportation from a warm body to a cold body, that is, to its re-establishment of equilibrium… In other words, what was important about a steam engine was that it was a “heat engine”, that “moved heat around”. His book is mostly words, with just a few formulas related to the behavior of ideal gases, and some tables of actual parameters for particular materials. But even though his underlying conceptual framework—of caloric theory—was not correct, the abstract arguments that he made (that involved essentially logical consequences of reversibility and of operating in a closed cycle) were robust enough that it didn’t matter, and in particular he was able to successfully show that there was a theoretical maximum efficiency for a heat engine, that depended only on the temperatures of its hot and cold reservoirs of heat. But what’s important for our purposes here is that in the setup Carnot constructed he basically ended up introducing the Second Law. At the time it appeared, however, Carnot’s book was basically ignored, and Carnot died in obscurity from cholera in 1832 (about 9 months after Évariste Galois (1811–1832)) at the age of 36. (The Sadi Carnot who would later become president of France was his nephew.) But in 1834, Émile Clapeyron (1799–1864)—a rather distinguished French engineering professor (and steam engine designer)—wrote a paper entitled “Memoir on the Motive Power of Heat”. He starts off by saying about Carnot’s book: The idea which serves as a basis of his researches seems to me to be both fertile and beyond question; his demonstrations are founded on the absurdity of the possibility of creating motive power or heat out of nothing. … This new method of demonstration seems to me worthy of the attention of theoreticians; it seems to me to be free of all objection … I believe that it is of some interest to take up this theory again; S. Carnot, avoiding the use of mathematical analysis, arrives by a chain of difficult and elusive arguments at results which can be deduced easily from a more general law which I shall attempt to prove… Clapeyron’s paper doesn’t live up to the claims of originality or rigor expressed here, but it served as a more accessible (both in terms of where it was published and how it was written) exposition of Carnot’s work, featuring, for example, for the first time a diagrammatic representation of a Carnot cycle as well as notations like Q-for-heat that are still in use today:
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.4 The Second Law Is Formulated : One of the implications of Newton’s Laws of Motion is that momentum is conserved. But what else might also be conserved? In the 1680s Gottfried Leibniz (1646–1716) suggested the quantity m v2, which he called, rather grandly, vis viva—or, in English, “life force”. And yes, in things like elastic collisions, this quantity did seem to be conserved. But in plenty of situations it wasn’t. By 1807 the term “energy” had been introduced, but the question remained of whether it could in any sense globally be thought of as conserved. It had seemed for a long time that heat was something a bit like mechanical energy, but the relation wasn’t clear—and the caloric theory of heat implied that caloric (i.e. the fluid corresponding to heat) was conserved, and so certainly wasn’t something that for example could be interconverted with mechanical energy. But in 1798 Benjamin Thompson (Count Rumford) (1753–1814) measured the heat produced by the mechanical process of boring a cannon, and began to make the argument that, in contradiction to the caloric theory, there was actually some kind of correspondence between mechanical energy and amount of heat. It wasn’t a very accurate experiment, and it took until the 1840s—with new experiments by the English brewer and “amateur” scientist James Joule (1818–1889) and the German physician Robert Mayer (1814–1878)—before the idea of some kind of equivalence between heat and mechanical work began to look more plausible. And in 1847 this was something William Thomson (1824–1907) (later Lord Kelvin)—a prolific young physicist recently graduated from the Mathematical Tripos in Cambridge and now installed as a professor of “natural philosophy” (i.e. physics) in Glasgow—began to be curious about. But first we have to go back a bit in the story. In 1845 Kelvin (as we’ll call him) had spent some time in Paris (primarily at at a lab that was measuring properties of steam for the French government), and there he’d learned about Carnot’s work from Clapeyron’s paper (at first he couldn’t get a copy of Carnot’s actual book). Meanwhile, one of the issues of the time was a proliferation of different temperature scales based on using different kinds of thermometers based on different substances. And in 1848 Kelvin realized that Carnot’s concept of a “pure heat engine”—assumed at the time to be based on caloric—could be used to define an “absolute” scale of temperature in which, for example, at absolute zero all caloric would have been removed from all substances: Having found Carnot’s ideas useful, Kelvin in 1849 wrote a 33-page summary of them (small world that it was then, the immediately preceding paper in the journal is “On the Theory of Rolling Curves”, written by the then-17-year-old James Clerk Maxwell (1831–1879), while the one that follows is “Theoretical Considerations on the Effect of Pressure in Lowering the Freezing Point of Water” by James Thomson (1786–1849), engineering-oriented older brother of William): He characterizes Carnot’s work as being based not so much on physics and experiment, but on the “strictest principles of philosophy”: He doesn’t immediately mention “caloric” (though it does slip in later), referring instead to a vaguer concept of “thermal agency”: In keeping with the idea that this is more philosophy than experimental science, he refers to “Carnot’s fundamental principle”—that after a complete cycle an engine can be treated as back in the “same state”—while adding the footnote that “this is tacitly assumed as an axiom”: In actuality, to say that an engine comes back to the same state is a nontrivial statement of the existence of some kind of unique equilibrium in the system, related to the Second Law. But in 1848 Kelvin brushes this off by saying that the “axiom” has “never, so far as I am aware, been questioned by practical engineers”. His next page is notable for the first-ever use of the term “thermo-dynamic” (then hyphenated) to discuss systems where what matters is “the dynamics of heat”: That same page has a curious footnote presaging what will come, and making the statement that “no energy can be destroyed”, and considering it “perplexing” that this seems incompatible with Carnot’s work and its caloric theory framework: After going through Carnot’s basic arguments, the paper ends with an appendix in which Kelvin basically says that even though the theory seems to just be based on a formal axiom, it should be experimentally tested: He proceeds to give some tests, which he claims agree with Carnot’s results—and finally ends with a very practical (but probably not correct) table of theoretical efficiencies for steam engines of his day: But now what of Joule’s and Mayer’s experiments, and their apparent disagreement with the caloric theory of heat? By 1849 a new idea had emerged: that perhaps heat was itself a form of energy, and that, when heat was accounted for, the total energy of a system would always be conserved. And what this suggested was that heat was somehow a dynamical phenomenon, associated with microscopic motion—which in turn suggested that gases might indeed consist just of molecules in motion. And so it was that in 1850 Kelvin (then still “William Thomson”) wrote a long exposition “On the Dynamical Theory of Heat”, attempting to reconcile Carnot’s ideas with the new concept that heat was dynamical in origin: He begins by quoting—presumably for some kind of “British-based authority”—an “anti-caloric” experiment apparently done by Humphry Davy (1778–1829) as a teenager, involving melting pieces of ice by rubbing them together, and included anonymously in a 1799 list of pieces of knowledge “principally from the west of England”: But soon Kelvin is getting to the main point: And then we have it: a statement of the Second Law (albeit with some hedging to which we’ll come back later): And there’s immediately a footnote that basically asserts the “absurdity” of a Second-Law-violating perpetual motion machine: But by the next page we find out that Kelvin admits he’s in some sense been “scooped”—by a certain Rudolf Clausius (1822–1888), who we’ll be discussing soon. But what’s remarkable is that Clausius’s “axiom” turns out to be exactly equivalent to Kelvin’s statement: And what this suggests is that the underlying concept—the Second Law—is something quite robust. And indeed, as Kelvin implies, it’s the main thing that ultimately underlies Carnot’s results. And so even though Carnot is operating on the now-outmoded idea of caloric theory, his main results are still correct, because in the end all they really depend on is a certain amount of “logical structure”, together with the Second Law (and a version of the First Law, but that’s a slightly trickier story). Kelvin recognized, though, that Carnot had chosen to look at the particular (“equilibrium thermodynamics”) case of processes that occur reversibly, effectively at an infinitesimal rate. And at the end of the first installment of his exposition, he explains that things will be more complicated if finite rates are considered—and that in particular the results one gets in such cases will depend on things like having a correct model for the nature of heat. Kelvin’s exposition on the “dynamical nature of heat” runs to four installments, and the next two dive into detailed derivations and attempted comparison with experiment: But before Kelvin gets to publish part four of his exposition he publishes two other pieces. In the first, he’s talking about sources of energy for human use (now that he believes energy is conserved): He emphasizes that the Sun is—directly or indirectly—the main source of energy on Earth (later he’ll argue that coal will run out, etc.): But he wonders how animals actually manage to produce mechanical work, noting that “the animal body does not act as a thermo-dynamic engine; and [it is] very probable that the chemical forces produce the external mechanical effects through electrical means”: And then, by April 1852, he’s back to thinking directly about the Second Law, and he’s cut through the technicalities, and is stating the Second Law in everyday (if slightly ponderous) terms: It’s interesting to see his apparently rather deeply held Presbyterian beliefs manifest themselves here in his mention that “Creative Power” is what must set the total energy of the universe. He ends his piece with: In (2) the hedging is interesting. He makes the definitive assertion that what amounts to a violation of the Second Law “is impossible in inanimate material processes”. And he’s pretty sure the same is true for “vegetable life” (recognizing that in his previous paper he discussed the harvesting of sunlight by plants). But what about “animal life”, like us humans? Here he says that “by our will” we can’t violate the Second Law—so we can’t, for example, build a machine to do it. But he leaves it open whether we as humans might have some innate (“God-given”?) ability to overcome the Second Law. And then there’s his (3). It’s worth realizing that his whole paper is less than 3 pages long, and right before his conclusions we’re seeing triple integrals: So what is (3) about? It’s presumably something like a Second-Law-implies-heat-death-of-the-universe statement (but what’s this stuff about the past?)—but with an added twist that there’s something (God?) beyond the “known operations going on at present in the material world” that might be able to swoop in to save the world for us humans. It doesn’t take people long to pick up on the “cosmic significance” of all this. But in the fall of 1852, Kelvin’s colleague, the Glasgow engineering professor William Rankine (1820–1872) (who was deeply involved with the First Law of thermodynamics), is writing about a way the universe might save itself: After touting the increasingly solid evidence for energy conservation and the First Law he goes on to talk about dissipation of energy and what we now call the Second Law and the fact that it implies an “end of all physical phenomena”, i.e. heat death of the universe. He continues: But now he offers a “ray of hope”. He believes that there must exist a “medium capable of transmitting light and heat”, i.e. an aether, “[between] the heavenly bodies”. And if this aether can’t itself acquire heat, he concludes that all energy must be converted into a radiant form: Now he supposes that the universe is effectively a giant drop of aether, with nothing outside, so that all this radiant energy will get totally internally reflected from its surface, allowing the universe to “[reconcentrate] its physical energies, and [renew] its activity and life”—and save it from heat death: He ends with the speculation that perhaps “some of the luminous objects which we see in distant regions of space may be, not stars, but foci in the interstellar aether”. But independent of cosmic speculations, Kelvin himself continues to study the “dynamical theory of gases”. It’s often a bit unclear what’s being assumed. There’s the First Law (energy conservation). And the Second Law. But there’s also reversibility. Equilibrium. And the ideal gas law (P V = R T). But it soon becomes clear that that’s not always correct for real gases—as the Joule–Thomson effect demonstrates: Kelvin soon returned to more cosmic speculations, suggesting that perhaps gravitation—rather than direct “Creative Power”—might “in reality [be] the ultimate created antecedent of all motion…”: Not long after these papers Kelvin got involved with the practical “electrical” problem of laying a transatlantic telegraph cable, and in 1858 was on the ship that first succeeded in doing this. (His commercial efforts soon allowed him to buy a 126-ton yacht.) But he continued to write physics papers, which ranged over many different areas, occasionally touching thermodynamics, though most often in the service of answering a “general science” question—like how old the Sun is (he estimated 32,000 years from thermodynamic arguments, though of course without knowledge of nuclear reactions). Kelvin’s ideas about the inevitable dissipation of “useful energy” spread quickly—by 1854, for example, finding their way into an eloquent public lecture by Hermann von Helmholtz (1821–1894). Helmholtz had trained as a doctor, becoming in 1843 a surgeon to a German military regiment. But he was also doing experiments and developing theories about “animal heat” and how muscles manage to “do mechanical work”, for example publishing an 1845 paper entitled “On Metabolism during Muscular Activity”. And in 1847 he was one of the inventors of the law of conservation of energy—and the First Law of thermodynamics—as well as perhaps its clearest expositor at the time (the word “force” in the title is what we now call “energy”): By 1854 Helmholtz was a physiology professor, beginning a distinguished career in physics, psychophysics and physiology—and talking about the Second Law and its implications. He began his lecture by saying that “A new conquest of very general interest has been recently made by natural philosophy”—and what he’s referring to here is the Second Law: Having discussed the inability of “automata” (he uses that word) to reproduce living systems, he starts talking about perpetual motion machines: First he disposes of the idea that perpetual motion can be achieved by generating energy from nothing (i.e. violating the First Law), charmingly including the anecdote: And then he’s on to talking about the Second Law and discussing how it implies the heat death of the universe: He notes, correctly, that the Second Law hasn’t been “proved”. But he’s impressed at how Kelvin was able to go from a “mathematical formula” to a global fact about the fate of the universe: He ends the whole lecture quite poetically: We’ve talked quite a bit about Kelvin and how his ideas spread. But let’s turn now to Rudolf Clausius, who in 1850 at least to some extent “scooped” Kelvin on the Second Law. At that time Clausius was a freshly minted German physics PhD. His thesis had been on an ingenious but ultimately incorrect theory of why the sky is blue. But he’d also worked on elasticity theory, and there he’d been led to start thinking about molecules and their configurations in materials. By 1850 caloric theory had become fairly elaborate, complete with concepts like “latent heat” (bound to molecules) and “free heat” (able to be transferred). Clausius’s experience in elasticity theory made him skeptical, and knowing Mayer’s and Joule’s results he decided to break with the caloric theory—writing his career-launching paper (translated from German in 1851, with Carnot’s puissance motrice [“motive power”] being rendered as “moving force”): The first installment of the English version of the paper gives a clear description of the ideal gas laws and the Carnot cycle, having started from a statement of the “caloric-busting” First Law: The general discussion continues in the second installment, but now there’s a critical side comment that describes the “general deportment of heat, which every-where exhibits the tendency to annul differences of temperature, and therefore to pass from a warmer body to a colder one”: Clausius “has” the Second Law, as Carnot basically did before him. But when Kelvin quotes Clausius he does so much more forcefully: But there it is: by 1852 the Second Law is out in the open, in at least two different forms. The path to reach it has been circuitous and quite technical. But in the end, stripped of its technical origins, the law seems somehow unsurprising and even obvious. For it’s a matter of common experience that heat flows from hotter bodies to colder ones, and that motion is dissipated by friction into heat. But the point is that it wasn’t until basically 1850 that the overall scientific framework existed to make it useful—or even really possible—to enunciate such observations as a formal scientific law. Of course the fact that a law “seems true” based on common experience doesn’t mean it’ll always be true, and that there won’t be some special circumstance or elaborate construction that will evade it. But somehow the very fact that the Second Law had in a sense been “technically hard won”—yet in the end seemed so “obvious”—appears to have given it a sense of inevitability and certainty. And it didn’t hurt that somehow it seemed to have emerged from Carnot’s work, which had a certain air of “logical necessity”. (Of course, in reality, the Second Law entered Carnot’s logical structure as an “axiom”.) But all this helped set the stage for some of the curious confusions about the Second Law that would develop over the century that followed.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.5 The Concept of Entropy : In the first half of the 1850s the Second Law had in a sense been presented in two ways. First, as an almost “footnote-style” assumption needed to support the “pure thermodynamics” that had grown out of Carnot’s work. And second, as an explicitly-stated-for-the-first-time—if “obvious”—“everyday” feature of nature, that was now realized as having potentially cosmic significance. But an important feature of the decade that followed was a certain progressive at-least-phenomenological “mathematicization” of the Second Law—pursued most notably by Rudolf Clausius. In 1854 Clausius was already beginning this process. Perhaps confusingly, he refers to the Second Law as the “second fundamental theorem [Hauptsatz]” in the “mechanical theory of heat”—suggesting it’s something that is proved, even though it’s really introduced just as an empirical law of nature, or perhaps a theoretical axiom: He starts off by discussing the “first fundamental theorem”, i.e. the First Law. And he emphasizes that this implies that there’s a quantity U (which we now call “internal energy”) that is a pure “function of state”—so that its value depends only on the state of a system, and not the path by which that state was reached. And as an “application” of this, he then points out that the overall change in U in a cyclic process (like the one executed by Carnot’s heat engine) must be zero. And now he’s ready to tackle the Second Law. He gives a statement that at first seems somewhat convoluted: But soon he’s deriving this from a more “everyday” statement of the Second Law (which, notably, is clearly not a “theorem” in any normal sense): After giving a Carnot-style argument he’s then got a new statement (that he calls “the theorem of the equivalence of transformations”) of the Second Law: And there it is: basically what we now call entropy (even with the same notation of Q for heat and T for temperature)—together with the statement that this quantity is a function of state, so that its differences are “independent of the nature of the process by which the transformation is effected”. Pretty soon there’s a familiar expression for entropy change: And by the next page he’s giving what he describes as “the analytical expression” of the Second Law, for the particular case of reversible cyclic processes: A bit later he backs out of the assumption of reversibility, concluding that: (And, yes, with modern mathematical rigor, that should be “non-negative” rather than “positive”.) He goes on to say that if something has changed after going around a cycle, he’ll call that an “uncompensated transformation”—or what we would now refer to as an irreversible change. He lists a few possible (now very familiar) examples: Earlier in his paper he’s careful to say that T is “a function of temperature”; he doesn’t say it’s actually the quantity we measure as temperature. But now he wants to determine what it is: He doesn’t talk about the ultimately critical assumption (effectively the Zeroth Law of thermodynamics) that the system is “in equilibrium”, with a uniform temperature. But he uses an ideal gas as a kind of “standard material”, and determines that, yes, in that case T can be simply the absolute temperature. So there it is: in 1854 Clausius has effectively defined entropy and described its relation to the Second Law, though everything is being done in a very “heat-engine” style. And pretty soon he’s writing about “Theory of the Steam-Engine” and filling actual approximate steam tables into his theoretical formulas: After a few years “off” (working, as we’ll discuss later, on the kinetic theory of gases) Clausius is back in 1862 talking about the Second Law again, in terms of his “theorem of the equivalence of transformations”: He’s slightly tightened up his 1854 discussion, but, more importantly, he’s now stating a result not just for reversible cyclic processes, but for general ones: But what does this result really mean? Clausius claims that this “theorem admits of strict mathematical proof if we start from the fundamental proposition above quoted”—though it’s not particularly clear just what that proposition is. But then he says he wants to find a “physical cause”: A little earlier in the paper he said: So what does he think the “physical cause” is? He says that even from his first investigations he’d assumed a general law: What are these “resistances”? He’s basically saying they are the forces between molecules in a material (which from his work on the kinetic theory of gases he now imagines exist): He introduces what he calls the “disgregation” to represent the microscopic effect of adding heat: For ideal gases things are straightforward, including the proportionality of “resistance” to absolute temperature. But in other cases, it’s not so clear what’s going on. A decade later he identifies “disgregation” with average kinetic energy per molecule—which is indeed proportional to absolute temperature. But in 1862 it’s all still quite muddy, with somewhat curious statements like: And then the main part of the paper ends with what seems to be an anticipation of the Third Law of thermodynamics: There’s an appendix entitled “On Terminology” which admits that between Clausius’s own work, and other people’s, it’s become rather difficult to follow what’s going on. He agrees that the term “energy” that Kelvin is using makes sense. He suggests “energy of the body” for what he calls U and we now call “internal energy”. He suggests “heat of the body” or “thermal content of the body” for Q. But then he talks about the fact that these are measured in thermal units (say the amount of heat needed to increase the temperature of water by 1°), while mechanical work is measured in units related to kilograms and meters. He proposes therefore to introduce the concept of “ergon” for “work measured in thermal units”: And pretty soon he’s talking about the “interior ergon” and “exterior ergon”, as well as concepts like “ergonized heat”. (In later work he also tries to introduce the concept of “ergal” to go along with his development of what he called—in a name that did stick—the “virial theorem”.) But in 1865 he has his biggest success in introducing a term. He’s writing a paper, he says, basically to clarify the Second Law, (or, as he calls it, “the second fundamental theorem”—rather confidently asserting that he will “prove this theorem”): Part of the issue he’s trying to address is how the calculus is done: The partial derivative symbol ∂ had been introduced in the late 1700s. He doesn’t use it, but he does introduce the now-standard-in-thermodynamics subscript notation for variables that are kept constant: A little later, as part of the “notational cleanup”, we see the variable S: And then—there it is—Clausius introduces the term “entropy”, “Greekifying” his concept of “transformation”: His paper ends with his famous crisp statements of the First and Second Laws of thermodynamics—manifesting the parallelism he’s been claiming between energy and entropy:
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.6 The Kinetic Theory of Gases : We began above by discussing the history of the question of “What is heat?” Was it like a fluid—the caloric theory? Or was it something more dynamical, and in a sense more abstract? But then we saw how Carnot—followed by Kelvin and Clausius—managed in effect to sidestep the question, and come up with all sorts of “thermodynamic conclusions”, by talking just about “what heat does” without ever really having to seriously address the question of “what heat is”. But to be able to discuss the foundations of the Second Law—and what it says about heat—we have to know more about what heat actually is. And the crucial development that began to clarify the nature of heat was the kinetic theory of gases. Central to the kinetic theory of gases is the idea that gases are made up of discrete molecules. And it’s important to remember that it wasn’t until the beginning of the 1900s that anyone knew for sure that molecules existed. Yes, something like them had been discussed ever since antiquity, and in the 1800s there was increasing “circumstantial evidence” for them. But nobody had directly “seen a molecule”, or been able, for example, until about 1870, to even guess what the size of molecules might be. Still, by the mid-1800s it had become common for physicists to talk and reason in terms of ordinary matter at least effectively being made of up molecules. But if a gas was made of molecules bouncing off each other like billiard balls according to the laws of mechanics, what would its overall properties be? Daniel Bernoulli had in 1738 already worked out the basic answer that pressure would vary inversely with volume, or in his notation, π = P/s (and he even also gave formulas for molecules of nonzero size—in a precursor of van der Waals): Results like Bernouilli’s would be rediscovered several times, for example in 1820 by John Herapath (1790–1868), a math teacher in England, who developed a fairly elaborate theory that purported to describe gravity as well as heat (but for example implied a P V = a T2 gas law): Then there was the case of John Waterston (1811–1883), a naval instructor for the East India company, who in 1843 published a book called Thoughts on the Mental Functions, which included results on what he called the “vis viva theory of heat”—that he developed in more detail in a paper he wrote in 1846. But when he submitted the paper to the Royal Society it was rejected as “nonsense”, and its manuscript was “lost” until 1891 when it was finally published (with an “explanation” of the “delay”): The paper had included a perfectly sensible mathematical analysis that included a derivation of the kinetic theory relation between pressure and mean-square molecular velocity: But with all these pieces of work unknown, it fell to a German high-school chemistry teacher (and sometime professor and philosophical/theological writer) named August Krönig (1822–1879) to publish in 1856 yet another “rediscovery”, that he entitled “Principles of a Theory of Gases”. He said it was going to analyze the “mechanical theory of heat”, and once again he wanted to compute the pressure associated with colliding molecules. But to simplify the math, he assumed that molecules went only along the coordinate directions, at a fixed speed—almost anticipating a cellular automaton fluid: What ultimately launched the subsequent development of the kinetic theory of gases, however, was the 1857 publication by Rudolf Clausius (by then an increasingly established German physics professor) of a paper entitled rather poetically “On the Nature of the Motion Which We Call Heat” (“Über die Art der Bewegung die wir Wärme nennen”): It’s a clean and clear paper, with none of the mathematical muddiness around Clausius’s work on the Second Law (which, by the way, isn’t even mentioned in this paper even though Clausius had recently worked on it). Clausius figures out lots of the “obvious” implications of his molecular theory, outlining for example what happens in different phases of matter: It takes him only a couple of pages of very light mathematics to derive the standard kinetic theory formula for the pressure of an ideal gas: He’s implicitly assuming a certain randomness to the motions of the molecules, but he barely mentions it (and this particular formula is robust enough that average values are actually all that matter): But having derived the formula for pressure, he goes on to use the ideal gas law to derive the relation between average molecular kinetic energy (which he still calls “vis viva”) and absolute temperature: From this he can do things like work out the actual average velocities of molecules in different gases—which he does without any mention of the question of just how real or not molecules might be. By knowing experimental results about specific heats of gases he also manages to determine that not all the energy (“heat”) in a gas is associated with “translatory motion”: he realizes that for molecules involving several atoms there can be energy associated with other (as we would now say) internal degrees of freedom: Clausius’s paper was widely read. And it didn’t take long before the Dutch meteorologist (and effectively founder of the World Meteorological Organization) Christophorus Buys Ballot (1817–1890) asked why—if molecules were moving as quickly as Clausius suggested—gases didn’t mix much more quickly than they’re observed to do: Within a few months, Clausius published the answer: the molecules didn’t just keep moving in straight lines; they were constantly being deflected, to follow what we would now call a random walk. He invented the concept of a mean free path to describe how far on average a molecule goes before it hits another molecule: As a capable theoretical physicist, Clausius quickly brings in the concept of probability and is soon computing the average number of molecules which will survive undeflected for a certain distance: Then he works out the mean free path λ (and it’s often still called λ): And he concludes that actually there’s no conflict between rapid microscopic motion and large-scale “diffusive” motion: Of course, he could have actually drawn a sample random walk, but drawing diagrams wasn’t his style. And in fact it seems as if the first published drawing of a random walk was something added by John Venn (1834–1923) in the 1888 edition of his Logic of Chance—and, interestingly, in alignment with my computational irreducibility concept from a century later he used the digits of π to generate his “randomness”: In 1859, Clausius’s paper came to the attention of the then-28-year-old James Clerk Maxwell, who had grown up in Scotland, done the Mathematical Tripos in Cambridge, and was now back in Scotland as professor of “natural philosophy” at Aberdeen. Maxwell had already worked on things like elasticity theory, color vision, the mechanics of tops, the dynamics of the rings of Saturn and electromagnetism—having published his first paper (on geometry) at age 14. And, by the way, Maxwell was quite a “diagrammist”—and his early papers include all sorts of pictures that he drew: But in 1859 Maxwell applied his talents to what he called the “dynamical theory of gases”: He models molecules as hard spheres, and sets about computing the “statistical” results of their collisions: And pretty soon he’s trying to compute distribution of their velocities: It’s a somewhat unconvincing (or, as Maxwell himself later put it, “precarious”) derivation (how does it work in 1D, for example?), but somehow it manages to produce what’s now known as the Maxwell distribution: Maxwell observes that the distribution is the same as for “errors … in the ‘method of least squares’”: Maxwell didn’t get back to the dynamical theory of gases until 1866, but in the meantime he was making a “dynamical theory” of something else: what he called the electromagnetic field: Even though he’d worked extensively with the inverse square law of gravity he didn’t like the idea of “action at a distance”, and for example he wanted magnetic field lines to have some underlying “material” manifestation imagining that they might be associated with arrays of “molecular vortices”: We now know, of course, that there isn’t this kind of “underlying mechanics” for the electromagnetic field. But—with shades of the story of Carnot—even though the underlying framework isn’t right, Maxwell successfully derives correct equations for the electromagnetic field—that are now known as Maxwell’s equations: His statement of how the electromagnetic field “works” is highly reminiscent of the dynamical theory of gases: But he quickly and correctly adds: And a few sections later he derives the idea of general electromagnetic waves noting that there’s no evidence that the medium through which he assumes they’re propagating has elasticity: By the way, when it comes to gravity he can’t figure out how to make his idea of a “mechanical medium” work: But in any case, after using it as an inspiration for thinking about electromagnetism, Maxwell in 1866 returns to the actual dynamical theory of gases, still feeling that he needs to justify looking at a molecular theory: And now he gives a recognizable (and correct, so far as it goes) derivation of the Maxwell distribution: He goes on to try to understand experimental results on gases, about things like diffusion, viscosity and conductivity. For some reason, Maxwell doesn’t want to think of molecules, as he did before, as hard spheres. And instead he imagines that they have “action at a distance” forces, which basically work like hard squares if it’s r-5 force law: In the years that followed, Maxwell visited the dynamical theory of gases several more times. In 1871, a few years before he died at age 48, he wrote a textbook entitled Theory of Heat, which begins, in erudite fashion, discussing what “thermodynamics” should even be called: Most of the book is concerned with the macroscopic “theory of heat”—though, as we’ll discuss later, in the very last chapter Maxwell does talk about the “molecular theory”, if in somewhat tentative terms. “Deriving” the Second Law from Molecular Dynamics The Second Law was in effect originally introduced as a formalization of everyday observations about heat. But the development of kinetic theory seemed to open up the possibility that the Second Law could actually be proved from the underlying mechanics of molecules. And this was something that Ludwig Boltzmann (1844–1906) embarked on towards the end of his physics PhD at the University of Vienna. In 1865 he’d published his first paper (“On the Movement of Electricity on Curved Surfaces”), and in 1866 he published his second paper, “On the Mechanical Meaning of the Second Law of Thermodynamics”: The introduction promises “a purely analytical, perfectly general proof of the Second Law”. And what he seemed to imagine was that the equations of mechanics would somehow inevitably lead to motion that would reproduce the Second Law. And in a sense what computational irreducibility, rule 30, etc. now show is that in the end that’s indeed basically how things work. But the methods and conceptual framework that Boltzmann had at his disposal were very far away from being able to see that. And instead what Boltzmann did was to use standard mathematical methods from mechanics to compute average properties of cyclic mechanical motions—and then made the somewhat unconvincing claim that combinations of these averages could be related (e.g. via temperature as average kinetic energy) to “Clausius’s entropy”: It’s not clear how much this paper was read, but in 1871 Boltzmann (now a professor of mathematical physics in Graz) published another paper entitled simply “On the Priority of Finding the Relationship between the Second Law of Thermodynamics and the Principle of Least Action” that claimed (with some justification) that Clausius’s then-newly-announced virial theorem was already contained in Boltzmann’s 1866 paper. But back in 1868—instead of trying to get all the way to Clausius’s entropy—Boltzmann instead uses mechanics to get a generalization of Maxwell’s law for the distribution of molecular velocities. His paper “Studies on the Equilibrium of [Kinetic Energy] between [Point Masses] in Motion” opens by saying that while analytical mechanics has in effect successfully studied the evolution of mechanical systems “from a given state to another”, it’s had little to say about what happens when such systems “have been left moving on their own for a long time”. He intends to remedy that, and spends 47 pages—complete with elaborate diagrams and formulas about collisions between hard spheres—in deriving an exponential distribution of energies if one assumes “equilibrium” (or, more specifically, balance between forward and backward processes): It’s notable that one of the mathematical approaches Boltzmann uses is to discretize (i.e. effectively quantize) things, then look at the “combinatorial” limit.  (Based on his later statements, he didn’t want to trust “purely continuous” mathematics—at least in the context of discrete molecular processes—and wanted to explicitly “watch the limits happening”.) But in the end it’s not clear that Boltzmann’s 1868 arguments do more than the few-line functional-equation approach that Maxwell had already used. (Maxwell would later complain about Boltzmann’s “overly long” arguments.) Boltzmann’s 1868 paper had derived what the distribution of molecular energies should be “in equilibrium”. (In 1871 he was talking about “equipartition” not just of kinetic energy, but also of energies associated with “internal motion” of polyatomic molecules.) But what about the approach to equilibrium? How would an initial distribution of molecular energies evolve over time? And would it always end up at the exponential (“Maxwell–Boltzmann”) distribution? These are questions deeply related to a microscopic understanding of the Second Law. And they’re what Boltzmann addressed in 1872 in his 22nd published paper “Further Studies on the Thermal Equilibrium of Gas Molecules”: Boltzmann explains that: Maxwell already found the value Av2 e–Bv2 [for the distribution of velocities] … so that the probability of different velocities is given by a formula similar to that for the probability of different errors of observation in the theory of the method of least squares. The first proof which Maxwell gave for this formula was recognized to be incorrect even by himself. He later gave a very elegant proof that, if the above distribution has once been established, it will not be changed by collisions. He also tries to prove that it is the only velocity distribution that has this property. But the latter proof appears to me to contain a false inference. It has still not yet been proved that, whatever the initial state of the gas may be, it must always approach the limit found by Maxwell. It is possible that there may be other possible limits. This proof is easily obtained, however, by the method which I am about to explain… (He gives a long footnote explaining why Maxwell might be wrong, talking about how a sequence of collisions might lead to a “cycle of velocity states”—which Maxwell hasn’t proved will be traversed with equal probability in each direction. Ironically, this is actually already an analog of where things are going to go wrong with Boltzmann’s own argument.) The main idea of Boltzmann’s paper is not to assume equilibrium, but instead to write down an equation (now called the Boltzmann Transport Equation) that explicitly describes how the velocity (or energy) distribution of molecules will change as a result of collisions. He begins by defining infinitesimal changes in time: He then goes through a rather elaborate analysis of velocities before and after collisions, and how to integrate over them, and eventually winds up with a partial differential equation for the time variation of the energy distribution (yes, he confusingly uses x to denote energy)—and argues that Maxwell’s exponential distribution is a stationary solution to this equation: A few paragraphs further on, something important happens: Boltzmann introduces a function that here he calls E, though later he’ll call it H: Ten pages of computation follow and finally Boltzmann gets his main result: if the velocity distribution evolves according to his equation, H can never increase with time, becoming zero for the Maxwell distribution. In other words, he is saying that he’s proved that a gas will always (“monotonically”) approach equilibrium—which seems awfully like some kind of microscopic proof of the Second Law. But then Boltzmann makes a bolder claim: It has thus been rigorously proved that, whatever the initial distribution of kinetic energy may be, in the course of a very long time it must always necessarily approach the one found by Maxwell. The procedure used so far is of course nothing more than a mathematical artifice employed in order to give a rigorous proof of a theorem whose exact proof has not previously been found. It gains meaning by its applicability to the theory of polyatomic gas molecules. There one can again prove that a certain quantity E can only decrease as a consequence of molecular motion, or in a limiting case can remain constant. One can also prove that for the atomic motion of a system of arbitrarily many material points there always exists a certain quantity which, in consequence of any atomic motion, cannot increase, and this quantity agrees up to a constant factor with the value found for the well-known integral ∫dQ/T in my [1871] paper on the “Analytical proof of the 2nd law, etc.”. We have therefore prepared the way for an analytical proof of the Second Law in a completely different way from those previously investigated. Up to now the object has been to show that ∫dQ/T = 0 for reversible cyclic processes, but it has not been proved analytically that this quantity is always negative for irreversible processes, which are the only ones that occur in nature. The reversible cyclic process is only an ideal, which one can more or less closely approach but never completely attain. Here, however, we have succeeded in showing that ∫dQ/T is in general negative, and is equal to zero only for the limiting case, which is of course the reversible cyclic process (since if one can go through the process in either direction, ∫dQ/T cannot be negative). In other words, he’s saying that the quantity H that he’s defined microscopically in terms of velocity distributions can be identified (up to a sign) with the entropy that Clausius defined as dQ/T. He says that he’ll show this in the context of analyzing the mechanics of polyatomic molecules. But first he’s going to take a break and show that his derivation doesn’t need to assume continuity. In a pre-quantum-mechanics pre-cellular-automaton-fluid kind of way he replaces all the integrals by limits of sums of discrete quantities (i.e. he’s quantizing kinetic energy, etc.): He says that this discrete approach makes everything clearer, and quotes Lagrange’s derivation of vibrations of a string as an example of where this has happened before. But then he argues that everything works out fine with the discrete approach, and that H still decreases, with the Maxwell distribution as the only possible end point. As an aside, he makes a jab at Maxwell’s derivation, pointing out that with Maxwell’s functional equation: … there are infinitely many other solutions, which are not useful however since ƒ(x) comes out negative or imaginary for some values of x. Hence, it follows very clearly that Maxwell’s attempt to prove a priori that his solution is the only one must fail, since it is not the only one but rather it is the only one that gives purely positive probabilities, and therefore the only useful one. But finally—after another aside about computing thermal conductivities of gases—Boltzmann digs into polyatomic molecules, and his claim about H being related to entropy. There’s another 26 pages of calculations, and then we get to a section entitled “Solution of Equation (81) and Calculation of Entropy”. More pages of calculation about polyatomic molecules ensue. But finally we’re computing H, and, yes, it agrees with the Clausius result—but anticlimactically he’s only dealing with the case of equilibrium for monatomic molecules, where we already knew we got the Maxwell distribution: And now he decides he’s not talking about polyatomic molecules anymore, and instead: In order to find the relation of the quantity [H] to the second law of thermodynamics in the form ∫dQ/T < 0, we shall interpret the system of mass points not, as previously, as a gas molecule, but rather as an entire body. But then, in the last couple of pages of his paper, Boltzmann pulls out another idea. He’s discussed the concept that polyatomic molecules (or, now, whole systems) can be in many different configurations, or “phases”. But now he says: “We shall replace [our] single system by a large number of equivalent systems distributed over many different phases, but which do not interact with each other”. In other words, he’s introducing the idea of an ensemble of states of a system. And now he says that instead of looking at the distribution just for a single velocity, we should do it for all velocities, i.e. for the whole “phase” of the system. [These distributions] may be discontinuous, so that they have large values when the variables are very close to certain values determined by one or more equations, and otherwise vanishingly small. We may choose these equations to be those that characterize visible external motion of the body and the kinetic energy contained in it. In this connection it should be noted that the kinetic energy of visible motion corresponds to such a large deviation from the final equilibrium distribution of kinetic energy that it leads to an infinity in H, so that from the point of view of the Second Law of thermodynamics it acts like heat supplied from an infinite temperature. There are a bunch of ideas swirling around here. Phase-space density (cf. Liouville’s equation). Coarse-grained variables. Microscopic representation of mechanical work. Etc. But the paper is ending. There’s a discussion about H for systems that interact, and how there’s an equilibrium value achieved. And finally there’s a formula for entropy that Boltzmann said “agrees … with the expression I found in my previous [1871] paper”. So what exactly did Boltzmann really do in his 1872 paper? He introduced the Boltzmann Transport Equation which allows one to compute at least certain non-equilibrium properties of gases. But is his ƒ log ƒ quantity really what we can call “entropy” in the sense Clausius meant? And is it true that he’s proved that entropy (even in his sense) increases? A century and a half later there’s still a remarkable level of confusion around both these issues. But in any case, back in 1872 Boltzmann’s “minimum theorem” (now called his “H theorem”) created quite a stir. But after some time there was an objection raised, which we’ll discuss below. And partly in response to this, Boltzmann (after spending time working on microscopic models of electrical properties of materials—as well as doing some actual experiments) wrote another major paper on entropy and the Second Law in 1877: The translated title of the paper is “On the Relation between the Second Law of Thermodynamics and Probability Theory with Respect to the Laws of Thermal Equilibrium”. And at the very beginning of the paper Boltzmann makes a statement that was pivotal for future discussions of the Second Law: he says it’s now clear to him that an “analytical proof” of the Second Law is “only possible on the basis of probability calculations”. Now that we know about computational irreducibility and its implications one could say that this was the point where Boltzmann and those who followed him went off track in understanding the true foundations of the Second Law. But Boltzmann’s idea of introducing probability theory was effectively what launched statistical mechanics, with all its rich and varied consequences. Boltzmann makes his basic claim early in the paper with the statement (quoting from a comment in a paper he’d written earlier the same year) that “it is clear” (always a dangerous thing to say!) that in thermal equilibrium all possible states of the system—say, spatially uniform and nonuniform alike—are equally probable … comparable to the situation in the game of Lotto where every single quintet is as improbable as the quintet 12345. The higher probability that the state distribution becomes uniform with time arises only because there are far more uniform than nonuniform state distributions… He goes on: [Thus] it is possible to calculate the thermal equilibrium state by finding the probability of the different possible states of the system. The initial state will in most cases be highly improbable but from it the system will always rapidly approach a more probable state until it finally reaches the most probable state, i.e., that of thermal equilibrium. If we apply this to the Second Law we will be able to identify the quantity which is usually called entropy with the probability of the particular state… He’s talked about thermal equilibrium, even in the title, but now he says: … our main purpose here is not to limit ourselves to thermal equilibrium, but to explore the relationship of the probabilistic formulation to the [Second Law]. He says his goal is to calculate probability distribution for different states, and he’ll start with as simple a case as possible, namely a gas of rigid absolutely elastic spherical molecules trapped in a container with absolutely elastic walls. (Which interact with central forces only within a certain small distance, but not otherwise; the latter assumption, which includes the former as a special case, does not change the calculations in the least). In other words, yet again he’s going to look at hard sphere gases. But, he says: Even in this case, the application of probability theory is not easy. The number of molecules is not infinite, in a mathematical sense, yet the number of velocities each molecule is capable of is effectively infinite. Given this last condition, the calculations are very difficult; to facilitate understanding, I will, as in earlier work, consider a limiting case. And this is where he “goes discrete” again—allowing (“cellular-automaton-style”) only discrete possible velocities for each molecule: He says that upon colliding, two molecules can exchange these discrete velocities, but nothing more. As he explains, though: Even if, at first sight, this seems a very abstract way of treating the problem, it rapidly leads to the desired objective, and when you consider that in nature all infinities are but limiting cases, one assumes each molecule can behave in this fashion only in the limiting case where each molecule can assume more and more values of the velocity. But now—much like in an earlier paper—he makes things even simpler, saying he’s going to ignore velocities for now, and just say that the possible energies of molecules are “in an arithmetic progression”: He plans to look at collisions, but first he just wants to consider the combinatorial problem of distributing these energies among n molecules in all possible ways, subject to the constraint of having a certain fixed total energy. He sets up a specific example, with 7 molecules, total energy 7, and maximum energy per molecule 7—then gives an explicit table of all possible states (up to, as he puts it, “immaterial permutations of molecular labels”): Tables like this had been common for nearly two centuries in combinatorial mathematics books like Jacob Bernoulli’s (1655–1705) Ars Conjectandi but this might have been the first place such a table had appeared in a paper about fundamental physics. And now Boltzmann goes into an analysis of the distribution of states—of the kind that’s now long been standard in textbooks of statistical physics, but will then have been quite unfamiliar to the pure-calculus-based physicists of the time: He derives the average energy per molecule, as well as the fluctuations: He says that “of course” the real interest is in the limit of an infinite number of molecules, but he still wants to show that for “moderate values” the formulas remain quite accurate. And then (even without Wolfram Language!) he’s off finding (using Newton’s method it seems) approximate roots of the necessary polynomials: Just to show how it all works, he considers a slightly larger case as well: Now he’s computing the probability that a given molecule has a particular energy and determining that in the limit it’s an exponential that is, as he says, “consistent with that known from gases in thermal equilibrium”. He claims that in order to really get a “mechanical theory of heat” it’s necessary to take a continuum limit. And here he concludes that thermal equilibrium is achieved by maximizing the quantity Ω (where the “l” stands for log, so this is basically ƒ log ƒ): He explains that Ω is basically the log of the number of possible permutations, and that it’s “of special importance”, and he’ll call it the “permutability measure”. He immediately notes that “the total permutability measure of two bodies is equal to the sum of the permutability measures of each body”. (Note that Boltzmann’s Ω isn’t the modern total-number-of-states Ω; confusingly, that’s essentially the exponential of Boltzmann’s Ω.) He goes through some discussion of how to handle extra degrees of freedom in polyatomic molecules, but then he’s on to the main event: arguing that Ω is (essentially) the entropy. It doesn’t take long: Basically he just says that in equilibrium the probability ƒ(…) for a molecule to have a particular velocity is given by the Maxwell distribution, then he substitutes this into the formula for Ω, and shows that indeed, up to a constant, Ω is exactly the “Clausius entropy” ∫dQ/T. So, yes, in equilibrium Ω seems to be giving the entropy. But then Boltzmann makes a bit of a jump. He says that in processes that aren’t reversible both “Clausius entropy” and Ω will increase, and can still be identified—and enunciates the general principle, printed in his paper in special doubled-spaced form: … [In] any system of bodies that undergoes state changes … even if the initial and final states are not in thermal equilibrium … the total permutability measure for the bodies will continually increase during the state changes, and can remain constant only so long as all the bodies during the state changes remain infinitely close to thermal equilibrium (reversible state changes). In other words, he’s asserting that Ω behaves the same way entropy is said to behave according to the Second Law. He gives various thought experiments about gases in boxes with dividers, gases under gravity, etc. And finally concludes that, yes, the relationship of entropy to Ω “applies to the general case”. There’s one final paragraph in the paper, though: Up to this point, these propositions may be demonstrated exactly using the theory of gases. If one tries, however, to generalize to liquid drops and solid bodies, one must dispense with an exact treatment from the outset, since far too little is known about the nature of the latter states of matter, and the mathematical theory is barely developed. But I have already mentioned reasons in previous papers, in virtue of which it is likely that for these two aggregate states, the thermal equilibrium is achieved when Ω becomes a maximum, and that when thermal equilibrium exists, the entropy is given by the same expression. It can therefore be described as likely that the validity of the principle which I have developed is not just limited to gases, but that the same constitutes a general natural law applicable to solid bodies and liquid droplets, although the exact mathematical treatment of these cases still seems to encounter extraordinary difficulties. Interestingly, Boltzmann is only saying that it’s “likely” that in thermal equilibrium his permutability measure agrees with Clausius’s entropy, and he’s implying that actually that’s really the only place where Clausius’s entropy is properly defined. But certainly his definition is more general (after all, it doesn’t refer to things like temperature that are only properly defined in equilibrium), and so—even though Boltzmann didn’t explicitly say it—one can imagine basically just using it as the definition of entropy for arbitrary cases. Needless to say, the story is actually more complicated, as we’ll see soon. But this definition of entropy—crispened up by Max Planck (1858–1947) and with different notation—is what ended up years later “written in stone” at Boltzmann’s grave:
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.7 The Concept of Ergodicity : In his 1877 paper Boltzmann had made the claim that in equilibrium all possible microscopic states of a system would be equally probable. But why should this be true? One reason could be that in its pure “mechanical evolution” the system would just successively visit all these states. And this was an idea that Boltzmann seems to have had—with increasing clarity—from the time of his very first paper in 1866 that purported to “prove the Second Law” from mechanics. In modern times—with our understanding of discrete systems and computational rules—it’s not difficult to describe the idea of “visiting all states”. But in Boltzmann’s time it was considerably more complicated. Did one expect to hit all the infinite possible infinitesimally separated configurations of a system? Or somehow just get close? The fact is that Boltzmann had certainly dipped his toe into thinking about things in terms of discrete quantities. But he didn’t make the jump to imagining discrete rules, even though he certainly did know about discrete iterative processes, like Newton’s method for finding roots. Boltzmann knew about cases—like circular motion—where everything was purely periodic. But maybe when motion wasn’t periodic, it’d inevitably “visit all states”. Already in 1868 Boltzmann was writing a paper entitled “Solution to a Mechanical Problem” where he studies a single point mass moving in an α/r – β/r2 potential and bouncing elastically off a line—and manages to show that it visits every position with equal probability. In this paper he’s just got traditional formulas, but by 1871, in “Some General Theorems about Thermal Equilibrium”—computing motion in the same potential as before—he’s got a picture: Boltzmann probably knew about Lissajous figures—cataloged in 1857 and the fact that in this case a rational ratio of x and y periods gives a periodic overall curve while an irrational one always gives a curve that visits every position might have led him to suspect that all systems would either be periodic, or would visit every possible configuration (or at least, as he identified in his paper, every configuration that had the same values of “constants of the motion”, like energy). In early 1877 Boltzmann returned to the same question, including as one section in his “Remarks on Some Problems in the Mechanical Theory of Heat” more analysis of the same potential as before, but now showing a diversity of more complicated pictures that almost seem to justify his rule-30-before-its-time idea that there could be “pure mechanics” that would lead to “Second Law” behavior: In modern times, of course, it’s easy to solve those equations of motion, and typical results obtained for an array of values of parameters are: Boltzmann returned to these questions in 1884, responding to Helmholtz’s analysis of what he was calling “monocyclic systems”. Boltzmann used the same potential again, but now with a name for the “visit-all-states” property: isodic. Meanwhile, Boltzmann had introduced the name “ergoden” for the collection of all possible configurations of a system with a given energy (what would now be called the microcanonical ensemble). But somehow, quite a few years later, Boltzmann’s student Paul Ehrenfest (1880–1933) (along with Tatiana Ehrenfest-Afanassjewa (1876–1964)) would introduce the term “ergodic” for Boltzmann’s isodic. And “ergodic” is the term that caught on. And in the twentieth century there was all sorts of development of “ergodic theory”, as we’ll discuss a bit later. But back in the 1800s people continued to discuss the possibility that what would become called ergodicity was somehow generic, and would explain why all states would somehow be equally probable, why the Maxwell distribution of velocities would be obtained, and ultimately why the Second Law was true. Maxwell worked out some examples. So did Kelvin. But it remained unclear how it would all work out, as Kelvin (now with many letters after his name) discussed in a talk he gave in 1900 celebrating the new century: The dynamical theory of light didn’t work out. And about the dynamical theory of heat, he quotes Maxwell (following Boltzmann) in one of his very last papers, published in 1878, as saying, in reference to what amounts to a proof of the Second Law from underlying dynamics: Kelvin talks about exploring test cases: When, for example, is the motion of a single particle bouncing around in a fixed region ergodic? He considers first an ellipse, and proves that, no, there isn’t in general ergodicity there: Then he goes on to the much more complicated case and now he does an “experiment” (with a rather Monte Carlo flavor): Kelvin considers a few other examples but mostly concludes that he can’t tell in general about ergodicity—and that probably something else is needed, or as he puts it (somehow wrapping the theory of light into the story as well):
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.8 But What about Reversibility? : Had Boltzmann’s 1872 H theorem proved the Second Law? Was the Second Law—with its rather downbeat implication about the heat death of the universe—even true? One skeptic was Boltzmann’s friend and former teacher, the chemist Josef Loschmidt (1821–1895), who in 1866 had used kinetic theory to (rather accurately) estimate the size of air molecules. And in 1876 Loschmidt wrote a paper entitled “On the State of Thermal Equilibrium in a System of Bodies with Consideration of Gravity” in which he claimed to show that when gravity was taken into account, there wouldn’t be uniform thermal equilibrium, the Maxwell distribution, or the Second Law—and thus, as he poetically explained: The terroristic nimbus of the Second Law is destroyed, a nimbus which makes that Second Law appear as the annihilating principle of all life in the universe—and at the same time we are confronted with the comforting perspective that, as far as the conversion of heat into work is concerned, mankind will not solely be dependent on the intervention of coal or of the Sun, but will have available an inexhaustible resource of convertible heat at all times. His main argument revolves around a thought experiment involving molecules in a gravitational field: Over the next couple of years, despite Loschmidt’s progressively more elaborate constructions Boltzmann and Maxwell will debunk this particular argument—even though to this day the role of gravity in relation to the Second Law remains incompletely resolved. But what’s more important for our narrative about Loschmidt’s original paper are a couple of paragraphs tucked away at the end of one section (that in fact Kelvin had basically anticipated in 1874): [Consider what would happen if] after a time t sufficiently long for the stationary state to obtain, we suddenly reversed the velocities of all atoms. Initially we would be in a state that would look like the stationary state. This would be true for some time, but in the long run the stationary state would deteriorate and after the time t we would inevitably return to the initial state… It is clear that in general in any system one can revert the entire course of events by suddenly inverting the velocities of all the elements of the system. This doesn’t give a solution to the problem of undoing everything that happens [in the universe] but it does give a simple prescription: just suddenly revert the instantaneous velocities of all atoms of the universe. How did this relate to the H theorem? The underlying molecular equations of motion that Boltzmann had assumed in his proof were reversible in time. Yet Boltzmann claimed that H was always going to a minimum. But why couldn’t one use Loschmidt’s argument to construct an equally possible “reverse evolution” in which H was instead going to a maximum? It didn’t take Boltzmann long to answer, in print, tucked away in a section of his paper “Remarks on Some Problems in the Mechanical Theory of Heat”. He admits that Loschmidt’s argument “has great seductiveness”. But he claims it is merely “an interesting sophism”—and then says he will “locate the source of the fallacy”. He begins with a classic setup: a collection of hard spheres in a box. Suppose that at time zero the distribution of spheres in the box is not uniform; for example, suppose that the density of spheres is greater on the right than on the left … The sophism now consists in saying that, without reference to the initial conditions, it cannot be proved that the spheres will become uniformly mixed in the course of time. But then he rather boldly claims that with the actual initial conditions described, the spheres will “almost always [become] uniform” at a future time t. Now he imagines (following Loschmidt) reversing all the velocities in this state at time t. Then, he says: … the spheres would sort themselves out as time progresses, and at [the analog of] time 0, they would have a completely nonuniform distribution, even though the [new] initial distribution [one had used] was almost uniform. But now he says that, yes—given this counterexample—it won’t be possible to prove that the final distribution of spheres will always be uniform. This is in fact a consequence of probability theory, for any nonuniform distribution, no matter how improbable it may be, is still not absolutely impossible. Indeed it is clear that any individual uniform distribution, which might arise after a certain time from some particular initial state, is just as improbable as an individual nonuniform distribution; just as in the game of Lotto, any individual set of five numbers is as improbable as the set 1, 2, 3, 4, 5. It is only because there are many more uniform distributions than nonuniform ones that the distribution of states will become uniform in the course of time. One therefore cannot prove that, whatever may be the positions and velocities of the spheres at the beginning, the distribution must become uniform after a long time; rather one can only prove that infinitely many more initial states will lead to a uniform one after a definite length of time than to a nonuniform one. He adds: One could even calculate, from the relative numbers of the different state distributions, their probabilities, which might lead to an interesting method for the calculation of thermal equilibrium. And indeed within a few months Boltzmann has followed up on that “interesting method” to produce his classic paper on the probabilistic interpretation of entropy. But in his earlier paper he goes on to argue: Since there are infinitely many more uniform than nonuniform distributions of states, the latter case is extraordinarily improbable [to arise] and can be considered impossible for practical purposes; just as it may be considered impossible that if one starts with oxygen and nitrogen mixed in a container, after a month one will find chemically pure oxygen in the lower half and nitrogen in the upper half, although according to probability theory this is merely very improbable but not impossible. He talks about how interesting it is that the Second Law is intimately connected with probability while the First Law is not. But at the end he does admit: Perhaps this reduction of the Second Law to the realm of probability makes its application to the entire universe appear dubious, but the laws of probability theory are confirmed by all experiments carried out in the laboratory. At this point it’s all rather unconvincing. The H theorem had purported to prove the Second Law. But now he’s just talking about probability theory. He seems to have given up on proving the Second Law. And he’s basically just saying that the Second Law is true because it’s observed to be true—like other laws of nature, but not like something that can be “proved”, say from underlying molecular dynamics. For many years not much attention was paid to these issues, but by the late 1880s there were attempts to clarify things, particularly among the rather active British circle of kinetic theorists. A published 1894 letter from the Irish mathematician Edward Culverwell (1855–1931) (who also wrote about ice ages and Montessori education) summed up some of the confusions that were circulating: At a lecture in England the next year, Boltzmann countered (conveniently, in English): He goes on, but doesn’t get much more specific: He then makes an argument that will be repeated many times in different forms, saying that there will be fluctuations, where H deviates temporarily from its minimum value, but these will be rare: Later he’s talking about what he calls the “H curve” (a plot of H as a function of time), and he’s trying to describe its limiting form: And he even refers to Weierstrass’s recent work on nondifferentiable functions: But he doesn’t pursue this, and instead ends his “rebuttal” with a more philosophical—and in some sense anthropic—argument that he attributes to his former assistant Ignaz Schütz (1867–1927): It’s an argument that we’ll see in various forms repeated over the century and a half that follows. In essence what it’s saying is that, yes, the Second Law implies that the universe will end up in thermal equilibrium. But there’ll always be fluctuations. And in a big enough universe there’ll be fluctuations somewhere that are large enough to correspond to the world as we experience it, where “visible motion and life exist”. But regardless of such claims, there’s a purely formal question about the H theorem. How exactly is it that from the Boltzmann transport equation—which is supposed to describe reversible mechanical processes—the H theorem manages to prove that the H function irreversibly decreases? It wasn’t until 1895—fully 25 years after Boltzmann first claimed to prove the H theorem—that this issue was even addressed. And it first came up rather circuitously through Boltzmann’s response to comments in a textbook by Gustav Kirchhoff (1824–1887) that had been completed by Max Planck. The key point is that Boltzmann’s equation makes an implicit assumption, that’s essentially the same as Maxwell made back in 1860: that before each collision between molecules, the molecules are statistically uncorrelated, so that the probability for the collision has the factored form ƒ(v1) ƒ(v2). But what about after the collision? Inevitably the collision itself will lead to correlations. So now there’s an asymmetry: there are no correlations before each collision, but there are correlations after. And that’s why the behavior of the system doesn’t have to be symmetrical—and the H theorem can prove that H irreversibly decreases. In 1895 Boltzmann wrote a 3-page paper (after half in footnotes) entitled “More about Maxwell’s Distribution Law for Speeds” where he explained what he thought was going on: [The reversibility of the laws of mechanics] has been recently applied in judging the assumptions necessary for a proof of [the H theorem]. This proof requires the hypothesis that the state of the gas is and remains molecularly disordered, namely, that the molecules of a given class do not always or predominantly collide in a specific manner and that, on the contrary, the number of collisions of a given kind can be found by the laws of probability. Now, if we assume that in general a state distribution never remains molecularly ordered for an unlimited time and also that for a stationary state-distribution every velocity is as probable as the reversed velocity, then it follows that by inversion of all the velocities after an infinitely long time every stationary state-distribution remains unchanged. After the reversal, however, there are exactly as many collisions occurring in the reversed way as there were collisions occurring in the direct way. Since the two state distributions are identical, the probability of direct and indirect collisions must be equal for each of them, whence follows Maxwell’s distribution of velocities. Boltzmann is introducing what we’d now call the “molecular chaos” assumption (and what Ehrenfest would call the Stosszahlansatz)—giving a rather self-fulfilling argument for why the assumption should be true. In Boltzmann’s time there wasn’t really anything better to do. By the 1940s the BBGKY hierarchy at least let one organize the hierarchy of correlations between molecules—though it still didn’t give one a tractable way to assess what correlations should exist in practice, and what not. Boltzmann knew these were all complicated issues. But he wrote about them at a technical level only a few more times in his life. The last time was in 1898 when, responding to a request from the mathematician Felix Klein (1849–1925), he wrote a paper about the H curve for mathematicians. He begins by saying that although this curve comes from the theory of gases, the essence of it can be reproduced by a process based on accumulating balls randomly picked from an urn. He then goes on to outline what amounts to a story of random walks and fractals. In another paper, he actually sketches the curve saying that his drawing “should be taken with a large grain of salt”, noting—in a remarkably fractal-reminiscent way—that “a zincographer [i.e. an engraver of printing plates] would not have been able to produce a real figure since the H-curve has a very large number of maxima and minima on each finite segment, and hence defies representation as a line of continuously changing direction.” Of course, in modern times it’s easy to produce an approximation to the H curve according to his prescription: But at the end of his “mathematical” paper he comes back to talking about gases. And first he makes the claim that the effective reversibility seen in the H curve will never be seen in actual physical systems because, in essence, there are always perturbations from outside. But then he ends, in a statement of ultimate reversibility that casts our everyday observation of irreversibility as tautological: There is no doubt that it is just as conceivable to have a world in which all natural processes take place in the wrong chronological order. But a person living in this upside-down world would have feelings no different than we do: they would just describe what we call the future as the past and vice versa.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.9 The Recurrence Objection : Probably the single most prominent research topic in mathematical physics in the 1800s was the three-body problem—of solving for the motion under gravity of three bodies, such as the Earth, Moon and Sun. And in 1890 the French mathematician Henri Poincaré (1854–1912) (whose breakout work had been on the three-body problem) wrote a paper entitled “On the Three-Body Problem and the Equations of Dynamics” in which, as he said: It is proved that there are infinitely many ways of choosing the initial conditions such that the system will return infinitely many times as close as one wishes to its initial position. There are also an infinite number of solutions that do not have this property, but it is shown that these unstable solutions can be regarded as “exceptional” and may be said to have zero probability. This was a mathematical result. But three years later Poincaré wrote what amounted to a philosophy paper entitled “Mechanism and Experience” which expounded on its significance for the Second Law: In the mechanistic hypothesis, all phenomena must be reversible; for example, the stars might traverse their orbits in the retrograde sense without violating Newton’s law; this would be true for any law of attraction whatever. This is therefore not a fact peculiar to astronomy; reversibility is a necessary consequence of all mechanistic hypotheses. Experience provides on the contrary a number of irreversible phenomena. For example, if one puts together a warm and a cold body, the former will give up its heat to the latter; the opposite phenomenon never occurs. Not only will the cold body not return to the warm one the heat which it has taken away when it is in direct contact with it; no matter what artifice one may employ, using other intervening bodies, this restitution will be impossible, at least unless the gain thereby realized is compensated by an equivalent or large loss. In other words, if a system of bodies can pass from state A to state B by a certain path, it cannot return from B to A, either by the same path or by a different one. It is this circumstance that one describes by saying that not only is there not direct reversibility, but also there is not even indirect reversibility. But then he continues: A theorem, easy to prove, tells us that a bounded world, governed only by the laws of mechanics, will always pass through a state very close to its initial state. On the other hand, according to accepted experimental laws (if one attributes absolute validity to them, and if one is willing to press their consequences to the extreme), the universe tends toward a certain final state, from which it will never depart. In this final state, which will be a kind of death, all bodies will be at rest at the same temperature. But in fact, he says, the recurrence theorem shows that: This state will not be the final death of the universe, but a sort of slumber, from which it will awake after millions of millions of centuries. According to this theory, to see heat pass from a cold body to a warm one … it will suffice to have a little patience. [And we may] hope that some day the telescope will show us a world in the process of waking up, where the laws of thermodynamics are reversed. By 1903, Poincaré was more strident in his critique of the formalism around the Second Law, writing (in English) in a paper entitled “On Entropy”: But back in 1896, Boltzmann and the H theorem had another critic: Ernst Zermelo (1871–1953), a recent German math PhD who was then working with Max Planck on applied mathematics—though would soon turn to foundations of mathematics and become the “Z” in ZFC set theory. Zermelo’s attack on the H theorem began with a paper entitled “On a Theorem of Dynamics and the Mechanical Theory of Heat”. After explaining Poincaré’s recurrence theorem, Zermelo gives some “mathematician-style” conditions (the gas must be in a finite region, must have no infinite energies, etc.), then says that even though there must exist states that would be non-recurrent and could show irreversible behavior, there would necessarily be infinitely more states that “would periodically repeat themselves … with arbitrarily small variations”. And, he argues, such repetition would affect macroscopic quantities discernable by our senses. He continues: In order to retain the general validity of the Second Law, we therefore would have to assume that just those initial states leading to irreversible processes are realized in nature, their small number notwithstanding, while the other ones, whose probability of existence is higher, mathematically speaking, do not actually occur. And he concludes that the Poincaré recurrence phenomenon means that: … it is certainly impossible to carry out a mechanical derivation of the Second Law on the basis of the existing theory without specializing the initial states. Boltzmann responded promptly but quite impatiently: I have pointed out particularly often, and as clearly as I possibly could … that the Second Law is but a principle of probability theory as far as the molecular-theoretic point of view is concerned. … While the theorem by Poincaré that Zermelo discusses in the beginning of his paper is of course correct, its application to heat theory is not. Boltzmann talks about the H curve, and first makes rather a mathematician-style point about the order of limits: If we first take the number of gas molecules to be infinite, as was clearly done in [my 1896 proof], and only then let the time grow very large, then, in the vast majority of cases, we obtain a curve asymptotically [always close to zero]. Moreover, as can easily be seen, Poincaré’s theorem is not applicable in this case. If, however, we take the time [span] to be infinitely great and, in contrast, the number of molecules to be very great but not absolutely infinite, then the H-curve has a different character. It almost always runs very close to [zero], but in rare cases it rises above that, in what we shall call a “hump” … at which significant deviations from the Maxwell velocity distribution can occur … Boltzmann then argues that even if you start “at a hump”, you won’t stay there, and “over an enormously long period of time” you’ll see something infinitely close to “equilibrium behavior”. But, he says: … it is [always] possible to reach again a greater hump of the H-curve by further extending the time … In fact, it is even the case that the original state must return, provided only that we continue to sufficiently extend the time… He continues: Mr. Zermelo is therefore right in claiming that, mathematically speaking, the motion is periodic. He has by no means succeeded, however, in refuting my theorems, which, in fact, are entirely consistent with this periodicity. After giving arguments about the probabilistic character of his results, and (as we would now say it) the fact that a 1D random walk is certain to repeatedly return to the origin, Boltzmann says that: … we must not conclude that the mechanical approach has to be modified in any way. This conclusion would be justified only if the approach had a consequence that runs contrary to experience. But this would be the case only if Mr. Zermelo were able to prove that the duration of the period within which the old state of the gas must recur in accordance with Poincaré’s theorem has an observable length… He goes on to imagine “a trillion tiny spheres, each with a [certain initial velocity] … in the one corner of a box” (and by “trillion” he means million million million, or today’s quintillion) and then says that “after a short time the spheres will be distributed fairly evenly in the box”, but the period for a “Poincaré recurrence” in which they all will return to their original corner is “so great that nobody can live to see it happen”. And to make this point more forcefully, Boltzmann has an appendix in which he tries to get an actual approximation to the recurrence time, concluding that its numerical value “has many trillions of digits”. He concludes: If we consider heat as a motion of molecules that occurs in accordance with the general equations of mechanics and assume that the arrangement of bodies that we perceive is currently in a highly improbable state, then a theorem follows that is in agreement with the Second Law for all phenomena so far observed. Of course, this theorem can no longer hold once we observe bodies of so small a scale that they only contain a few molecules. Since, however, we do not have at hand any experimental results on the behavior of bodies so small, this assumption does not run counter to previous experience. In fact, certain experiments conducted on very small bodies in gases seem rather to support the assumption, although we are still far from being able to assert its correctness on the basis of experimental proof. But then he gives an important caveat—with a small philosophical flourish: Of course, we cannot expect natural science to answer the question as to why the bodies surrounding us currently exist in a highly improbable state, just as we cannot expect it to answer the question as to why there are any phenomena at all and why they adhere to certain given principles. Unsurprisingly—particularly in view of his future efforts in the foundations of mathematics—Zermelo is unconvinced by all of this. And six months later he replies again in print. He admits that a full Poincaré recurrence might take astronomically long, but notes that (where, by “physical state”, he means one that we perceive): … we are after all always concerned only with the “physical state”, which can be realized by many different combinations, and hence can recur much sooner. Zermelo zeroes in on many of the weaknesses in Boltzmann’s arguments, saying that the thing he particularly “contests … is the analogy that is supposed to exist between the properties of the H curve and the Second Law”. He claims that irreversibility cannot be explained from “mechanical suppositions” without “new physical assumptions”—and in particular criteria for choosing appropriate initial states. He ends by saying that: From the great successes of the kinetic theory of gases in explaining the relationships among states we must not deduce its … applicability also to temporal processes. … [For in this case I am] convinced that it necessarily fails in the absence of entirely new assumptions. Boltzmann replies again—starting off with the strangely weak argument: The Second Law receives a mechanical explanation by virtue of the assumption, which is of course unprovable, that the universe, when considered as a mechanical system, or at least a very extensive part thereof surrounding us, started out in a highly improbable state and still is in such a state. And, yes, there’s clearly something missing in the understanding of the Second Law. And even as Zermelo pushes for formal mathematician-style clarity, Boltzmann responds with physicist-style “reasonable arguments”. There’s lots of rhetoric: The applicability of the calculus of probabilities to a particular case can of course never be proved with precision. If 100 out of 100,000 objects of a particular sort are consumed by fire per year, then we cannot infer with certainty that this will also be the case next year. On the contrary! If the same conditions continue to obtain for 1010 years, then it will often be the case during this period that the 100,000 objects are all consumed by fire at once on a single day, and even that not a single object suffers damage over the course of an entire year. Nevertheless, every insurance company places its faith in the calculus of probabilities. Or, in justification of the idea that we live in a highly improbable “low-entropy” part of the universe: I refuse to grant the objection that a mental picture requiring so great a number of dead parts of the universe for the explanation of so small a number of animated parts is wasteful, and hence inexpedient. I still vividly remember someone who adamantly refused to believe that the Sun’s distance from the Earth is 20 million miles on the ground that it would simply be foolish to assume so vast a space only containing luminiferous aether alongside so small a space filled with life. Curiously—given his apparent reliance on “commonsense” arguments—Boltzmann also says: I myself have repeatedly cautioned against placing excessive trust in the extension of our mental pictures beyond experience and issued reminders that the pictures of contemporary mechanics, and in particular the conception of the smallest particles of bodies as material points, will turn out to be provisional. In other words, we don’t know that we can think of atoms (even if they exist at all) as points, and we can’t really expect our everyday intuition to tell us about how they work. Which presumably means that we need some kind of solid, “formal” argument if we’re going to explain the Second Law. Zermelo didn’t respond again, and moved on to other topics. But Boltzmann wrote one more paper in 1897 about “A Mechanical Theorem of Poincaré” ending with two more why-it-doesn’t-apply-in-practice arguments: Poincaré’s theorem is of course never applicable to terrestrial bodies which we can hold in our hands as none of them is entirely closed. Nor it is applicable to an entirely closed gas of the sort considered by the kinetic theory if first the number of molecules and only then the quotients of the intervals between two neighboring collisions in the observation time is allowed to become infinite.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.10 Ensembles, and an Effort to Make Things Rigorous : Boltzmann—and Maxwell before him—had introduced the idea of using probability theory to discuss the emergence of thermodynamics and potentially the Second Law. But it wasn’t until around 1900—with the work of J. Willard Gibbs (1839–1903)—that a principled mathematical framework for thinking about this developed. And while we can now see that this framework distracts in some ways from several of the key issues in understanding the foundations of the Second Law, it’s been important in framing the discussion of what the Second Law really says—as well as being central in defining the foundations for much of what’s been done over the past century or so under the banner of “statistical mechanics”. Gibbs seems to have first gotten involved with thermodynamics around 1870. He’d finished his PhD at Yale on the geometry of gears in 1863—getting the first engineering PhD awarded in the US. After traveling in Europe and interacting with various leading mathematicians and physicists, he came back to Yale (where he stayed for the remaining 34 years of his life) and in 1871 became professor of mathematical physics there. His first papers (published in 1873 when he was already 34 years old) were in a sense based on taking seriously the formalism of equilibrium thermodynamics defined by Clausius and Maxwell—treating entropy and internal energy, just like pressure, volume and temperature, as variables that defined properties of materials (and notably whether they were solids, liquids or gases). Gibbs’s main idea was to “geometrize” this setup, and make it essentially a story of multivariate calculus: Unlike the European developers of thermodynamics, Gibbs didn’t interact deeply with other scientists—with the possible exception of Maxwell, who (a few years before his death in 1879) made a 3D version of Gibbs’s thermodynamic surface out of clay—and supplemented his 2D thermodynamic diagrams after the first edition of his textbook Theory of Heat with renderings of 3D versions: Three years later, Gibbs began publishing what would be a 300-page work defining what has become the standard formalism for equilibrium chemical thermodynamics. He began with a quote from Clausius: In the years that followed, Gibbs’s work—stimulated by Maxwell—mostly concentrated on electrodynamics, and later quaternions and vector analysis. But Gibbs published a few more small papers on thermodynamics—always in effect taking equilibrium (and the Second Law) for granted. In 1882—a certain Henry Eddy (1844–1921) (who in 1879 had written a book on thermodynamics, and in 1890 would become president of the University of Cincinnati), claimed that “radiant heat” could be used to violate the Second Law: Gibbs soon published a 2-page rebuttal (in the 6th-ever issue of Science magazine): Then in 1889 Clausius died, and Gibbs wrote an obituary—praising Clausius but making it clear he didn’t think the kinetic theory of gases was a solved problem: That same year Gibbs announced a short course that he would teach at Yale on “The a priori Deduction of Thermodynamic Principles from the Theory of Probabilities”. After a decade of work, this evolved into Gibbs’s last publication—an original and elegant book that’s largely defined how the Second Law has been thought about ever since: The book begins by explaining that mechanics is about studying the time evolution of single systems: But Gibbs says he is going to do something different: he is going to look at what he’ll call an ensemble of systems, and see how the distribution of their characteristics changes over time: He explains that these “inquiries” originally arose in connection with deriving the laws of thermodynamics: But he argues that this area—which he’s calling statistical mechanics—is worth investigating even independent of its connection to thermodynamics: Still, he expects this effort will be relevant to the foundations of thermodynamics: He immediately then goes on to what he’ll claim is the way to think about the relation of “observed thermodynamics” to his exact statistical mechanics: Soon he makes the interesting—if, in the light of history, very overly optimistic—claim that “the laws of thermodynamics may be easily obtained from the principles of statistical mechanics”: At first the text of the book reads very much like a typical mathematical work on mechanics: But soon it’s “going statistical”, talking about the “density” of systems in “phase” (i.e. with respect to the variables defining the configuration of the system). And a few pages in, he’s proving the fundamental result that the density of “phase fluid” satisfies a continuity equation (which we’d now call the Liouville equation): It’s all quite elegant, and all very rooted in the calculus-based mathematics of its time. He’s thinking about a collection of instances of a system. But while with our modern computational paradigm we’d readily be able to talk about a discrete list of instances, with his calculus-based approach he has to consider a continuous collection of instances—whose treatment inevitably seems more abstract and less explicit. He soon makes contact with the “theory of errors”, discussing in effect how probability distributions over the space of possible states evolve. But what probability distributions should one consider? By chapter 4, he’s looking at what he calls (and is still called) the “canonical distribution”: He gives a now-classic definition for the probability as a function of energy ϵ: He observes that this distribution combines nicely when independent parts of a system are brought together, and soon he’s noting that: But so far he’s careful to just talk about how things are “analogous”, without committing to a true connection: More than halfway through the book he’s defined certain properties of his probability distributions that “may … correspond to the thermodynamic notions of entropy and temperature”: Next he’s on to the concept of a “microcanonical ensemble” that includes only states of a given energy. For him—with his continuum-based setup—this is a slightly elaborate thing to define; in our modern computational framework it actually becomes more straightforward than his “canonical ensemble”. Or, as he already says: But what about the Second Law? Now he’s getting a little closer: When he says “index of probability” he’s talking about the log of a probability in his ensemble, so this result is about the fact that this quantity is extremized when all the elements of the ensemble have equal probability: Soon he’s discussing whether he can use his index as a way—like Boltzmann tried to do with his version of entropy—to measure deviations from “statistical equilibrium”: But now Gibbs has hit one of the classic gotchas of his approach: if you look in perfect detail at the evolution of an ensemble of systems, there’ll never be a change in the value of his index—essentially because of the overall conservation of probability. Gibbs brings in what amounts to a commonsense physics argument to handle this. He says to consider putting “coloring matter” in a liquid that one stirs. And then he says that even though the liquid (like his phase fluid) is microscopically conserved, the coloring matter will still end up being “uniformly mixed” in the liquid: He talks about how the conclusion about whether mixing happens in effect depends on what order one takes limits in. And while he doesn’t put it quite this way, he’s essentially realized that there’s a competition between the system “mixing things up more and more finely” and the observer being able to track finer and finer details. He realizes, though, that not all systems will show this kind of mixing behavior, noting for example that there are mechanical systems that’ll just keep going in simple cycles forever. He doesn’t really resolve the question of why “practical systems” should show mixing, more or less ending with a statement that even though his underlying mechanical systems are reversible, it’s somehow “in practice” difficult to go back: Despite things like this, Gibbs appears to have been keen to keep the majority of his book “purely mathematical”, in effect proving theorems that necessarily followed from the setup he had given. But in the penultimate chapter of the book he makes what he seems to have viewed as a less-than-satisfactory attempt to connect what he’s done with “real thermodynamics”. He doesn’t really commit to the connection, though, characterizing it more as an “analogy”: But he soon starts to be pretty clear that he actually wants to prove the Second Law: He quickly backs off a little, in effect bringing in the observer to soften the requirements: But then he fires his best shot. He says that the quantities he’s defined in connection with his canonical ensemble satisfy the same equations as Clausius originally set up for temperature and entropy: He adds that fluctuations (or “anomalies”, as he calls them) become imperceptible in the limit of a large system: But in physical reality, why should one have a whole collection of systems as in the canonical ensemble? Gibbs suggests it would be more natural to look at the microcanonical ensemble—and in fact to look at a “time ensemble”, i.e. an averaging over time rather than an averaging over different possible states of the system: Gibbs has proved some results (e.g. related to the virial theorem) about the relation between time and ensemble averages. But as the future of the subject amply demonstrates, they’re not nearly strong enough to establish any general equivalence. Still, Gibbs presses on. In the end, though, as he himself recognized, things weren’t solved—and certainly the canonical ensemble wasn’t the whole story: He discusses the tradeoff between having a canonical ensemble “heat bath” of a known temperature, and having a microcanonical ensemble with known energy. At one point he admits that it might be better to consider the time evolution of a single state, but basically decides that—at least in his continuous-probability-distribution-based formalism—he can’t really set this up: Gibbs definitely encourages the idea that his “statistical mechanics” has successfully “derived” thermodynamics. But he’s ultimately quite careful and circumspect in what he actually says. He mentions the Second Law only once in his whole book—and then only to note that he can get the same “mathematical expression” from his canonical ensemble as Clausius’s form of the Second Law. He doesn’t mention Boltzmann’s H theorem anywhere in the book, and—apart from one footnote concerning “difficulties long recognized by physicists”—he mentions only Boltzmann’s work on theoretical mechanics. One can view the main achievement of Gibbs’s book as having been to define a framework in which precise results about the statistical properties of collections of systems could be defined and in some cases derived. Within the mathematics and other formalism of the time, such ensemble results represented in a sense a distinctly “higher-order” description of things. Within our current computational paradigm, though, there’s much less of a distinction to be made: whether one’s looking at a single path of evolution, or a whole collection, one’s ultimately still just dealing with a computation. And that makes it clearer that—ensembles or not—one’s thrown back into the same kinds of issues about the origin of the Second Law. But even so, Gibbs provided a language in which to talk with some clarity about many of the things that come up.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.11 Maxwell’s Demon : In late 1867 Peter Tait (1831–1901)—a childhood friend of Maxwell’s who was by then a professor of “natural philosophy” in Edinburgh—was finishing his sixth book. It was entitled Sketch of Thermodynamics and gave a brief, historically oriented and not particularly conceptual outline of what was then known about thermodynamics. He sent a draft to Maxwell, who responded with a fairly long letter: The letter begins: I do not know in a controversial manner the history of thermodynamics … [and] I could make no assertions about the priority of authors … Any contributions I could make … [involve] picking holes here and there to ensure strength and stability. Then he continues (with “ΘΔcs” being his whimsical Greekified rendering of the word “thermodynamics”): To pick a hole—say in the 2nd law of ΘΔcs, that if two things are in contact the hotter cannot take heat from the colder without external agency. Now let A and B be two vessels divided by a diaphragm … Now conceive a finite being who knows the paths and velocities of all the molecules by simple inspection but who can do no work except open and close a hole in the diaphragm by means of a slide without mass. Let him … observe the molecules in A and when he sees one coming … whose velocity is less than the mean [velocity] of the molecules in B let him open the hole and let it go into B [and vice versa]. Then the number of molecules in A and B are the same as at first, but the energy in A is increased and that in B diminished, that is, the hot system has got hotter and the cold colder and yet no work has been done, only the intelligence of a very observant and neat-fingered being has been employed. Or in short [we can] … restore a uniformly hot system to unequal temperatures… Only we can’t, not being clever enough. And so it was that the idea of “Maxwell’s demon” was launched. Tait must at some point have shown Maxwell’s letter to Kelvin, who wrote on it: Very good. Another way is to reverse the motion of every particle of the Universe and to preside over the unstable motion thus produced. But the first place Maxwell’s demon idea appeared in print was in Maxwell’s 1872 textbook Theory of Heat: Much of the book is devoted to what was by then quite traditional, experimentally oriented thermodynamics. But Maxwell included one final chapter: Even in 1871, after all his work on kinetic theory, Maxwell is quite circumspect in his discussion of molecules: But Maxwell’s textbook goes through a series of standard kinetic theory results, much as a modern textbook would. The second-to-last section in the whole book sounds a warning, however: Interestingly, Maxwell continues, somewhat in anticipation of what Gibbs will say 30 years later: But then there’s a reminder that this is being written in 1871, several decades before any clear observation of molecules was made. Maxwell says: In other words, if there are water molecules, there must be something other than a law of averages that makes them all appear the same. And, yes, it’s now treated as a fundamental fact of physics that, for example, all electrons have exactly—not just statistically—the same properties such as mass and charge. But back in 1871 it was much less clear what characteristics molecules—if they existed as real entities at all—might have. Maxwell included one last section in his book that to us today might seem quite wild: In other words, aware of Darwin’s (1809–1882) 1859 Origin of Species, he’s considering a kind of “speciation” of molecules, along the lines of the discrete species observed in biology. But then he notes that unlike biological organisms, molecules are “permanent”, so their “selection” must come from some kind of pure separation process: And at the very end he suggests that if molecules really are all identical, that suggests a level of fundamental order in the world that we might even be able to flow through to “exact principles of distributive justice” (presumably for people rather than molecules): Maxwell has described rather clearly his idea of demons. But the actual name “demon” first appears in print in a paper by Kelvin in 1874: It’s a British paper, so—in a nod to future nanomachinery—it’s talking about (molecular) cricket bats: Kelvin’s paper—like his note written on Maxwell’s letter—imagines that the demons don’t just “sort” molecules; they actually reverse their velocities, thus in effect anticipating Loschmidt’s 1876 “reversibility objection” to Boltzmann’s H theorem. In an undated note, Maxwell discusses demons, attributing the name to Kelvin—and then starts considering the “physicalization” of demons, simplifying what they need to do: 
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.12 Concerning Demons : I. Who gave them this name? Thomson. 2. What were they by nature? Very small BUT lively beings incapable of doing work but able to open and shut valves which move without friction or inertia. 3. What was their chief end? To show that the 2nd Law of Thermodynamics has only a statistical certainty. 4. Is the production of an inequality of temperature their only occupation? No, for less intelligent demons can produce a difference in pressure as well as temperature by merely allowing all particles going in one direction while stopping all those going the other way. This reduces the demon to a valve. As such value him. Call him no more a demon but a valve like that of the hydraulic ram, suppose. It didn’t take long for Maxwell’s demon to become something of a fixture in expositions of thermodynamics, even if it wasn’t clear how it connected to other things people were saying about thermodynamics. And in 1879, for example, Kelvin gave a talk all about Maxwell’s “sorting demon” (like other British people of the time he referred to Maxwell as “Clerk Maxwell”): Kelvin describes—without much commentary, and without mentioning the Second Law—some of the feats of which the demon would be capable. But he adds: The description of the lecture ends: Presumably no actual Maxwell’s demon was shown—or Kelvin wouldn’t have continued for the rest of his life to treat the Second Law as an established principle. But in any case, Maxwell’s demon has always remained something of a fixture in discussions of the foundations of the Second Law. One might think that the observability of Brownian motion would make something like a Maxwell’s demon possible. And indeed in 1912 Marian Smoluchowski (1872–1917) suggested experiments that one could imagine would “systematically harvest” Brownian motion—but showed that in fact they couldn’t. In later years, a sequence of arguments were advanced that the mechanism of a Maxwell’s demon just couldn’t work in practice—though even today microscopic versions of what amount to Maxwell’s demons are routinely being investigated. What Happened to Those People? We’ve finally now come to the end of the story of how the original framework for the Second Law came to be set up. And, as we’ve seen, only a fairly small number of key players were involved: So what became of these people? Carnot lived a generation earlier than the others, never made a living as a scientist, and was all but unknown in his time. But all the others had distinguished careers as academic scientists, and were widely known in their time. Clausius, Boltzmann and Gibbs are today celebrated mainly for their contributions to thermodynamics; Kelvin and Maxwell also for other things. Clausius and Gibbs were in a sense “pure professors”; Boltzmann, Maxwell and especially Kelvin also had engagement with the more general public. All of them spent the majority of their lives in the countries of their birth—and all (with the exception of Carnot) were able to live out the entirety of their lives without time-consuming disruptions from war or other upheavals:
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.13 Sadi Carnot (1796–1832) : Almost all of what is known about Sadi Carnot as a person comes from a single biographical note written nearly half a century after his death by his younger brother Hippolyte Carnot (who was a distinguished French politician—and sometime education minister—and father of the Sadi Carnot who would become president of France). Hippolyte Carnot began by saying that: As the life of Sadi Carnot was not marked by any notable event, his biography would have occupied only a few lines; but a scientific work by him, after remaining long in obscurity, brought again to light many years after his death, has caused his name to be placed among those of great inventors. The Carnots’ father was close to Napoleon, and Hippolyte explains that when Sadi was a young child he ended up being babysat by “Madame Bonaparte”—but one day wandered off, and was found inspecting the operation of a nearby mill, and quizzing the miller about it. For the most part, however, throughout his life, Sadi Carnot apparently kept very much to himself—while with quiet intensity showing a great appetite for intellectual pursuits from mathematics and science to art, music and literature, as well as practical engineering and the science of various sports. Even his brother Hippolyte can’t explain quite how Sadi Carnot—at the age of 28—suddenly “came out” and in 1824 published his book on thermodynamics. (As we discussed above, it no doubt had something to do with the work of his father, who died two years earlier.) Sadi Carnot funded the publication of the book himself—having 600 copies printed (at least some of which remained unsold a decade later). But after the book was published, Carnot appears to have returned to just privately doing research, living alone, and never publishing again in his lifetime. And indeed he lived only another eight years, dying (apparently after some months of ill health) in the same Paris cholera outbreak that claimed General Lamarque of Les Misérables fame. Twenty-three pages of unpublished personal notes survive from the period after the publication of Carnot’s book. Some are general aphorisms and life principles: Speak little of what you know, and not at all of what you do not know. Why try to be witty? I would rather be thought stupid and modest than witty and pretentious. God cannot punish man for not believing when he could so easily have enlightened and convinced him. The belief in an all-powerful Being, who loves us and watches over us, gives to the mind great strength to endure misfortune. When walking, carry a book, a notebook to preserve ideas, and a piece of bread in order to prolong the walk if need be. But others are more technical—and in fact reveal that Carnot, despite having based his book on caloric theory, had realized that it probably wasn’t correct: When a hypothesis no longer suffices to explain phenomena, it should be abandoned. This is the case with the hypothesis which regards caloric as matter, as a subtile fluid. The experimental facts tending to destroy this theory are as follows: The development of heat by percussion or friction of bodies … The elevation of temperature which takes place [when] air [expands into a] vacuum … He continues: At present, light is generally regarded as the result of a vibratory movement of the ethereal fluid. Light produces heat, or at least accompanies radiating heat, and moves with the same velocity as heat. Radiating heat is then a vibratory movement. It would be ridiculous to suppose that it is an emission of matter while the light which accompanies it could be only a movement. Could a motion (that of radiating heat) produce matter (caloric)? No, undoubtedly; it can only produce a motion. Heat is then the result of a motion. And then—in a rather clear enunciation of what would become the First Law of thermodynamics: Heat is simply motive power, or rather motion which has changed form. It is a movement among the particles of bodies. Wherever there is destruction of motive power there is, at the same time, production of heat in quantity exactly proportional to the quantity of motive power destroyed. Reciprocally, wherever there is destruction of heat, there is production of motive power. Carnot also wonders: Liquefaction of bodies, solidification of liquids, crystallization—are they not forms of combinations of integrant molecules? Supposing heat due to a vibratory movement, how can the passage from the solid or the liquid to the gaseous state be explained? There is no indication of how Carnot felt about this emerging rethinking of thermodynamics, or of how it might affect the results in his book. But Carnot clearly hoped to do experiments (as outlined in his notes) to test what was really going on. But as it was, he presumably didn’t get around to any of them—and his notes, ahead of their time as they were, did not resurface for many decades, by which time the ideas they contained had already been discovered by others.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.14 Rudolf Clausius (1822–1888) : Rudolf Clausius was born in what’s now Poland (and was then Prussia), one of more than 14 children of an education administrator and pastor. He went to university in Berlin, and, after considering doing history, eventually specialized in math and physics. After graduating in 1844 he started teaching at a top high school in Berlin (which he did for 6 years), and meanwhile earned his PhD in physics. His career took off after his breakout paper on thermodynamics appeared in 1850. For a while he was a professor in Berlin, then for 12 years in Zürich, then briefly in Würzburg, then—for the remaining 19 years of his life—in Bonn. He was a diligent—if, one suspects, somewhat stiff—professor, notable for the clarity of his lectures, and his organizational care with students. He seems to have been a competent administrator, and late in his career he spent a couple of years as the president (“rector”) of his university. But first and foremost, he was a researcher, writing about a hundred papers over the course of his career. Most physicists of the time devoted at least some of their efforts to doing actual physics experiments. But Clausius was a pioneer in the idea of being a “pure theoretical physicist”, inspired by experiments and quoting their results, but not doing them himself. The majority of Clausius’s papers were about thermodynamics, though late in his career his emphasis shifted more to electrodynamics. Clausius’s papers were original, clear, incisive and often fairly mathematically sophisticated. But from his very first paper on thermodynamics in 1850, he very much adopted a macroscopic approach, talking about what he considered to be “bulk” quantities like energy, and later entropy. He did explore some of the potential mechanics of molecules, but he never really made the connection between molecular phenomena and entropy—or the Second Law. He had a number of run-ins about academic credit with Kelvin, Tait, Maxwell and Boltzmann, but he didn’t seem to ever pay much attention to, for example, Boltzmann’s efforts to find molecular-based probabilistic derivations of Clausius’s results. It probably didn’t help that after two decades of highly productive work, two misfortunes befell Clausius. First, in 1870, he had volunteered to lead an ambulance corps in the Franco-Prussian war, and was wounded in the knee, leading to chronic pain (as well as to his habit of riding to class on horseback). And then, in 1875, Clausius’s wife died in the birth of their sixth child—leaving him to care for six young children (which apparently he did with great conscientiousness). Clausius nevertheless continued to pursue his research—even to the end of his life—receiving many honors along the way (like election to no less than 40 professional societies), but it never again rose to the level of significance of his early work on thermodynamics and the Second Law.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.15 Kelvin (William Thomson) (1824–1907) : Of the people we’re discussing here, by far the most famous during their lifetime was Kelvin. In his long career he wrote more than 600 scientific papers, received dozens of patents, started several companies and served in many administrative and governmental roles. His father was a math professor, ultimately in Glasgow, who took a great interest in the education of his children. Kelvin himself got an early start, effectively going to college at the age of 10, and becoming a professor in Glasgow at the age of 22—a position in which he continued for 53 years. Kelvin’s breakout work, done in his twenties, was on thermodynamics. But over the years he also worked on many other areas of physics, and beyond, mixing theory, experiment and engineering. Beginning in 1854 he became involved in a technical megaproject of the time: the attempt to lay a transatlantic telegraph cable. He wound up very much on the front lines, helping out as a just-in-time physicist + engineer on the cable-laying ship. The first few attempts didn’t work out, but finally in 1866—in no small part through Kelvin’s contributions—a cable was successfully laid, and Kelvin (or William Thomson, as he then was) became something of a celebrity. He was made “Sir William Thomson” and—along with two other techies—formed his first company, which had considerable success in exploiting telegraph-cable-related engineering innovations. Kelvin’s first wife died after a long illness in 1870, and Kelvin, with no children and already enthusiastic about the sea, bought a fairly large yacht, and pursued a number of nautical-related projects. One of these—begun in 1872—was the construction of an analog computer for calculating tides (basically with 10 gears for adding up 10 harmonic tide components), a device that, with progressive refinements, continued to be used for close to a century. Being rather charmed by Kelvin’s physicist-with-a-big-yacht persona, I once purchased a letter that Kelvin wrote in 1877 on the letterhead of “Yacht Lalla Rookh”: The letter—in true academic style—promises that Kelvin will soon send an article he’s been asked to write on elasticity theory. And in fact he did write the article, and it was an expository one that appeared in the 9th edition of the Encyclopedia Britannica. Kelvin was a prolific (if, to modern ears, sometimes rather pompous) writer, who took exposition seriously. And indeed—finding the textbooks available to him as a professor inadequate—he worked over the course of a dozen years (1855–1867) with his (and Maxwell’s) friend Peter Guthrie Tait to produce the influential Treatise on Natural Philosophy. Kelvin explored many topics and theories, some more immediately successful than others. In the 1870s he suggested that perhaps atoms might be knotted vortices in the (luminiferous) aether (causing Tait to begin developing knot theory)—a hypothesis that’s in some sense a Victorian prelude to modern ideas about particles in our Physics Project. Throughout his life, Kelvin was a devout Christian, writing that “The more thoroughly I conduct scientific research, the more I believe science excludes atheism.” And indeed this belief seems to make an appearance in his implication that humans—presumably as a result of their special relationship with God—might avoid the Second Law. But more significant at the time was Kelvin’s skepticism about Charles Darwin’s 1859 theory of natural selection, believing that there must in the end be a “continually guiding and controlling intelligence”. Despite being somewhat ridiculed for it, Kelvin talked about the possibility that life might have come to Earth from elsewhere via meteorites, believing that his estimates of the age of the Earth (which didn’t take into account radioactivity) made it too young for the things Darwin described to have occurred. By the 1870s, Kelvin had become a distinguished man of science, receiving all sorts of honors, assignments and invitations. And in 1876, for example, he was invited to Philadelphia to chair the committee judging electrical inventions at the US Centennial International Exhibition, notably reporting, in the terms of the time: Then in 1892 a “peerage of the realm” was conferred on him by Queen Victoria. His wife (he had remarried) and various friends (including Charles Darwin’s son George) suggested he pick the title “Kelvin”, after the River Kelvin that flowed by the university in Glasgow. And by the end of his life “Lord Kelvin” had accumulated enough honorifics that they were just summarized with “…” (the MD was an honorary degree conferred by the University of Heidelberg because “it was the only one at their disposal which he did not already possess”): And when Kelvin died in 1907 he was given a state funeral and buried in Westminster Abbey near Newton and Darwin.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.16 James Clerk Maxwell (1831–1879) : James Clerk Maxwell lived only 48 years but in that time managed to do a remarkable amount of important science. His early years were spent on a 1500-acre family estate (inherited by his father) in a fairly remote part of Scotland—to which he would return later. He was an only child and was homeschooled—initially by his mother, until she died, when he was 8. At 10 he went to an upscale school in Edinburgh, and by the age of 14 had written his first scientific paper. At 16 he went as an undergraduate to the University of Edinburgh, then, effectively as a graduate student, to Cambridge—coming second in the final exams (“Second Wrangler”) to a certain Edward Routh, who would spend most of his life coaching other students on those very same exams. Within a couple of years, Maxwell was a professor, first in Aberdeen, then in London. In Aberdeen he married the daughter of the university president, who would soon be his “Observer K” (for “Katherine”) in his classic work on color vision. But after nine fairly strenuous years as a professor, Maxwell in 1865 “retired” to his family estate, supervising a house renovation, and in “rural solitude” (recreationally riding around his estate on horseback with his wife) having the most scientifically productive time of his life. In addition to his work on things like the kinetic theory of gases, he also wrote his 2-volume Treatise on Electricity and Magnetism, which ultimately took 7 years to finish, and which, with considerable clarity, described his approach to electromagnetism and what are now called “Maxwell’s Equations”. Occasionally, there were hints of his “country life”—like his 1870 “On Hills and Dales” that in his characteristic mathematicize-everything way gave a kind of “pre-topological” analysis of contour maps (perhaps conceived as he walked half a mile every day down to the mailbox at which journals and correspondence would arrive): As a person, Maxwell was calm, reserved and unassuming, yet cheerful and charming—and given to writing (arguably sometimes sophomoric) poetry: With a certain sense of the absurd, he would occasionally publish satirical pieces in Nature, signing them dp/dt, which in the thermodynamic notation created by his friend Tait was equal to JCM, which were his initials. Maxwell liked games and tricks, and spinning tops featured prominently in some of his work. He enjoyed children, though never had any of his own. As a lecturer, he prepared diligently, but often got too sophisticated for his audience. In writing, though, he showed both great clarity and great erudition, for example freely quoting Latin and Greek in articles he wrote for the 9th edition of the Encyclopedia Britannica (of which he was scientific co-editor) on topics such as “Atom” and “Ether”. As we mentioned above, Maxwell was quite an enthusiast of diagrams and visual presentation (even writing an article on “Diagrams” for the Encyclopedia Britannica). He was also a capable experimentalist, making many measurements (sometimes along with his wife), and in 1861 creating the first color photograph. In 1871 William Cavendish, 7th Duke of Devonshire, who had studied math in Cambridge, and was now chancellor of the university, agreed to put up the money to build what became the Cavendish Laboratory and to endow a new chair of experimental physics. Kelvin having turned down the job, it was offered to the still-rather-obscure Maxwell, who somewhat reluctantly accepted—with the result that for several years he spent much of his time supervising the design and building of the lab. The lab was finished in 1874, but then William Cavendish dropped on Maxwell a large collection of papers from his great uncle Henry Cavendish, who had been a wealthy “gentleman scientist” of the late 1700s and (among other things) the discoverer of hydrogen. Maxwell liked history (as some of us do!), noticed that Cavendish had discovered Ohm’s law 50 years before Ohm, and in the end spent several years painstakingly editing and annotating the papers into a 500-page book. By 1879 Maxwell was finally ready to energetically concentrate on physics research again, but, sadly, in the fall of that year his health failed, and he died at the age of 48—having succumbed to stomach cancer, as his mother also had at almost the same age.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.17 J. Willard Gibbs (1839–1903) : Gibbs was born near the Yale campus, and died there 64 years later, in the same house where he had lived since he was 7 years old (save for three years spent visiting European universities as a young man, and regular summer “out-in-nature” vacations). His father (who, like, “our Gibbs” was named “Josiah Willard”—making “our Gibbs” be called “Willard”) came from an old and distinguished intellectual and religious New England family, and was a professor of sacred languages at Yale. Willard Gibbs went to college and graduate school at Yale, and then spent his whole career as a professor at Yale. He was, it seems, a quiet, modest and rather distant person, who radiated a certain serenity, regularly attended church, had a small circle of friends and lived with his two sisters (and the husband and children of one of them). He diligently discharged his teaching responsibilities, though his lectures were very sparsely attended, and he seems not to have been thought forceful enough in dealing with people to have been called on for many administrative tasks—though he became the treasurer of his former high school, and himself was careful enough with money that by the end of his life he had accumulated what would now be several million dollars. He had begun his academic career in practical engineering, for example patenting an “improved [railway] car-brake”, but was soon drawn in more mathematical directions, favoring a certain clarity and minimalism of formulation, and a cleanliness, if not brevity, of exposition. His work on thermodynamics (initially published in the rather obscure Transactions of the Connecticut Academy) was divided into two parts: the first, in the 1870s, concentrating on macroscopic equilibrium properties, and second, in the 1890s, concentrating on microscopic “statistical mechanics” (as Gibbs called it). Even before he started on thermodynamics, he’d been interested in electromagnetism, and between his two “thermodynamic periods”, he again worked on electromagnetism. He studied Maxwell’s work, and was at first drawn to the then-popular formalism of quaternions—but soon decided to invent his own approach and notation for vector analysis, which at first he presented only in notes for his students, though it later became widely adopted. And while Gibbs did increasingly mathematical work, he never seems to have identified as a mathematician, modestly stating that “If I have had any success in mathematical physics, it is, I think, because I have been able to dodge mathematical difficulties.” His last work was his book on statistical mechanics, which—with considerable effort and perhaps damage to his health—he finished in time for publication in connection with the Yale bicentennial in 1901 (an event which notably also brought a visit from Kelvin), only to die soon thereafter. Gibbs had a few graduate students at Yale, a notable one being Lee de Forest, inventor of the vacuum tube (triode) electronic amplifier, and radio entrepreneur. (de Forest’s 1899 PhD thesis was entitled “Reflection of Hertzian Waves from the Ends of Parallel Wires”.) Another student of Gibbs was Lynde Wheeler, who became a government radio scientist, and who wrote a biography of Gibbs, of which I have a copy bought years ago at a used bookstore—that I was now just about to put back on a shelf when I opened its front cover and found an inscription: And, yes, it’s a small world, and “To Willard” refers to Gibbs’s sister’s son (Willard Gibbs Van Name, who became a naturalist and wrote a 1929 book about national park deforestation).
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.18 Ludwig Boltzmann (1844–1906) : Of the people we’re discussing, Boltzmann is the one whose career was most focused on the Second Law. Boltzmann grew up in Austria, where his father was a civil servant (who died when Boltzmann was 15) and his mother was something of an heiress. Boltzmann did his PhD at the University of Vienna, where his professor notably gave him a copy of some of Maxwell’s papers, together with an English grammar book. Boltzmann started publishing his own papers near the end of his PhD, and soon landed a position as a professor of mathematical physics in Graz. Four years later he moved to Vienna as a professor of mathematics, soon moving back to Graz as a professor of “general and experimental physics”—a position he would keep for 14 years. He’d married in 1876, and had 5 children, though a son died in 1889, leaving 3 daughters and another son. Boltzmann was apparently a clear and lively lecturer, as well as a spirited and eager debater. He seems, at least in his younger years, to have been a happy and gregarious person, with a strong taste for music—and some charming do-it-your-own-way tendencies. For example, wanting to provide fresh milk for his children, he decided to just buy a cow, which he then led from the market through the streets—though had to consult his colleague, the professor of zoology, to find out how to milk it. Boltzmann was a capable experimental physicist, as well as a creator of gadgets, and a technology enthusiast—promoting the idea of airplanes (an application for gas theory!) and noting their potential power as a means of transportation. Boltzmann had always had mood swings, but by the early 1890s he claimed they were getting worse. It didn’t help that he was worn down by administrative work, and had worsening asthma and increasing nearsightedness (that he’d thought might be a sign of going blind). He moved positions, but then came back to Vienna, where he embarked on writing what would become a 2-volume book on Gas Theory—in effect contextualizing his life’s work. The introduction to the first volume laments that “gas theory has gone out of fashion in Germany”. The introduction to the second volume, written in 1898 when Boltzmann was 54, then says that “attacks on the theory of gases have begun to increase”, and continues: … it would be a great tragedy for science if the theory of gases were temporarily thrown into oblivion because of a momentary hostile attitude toward it, as, for example, was the wave theory [of light] because of Newton’s authority. I am conscious of being only an individual struggling weakly against the stream of time. But it still remains in my power to contribute in such a way that, when the theory of gases is again revived, not too much will have to be rediscovered. But even as he was writing this, Boltzmann had pretty much already wound down his physics research, and had basically switched to exposition, and to philosophy. He moved jobs again, but in 1902 again came back to Vienna, but now also as a professor of philosophy. He gave an inaugural lecture, first quoting his predecessor Ernst Mach (1838–1916) as saying “I do not believe that atoms exist”, then discussing the philosophical relations between reality, perception and models. Elsewhere he discussed things like his view of the different philosophical character of models associated with differential equations and with atomism—and he even wrote an article on the general topic of “Models” for Encyclopedia Britannica (which curiously also talks about “in pure mathematics, especially geometry, models constructed of papier-mâché and plaster”). Sometimes Boltzmann’s philosophy could be quite polemical, like his attack on Schopenhauer, that ends by saying that “men [should] be freed from the spiritual migraine that is called metaphysics”. Then, in 1904, Boltzmann addressed the Vienna Philosophical Society (a kind of predecessor of the Vienna Circle) on the subject of a “Reply to a Lecture on Happiness by Professor Ostwald”. Wilhelm Ostwald (1853–1932) (a chemist and social reformer, who was a personal friend of Boltzmann’s, but intellectual adversary) had proposed the concept of “energy of will” to apply mathematical physics ideas to psychology. Boltzmann mocked this, describing its faux formalism as “dangerous for science”. Meanwhile, Boltzmann gives his own Darwinian theory for the origin of happiness, based essentially on the idea that unhappiness is needed as a way to make organisms improve their circumstances in the struggle for survival. Boltzmann himself was continuing to have problems that he attributed to then-popular but very vague “diagnosis” of “neurasthenia”, and had even briefly been in a psychiatric hospital. But he continued to do things like travel. He visited the US three times, in 1905 going to California (mainly Berkeley)—which led him to write a witty piece entitled “A German Professor’s Trip to El Dorado” that concluded: Yes, America will achieve great things. I believe in these people, even after seeing them at work in a setting where they’re not at their best: integrating and differentiating at a theoretical physics seminar… In 1905 Einstein published his Boltzmann-and-atomism-based results on Brownian motion and on photons. But it’s not clear Boltzmann ever knew about them. For Boltzmann was sinking further. Perhaps he’d overexerted himself in California, but by the spring of 1906 he said he was no longer able to teach. In the summer he went with his family to an Italian seaside resort in an attempt to rejuvenate. But a day before they were to return to Vienna he failed to join his family for a swim, and his youngest daughter found him hanged in his hotel room, dead at the age of 62.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.19 Coarse-Graining and the “Modern Formulation” : After Gibbs’s 1902 book introducing the idea of ensembles, most of the language used (at least until now!) to discuss the Second Law was basically in place. But in 1912 one additional term—representing a concept already implicit in Gibbs’s work—was added: coarse-graining. Gibbs had discussed how the phase fluid representing possible states of a system could be elaborately mixed by the mechanical time evolution of the system. But realistic practical measurements could not be expected to probe all the details of the distribution of phase fluid; instead one could say that they would only sample “coarse-grained” aspects of it. The term “coarse-graining” first appeared in a survey article entitled “The Conceptual Foundations of the Statistical Approach in Mechanics”, written for the German-language Encyclopaedia of the Mathematical Sciences by Boltzmann’s former student Paul Ehrenfest, and his wife Tatiana Ehrenfest-Afanassjewa: The article also introduced all sorts of now-standard notation, and in many ways can be read as a final summary of what was achieved in the original development around the foundations of thermodynamics and the Second Law. (And indeed the article was sufficiently “final” that when it was republished as a book in 1959 it could still be presented as usefully summarizing the state of things.) Looking at the article now, though, it’s notable how much it recognized was not at all settled about the Second Law and its foundations. It places Boltzmann squarely at the center, stating in its preface: The section titles are already revealing: And soon they’re starting to talk about “loose ends”, and lots of them. Ergodicity is something one can talk about, but there’s no known example (and with this definition it was later proved that there couldn’t be): But, they point out, it’s something Boltzmann needed in order to justify his results: Soon they’re talking about Boltzmann’s sloppiness in his discussion of the H curve: And then they’re on to talking about Gibbs, and the gaps in his reasoning: In the end they conclude: In other words, even though people now seem to be buying all these results, there are still plenty of issues with their foundations. And despite people’s implicit assumptions, we can in no way say that the Second Law has been “proved”.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.20 Radiant Heat, the Second Law and Quantum Mechanics : It was already realized in the 1600s that when objects get hot they emit “heat radiation”—which can be transferred to other bodies as “radiant heat”. And particularly following Maxwell’s work in the 1860s on electrodynamics it came to be accepted that radiant heat was associated with electromagnetic waves propagating in the “luminiferous aether”. But unlike the molecules from which it was increasingly assumed that one could think of matter as being made, these electromagnetic waves were always treated—particularly on the basis of their mathematical foundations in calculus—as fundamentally continuous. But how might this relate to the Second Law? Could it be, perhaps, that the Second Law should ultimately be attributed not to some property of the large-scale mechanics of discrete molecules, but rather to a feature of continuous radiant heat? The basic equations assumed for mechanics—originally due to Newton—are reversible. But what about the equations for electrodynamics? Maxwell’s equations are in and of themselves also reversible. But when one thinks about their solutions for actual electromagnetic radiation, there can be fundamental irreversibility. And the reason is that it’s natural to describe the emission of radiation (say from a hot body), but then to assume that, once emitted, the radiation just “escapes to infinity”—rather than ever reversing the process of emission by being absorbed by some other body. All the various people we’ve discussed above, from Clausius to Gibbs, made occasional remarks about the possibility that the Second Law—whether or not it could be “derived mechanically”—would still ultimately work, if nothing else, because of the irreversible emission of radiant heat. But the person who would ultimately be most intimately connected to these issues was Max Planck—though in the end the somewhat-confused connection to the Second Law would recede in importance relative to what emerged from it, which was basically the raw material that led to quantum theory. As a student of Helmholtz’s in Berlin, Max Planck got interested in thermodynamics, and in 1879 wrote a 61-page PhD thesis entitled “On the Second Law of Mechanical Heat Theory”. It was a traditional (if slightly streamlined) discussion of the Second Law, very much based on Clausius’s approach (and even with the same title as Clausius’s 1867 paper)—and without any mention whatsoever of Boltzmann: For most of the two decades that followed, Planck continued to use similar methods to study the Second Law in various settings (e.g. elastic materials, chemical mixtures, etc.)—and meanwhile ascended the German academic physics hierarchy, ending up as a professor of theoretical physics in Berlin. Planck was in many ways a physics traditionalist, not wanting to commit to things like “newfangled” molecular ideas—and as late as 1897 (with his assistant Zermelo having made his “recurrence objection” to Boltzmann’s work) still saying that he would “abstain completely from any definite assumption about the nature of heat”. But regardless of its foundations, Planck was a true believer in the Second Law, for example in 1891 asserting that it “must extend to all forces of nature … not only thermal and chemical, but also electrical and other”. And in 1895 he began to investigate how the Second Law applied to electrodynamics—and in particular to the “heat radiation” that it had become clear (particularly through Heinrich Hertz’s (1857–1894) experiments) was of electromagnetic origin. In 1896 Wilhelm Wien (1864–1928) suggested that the heat radiation (or what we now call blackbody radiation) was in effect produced by tiny Hertzian oscillators with velocities following a Maxwell distribution. Planck, however, had a different viewpoint, instead introducing the concept of “natural radiation”—a kind of intrinsic thermal equilibrium state for radiation, with an associated intrinsic entropy. He imagined “resonators” interacting through Maxwell’s equations with this radiation, and in 1899 invented a (rather arbitrary) formula for the entropy of these resonators, that implied (through the laws of electrodynamics) that overall entropy would increase—just like the Second Law said—and when the entropy was maximized it gave the same result as Wien for the spectrum of blackbody radiation. In early 1900 he sharpened his treatment and began to suggest that with his approach Wien’s form of the blackbody spectrum would emerge as a provable consequence of the universal validity of the Second Law. But right around that time experimental results arrived that disagreed with Wien’s law. And by the end of 1900 Planck had a new hypothesis, for which he finally began to rely on ideas from Boltzmann. Planck started from the idea that he should treat the behavior of his resonators statistically. But how then could he compute their entropy? He quotes (for the first time ever) his simplification of Boltzmann’s formula for entropy: As he explains it—claiming now, after years of criticizing Boltzmann, that this is a “theorem”: We now set the entropy S of the system proportional to the logarithm of its probability W… In my opinion this actually serves as a definition of the probability W, since in the basic assumptions of electromagnetic theory there is no definite evidence for such a probability. The suitability of this expression is evident from the outset, in view of its simplicity and close connection with a theorem from kinetic gas theory. But how could he figure out the probability for a resonator to have a certain energy, and thus a certain entropy? For this he turns directly to Boltzmann—who, as a matter of convenience in his 1877 paper had introduced discrete values of energy for molecules. Planck simply states that it’s “necessary” (i.e. to get the experimentally right answer) to treat the resonator energy “not as a continuous, infinitely divisible quantity, but as a discrete quantity composed of an integral number of finite equal parts”. As an example of how this works he gives a table just like the one in Boltzmann’s paper from nearly a quarter of a century earlier: Pretty soon he’s deriving the entropy of a resonator as a function of its energy, and its discrete energy unit ϵ: Connecting this to blackbody radiation he claims that each resonator’s energy unit is connected to its frequency according to so that its entropy is “[where] h and k are universal constants”. In a similar situation Boltzmann had effectively taken the limit ϵ→0, because that’s what he believed corresponded to (“calculus-based”) physical reality. But Planck—in what he later described as an “act of desperation” to fit the experimental data—didn’t do that. So in computing things like average energies he’s evaluating Sum[x Exp[-a x], {x, 0, ∞}] rather than Integrate[x Exp [-a x], {x, 0, Infinity}]. And in doing this it takes him only a few lines to derive what’s now called the Planck spectrum for blackbody radiation (i.e. for “radiation in equilibrium”): And then by fitting this result to the data of the time he gets “Planck’s constant” (the correct result is 6.62): And, yes, this was essentially the birth of quantum mechanics—essentially as a spinoff from an attempt to extend the domain of the Second Law. Planck himself didn’t seem to internalize what he’d done for at least another decade. And it was really Albert Einstein’s 1905 analysis of the photoelectric effect that made the concept of the quantization of energy that Planck had assumed (more as a calculational hypothesis than anything else) seem to be something of real physical significance—that would lead to the whole development of quantum mechanics, notably in the 1920s. 
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.21 Are Molecules Real? Continuous Versus Discrete : As we discussed at the very beginning above, already in antiquity there was a notion that at least things like solids and liquids might not ultimately be continuous (as they seemed), but might instead be made of large numbers of discrete “atomic” elements. By the 1600s there was also the idea that light might be “corpuscular”—and, as we discussed above, gases too. But meanwhile, there were opposing theories that espoused continuity—like the caloric theory of heat. And particularly with the success of calculus, there was a strong tendency to develop theories that showed continuity—and to which calculus could be applied. But in the early 1800s—notably with the work of John Dalton (1766–1844)—there began to be evidence that there were discrete entities participating in chemical reactions. Meanwhile, as we discussed above, the success of the kinetic theory of gases gave increasing evidence for some kind of—at least effectively—discrete elements in gases. But even people like Boltzmann and Maxwell were reluctant to assert that gases really were made of molecules. And there were plenty of well-known scientists (like Ernst Mach) who “opposed atomism”, often effectively on the grounds that in science one should only talk about things one can actually see or experience—not things like atoms that were too small for that. But there was something else too: with Newton’s theory of gravitation as a precursor, and then with the investigation of electromagnetic phenomena, there emerged in the 1800s the idea of a “continuous field”. The interpretation of this was fairly clear for something like an elastic solid or a fluid that exhibited continuous deformations. Mathematically, things like gravity, magnetism—and heat—seemed to work in similar ways. And it was assumed that this meant that in all cases there had to be some fluid-like “carrier” for the field. And this is what led to ideas like the luminiferous aether as the “carrier” of electromagnetic waves. And, by the way, the idea of an aether wasn’t even obviously incompatible with the idea of atoms; Kelvin, for example, had a theory that atoms were vortices (perhaps knotted) in the aether. But how does this all relate to the Second Law? Well, particularly through the work of Boltzmann there came to be the impression that given atomism, probability theory could essentially “prove” the Second Law. A few people tried to clarify the formal details (as we discussed above), but it seemed like any final conclusion would have to await the validation (or not) of atomism, which in the late 1800s was still a thoroughly controversial theory. By the first decade of the 1900s, however, the fortunes of atomism began to change. In 1897 J. J. Thomson (1856–1940) discovered the electron, showing that electricity was fundamentally “corpuscular”. And in 1900 Planck had (at least calculationally) introduced discrete quanta of energy. But it was the three classic papers of Albert Einstein in 1905 that—in their different ways—began to secure the ultimate success of atomism. First there was his paper “On a Heuristic View about the Production and Transformation of Light”, which began: Maxwell’s theory of electromagnetic [radiation] differs in a profound, essential way from the current theoretical models of gases and other matter. We consider the state of a material body to be completely determined by the positions and velocities of a finite number of atoms and electrons, albeit a very large number. But the electromagnetic state of a region of space is described by continuous functions … He then points out that optical experiments look only at time-averaged electromagnetic fields, and continues: In particular, blackbody radiation, photoluminescence, [the photoelectric effect] and other phenomena associated with the generation and transformation of light seem better modeled by assuming that the energy of light is distributed discontinuously in space. According to this picture, the energy of a light wave emitted from a point source is not spread continuously over ever larger volumes, but consists of a finite number of energy quanta that are spatially localized at points of space, move without dividing and are absorbed or generated only as a whole. In other words, he’s suggesting that light is “corpuscular”, and that energy is quantized. When he begins to get into details, he’s soon talking about the “entropy of radiation”—and, then, in three core sections of his paper, he’s basing what he’s doing on “Boltzmann’s principle”: Two months later, Einstein produced another paper: “Investigations on the Theory of Brownian Motion”. Back in 1827 the British botanist Robert Brown (1773–1858) had seen under a microscope tiny grains (ejected by pollen) randomly jiggling around in water. Einstein began his paper: In this paper it will be shown that according to the molecular-kinetic theory of heat, bodies of microscopically visible size suspended in a liquid will perform movements of such magnitude that they can be easily observed in a microscope, on account of the molecular motions of heat. He doesn’t explicitly mention Boltzmann in this paper, but there’s Boltzmann’s formula again: And by the next year it’s become clear experimentally that, yes, the jiggling Robert Brown had seen was in fact the result of impacts from discrete, real water molecules. Einstein’s third 1905 paper, “On the Electrodynamics of Moving Bodies”—in which he introduced relativity theory—wasn’t so obviously related to atomism. But in showing that the luminiferous aether will (as Einstein put it) “prove superfluous” he was removing what was (almost!) the last remaining example of something continuous in physics. In the years after 1905, the evidence for atomism mounted rapidly, segueing in the 1920s into the development of quantum mechanics. But what happened with the Second Law? By the time atomism was generally accepted, the generation of physicists that had included Boltzmann and Gibbs was gone. And while the Second Law was routinely invoked in expositions of thermodynamics, questions about its foundations were largely forgotten. Except perhaps for one thing: people remembered that “proofs” of the Second Law had been controversial, and had depended on the controversial hypothesis of atomism. But—they appear to have reasoned—now that atomism isn’t controversial anymore, it follows that the Second Law is indeed “satisfactorily proved”. And, after all, there were all sorts of other things to investigate in physics. There are a couple of “footnotes” to this story. The first has to do with Einstein. Right before Einstein’s remarkable series of papers in 1905, what was he working on? The answer is: the Second Law! In 1902 he wrote a paper entitled “Kinetic Theory of Thermal Equilibrium and of the Second Law of Thermodynamics”. Then in 1903: “A Theory of the Foundations of Thermodynamics”. And in 1904: “On the General Molecular Theory of Heat”. The latter paper claims: I derive an expression for the entropy of a system, which is completely analogous to the one found by Boltzmann for ideal gases and assumed by Planck in his theory of radiation. Then I give a simple derivation of the Second Law. But what’s actually there is not quite what’s advertised: It’s a short argument—about interactions between a collection of heat reservoirs. But in a sense it already assumes its answer, and certainly doesn’t provide any kind of fundamental “derivation of the Second Law”. And this was the last time Einstein ever explicitly wrote about deriving the Second Law. Yes, in those days it was just too hard, even for Einstein. There’s another footnote to this story too. As we said, at the beginning of the twentieth century it had become clear that lots of things that had been thought to be continuous were in fact discrete. But there was an important exception: space. Ever since Euclid (~300 BC), space had almost universally been implicitly assumed to be continuous. And, yes, when quantum mechanics was being built, people did wonder about whether space might be discrete too (and even in 1917 Einstein expressed the opinion that eventually it would turn out to be). But over time the idea of continuous space (and time) got so entrenched in the fabric of physics that when I started seriously developing the ideas that became our Physics Project based on space as a discrete network (or what—in homage to the dynamical theory of heat one might call the “dynamical theory of space”) it seemed to many people quite shocking. And looking back at the controversies of the late 1800s around atomism and its application to the Second Law it’s charming how familiar many of the arguments against atomism seem. Of course it turns out they were wrong—as they seem again to be in the case of space.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.22 The Twentieth Century : The foundations of thermodynamics were a hot topic in physics in the latter half of the nineteenth century—worked on by many of the most prominent physicists of the time. But by the early twentieth century it’d been firmly eclipsed by other areas of physics. And going forward it’d receive precious little attention—with most physicists just assuming it’d “somehow been solved”, or at least “didn’t need to be worried about”. As a practical matter, thermodynamics in its basic equilibrium form nevertheless became very widely used in engineering and in chemistry. And in physics, there was steadily increasing interest in doing statistical mechanics—typically enumerating states of systems (quantum or otherwise), weighted as they would be in idealized thermal equilibrium. In mathematics, the field of ergodic theory developed, though for the most part it concerned itself with systems (such as ordinary differential equations) involving few variables—making it relevant to the Second Law essentially only by analogy. There were a few attempts to “axiomatize” the Second Law, but mostly only at a macroscopic level, not asking about its microscopic origins. And there were also attempts to generalize the Second Law to make robust statements not just about equilibrium and the fact that it would be reached, but also about what would happen in systems driven to be in some manner away from equilibrium. The fluctuation-dissipation theorem about small perturbations from equilibrium—established in the mid-1900s, though anticipated in Einstein’s work on Brownian motion—was one example of a widely applicable result. And there were also related ideas of “minimum entropy production”—as well as “maximum entropy production”. But for large deviations from equilibrium there really weren’t convincing general results, and in practice most investigations basically used phenomenological models that didn’t have obvious connections to the foundations of thermodynamics, or derivations of the Second Law. Meanwhile, through most of the twentieth century there were progressively more elaborate mathematical analyses of Boltzmann’s equation (and the H theorem) and their relation to rigorously derivable but hard-to-manage concepts like the BBGKY hierarchy. But despite occasional claims to the contrary, such approaches ultimately never seem to have been able to make much progress on the core problem of deriving the Second Law. And then there’s the story of entropy. And in a sense this had three separate threads. The first was the notion of entropy—essentially in the original form defined by Clausius—being used to talk quantitatively about heat in equilibrium situations, usually for either engineering or chemistry. The second—that we’ll discuss a little more below—was entropy as a qualitative characterization of randomness and degradation. And the third was entropy as a general and formal way to measure the “effective number of degrees of freedom” in a system, computed from the log of the number of its achievable states. There are definitely correspondences between these different threads. But they’re in no sense “obviously equivalent”. And much of the mystery—and confusion—that developed around entropy in the twentieth century came from conflating them. Another piece of the story was information theory, which arose in the 1940s. And a core question in information theory is how long an “optimally compressed” message will be. And (with various assumptions) the average such length is given by a ∑p log p form that has essentially the same structure as Boltzmann’s expression for entropy. But even though it’s “mathematically like entropy” this has nothing immediately to do with heat—or even physics; it’s just an abstract consequence of needing log Ω bits (i.e. log Ω degrees of freedom) to specify one of Ω possibilities. (Still, the coincidence of definitions led to an “entropy branding” for various essentially information-theoretic methods, with claims sometimes being made that, for example, the thing called entropy must always be maximized “because we know that from physics”.) There’d been an initial thought in the 1940s that there’d be an “inevitable Second Law” for systems that “did computation”. The argument was that logical gates (like And and Or) take 2 bits of input (with 4 overall states 11, 10, 01, 00) but give only 1 bit of output (1 or 0), and are therefore fundamentally irreversible. But in the 1970s it became clear that it’s perfectly possible to do computation reversibly (say with 2-input, 2-output gates)—and indeed this is what’s used in the typical formalism for quantum circuits. As I’ve mentioned elsewhere, there were some computer experiments in the 1950s and beyond on model systems—like hard sphere gases and nonlinear springs—that showed some sign of Second Law behavior (though less than might have been expected). But the analysis of these systems very much concentrated on various regularities, and not on the effective randomness associated with Second Law behavior. In another direction, the 1970s saw the application of thermodynamic ideas to black holes. At first, it was basically a pure analogy. But then quantum field theory calculations suggested that black holes should produce thermal radiation as if they had a certain effective temperature. By the late 1990s there were more direct ways to “compute entropy” for black holes, by enumerating possible (quantum) configurations consistent with the overall characteristics of the black hole. But such computations in effect assume (time-invariant) equilibrium, and so can’t be expected to shed light directly on the Second Law. Talking about black holes brings up gravity. And in the course of the twentieth century there were scattered efforts to understand the effect of gravity on the Second Law. Would a self-gravitating gas achieve “equilibrium” in the usual sense? Does gravity violate the Second Law? It’s been difficult to get definitive answers. Many specific simulations of n-body gravitational systems were done, but without global conclusions for the Second Law. And there were cosmological arguments, particularly about the role of gravity in accounting for entropy in the early universe—but not so much about the actual evolution of the universe and the effect of the Second Law on it. Yet another direction has involved quantum mechanics. The standard formalism of quantum mechanics—like classical mechanics—is fundamentally reversible. But the formalism for measurement introduced in the 1930s—arguably as something of a hack—is fundamentally irreversible, and there’ve been continuing arguments about whether this could perhaps “explain the Second Law”. (I think our Physics Project finally provides more clarity about what’s going on here—but also tells us this isn’t what’s “needed” for the Second Law.) From the earliest days of the Second Law, there had always been scattered but ultimately unconvincing assertions of exceptions to the Second Law—usually based on elaborately constructed machines that were claimed to be able to achieve perpetual motion “just powered by heat”. Of course, the Second Law is a claim about large numbers of molecules, etc.—and shouldn’t be expected to apply to very small systems. But by the end of the twentieth century it was starting to be possible to make micromachines that could operate on small numbers of molecules (or electrons). And with the right control systems in place, it was argued that such machines could—at least in principle—effectively be used to set up Maxwell’s demons that would systematically violate the Second Law, albeit on a very small scale. And then there was the question of life. Early formulations of the Second Law had tended to talk about applying only to “inanimate matter”—because somehow living systems didn’t seem to follow the same process of inexorable “dissipation to heat” as inanimate, mechanical systems. And indeed, quite to the contrary, they seemed able to take disordered input (like food) and generate ordered biological structures from it. And indeed, Erwin Schrödinger (1887–1961), in his 1944 book What Is Life? talked about “negative entropy” associated with life. But he—and many others since—argue that life doesn’t really violate the Second Law because it’s not operating in a closed environment where one should expect evolution to equilibrium. Instead, it’s constantly being driven away from equilibrium, for example by “organized energy” ultimately coming from the Sun. Still, the concept of at least locally “antithermodynamic” behavior is often considered to be a potential general signature of life. But already by the early part of the 1900s, with the rise of things like biochemistry, and the decline of concepts like “life force” (which seemed a little like “caloric”), there developed a strong belief that the Second Law must at some level always apply, even to living systems. But, yes, even though the Second Law seemed to say that one can’t “unscramble an egg”, there was still the witty rejoinder: “unless you feed it to a chicken”. What about biological evolution? Well, Boltzmann had been an enthusiast of Darwin’s idea of natural selection. And—although it’s not clear he made this connection—it was pointed out many times in the twentieth century that just as in the Second Law reversible underlying dynamics generate an irreversible overall effect, so also in Darwinian evolution effectively reversible individual changes aggregate to what at least Darwin thought was an “irreversible” progression to things like the formation of higher organisms. The Second Law also found its way into the social sciences—sometimes under names like “entropy pessimism”—most often being used to justify the necessity of “Maxwell’s-demon-like” active intervention or control to prevent the collapse of economic or social systems into random or incoherent states. But despite all these applications of the Second Law, the twentieth century largely passed without significant advances in understanding the origin and foundations of the Second Law. Though even by the early 1980s I was beginning to find results—based on computational ideas—that seemed as if they might finally give a foundational understanding of what’s really happening in the Second Law, and the extent to which the Second Law can in the end be “derived” from underlying “mechanical” rules.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.23 What the Textbooks Said: The Evolution of Certainty : Ask a typical physicist today about the Second Law and they’re likely to be very sure that it’s “just true”. Maybe they’ll consider it “another law of nature” like the conservation of energy, or maybe they’ll think it is something that was “proved long ago” from basic principles of mathematics and mechanics. But as we’ve discussed here, there’s really nowhere in the history of the Second Law that should give us this degree of certainty. So where did all the certainty come from? I think in the end it’s a mixture of a kind of don’t-question-this-it-comes-from-sophisticated-science mystique about the Second Law, together with a century and a half of “increasingly certain” textbooks. So let’s talk about the textbooks. While early contributions to what we now call thermodynamics (and particularly those from continental Europe) often got published as monographs, the first “actual textbooks” of thermodynamics already started to appear in the 1860s, with three examples (curiously, all in French) being: And in these early textbooks what one repeatedly sees is that the Second Law is simply cited—without much comment—as a “principle” or “axiom” (variously attributed to Carnot, Kelvin or Clausius, and sometimes called “the Principle of Carnot”), from which theory will be developed. By the 1870s there’s a bit of confusion starting to creep in, because people are talking about the “Theorem of Carnot”. But, at least at first, by this they mean not the Second Law, but the result on the efficiency of heat engines that Carnot derived from this. Occasionally, there are questions in textbooks about the validity of the Second Law. A notable one, that we discussed above when we talked about Maxwell’s demon, shows up under the title “Limitation of the Second Law of Thermodynamics” at the end of Maxwell’s 1871 Theory of Heat. Tait’s largely historical 1877 Sketch of Thermodynamics notes that, yes, the Second Law hasn’t successfully been proved from the laws of mechanics: In 1879, Eddy’s Thermodynamics at first shows even more skepticism but soon he’s talking about how “Rankine’s theory of molecular vortices” has actually “proved the Second Law”: He goes on to give some standard “phenomenological” statements of the Second Law, but then talks about “molecular hypotheses from which Carnot’s principle has been derived”: Pretty soon there’s confusion like the section in Alexandre Gouilly’s (1842–1906) 1877 Mechanical Theory of Heat that’s entitled “Second Fundamental Theorem of Thermodynamics or the Theorem of Carnot”: More textbooks on thermodynamics follow, but the majority tend to be practical expositions (that are often incredibly similar to each other) with no particular theoretical discussion of the Second Law, its origins or validity. In 1891 there’s an “official report about the Second Law” commissioned by the British Association for the Advancement of Science (and written by a certain George Bryan (1864–1928) who would later produce a thermodynamics textbook): There’s an enumeration of approaches so far: Somewhat confusingly it talks about a “proof of the Second Law”—actually referring to an already-in-equilibrium result: There’s talk of mechanical instability leading to irreversibility: The conclusions say that, yes, the Second Law isn’t proved “yet” but imply that if only we knew more about molecules that might be enough to nail it: But back to textbooks. In 1895 Boltzmann published his Lectures on Gas Theory, which includes a final chapter about the H theorem and its relation to the Second Law. Boltzmann goes through his mathematical derivations for gases, then (rather over-optimistically) asserts that they’ll also work for solids and liquids: We have looked mainly at processes in gases and have calculated the function H for this case. Yet the laws of probability that govern atomic motion in the solid and liquid states are clearly not qualitatively different … from those for gases, so that the calculation of the function H corresponding to the entropy would not be more difficult in principle, although to be sure it would involve greater mathematical difficulties. But soon he’s discussing the more philosophical aspects of things (and by the time Boltzmann wrote this book, he was a professor of philosophy as well as physics). He says that the usual statement of the Second Law is “asserted phenomenologically as an axiom” (just as he says the infinite divisibility of matter also is at that time): … the Second Law is formulated in such a way that the unconditional irreversibility of all natural processes is asserted as an axiom, just as general physics based on a purely phenomenological standpoint asserts the unconditional divisibility of matter without limit as an axiom. One might then expect him to say that actually the Second Law is somehow provable from basic physical facts, such as the First Law. But actually his claims about any kind of “general derivation” of the Second Law are rather subdued: Since however the probability calculus has been verified in so many special cases, I see no reason why it should not also be applied to natural processes of a more general kind. The applicability of the probability calculus to the molecular motion in gases cannot of course be rigorously deduced from the differential equations for the motion of the molecules. It follows rather from the great number of the gas molecules and the length of their paths, by virtue of which the properties of the position in the gas where a molecule undergoes a collision are completely independent of the place where it collided the previous time. But he still believes in the ultimate applicability of the Second Law, and feels he needs to explain why—in the face of the Second Law—the universe as we perceive “still has interesting things going on”: … small isolated regions of the universe will always find themselves “initially” in an improbable state. This method seems to me to be the only way in which one can understand the Second Law—the heat death of each single world—without a unidirectional change of the entire universe from a definite initial state to a final state. Meanwhile, he talks about the idea that elsewhere in the universe things might be different—and that, for example, entropy might be systematically decreasing, making (he suggests) perceived time run backwards: In the entire universe, the aggregate of all individual worlds, there will however in fact occur processes going in the opposite direction. But the beings who observe such processes will simply reckon time as proceeding from the less probable to the more probable states, and it will never be discovered whether they reckon time differently from us, since they are separated from us by eons of time and spatial distances 101010 times the distance of Sirius—and moreover their language has no relation to ours. Most other textbook discussions of thermodynamics are tamer than this, but the rather anthropic-style argument that “we live in a fluctuation” comes up over and over again as an ultimate way to explain the fact that the universe as we perceive it isn’t just a featureless maximum-entropy place. It’s worth noting that there are roughly three general streams of textbooks that end up discussing the Second Law. There are books about rather practical thermodynamics (of the type pioneered by Clausius), that typically spend most of their time on the equilibrium case. There are books about kinetic theory (effectively pioneered by Maxwell), that typically spend most of their time talking about the dynamics of gas molecules. And then there are books about statistical mechanics (as pioneered by Gibbs) that discuss with various degrees of mathematical sophistication the statistical characteristics of ensembles. In each of these streams, many textbooks just treat the Second Law as a starting point that can be taken for granted, then go from there. But particularly when they are written by physicists with broader experience, or when they are intended for a not-totally-specialized audience, textbooks will quite often attempt at least a little justification or explanation for the Second Law—though rather often with a distinct sleight of hand involved. For example, when Planck in 1903 wrote his Treatise on Thermodynamics he had a chapter in his discussion of the Second Law, misleadingly entitled “Proof”. Still, he explains that: The second fundamental principle of thermodynamics [Second Law] being, like the first, an empirical law, we can speak of its proof only in so far as its total purport may be deduced from a single self-evident proposition. We, therefore, put forward the following proposition as being given directly by experience. It is impossible to construct an engine which will work in a complete cycle, and produce no effect except the raising of a weight and the cooling of a heat-reservoir. In other words, his “proof” of the Second Law is that nobody has ever managed to build a perpetual motion machine that violates it. (And, yes, this is more than a little reminiscent of P ≠ NP, which, through computational irreducibility, is related to the Second Law.) But after many pages, he says: In conclusion, we shall briefly discuss the question of the possible limitations to the Second Law. If there exist any such limitations—a view still held by many scientists and philosophers—then this [implies an error] in our starting point: the impossibility of perpetual motion … (In the 1905 edition of the book he adds a footnote that frankly seems bizarre in view of his—albeit perhaps initially unwilling—role in the initiation of quantum theory five years earlier: “The following discussion, of course, deals with the meaning of the Second Law only insofar as it can be surveyed from the points of view contained in this work avoiding all atomic hypotheses.”) He ends by basically saying “maybe one day the Second Law will be considered necessarily true; in the meantime let’s assume it and see if anything goes wrong”: Presumably the time will come when the principle of the increase of the entropy will be presented without any connection with experiment. Some metaphysicians may even put it forward as being a priori valid. In the meantime, no more effective weapon can be used by both champions and opponents of the Second Law than the indefatigable endeavour to follow the real purport of this law to the utmost consequences, taking the latter one by one to the highest court of appeal experience. Whatever the decision may be, lasting gain will accrue to us from such a proceeding, since thereby we serve the chief end of natural science the enlargement of our stock of knowledge. Planck’s book came in a sense from the Clausius tradition. James Jeans’s (1877–1946) 1904 book The Dynamical Theory of Gases came instead from the Maxwell + Boltzmann tradition. He says at the beginning—reflecting the fact the existence of molecules had not yet been firmly established in 1904—that the whole notion of the molecular basis of heat “is only a hypothesis”: Later he argues that molecular-scale processes are just too “fine-grained” to ever be directly detected: But soon Jeans is giving a derivation of Boltzmann’s H theorem, though noting some subtleties: His take on the “reversibility objection” is that, yes, the H function will be symmetric at every maximum, but, he argues, it’ll also be discontinuous there: And in the time-honored tradition of saying “it is clear” right when an argument is questionable, he then claims that an “obvious averaging” will give irreversibility and the Second Law: Later in his book Jeans simply quotes Maxwell and mentions his demon: Then effectively just tells readers to go elsewhere: In 1907 George Bryan (whose 1891 report we mentioned earlier) published Thermodynamics, an Introductory Treatise Dealing Mainly with First Principles and Their Direct Applications. But despite its title, Bryan has now “walked back” the hopes of his earlier report and is just treating the Second Law as an “axiom”: And—presumably from his interactions with Boltzmann—is saying that the Second Law is basically an empirical fact of our particular experience of the universe, and thus not something fundamentally derivable: As the years went by, many thermodynamics textbooks appeared, increasingly with an emphasis on applications, and decreasingly with a mention of foundational issues—typically treating the Second Law essentially just as an absolute empirical “law of nature” analogous to the First Law. But in other books—including some that were widely read—there were occasional mentions of the foundations of the Second Law. A notable example was in Arthur Eddington’s (1882–1944) 1929 The Nature of the Physical World—where now the Second Law is exalted as having the “supreme position among the laws of Nature”: Although Eddington does admit that the Second Law is probably not “mathematically derivable”: And even though in the twentieth century questions about thermodynamics and the Second Law weren’t considered “top physics topics”, some top physicists did end up talking about them, if nothing else in general textbooks they wrote. Thus, for example, in the 1930s and 1940s people like Enrico Fermi (1901–1954) and Wolfgang Pauli (1900–1958) wrote in some detail about the Second Law—though rather strenuously avoided discussing foundational issues about it. Lev Landau (1908–1968), however, was a different story. In 1933 he wrote a paper “On the Second Law of Thermodynamics and the Universe” which basically argues that our everyday experience is only possible because “the world as a whole does not obey the laws of thermodynamics”—and suggests that perhaps relativistic quantum mechanics (which he says, quoting Niels Bohr (1885–1962), could be crucial in the center of stars) might fundamentally violate the Second Law. (And yes, even today it’s not clear how “relativistic temperature” works.) But this kind of outright denial of the Second Law had disappeared by the time Lev Landau and Evgeny Lifshitz (1915–1985) wrote the 1951 version of their book Statistical Mechanics—though they still showed skepticism about its origins: There is no doubt that the foregoing simple formulations [of the Second Law] accord with reality; they are confirmed by all our everyday observations. But when we consider more closely the problem of the physical nature and origin of these laws of behaviour, substantial difficulties arise, which to some extent have not yet been overcome. Their book continues, discussing Boltzmann’s fluctuation argument: Firstly, if we attempt to apply statistical physics to the entire universe … we immediately encounter a glaring contradiction between theory and experiment. According to the results of statistics, the universe ought to be in a state of complete statistical equilibrium. … Everyday experience shows us, however, that the properties of Nature bear no resemblance to those of an equilibrium system; and astronomical results show that the same is true throughout the vast region of the Universe accessible to our observation. We might try to overcome this contradiction by supposing that the part of the Universe which we observe is just some huge fluctuation in a system which is in equilibrium as a whole. The fact that we have been able to observe this huge fluctuation might be explained by supposing that the existence of such a fluctuation is a necessary condition for the existence of an observer (a condition for the occurrence of biological evolution). This argument, however, is easily disproved, since a fluctuation within, say, the volume of the solar system only would be very much more probable, and would be sufficient to allow the existence of an observer. What do they think is the way out? The effect of gravity: … in the general theory of relativity, the Universe as a whole must be regarded not as a closed system but as a system in a variable gravitational field. Consequently the application of the law of increase of entropy does not prove that statistical equilibrium must necessarily exist. But they say this isn’t the end of the problem, essentially noting the reversibility objection. How should this be overcome? First, they suggest the solution might be that the observer somehow “artificially closes off the history of a system”, but then they add: Such a dependence of the laws of physics on the nature of an observer is quite inadmissible, of course. They continue: At the present time it is not certain whether the law of increase of entropy thus formulated can be derived on the basis of classical mechanics. … It is more reasonable to suppose that the law of increase of entropy in the above general formulation arises from quantum effects. They talk about the interaction of classical and quantum systems, and what amounts to the explicit irreversibility of the traditional formalism of quantum measurement, then say that if quantum mechanics is in fact the ultimate source of irreversibility: … there must exist an inequality involving the quantum constant ℏ which ensures the validity of the law and is satisfied in the real world… What about other textbooks? Joseph Mayer (1904–1983) and Maria Goeppert Mayer’s (1906–1972) 1940 Statistical Mechanics has the rather charming though in the end they sidestep difficult questions about the Second Law by basically making convenient definitions of what S and Ω mean in S = k log Ω. For a long time one of the most cited textbooks in the area was Richard Tolman’s (1881–1948) 1938 Principles of Statistical Mechanics. Tolman (basically following Gibbs) begins by explaining that statistical mechanics is about making predictions when all you know are probabilistic statements about initial conditions: Tolman continues: He notes that, historically, statistical mechanics was developed for studying systems like gases, where (in a vague foreshadowing of the concept of computational irreducibility) “it is evident that we should be quickly lost in the complexities of our computations” if we try to trace every molecule, but where, he claims, statistical mechanics can still accurately tell us “statistically” what will happen: But where exactly should we get the probability distributions for initial states from? Tolman says he’s going to consider the kinds of mathematically defined ensembles that Gibbs discusses. And tucked away at the end of a chapter he admits that, well, yes, this setup is really all just a postulate—set up so as to make the results of statistical mechanics “merely a matter for computation”: On this basis Tolman then derives Boltzmann’s H theorem, and his  “coarse-grained” generalization (where, yes, the coarse-graining ultimately operates according to his postulate). For 530 pages, there’s not a single mention of the Second Law. But finally, on page 558 Tolman is at least prepared to talk about an “analog of the Second Law”: And basically what Tolman argues is that his  can reasonably be identified with thermodynamic entropy S. In the end, the argument is very similar to Boltzmann’s, though Tolman seems to feel that it has achieved more: Very different in character from Tolman’s book, another widely cited book is Percy Bridgman’s (1882–1961) largely philosophical 1943 The Nature of Thermodynamics. His chapter on the Second Law begins: A decade earlier Bridgman had discussed outright violations of the Second Law, saying that he’d found that the younger generation of physicists at the time seemed to often think that “it may be possible some day to construct a machine which shall violate the Second Law on a scale large enough to be commercially profitable”—perhaps, he said, by harnessing Brownian motion: At a philosophical level, a notable treatment of the Second Law appeared in Hans Reichenbach’s (1891–1953) (unfinished-at-his-death) 1953 work The Direction of Time. Wanting to make use of the Second Law, but concerned about the reversibility objections, Reichenbach introduces the notion of “branch systems”—essentially parts of the universe that can eventually be considered isolated, but which were once connected to other parts that were responsible for determining their (“nonrandom”) effective initial conditions: Most textbooks that cover the Second Law use one of the formulations that we’ve already discussed. But there is one more formulation that also sometimes appears, usually associated with the name “Carathéodory” or the term “axiomatic thermodynamics”. Back in the first decade of the twentieth century—particularly in the circle around David Hilbert (1862–1943)—there was a lot of enthusiasm for axiomatizing things, including physics. And in 1908 the mathematician Constantin Carathéodory (1873–1950) suggested an axiomatization of thermodynamics. His essential idea—that he developed further in the 1920s—was to consider something like Gibbs’s phase fluid and then roughly to assert that it gets (in some measure-theoretic sense) “so mixed up” that there aren’t “experimentally doable” transformations that can unmix it. Or, in his original formulation: In any arbitrary neighborhood of an arbitrarily given initial point there is a state that cannot be arbitrarily approximated by adiabatic changes of state. There wasn’t much pickup of this approach—though Max Born (1882–1970) supported it, Max Planck dismissed it, and in 1939 S. Chandrasekhar (1910–1995) based his exposition of stellar structure on it. But in various forms, the approach did make it into a few textbooks. An example is Brian Pippard’s (1920–2008) otherwise rather practical 1957 The Elements of Classical Thermodynamics: Yet another (loosely related) approach is the “postulatory formulation” on which Herbert Callen’s (1919–1993) 1959 textbook Thermodynamics is based: In effect this is now “assuming the result” of the Second Law: Though in an appendix he rather tautologically states: So what about other textbooks? A famous set are Richard Feynman’s (1918–1988) 1963 Lectures on Physics. Feynman starts his discussion of the Second Law quite carefully, describing it as a “hypothesis”: Feynman says he’s not going to go very far into thermodynamics, though quotes (and criticizes) Clausius’s statements: But then he launches into a whole chapter on “Ratchet and pawl”: His goal, he explains, is to analyze a device (similar to what Marian Smoluchowski had considered in 1912) that one might think by its one-way ratchet action would be able to “harvest random heat” and violate the Second Law. But after a few pages of analysis he claims that, no, if the system is in equilibrium, thermal fluctuations will prevent systematic “one-way” mechanical work from being achieved, so that the Second Law is saved. But now he applies this to Maxwell’s demon, claiming that the same basic argument shows that the demon can’t work: But what about reversibility? Feynman first discusses what amounts to Boltzmann’s fluctuation idea: But then he opts instead for the argument that for some reason—then unknown—the universe started in a “low-entropy” state, and has been “running down” ever since: By the beginning of the 1960s an immense number of books had appeared that discussed the Second Law. Some were based on macroscopic thermodynamics, some on kinetic theory and some on statistical mechanics. In all three of these cases there was elegant mathematical theory to be described, even if it never really addressed the ultimate origin of the Second Law. But by the early 1960s there was something new on the scene: computer simulation. And in 1965 that formed the core of Fred Reif’s (1927–2019) textbook Statistical Physics: In a sense the book is an exploration of what simulated hard sphere gases do—as analyzed using ideas from statistical mechanics. (The simulations had computational limitations, but they could go far enough to meaningfully see most of the basic phenomena of statistical mechanics.) Even the front and back covers of the book provide a bold statement of both reversibility and the kind of randomization that’s at the heart of the Second Law: But inside the book the formal concept of entropy doesn’t appear until page 147—where it’s defined very concretely in terms of states one can explicitly enumerate: And finally, on page 283—after all necessary definitions have been built up—there’s a rather prosaic statement of the Second Law, almost as a technical footnote: Looking though many textbooks of thermodynamics and statistical mechanics it’s striking how singular Reif’s “show-don’t-tell” computer-simulation approach is. And, as I describe in detail elsewhere, for me personally it has a particular significance, because this is the book that in 1972, at the age of 12, launched me on what has now been a 50-year journey to understand the Second Law and its origins. When the first textbooks that described the Second Law were published nearly a century and a half ago they often (though even then not always) expressed uncertainty about the Second Law and just how it was supposed to work. But it wasn’t long before the vast majority of books either just “assumed the Second Law” and got on with whatever they wanted to apply it to, or tried to suggest that the Second Law had been established from underlying principles, but that it was a sophisticated story that was “out of the scope of this book” but to be found elsewhere. And so it was that a strong sense emerged that the Second Law was something whose ultimate character and origins the typical working scientist didn’t need to question—and should just believe (and protect) as part of the standard canon of science.
How Did We Get Here? The Tangled History of the Second Law of Thermodynamics 35.24 So Where Does This Leave the Second Law? : The Second Law is now more than 150 years old. But—at least until now—I think it’s fair to say that the fundamental ideas used to discuss it haven’t materially changed in more than a century. There’s a lot that’s been written about the Second Law. But it’s always tended to follow lines of development already defined over a century ago—and mostly those from Clausius, or Boltzmann, or Gibbs. Looking at word clouds of titles of the thousands of publications about the Second Law over the decades we see just a few trends, like the appearance of the “generalized Second Law” in the 1990s relating to black holes: But with all this activity why hasn’t more been worked out about the Second Law? How come after all this time we still don’t really even understand with clarity the correspondence between the Clausius, Boltzmann and Gibbs approaches—or how their respective definitions of “entropy” are ultimately related? In the end, I think the answer is that it needs a new paradigm—that, yes, is fundamentally based on computation and on ideas like computational irreducibility. A little more than a century ago—with people still actively arguing about what Boltzmann was saying—I don’t think anyone would have been too surprised to find out that to make progress would need a new way of looking at things. (After all, just a few years earlier Boltzmann and Gibbs had needed to bring in the new idea of using probability theory.) But as we discussed, by the beginning of the twentieth century—with other areas of physics heating up—interest in the Second Law was waning. And even with many questions unresolved people moved on. And soon several academic generations had passed. And as is typical in the history of science, by that point nobody was questioning the foundations anymore. In the particular case of the Second Law there was some sense that the uncertainties had to do with the assumption of the existence of molecules, which had by then been established. But more important, I think, was just the passage of “academic time” and the fact that what might once have been a matter of discussion had now just become a statement in the textbooks—that future academic generations should learn and didn’t need to question. One of the unusual features of the Second Law is that at the time it passed into the “standard canon of science” it was still rife with controversy. How did those different approaches relate? What about those “mathematical objections”? What about the thought experiments that seemed to suggest exceptions? It wasn’t that these issues were resolved. It was just that after enough time had passed people came to assume that “somehow that must have all been worked out ages ago”. And it wasn’t that there was really any pressure to investigate foundational issues. The Second Law—particularly in its implications for thermal equilibrium—seemed to work just fine in all its standard applications. And it even seemed to work in new domains like black holes. Yes, there was always a desire to extend it. But the difficulties encountered in trying to do so didn’t seem in any obvious way related to issues about its foundations. Of course, there were always a few people who kept wondering about the Second Law. And indeed I’ve been surprised at how much of a Who’s Who of twentieth-century physics this seems to have included. But while many well-known physicists seem to have privately thought about the foundations of the Second Law they managed to make remarkably little progress—and as a result left very few visible records of their efforts. But—as is so often the case—the issue, I believe, is that a fundamentally new paradigm was needed in order to make real progress. When the “standard canon” of the Second Law was formed in the latter part of the nineteenth century, calculus was the primary tool for physics—with probability theory a newfangled addition introduced specifically for studying the Second Law. And from that time it would be many decades before even the beginnings of the computational paradigm began to emerge, and nearly a century before phenomena like computational irreducibility were finally discovered. Had the sequence been different I have no doubt that what I have now been able to understand about the Second Law would have been worked out by the likes of Boltzmann, Maxwell and Kelvin. But as it is, we’ve had to wait more than a century to get to this point. And having now studied the history of the Second Law—and seen the tangled manner in which it developed—I believe that we can now be confident that we have indeed successfully been able to resolve many of the core issues and mysteries that have plagued the Second Law and its foundations over the course of nearly 150 years. Almost all of what I say here is based on my reading of primary literature, assisted by modern tools and by my latest understanding of the Second Law. About some of what I discuss, there is—sometimes quite extensive—existing scholarship; some references are given in the bibliography.
A Class of Models with the Potential to Represent Fundamental Physics. BY STEPHEN WOLFRAM. A class of models intended to be as minimal and structureless as possible is introduced. The models can be viewed as describing networks, relations, sets, or a variety of other structures. Even in cases with simple rules, highly complex behavior can emerge, some of which has striking similarities to known core features of fundamental physics.
Introduction : Quantum mechanics and general relativity—both introduced more than a century ago—have delivered many impressive successes in physics. But so far they have not allowed the formulation of a complete, fundamental theory of our universe, and at this point it seems worthwhile to try exploring other foundations from which space, time, general relativity, quantum mechanics and all the other known features of physics could emerge. The purpose here is to introduce a class of models that could be relevant. The models are set up to be as minimal and structureless as possible, but despite the simplicity of their construction, they can nevertheless exhibit great complexity and structure in their behavior. Even independent of their possible relevance to fundamental physics, the models appear to be of significant interest in their own right, not least as sources of examples amenable to rich analysis by modern methods in mathematics and mathematical physics. But what is potentially significant for physics is that with exceptionally little input, the models already seem able to reproduce some important and sophisticated features of known fundamental physics—and give suggestive indications of being able to reproduce much more.  Our approach here is to carry out a fairly extensive empirical investigation of the models, then to use the results of this to make connections with known mathematical and other features of physics. We do not know a priori whether any model that we would recognize as simple can completely describe the operation of our universe—although the very existence of physical laws does seem to indicate some simplicity. But it is basically inevitable that if a simple model exists, then almost nothing about the universe as we normally perceive it—including notions like space and time—will fit recognizably into the model. And given this, the approach we take is to consider models that are as minimal and structureless as possible, so that in effect there is the greatest opportunity for the phenomenon of emergence to operate. The models introduced here have their origins in network-based models studied in the 1990s for [1], but the present models are more minimal and structureless. They can be thought of as abstracted versions of a surprisingly wide range of types of mathematical and computational systems, including combinatorial, functional, categorical, algebraic and axiomatic ones. In what follows, sections 2 through 7 describe features of our models, without specific reference to physics. Section 8 discusses how the results of the preceding sections can potentially be used to understand known fundamental features of physics. An informal introduction to the ideas described here is given in [2].
Basic Form of Models 2.1 Basic Structure : At the lowest level, the structures on which our models operate consist of collections of relations between identical (but labeled) discrete elements. One convenient way to represent such structures is as graphs (or, in general, hypergraphs). The elements are the nodes of the graph or hypergraph. The relations are the (directed) edges or hyperedges that connect these elements. For example, the graph {{1, 2}, {1, 3}, {2, 3}, {4, 1}} corresponds to the collection of relations {{1, 2}, {1, 3}, {2, 3}, {4, 1}} The order in which these relations are stated is irrelevant, but the order in which elements appear within each relation is considered significant (and is reflected by the directions of the edges in the graph). The specific labels used for the elements (here 1, 2, 3, 4) are arbitrary; all that matters is that a particular label always refer to the same element.
Basic Form of Models 2.2 First Example of a Rule : The core of our models are rules for rewriting collections of relations. A very simple example of a rule is: {{x, y}} -> {{x, y}, {y, z}} Here x, y and z stand for any elements. (The elements they stand for need not be distinct; for example, x and y could both stand for the element 1.) The rule states that wherever a relation that matches {x,y} appears, it should be replaced by {{x,y},{y,z}}, where z is a new element. So given {{1,2}} the rule will produce {{1,2},{2,}} where  is a new element. The label for the new element could be anything—so long as it is distinct from 1 and 2. Here we will use 3, so that the result of applying the rule to {{1,2}} becomes: {{1, 2}, {2, 3}} If one applies the rule again, it will now operate again on {1,2}, and also on {2,3}. On {1,2} it again gives {{1,2},{2,n}}, but now the new node n cannot be labeled 3, because that label is already taken—so instead we will label it 4. When the rule operates on {2,3} it gives {{2,3},{3,n}}, where again n is a new node, which can now be labeled 5. Combining these gives the final result: {{1, 2}, {2, 4}, {2, 3}, {3, 5}} (We have written this so that the results from {{1,2}} are followed by those from {{2,3}}—but there is no significance to the order in which the relations appear.) In graphical terms, the rule we have used is: and the sequence of steps is: It is important to note that all that matters in these graphs is their connectivity. Where nodes are placed on the page in drawing the graph has no fundamental significance; it is usually just done to make the graphs as easy to read as possible. Continuing to apply the same rule for three more steps gives: Laying out nodes differently makes it easier to see some features of the graphs: Continuing for a few more steps with the original layout gives the result: Showing the last 3 steps with the other layout makes it a little clearer what is going on: The rule is generating a binomial tree, with 2n edges (relations) and 2n+1 nodes (distinct elements) at step n (and with Binomial[n, s–1] nodes at level s).
Basic Form of Models 2.3 A Slightly Different Rule : Since order within each relation matters, the following is a different rule: {{x, y}} -> {{z, y}, {y, x}} This rule can be represented graphically as: Like the previous rule, running this rule also gives a tree, but now with a somewhat different structure: With the other rendering from above, the last 3 steps here are:
Basic Form of Models 2.4 Self-Loops : A relation can contain two identical elements, as in {0,0}, corresponding to a self-loop in a graph. Starting our first rule from a single self-loop, the self-loop effectively just stays marking the original node: However, with for example the rule: {{x, y}} -> {{y, z}, {z, x}} the self-loop effectively “takes over” the system, “inflating” to a 2n – gon: The rule can also contain self-loops. An example is {{x, x}} -> {{y, y}, {y, y}, {x, y}} represented graphically as: Starting from a single self-loop, this rule produces a simple binary tree:
Basic Form of Models 2.5 Multiedges : Rules can involve several copies of the same relation, corresponding to multiedges in a graph. A simple example is the rule: {{x, y}} -> {{x, z}, {x, z}, {y, z}} Running this rule produces a structure with 3n edges and  nodes at step n: Rules can both create and destroy multiedges. The rule {{x, y}} -> {{x, z}, {z, w}, {y, z}} generates a multiedge after one step, but then destroys it:
Basic Form of Models 2.6 The Representation of Rules : The examples we have discussed so far all contain only relations involving two elements, which can readily be represented as ordinary directed graphs. But in the class of models we consider, it is also possible to have relations involving other numbers of elements, say three. As an example, consider: {{1, 2, 3}, {3, 4, 5}} which consists of two ternary relations. Such an object can be represented as a hypergraph consisting of two ternary hyperedges: Because our relations are ordered, the hypergraph is directed, as indicated by the arrows around each hyperedge. Note that hypergraphs can contain full or partial self-loops, as in the example of {{1, 1, 1}, {1, 2, 3}, {3, 4, 4}} which can be drawn as: Rules can involve k-ary relations. Here is an example with ternary relations: {{x, y, z}} -> {{x, y, w}, {y, w, z}} This rule can be represented as: Starting from a single ternary self-loop, here are the first few steps obtained with this rule: Continuing with this rule gives the following result: It is worth noting that in addition to having relations involving 3 or more elements, it is also possible to have relations with just one element. Here is an example of a rule involving unary relations: {{x}} -> {{x, y}, {y}, {y}} Starting from a unary self-loop, this rule leads to a binary tree with double-unary self-loops as leaves:
Basic Form of Models 2.7 Rules Depending on More Than One Relation : A crucial simplifying feature of the rules we have considered so far is that they depend only on one relation, so that in a collection of relations, the rule can be applied separately to each relation (cf. [1:p82]). Put another way, this means that all the rules we have considered always transform single edges or hyperedges independently. But consider a rule like: {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} This can be represented graphically as: Here is the result of running the rule for several steps: Here is the result for 10 steps: Despite the simplicity of the underlying rule, the structure that is built (here after 15 steps, and involving 6974 elements and 13,944 relations) is complex: In getting this result, we are, however, glossing over an important issue that will occupy us extensively in later sections, and that potentially seems intimately connected with foundational features of physics. With a rule that just depends on a single relation, there is in a sense never any ambiguity in where the rule should be applied: it can always separately be used on any relation. But with a rule that depends on multiple relations, ambiguity is possible. Consider the configuration: {{1, 2}, {1, 3}, {1, 4}, {1, 4}} The rule {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} can be applied here in two distinct, but overlapping, ways. First, one can take {x -> 1, y -> 2, z -> 3} giving the result: {{1, 3}, {1, 5}, {2, 5}, {3, 5}, {1, 4}, {1, 4}} But one can equally well take {x -> 1, y -> 3, z -> 4} giving the inequivalent result: {{1, 2}, {1, 4}, {1, 5}, {3, 5}, {4, 5}, {1, 4}} With a rule that just depends on a single relation, there is an obvious way to define a single complete step in the evolution of the system: just make it correspond to the result of applying the rule once to each relation. But when the rule involves multiple relations, we have seen that there can be ambiguity in how it is applied (cf. [1:p501]), and one consequence of this is that there is no longer an obvious unique way to define a single complete step of evolution. For our purposes at this point, however, we will take each step to be what is obtained by scanning the configuration of the system, and finding the largest number of non-overlapping updates that can be made (cf. [1:p487]). In other words, in a single step, we update as many edges (or hyperedges) as possible, while never updating any edge more than once. For now, this will give us a good indication of what kind of typical behavior different rules can produce. Later, we will study the results of all possible updating orders. And while this will not affect our basic conclusions about typical behavior, it will have many important consequences for our understanding of the models presented here, and their potential relevance to fundamental physics.
Basic Form of Models 2.8 Termination : We have seen that there can be several ways to apply a particular rule to a configuration of one of our systems. It is also possible that there may be no way to apply a rule. This can happen trivially if the evolution of the system reduces the number of relations it contains, and at some point there are simply no relations left. It can also happen if the rule involves, say, only k-ary relations, but there are no k-ary relations in the configuration of the system. In general, however, a rule can continue for any number of steps, but then get to a configuration where it can no longer apply. The rule below, for example, takes 9 steps to go from {{0,0,0},{0,0}} to a configuration that contains only a single 3-edge, and no 2-edges that match the pattern for the rule: {{x, y, z}, {u, x}} -> {{x, u, v}, {z, y}, {z, u}} It can be arbitrarily difficult to predict if or when a particular rule will “halt”, and we will see later that this is to be expected on the basis of computational irreducibility [1:12.6].
Basic Form of Models 2.9 Connectedness : All the rules we have seen so far maintain connectedness. It is, however, straightforward to set up rules that do not. An obvious example is: {{x, y}} -> {{y, y}, {x, z}} At step n, there are 2n+1 components altogether, with the largest component having n + 1 relations. Rules that are themselves connected can produce disconnected results: {{x, y}} -> {{x, x}, {z, x}} Rules whose left-hand sides are connected in a sense operate locally on hypergraphs. But rules with disconnected left-hand sides (such as {{x},{y}}→{{x,y}}) can operate non-locally and in effect knit together elements from anywhere—though such a process is almost inevitably rife with ambiguity.
Typical Behaviors 3.1 The Representation of Rules : Having introduced our class of models, we now begin to study the general distribution of behavior in them. Like with cellular automata [1:2] and other kinds of systems defined by what can be thought of as simple computational rules [1:3, 4, 5], we will find great diversity in behavior as well as unifying trends. Any one of our models is defined by a rule that specifies transformations between collections of relations. It is convenient to introduce the concept of the “signature” of a rule, defined as the number of relations of each arity that appear on the left and right of each transformation. Thus, for example, the rule {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} has signature 22->42 (and involves a total of 4 distinct elements). Similarly, the rule {{a, a, b}, {c, d}} -> {{b, b, d}, {a, e, d}, {b, b}, {c, a}} has signature 1312->2322 (and involves 5 distinct elements). So far, we have always used letters to indicate elements in a rule, to highlight the fact that these are merely placeholders for the particular elements that appear in the configuration to which the rule is applied. But in systematic studies it is often convenient just to use integers to represent elements in rules, even though these are still to be considered placeholders (or pattern variables), not specific elements. So as a result, the rule just mentioned can be written: It is important to note that there is a certain arbitrariness in the way rules are written. The names assigned to elements, and the order in which relations appear, can both be rearranged without changing the meaning of the rule. In general, determining whether two presentations of a rule are equivalent is essentially a problem of hypergraph isomorphism. Here we will give rules in a particular canonical form obtained by permuting names of elements and orders of relations in all possible ways, numbering elements starting at 1, and using the lexicographically first form obtained. (This form has the property that DeleteDuplicates[Flatten[{lhs,rhs}]] is always a sequence of successive integers starting at 1.) Thus for example, both {{1, 1}, {2, 4, 5}, {7, 5}} -> {{3, 8}, {2, 7}, {5, 4, 1}, {4, 6}, {5, {{7, 3}, {4, 4}, {8, 5, 3}} -> {{3, 4, 7}, {5, 6}, {8, 7}, {3, 5, would be given in the canonical form: From the canonical form, it is possible to derive a single integer to represent the rule. The basic idea is to get the sequence Flatten[{lhs,rhs}] (in this case {1, 2, 3, 4, 4, 5, 3, 3, 2, 4, 3, 4, 5, 1, 5, 2, 6, 7, 8}) and then find out (through a generalized pairing or “tupling” function [3]) where in a list of all possible tuples of this length this sequence occurs [4]. In this example, the result is 310528242279018009. But unlike for systems like cellular automata [5][1:p53][6] or Turing machines [1:p888][7] where it is straightforward to set up a dense sequence of rule numbers, only a small fraction of integers constructed like this represent inequivalent rules (most correspond to non-canonical rule specifications). In addition—for example for applications in physics—one is usually not even interested in all possible rules, but instead in a small number of somehow “notable” rules. And it is often convenient to refer to such notable rules by “short codes”. These can be obtained by hashing the canonical form of the rule, but since hashes can collide, it is necessary to maintain a central repository to ensure that short codes remain unique. In our Registry of Notable Universes [8], the rule just presented has short code wm8678.
Typical Behaviors 3.2 The Number of Possible Rules : Given a particular signature, one may ask how many distinct possible canonical rules there are with that signature. As a first step, one can ask how many distinct elements can occur in the rule. If the rule signature has terms niki on both left and right, the maximum conceivable number of distinct elements is ∑niki. (For example, a possible canonical 22 -> 22 rule is {{1,2},{3,4}}->{{5,6},{7,8}}.) But for many purposes we will want to impose connectivity constraints on the rule. For example, we may want the hypergraph corresponding to the relations on the left-hand side of the rule to be connected [9], and for elements in these relations to appear in some way on the right. Requiring this kind of “left connectivity” reduces the maximum conceivable number of distinct elements to  (or 6 for 22 -> 22). (If the right-hand side is also required to be a connected hypergraph, the maximum number of distinct elements is 1 + ∑ni(ki–1), or 5 for 22 -> 22.) Given a maximum number of possible elements m, an immediate upper bound on the number of rules is m∑niki. But this is usually a dramatic overestimate, because most rules are not canonical. For example, it would imply 1,679,616 left-connected 22 -> 22 rules, but actually there are only 562 canonical such rules. The following gives the number of left-connected canonical rules for various rule signatures (for n1->anything there is always only one inequivalent left-connected rule): Although the exact computation of these numbers seems to be comparatively complex, it is possible to obtain fairly accurate lower-bound estimates in terms of Bell numbers [10]. If one ignores connectivity constraints, the number of canonical rules is bounded below by BellB[∑ni ki]/∏ni!. Here are some examples comparing the estimate with exact results both for the unconstrained and left-connected cases: Based on the estimates, we can say that the number of canonical rules typically increases faster than exponentially as either ni or ki increases. (For 5≤n≤10874, one finds 2n<BellB[n]<2n log n, and for larger n, 2n<BellB[n]<nn.) Note that given an estimate for unconstrained rules, an estimate for the number of left-connected rules can be found from the fraction of randomly sampled unconstrained rules that are left connected. For signature 1p -> 1q, the number of unconstrained canonical rules is BellB[p+q], but given the constraint of left-connectedness there is only ever one canonical rule in this case. When there are no connectivity constraints, the number of canonical rules for signature a -> b is the same as for signature b -> a. With the constraint of left connectivity, the number of  12 -> 52 rules is slightly larger than 52 -> 12 rules, because there are fewer constraints in the former case. For any given signature, we can ask how many distinct elements occur in different canonical rules. Here are histograms for a few cases: There are a number of features of rules that are important when using them in our models. First, if there are relations of arity k on the left-hand side of the rule, there must be relations of the same arity on the right-hand side if those relations are not just going to be inert under that rule. Thus, for example, a rule with signature a2 -> b3 will never (on its own) apply more than once, regardless of the values of a and b. In addition, if a rule is going to have a chance of leading to growth, the number of relations of some arity on the right-hand side must be greater than the number of that arity on the left. These two constraints, however, do not always apply if a complete rule involves several individual rules. Thus, for example, a complete rule containing individual rules with signatures 22 -> 3221, 2211 -> 12 can show growth, and can involve all relations. Note that since canonicalization is independent between different individual rules, the total number of possible inequivalent complete rules is just the product of the number of possible individual inequivalent rules. When investigating cases where a large number of inequivalent rules are possible, it will often be convenient to do random sampling. If one picks a random rule first, and then canonicalizes it, some canonical rules will be significantly more common than others. But it is possible to pick with equal probability among canonical rules by choosing an integer between 1 and the total number of rules, then decoding this integer as discussed above to give the canonical rule.
Typical Behaviors 3.3 Initial Conditions : In addition to enumerating rules, we can also consider enumerating possible initial conditions. Like each side of a rule, these can be characterized by sequences n1k1n2k2... which give the number of relations ni of arity ki. The only possible inequivalent 12 initial conditions are {{1,1}}, corresponding a graph consisting of a single self-loop, and {{1,2}}, consisting of a single edge. The possible inequivalent connected 22 initial conditions are: These correspond to the graphs: The possible inequivalent 13 initial conditions are: These correspond to the hypergraphs: There are 102 inequivalent connected 23 initial conditions. Ignoring ordering of relations, these correspond to hypergraphs with the following structures: Ignoring connectivity, the number of possible inequivalent n1 initial conditions is PartitionsP[n], the number of 1n ones is BellB[n], while the number of n2 ones can be derived using cycle index polynomials (see also [11:A137975]). The number of inequivalent connected initial conditions for various small signatures is as follows (with essentially the same Bell number estimates applying as for rules) [12]: A rule can only apply to a given initial condition if the initial condition contains at least enough relations to match all elements of the left-hand side of the rule. In other words, for a rule with signature nk -> … there must be at least n k-ary relations in the initial condition.
Typical Behaviors 3.4 Rules Depending on a Single Unary Relation : The very simplest possible rules are ones that transform a single unary relation, for example the 11 -> 21 rule: {{x}} -> {{x}, {y}} This rule generates a disconnected hypergraph, containing 2n disconnected unary hyperedges at step n: To get less trivial behavior, one must introduce at least one binary relation. With the 11 -> 1211 rule {{x}} -> {{x, y}, {x}} one just gets a figure with progressively more binary-edge “arms” being added to the central unary hyperedge: The rule {{x}} -> {{x, y}, {y}} produces a growing linear structure, progressively “extruding” binary edges from the unary hyperedge: With two unary relations and one binary relation (signature 11 -> 1221) there are 16 possible rules; after 4 steps starting from a single unary relation, these give: Many lead to disconnected hypergraphs; four lead to binary trees with structures we have already seen. ({{x}}→{{x,y},{x},{y}} is a 12 -> 1221 rule that gives the same result as the very first 12 -> 22 rule we saw. Rules for a single unary relation can never give structures more complex than trees, though the morphology of the trees can become slightly more elaborate:
Typical Behaviors 3.5 Rules Depending on a Single Binary Relation : There are 73 inequivalent left-connected 12 -> 22 rules, but none lead to structures more complex than trees. Starting each from a single self-loop, the results after 5 steps are (note that even a connected rule like {{x,y}}→{{x,z},{z,x}} can give a disconnected result): With an initial condition consisting of a square graph, the following very similar results are obtained: There are 506 inequivalent left-connected 12 -> 32 rules. Running all these rules for 5 steps starting from a single self-loop, and keeping only distinct connected results, one gets (note that similar-looking results can differ in small-scale details): Several distinct classes of behavior are visible. Beyond simple lines, loops, trees and radial “bursts”, there are nested (“cactus-like”) graphs such as obtained from the rule: {{x, y}} -> {{z, z}, {x, z}, {y, z}} The only slightly different rule {{x, y}} -> {{x, x}, {x, y}, {z, x}} gives a rather different structure: A layered rendering makes the behavior slightly clearer: Another notable rule similar to one we saw in the previous section is: {{x, y}} -> {{x, z}, {x, z}, {z, y}} From a single edge this gives: Starting from a single self-loop gives a more complex topological structure (and copies of this structure appear when the initial condition is more complex): Another notable 12 -> 22 rule is {{x, y}} -> {{x, y}, {y, z}, {z, x}} which produces an elaborately filled-in structure: After 8 steps, the structure has the form: After t steps, there are 3t–1 nodes, and  edges. The graph diameter is 2t – 1 if directions of edges are taken into account, and t – 1 if they are not. The maximum degree of any vertex is 2t—and all vertices have degrees of the form 2s, with the number of vertices of degree 2s being proportional to 3t–s. Starting from a single edge makes it slightly easier to understand what is going on: As the rule indicates, every edge of every triangle “sprouts” a new triangle at every step, in effect producing a sequence of “frills upon frills”. But even though this may seem complicated, the whole structure basically corresponds just to a ternary tree in which each node is replaced by a triangle: Starting from a single self-loop, all 12 -> 32 rules give after n steps a number of relations that is either constant, or goes like 2t – 1, 2t – 1 or 3t–1. For 12 -> 42, there are 3740 distinct left-connected rules. As suggested by the random cases below, their behavior is typically similar to 12 -> 32 rules, though the forms obtained can be somewhat more elaborate: For example, the rule {{x, y}} -> {{y, z}, {y, z}, {z, y}, {z, x}} gives the following: The rule {{x, y}} -> {{y, z}, {y, w}, {z, w}, {z, x}} while the similar rule gives: Successive steps in effect just fill in this shape, which seems somewhat irregular when rendered in 2D, but appears more regular if rendered in 3D. Another rule with a simple structure when rendered in 3D is {{x, y}} -> {{y, z}, {y, z}, {z, x}, {z, x}} which yields: The outputs from 12 -> 42 rules all grow either linearly (for example, like 3t – 2), or exponentially, asymptotically like 2t, 3t or 4t. The number of relations after t steps is always given by a linear recurrence relation; for the rule {{x,x}}->{{x,x},{x,x},{x,y},{x,y}} the recurrence is f[t]=3f[t–1]–2f[t–2] (with f[1]=1, f[2]=4), giving size.
Typical Behaviors 3.6 Rules Depending on One Ternary Relation : There are 9373 inequivalent left-connected 13 -> 23 rules. Here are typical examples of their behavior after 5 steps, starting from a single ternary self-loop: Here are results from a few of these rules after 10 steps: The number of relations in the evolution of 13 -> 23 rules can grow in a slightly more complicated way than for 12 -> n2 rules. In addition to linear and 2t growth, there is also, for example, quadratic growth: in the rule each existing “arm” effectively grows by one element each step, and there is one new arm generated, yielding a total size of  = : The rule {{x, x, y}} -> {{y, y, y}, {x, y, z}} yields a Fibonacci tree, with size Fibonacci[t+2]–1 ∼ : 13 -> 23 rules can produce results that look fairly complex. But it is a consequence of their dependence only on a single relation that once such rules have established a large-scale structure, later updates (which are necessarily purely local) can in a sense only embellish it, not fundamentally change it: There are 637,568 inequivalent left-connected 13 -> 33 rules; here are samples of their behavior: The results can be more elaborate than for 13 -> 23 rules—as the following examples illustrate—but remain qualitatively similar: One notable 13 -> 33 rule (that we will discuss below) in a sense directly implements the recursive formation of a nested Sierpiński pattern: {{x, y, z}} -> {{x, u, v}, {z, v, w}, {y, w, u}}
Typical Behaviors 3.7 Rules Depending on More Than One Relation :  The 22->32 Case The smallest nontrivial signature that can lead to growth (and therefore unbounded evolution) is 22 -> 32. There are 4702 distinct left-connected rules with this signature. Here is a random sample of the behavior they generate, starting from a double self-loop {{0,0},{0,0}} and run for 8 steps: Restricting to connected cases, there are 291 distinct outputs involving more than 10 relations after 8 steps: The overall behavior we see here is very similar to what we saw with rules depending only on a single relation. But there is a new issue now to be addressed. With rules depending only on a single relation there is never any ambiguity about where the rule should be applied. But with rules that depend on more than one relation, there can be ambiguity, and the results one gets can potentially depend on the order in which updating is done. Consider the rule {{x, y}, {x, z}} -> {{x, w}, {y, w}, {z, w}} With our standard updating order, the result of running this rule for 30 steps is: But with 6 different choices of random updating orders one gets instead: None of these graphs are isomorphic, but all of them are qualitatively similar. Later on, we will discuss in detail the consequences of different updating orders, and their potentially important implications for physics. But for now, suffice it to say that at a qualitative level different updating orders typically lead to similar behavior. As an example of something of an exception, consider the 22 -> 32 rule shown in the array above: {{x, y}, {y, z}} -> {{x, w}, {w, z}, {z, x}} With our standard updating order, this rule behaves as follows, yielding complicated-looking results with about 1.5n relations after n steps: But with random updating order, the behavior is typically quite different. Here are six examples of results obtained after 10 steps—and all of them are disconnected:
Typical Behaviors 3.8 Rules with Signature 22->42 : For 22 -> 42, there are 40,405 inequivalent left-connected rules. Of these, about 36% stay connected when they evolve. Starting from two self-loops {{0,0},{0,0}}, and running for 8 steps, here is a sample of the 4000 or so distinct behaviors that are produced: urules24 = 2x0-unioned-summary.wxf"]; Most of these rules show the same kinds of behaviors we have seen before. But there is one major new kind of behavior that is observed: in a little less than 1% of all cases, the rules produce globular structures that in effect continually add various forms of cross-connections. Here are a few examples (notably, even though 22 -> 42 rules can involve up to 7 distinct elements, these rules all involve just 4): We will study these kinds of structures in more detail later. Note that the specific forms shown here depend on the underlying updating order used—though for example random orders typically seem to give similar results. It is also the case that the detailed visual layout of graphs can affect the impression of these structures; we will address this in the next section when we discuss various forms of quantitative analysis. It is remarkable how complex the structures are that can be created even from very simple rules. Here are three examples (with short codes wm5583, wm4519, wm2469) shown in more detail: {{x, y}, {x, z}} -> {{y, z}, {y, w}, {z, w}, {w, x}} {{x, y}, {y, z}} -> {{x, y}, {y, x}, {w, x}, {w, z}} {{x, y}, {y, z}} -> {{w, y}, {y, z}, {z, w}, {x, w}} Much as we have seen in other systems such as cellular automata [1], there seems to be no simple way to deduce from the rules from our systems here what their behavior will be. And indeed even seemingly very similar rules can give dramatically different behavior, sometimes simple, and sometimes complex.
Typical Behaviors 3.9 Binary Rules with Signatures Beyond 22->42 : Going from signature 22 -> 32 to signature 22 -> 42 brought us the phenomenon of globular structures. Going to signature 22 -> 52 and beyond does not seem to bring us any similarly widespread significant new form of behavior. The fraction of rules that yield connected results decreases, but among connected results, similar fractions of globular structures are seen, with examples from 22 -> 52 including: The last rule shown here has a feature that is seen in a few 22 -> 42 rules, but is more prominent in 22 -> 52 rules: the presence of many “dangling ends” that at least visually obscure the structure. To see the structure better, one can take the evolution of this rule and effectively just “edit” the graphs obtained at each step, removing all dangling ends: In addition to increasing the number of relations on the right-hand side of the rule, one can also increase the number on the left. For example, one can consider 32 -> 42 rules. These much more often lead to termination than 22 -> … rules, and appear to produce results generally similar to 22 -> 32 rules. 32 -> 52 rules also produce globular structures, though more rarely than 22 -> 42 rules, and with slower growth. A few examples are:
Typical Behaviors 3.10 Rules Depending on Two Ternary Relations : the 23->33 Case There are 79,359,764 inequivalent left-connected 23 -> 33 rules. The fraction of these rules showing continued growth is considerably smaller than for 22 -> … rules. But here is a typical sample of growth rules (note that different rules are run for different numbers of steps to achieve a roughly balanced level of detail): rulesample = {{{{1, 2, 1}, {3, 4, 2}} -> {{5, 6, 5}, {6, 7, 6}, {7, 4, And even though there are only 3 relations on the right-hand side (rather than the 4 in 22 -> 42) these rules can produce globular structures. Some examples are: ruleset = {{{{1, 2, 3}, {3, 4, 5}} -> {{6, 3, 5}, {3, 1, 6}, {6, 2, A new phenomenon exhibited by 23 -> 33 rules is the formation of globular structures by what amounts to slow grow. This is exemplified by a rule like: {{x, y, z}, {x, u, v}} -> {{x, w, u}, {v, w, y}, {w, y, z}} This rule progressively builds up a structure by growing only in one place at a time (the position of the surviving self-loop): After 1000 steps the rule has produced this structure containing 1000 ternary relations (plus the 2 already present in the initial condition): Another example of slow growth occurs in the rule {{x, x, y}, {z, u, x}} -> {{u, u, z}, {v, u, v}, {v, y, x}} which after 1000 steps generates: Note the presence here of regions of square grids. These occur even more prominently in the rule {{x, y, z}, {u, y, v}} -> {{w, z, x}, {z, w, u}, {x, y, w}} which after 500 steps produces: As we will discuss in the next section, the grid here becomes quite explicit when the hypergraph is rendered in 3D. Notice that the grid is not evident even after 20 steps in the evolution of the rule; it takes longer to emerge: Once again, though, the rule adds just a single relation at each generation; in effect the grid is being “knitted” one node at a time. The emergence of a grid is still easier to see in the rule {{x, y, z}, {x, u, v}} -> {{z, z, w}, {w, w, v}, {u, v, w}} which after 200 steps yields: Once again, the “knitting” of this form is far from obvious in the first 20 steps of evolution: Just sometimes, however, the behavior is quite easy to trace, as in this particularly direct example of “knitting” {{x, y, y}, {z, x, u}} -> {{y, v, y}, {y, z, v}, {u, v, v}} which after 200 steps yields: As a different example of slow growth, consider the rule: {{x, y, y}, {y, z, u}} -> {{u, z, z}, {u, x, v}, {y, u, v}} After 200 steps this rule gives while after 500 steps it gives: Looking at all 79 million or so 23 -> 33 rules in canonical order, one finds that rules with slow growth are quite rare and are strongly localized to about 10 broad regions in the space of possible rules. Of rules with slow growth, only a few percent form nontrivial globular structures. And of these, perhaps 10% exhibit obvious lattice-like patterns. The pictures below show additional examples. Note that—as we will discuss later—many of the patterns here are best visualized in 3D.
Typical Behaviors 3.11 Rules Involving More Ternary Relations : There are about 9 billion inequivalent left-connected 23 -> 43 rules. About 20% lead to connected results, and of these about half show continued growth. Here is a random sampling of the behavior of such rules: rulesample = {{{{1, 2, 3}, {2, 4, 5}} -> {{6, 7, 4}, {6, 8, 4}, {4, 7, The fraction of complex behavior appears to be no higher than for 23 -> 33 rules, and no obvious major new phenomena are seen. Much like in systems such as cellular automata (and as suggested by the Principle of Computational Equivalence [1:12]), above some low threshold, adding complexity to the rules does not appear to add complexity to the typical behavior produced. The trend continues with 33 -> 43 rules, with one notable feature here being an increased propensity for rules to yield results that become disconnected, though only after many steps. The general difficulty of predicting long-term behavior is illustrated for example by the evolution of this 33 -> 53 rule, sampled every 10 steps:
Typical Behaviors 3.12 Rules with Mixed Arity : So far essentially all the rules we have considered have “pure signatures” of the form mk -> nk for some arity k. Continued growth is never possible unless the right-hand side of a rule contains some relations with the same arity as appear on the left. But, for example, it is perfectly possible to have growth in rules with signatures like 12 -> 2221. Such rules produce unary relations, which can serve as “markers” for the application of the rule, but cannot themselves affect how or where the rule is used: The 634 rules with signature 12 -> 1312 all show very simple behavior (as do the 2212 rules with signature 12 -> 131211), with not even trees being possible. But among the 7652 12 -> 1322 rules there are not only many trees, but also closed structures such as: Previously we had only seen structures like the first one above in rules that depend on more than one relation. But as this illustrates, such structures can be produced even with just a single relation on the left: {{x, y}} -> {{x, x, y}, {x, z}, {z, y}} The 44,686 rules with signature 12 -> 2312 cannot even produce trees. Rules with signature 13 -> 2312 can produce trees, as well as closed structures similar to those seen in 12 -> 1322 rules. A minimal way to add mixed arity to the left-hand sides of rule is to introduce unary relations—but the presence of these seems to inhibit the production of any more complex forms of behavior. Looking at mixed binary and ternary left-hand sides, none of the 1,141,692 rules with signature 1312 -> 1322 seem to produce even trees. But rules with signature 1312 -> 2322 readily produce structures such as: One can go on and look at rules with higher signatures, and probably the most notable finding is that—in keeping with the Principle of Computational Equivalence [1:12]—the overall behavior seen does not appear to change at all. Here are nevertheless a few examples of slightly unusual behavior found in 2312 -> 3322 and 2312 -> 4342 rules:
Typical Behaviors 3.13 Multiple Transformation Rules : So far we have always considered having just a single possible transformation rule which can be used wherever it applies. It is also possible to have multiple transformation rules which are used wherever they apply. A single transformation rule can either increase or decrease the number of relations, but must do the same every time it is used. With multiple transformations, some can increase the number of relations while others decrease it. As a minimal example, consider the rule: {{{x, x}} -> {{y, x}, {x, z}}, {{x, y}, {y, z}} -> {{x, x}}} On successive steps, this rule simply alternates between two cases: As another example, consider the rule: {{{x, x}} -> {{y, x}, {y, x}, {z, x}}, {{x, y}, {z, y}} -> {{y, y}}} This rule produces results that alternately grow and shrink on successive steps: It is fairly common with multiple transformation rules to find that one transformation is occasionally applied. But at least with our standard updating order, it is difficult to find rules in which, for example, the total size of the results varies in anything but a fairly regular way from step to step.
Typical Behaviors 3.14 Rules Involving Disconnected Pieces : We have mostly restricted ourselves so far to cases where the results generated by a rule remain connected. But in fact if one looks at all possible rules the majority generate disconnected pieces, or at least can do so for certain initial conditions. Among the 73 rules with signature 12 -> 22, only 33 generate connected results starting from initial condition {{0,0}} (and a further 10 terminate from this initial condition). (Note that we are ignoring order in determining connectivity, so that, for example, the relation {1,2} is considered connected not only to {2,3} but also to {1,3}. Translating binary relations like these into directed edges in a graph, this means we are considering weak connectivity, or, equivalently, we are looking only at the undirected version of the graph.) Most 12 -> 22 rules that yield disconnected results essentially just produce exponentially more copies of the same structure: {{x, y}} -> {{y, z}, {y, z}} (Note that this rule is an example of one that yields disconnected results even though the rule itself is not disconnected.) A few rules show slightly more complicated behavior. Examples are (wm575, wm879): {{x, y}} -> {{y, y}, {x, z}} {{x, y}} -> {{x, x}, {y, z}} Both these rules still show exponentially increasing numbers of connected components. In the first case, at step t there are components of all sizes 1 through t, in exponentially decreasing numbers. In the second case, the size of the largest component is or asymptotically ~ϕn (it follows the recurrence f[n]=2f[n–1]–f[n–3]). Note that if one tracks only the largest component, one gets a sequence of results that could only be generated by a rule involving several separate transformations (in this case {{x,x}}→{{x,y},{x,x}} and {{x,y}}→{{x,x}}). In general, with a single transformation, the total number of relations must either always increase or always decrease. But if there are disconnected pieces, and one tracks, say, only the largest component, one can get a sequence of results that can both increase and decrease in size. As an example, consider the rule: {{x, y}, {x, z}} -> {{y, z}, {z, y}, {x, w}} Evolving this rule with our standard updating order gives: The total number of relations increases roughly exponentially. But tracing only the largest component, we see that it oscillates in size, eventually settling into the cycle 5,8,9,8: Note that this result is quite specific to the use of our standard updating order. A random updating order, for example, will typically give larger results for the largest component, and no cycle will normally be seen. It is quite common to see rules that sometimes yield connected results, and sometimes do not. (In fact, proving that a given rule in a given case can never generate disconnected components can be arbitrarily difficult.) Sometimes there can be a large component with a complex structure, with small disconnected pieces occasionally getting “thrown off”. Consider for example the rule: {{x, y}, {y, z}} -> {{x, w}, {w, x}, {z, x}} With the standard updating order, it remains connected for 10 steps, then suddenly starts throwing off small disconnected pieces. As a more elaborate example, consider the rule: {{x, y, z}, {u, v, z}} -> {{y, w, u}, {w, x, y}, {u, y, x}} This remains connected for 16 steps, then starts throwing off disconnected pieces: With a rule like this, once components become disconnected, they can in a sense never interact again; their evolutions become completely separate. The only way for disconnected components to interact is to have a rule which itself has a disconnected left-hand side. For example, a rule like will collect even completely disconnected unary relations, and connect pairs of them into binary relations: Connected unary relations (i.e. such as {1}, {1}, ...) can end up in the same component, but the result depends critically on the order in which updates are done: For now, we will not consider any further rules with disconnected left-hand sides—and the extreme nonlocality they represent.
Typical Behaviors 3.15 Termination : Not all rules continue to evolve forever from a given initial state. Instead they can reach a fixed point where the rule no longer applies. If the rule depends only on a single relation, this can only happen at the very first step. But if the rule depends on multiple relations, it can happen after multiple steps. Among the 4702 rules with signature 22 -> 32, 1788 rules eventually reach a fixed point starting from a self-loop initial condition, at least using our standard updating order. Their “halting time” decreases roughly exponentially, with the maximum being 7 steps, achieved by the rule: {{x, y}, {z, y}} -> {{y, u}, {u, x}, {v, z}} The longest halting time for which connectedness is maintained is 3 steps, achieved for example by: {{x, y}, {y, z}} -> {{y, z}, {y, u}, {v, z}} Among the 40,405 22 -> 42 rules, 10,480 evolve to fixed points starting from self-loops. The maximum halting time is 13 steps; the maximum maintaining connectedness is 6 steps, achieved by: Among the 353,462 22 -> 52 rules, 67,817 (or about 19%) evolve to fixed points. The maximum halting time is 24 steps; the maximum maintaining connectedness is 10 steps, achieved for example by: Among 23 -> 33 rules {{x, y, z}, {x, u, v}} -> {{y, x, w}, {w, u, s}, {v, z, u}} has halting time 20:
Typical Behaviors 3.16 The Effect of Initial Conditions : For rules that depend on only a single relation, adding relations to initial conditions always just leads to replication of identical structures, as in these examples for the rule: Sometimes, however, the layout of hypergraphs for visualization can make the replication of structures a little less obvious, as in this example for the rule: For rules depending on more than one relation, initial conditions can have more important effects. Starting with the rule from all 8 inequivalent 2-relation and all 32 inequivalent 3-relation initial conditions, one sees quite a range of behavior: But in other rules—particularly many of those such as {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} that yield globular structures—different initial conditions (so long as they lead to growth at all) produce behavior that is different in detail but similar in overall features, a bit like what happens in class 3 cellular automata such as rule 30 [1:p251]): For the evolution of a rule to not just immediately terminate, the left-hand side of the rule must be able to match the initial conditions given (and so must be a sub-hypergraph of the initial conditions). This is guaranteed if the initial conditions are in effect just a copy of the left-hand side. But the most “fertile” initial conditions, with the most possibility for different matches, are always self-loops: in particular, n k-ary self-loops for a rule with signature nk ->…. And in what follows, this is the form of initial conditions that we will most often use. One practical issue with self-loop initial conditions, however, is that they can make it visually more difficult to tell what is going on. Sometimes, for example, initial conditions that lead to slightly less activity, or enforce some particular symmetry, can help. Note, however, that in the evolution of rules that depend on more than one relation, there may be no way to preserve symmetry, at least with any specific updating order (see section 6). Thus, for example, the rule {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} with our standard updating order gives: Another feature of initial conditions is that they can affect the connectivity of the results from a rule. Thus, for example, even in the case of the rule above that generates a grid, initial conditions consisting of different numbers of 3-ary self-loops lead to differently connected results:
Typical Behaviors 3.17 Behavior without Growth : Essentially all the rules we have considered so far have been set up to add relations. But with extended initial conditions, it makes sense also to consider rules that maintain a fixed number of relations. Rules with signatures like 12 -> 12 that depend only on one relation cannot give rise to nontrivial behavior. But rules with signature 22 -> 22 (of which a total of 562 are inequivalent) already can. Consider the rule: {{x, y}, {y, z}} -> {{y, x}, {z, x}} Starting from a chain of 5 binary relations, this is the behavior of the rule: Given that only a finite number of elements and relations are involved, the total number of possible states of the system is finite, so it is inevitable that the evolution of the system must eventually repeat. In the particular case shown here, the repetition period is only 3 steps. (Note that the detailed behavior—and the repetition period—can depend on the updating order used.) In general, the total number of possible states of the system is given by the number of distinct hypergraphs of a certain size. One can then construct a state transition graph for these states under a rule. Here is the result for the rule above with the 32 distinct connected 32 hypergraphs (note that with this rule, the hypergraphs always remain connected): The result for all 928 52 hypergraphs is: This graph contains trees corresponding to transients, leading into cycles. The maximum cycle length in this case is 5. But when the size of the system increases, the lengths of cycles can increase rapidly (cf. [1:6.4]). The length is bounded by the number of distinct nk hypergraphs, which grows faster than exponentially with n. The plot below shows the lengths of cycles and transients in the rule above for initial conditions consisting of progressively longer chains of relations:
Typical Behaviors 3.18 Random Rules and Overall Classification of Behavior : Here are samples of random rules with various signatures (only connected results are included): As expected from the Principle of Computational Equivalence [1:12], above a low threshold more complex rules do not generally lead to more complex behavior, although the frequencies of different kinds of behavior do change somewhat. At a basic visual level, one can identify several general types of behavior: Line-like: elements are connected primarily in sequences (lines, circles, etc.) Radial: most elements are connected to just a few core elements Tree-like: elements repeatedly form independent branches Globular: more complex, closed structures Inevitably, these types of behavior are neither mutually exclusive, nor precisely defined. There are certainly specific graph-theoretic and other methods that could be used to discriminate different types, but there will always be ambiguous cases (and sometimes it will even be formally undecidable what category something is in). But just like for cellular automata—or for many systems in the empirical sciences—classifications can still be useful in practice even if their definitions are not unique or precise. As an alternative to categorical classification, one can also consider systematically arranging behaviors in a continuous feature space (e.g. [13]). The results inevitably depend on how features are extracted. Here is what happens if one takes images like the ones above, and directly applies a feature extractor trained on images of picturable nouns in human language [14]: Here is the fairly similar result based on feature extraction of underlying adjacency matrices: In addition to characterizing the behavior of individual rules, one can also ask to what extent behavior is clustered in rule space. Here are samples of what happens if one starts from particular 22 -> 72 rules, then looks at a collection of “nearby” rules that differ by one element in one relation: And what we see is that even though there are only 68 million or so 22 -> 72 rules, changing one element (out of 14) still usually gives a rule whose overall behavior is similar.
Limiting Behavior and Emergent Geometry 4.1 Recognizable Geometry : Particularly for potential applications to fundamental physics, it will be of great importance to understand what happens if we run our models for many steps—and to find ways to characterize overall behavior that emerges. Sometimes the characterization is easy. One gets a loop with progressively more links: {{x, y}} -> {{y, z}, {z, x}} Or one gets a tree with progressively more levels of branching: {{x}} -> {{x, y}, {y}, {y}} Flatten[Prepend[ But what about a case like the following? Is there any way to characterize the limiting behavior here? {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} It turns out that a rare phenomenon that we saw in the previous section gives a critical clue. Consider the rule: {{x, y, y}, {z, x, u}} -> {{y, v, y}, {y, z, v}, {u, v, v}} Looking at the first 10 steps it is not clear what it will do: But showing the results every 10 steps thereafter it starts to become clearer: And after 1000 steps it is very clear: the rule has basically produced a simple grid: Geometrical though this looks, it is important to understand that at a fundamental level there is no geometry in our model: it just involves abstract collections of relations. Our visualization methods make it evident, however, that the pattern of these relations corresponds to the pattern of connections in a grid. In other words, from the purely combinatorial structure of the model, what we can interpret as geometrical structure has emerged. And if we continue running the model, the grid in our picture will get finer and finer, until eventually it approximates a triangular piece of continuous two-dimensional space. Consider now the rule: {{x, x, y}, {x, z, u}} -> {{u, u, z}, {y, v, z}, {y, v, z}} Looking at the first 10 steps of evolution it is again not clear what will happen: But after 1000 steps a definite geometric structure has emerged: There is evidence of a grid, but now it is no longer flat. Visualizing in 3D makes it clearer what is going on: the grid is effectively defining a 2D surface in 3D: To make its form clearer, we can go for 2000 steps, and include an approximate surface reconstruction [15]: The result is that we can identify that in the limit this rule can be characterized as creating what is essentially a cone. Other rules produce other shapes. For example, the rule {{x, y, z}, {u, y, v}} -> {{w, z, x}, {z, w, u}, {x, y, w}} gives after 1000 steps: The structure is clearer when visualized in 3D: Despite its smooth form, there does not seem to be a simple mathematical characterization of this surface. (Its three-lobed structure means it cannot be an ordinary algebraic surface [16]; it is similar but not the same as the surface r = sin(ϕ) in spherical coordinates.) Changing the initial condition from  to  yields the rather different surface: This rule gives a closer approximation to a sphere, though there is a definite threefold structure to be seen: {{x, y, y}, {x, z, u}} -> {{u, v, v}, {v, z, y}, {x, y, v}} Simpler forms, such as cylindrical tubes, also appear: {{x, x, y}, {z, u, y}} -> {{u, u, y}, {x, v, y}, {x, v, z}} It is worth pausing for a moment to consider in what sense the limiting object here “is” a tube. What we ultimately have is a collection of relations which define a hypergraph. But there is an obvious measure of distance on the hypergraph: how many relations you have to follow to go from one element to another. So now one can ask whether there is a way to make this hypergraph distance between elements correspond to an ordinary geometrical distance. Can one assign positions in space to the elements so that the spatial distances between them agree with the hypergraph distances? The answer in this case is that one can—by placing the elements at a lattice of positions on the surface of a cylinder in three-dimensional space. (And, conveniently, it so happens that our visualization method for hypergraphs basically automatically does this.) But it is important to realize that such a direct correspondence with an easy-to-describe surface is a rare and special feature of the particular rule used here. Consider the rule: {{x, x, y}, {x, z, u}} -> {{u, u, v}, {v, u, y}, {z, y, v}} After 1000 steps, this rule produces: In 3D, this can be visualized as: There are many subtle issues here. First, at every step the rule adds more elements, and in principle this could change the emergent geometry. But it appears that after enough steps, there is a definite limiting shape. Unlike in the case of a cylinder, however, it is much less clear how to assign spatial coordinates to different elements. It does not help that the limiting shape does not appear to have a completely smooth surface; instead there are places at which it appears to form cusps (reminiscent of an orbifold [17]). There are rules that give more obvious “singularities”; an example is: {{x, x, y}, {y, z, u}} -> {{v, v, u}, {v, u, x}, {z, y, v}} Some rules produce surfaces with complex folds: {{x, x, y}, {z, u, x}} -> {{z, z, v}, {y, v, x}, {y, w, v}} It is also perfectly possible for the emergent geometry to have nontrivial topology. This rule produces a (strangely twisted) torus: {{x, x, y}, {z, u, x}} -> {{x, x, z}, {u, v, x}, {y, v, z}} All the emergent geometries we have seen so far in effect involve a regular mesh. But this rule, instead uses a mixture of triangles, quadrilaterals and pentagons to cover a region: {{x, y, x}, {x, z, u}} -> {{u, v, u}, {v, u, z}, {x, y, v}}
Limiting Behavior and Emergent Geometry 4.2 Hyperbolic Space : Among all possible rules, the formation of geometrical shapes of the kind we have just been discussing is very rare. Slightly more common is the type of behavior that we see in a rule like: {{x, y}, {y, z}} -> {{w, x}, {w, y}, {x, y}, {y, z}} Essentially the same behavior also occurs in a mixed-arity rule with a single relation on the left-hand side: {{x, y}} -> {{z, y, x}, {y, z}, {z, x}} Flatten[Append[ We can think of the structure that is produced as being like a binary tree of triangles: The same structure can be produced from an Apollonian circle packing (e.g. [18][1: p985]): If each triangle is required to have the same area, the structure can be rendered in 2D as: If we tried to render this with every triangle roughly the same size, then even in 3D the best we could do would be to have something that crinkles up more and more at the edge, like an idealized lettuce leaf: But just as we can think of the grids we discussed before as being regularly laid out in ordinary 2D or 3D space, so now we can think of the object we have here as being regularly laid out in a hyperbolic space [19][20] of constant negative curvature. In particular, the object corresponds to an infinite-order triangular tiling of the hyperbolic plane (with Schläfli symbol {3,∞}). There are a variety of ways to visualize the hyperbolic plane. One example is the Poincaré disk model in which hyperbolic-space straight lines are rendered as arcs of circles orthogonal to the boundary: (The particular graph here happens to be the Farey graph [21].)
Limiting Behavior and Emergent Geometry 4.3 Geometry from Subdivision : The grids and surfaces that we saw above were all produced by rules that end up executing a laborious “knitting” process in which they add just a single relation at each step. But it is also possible to generate recognizable geometric forms more quickly—in effect by a process of repeated subdivision. Consider the 2312 -> 4342 rule: At each step, this rule doubles the number of relations—and quickly produces a structure with a definite emergent geometrical form: After 10 steps the rule has generated 2560 relations, in the following structure: Visualized in 3D, this becomes: Once again, this corresponds to a smooth surface, but with 3 cusps. The surface is defined not by a simple triangular grid, but instead by an octagon-square (“truncated square”) tiling—that in this case becomes twice as fine at every step. Changing the initial conditions can give a somewhat different structure: Visualized in 3D after 10 steps (and reconstructing less of the surface), this becomes:
Limiting Behavior and Emergent Geometry 4.4 Nested Patterns : Consider the 13 -> 33 rule: Starting from a single ternary relation with three distinct elements {{1,2,3}}, this gives a classic Sierpiński triangle structure: Starting instead from a ternary self-loop {{0,0,0}} one gets what amounts to a tetrahedron of Sierpiński triangles: This is exactly the same as one would get by starting with a tetrahedron graph, and repeatedly replacing every trivalent vertex with a triangle of vertices [1:p509]: In an ordinary Sierpiński triangle, the points on the edges have different neighborhoods from those in the interior. But in the structure shown here, all points have the same neighborhoods (so there is an isometry). Many of the rules we have used have completely different behavior if the order of elements in their relations are changed. But in this case the limiting shape is always the same, regardless of ordering, as in these examples: The rule we have discussed so far in this section in a sense directly implements the recursive construction of nested patterns [1:5.4]. But the formation of nested patterns is also a common feature of the limiting behavior of many rules that do not exhibit any such obvious construction. As an example, consider the 13 -> 23 rule: {{x, y, z}} -> {{z, w, w}, {y, w, x}} This rule effectively constructs a nested sequence of self-similar “segments”: Similar behavior is seen in rules with binary relations, such as the 12 -> 42 rule: {{x, y}} -> {{z, w}, {z, x}, {w, x}, {y, w}} A clear “naturally occurring” Sierpiński pattern appears in the limiting behavior of the 22 -> 42 rule: {{x, y}, {z, y}} -> {{y, w}, {y, w}, {w, x}, {z, w}} After 15 steps, the rule yields:
Limiting Behavior and Emergent Geometry 4.5 The Notion of Dimension : In traditional geometry, a basic feature of any continuous space is its dimension. And we have seen that at least in certain cases we can characterize the limiting behavior of our models in terms of the emergence of recognizable geometry—with definite dimension. So this suggests that perhaps we might be able to use a notion of dimension to characterize the limiting behavior of our models even when we do not readily recognize traditional geometrical structure in them. For standard continuous spaces it is straightforward to define dimension, normally in terms of the number of coordinates needed to specify a position. If we make a discrete approximation to a continuous space, say with a progressively finer grid, we can still identify dimension in terms of the number of coordinates on the grid. But now imagine we only have a connectivity graph for a grid. Can we deduce what dimension it corresponds to? We might choose to draw the grids so they lay out according to coordinates, here in 1-, 2- and 3-dimensional Euclidean space: But these are all the same graph, with the same connectivity information: So just from intrinsic information about a graph—or, more accurately, from information about a sequence of larger and larger graphs—can we deduce what dimension of space it might correspond to? The procedure we will follow is straightforward (cf. [1:p479][22]). For any point X in the graph define Vr(X) to be the number of points in the graph that can be reached by going at most graph distance r. This can be thought of as the volume of a ball of radius r in the graph centered at X. For a square grid, the region that defines Vr(X) for successive r starting at a point in the center is: For an infinite grid we then have: For a 1D grid the corresponding result is: And for a 3D grid it is: In general, for a d-dimensional cubic grid (cf. [1:p1031]) the result is a terminating hypergeometric series (and the coefficient of zd in the expansion of (z+1)r/(z-1)r+1): But the important feature for us is that the leading term—which is computable purely from connectivity information about the graph—is proportional to rd.  What will happen for a graph that is less regular than a grid? Here is a graph made by random triangulation of a 2D region: And once again, the number of points reached at graph distance r grows like r2: In ordinary d-dimensional continuous Euclidean space, the volume of a ball is exactly And we should expect that if in some sense our graphs limit to d-dimensional space, then in correspondence with this, Vr should always show rd growth. There are, however, many subtle issues. The first—immediately evident in practice—is that if our graph is finite (like the grids above) then there are edge effects that prevent rd growth in Vr when the radius of the ball becomes comparable to the radius of the graph. The pictures below show what happens for a grid with side length 11, compared to an infinite grid, and the rd term on its own: One might imagine that edge effects would be avoided if one had a toroidal grid graph such as: But actually the results for Vr(X) for any point on a toroidal graph are exactly the same as those for the center point in an ordinary grid; it is just that now finite-size effects come from paths in the graph that wrap around the torus. Still, so long as r is small compared to the radius of the graph—but large enough that we can see overall rd growth—we can potentially deduce an effective dimension from measurements of Vr. In practice, a convenient way to assess the form of Vr, and to make estimates of dimension, is to compute log differences as a function of r: Here are results for the center points of grid graphs (or for any point in the analogous toroidal graphs): The results are far from perfect. For small r one is sensitive to the detailed structure of the grid, and for large r to the finite overall size of the graph. But, for example, for a 2D grid graph, as the size of the graph is progressively increased, we see that there is an expanding region of values of r at which our estimate of dimension is accurate: A notable feature of measuring dimension from the growth rate of Vr(X) is that the measurement is in some sense local: it starts from a particular position X. Of course, in looking at successively larger balls, Vr(X) will be sensitive to parts of the graph progressively further away from X. But still, the results can depend on the choice of X. And unless the graph is homogeneous (like our toroidal grids above), one will often want to average over at least a range of possible positions X. Here is an example of doing such averaging for a collection of starting points in the center of the random 2D graph above. The error bars indicate 1σ ranges in the distribution of values obtained from different points X. So far we have looked at graphs that approximate standard integer-dimensional spaces. But what about fractal spaces [23]? Let us consider a Sierpiński graph, and look at the growth of a ball in the graph: Estimating dimension from Vr(X) averaged over all points we get (for graphs made from 6 and 7 recursive subdivisions): The dotted line indicates the standard Hausdorff dimension log2(3)≈1.58 for a Sierpiński triangle [23]. And what the pictures suggest is that the growth rate of Vr approximates this value. But to get the exact value we see that in addition to everything else, we will need average estimates of dimension over different values of r. In the end, therefore, we have quite a collection of limits to take. First, we need the overall size of our graph to be large. Second, we need the range of values of r for measuring Vr to be small compared to the size of the graph. Third, we need these values to be large relative to individual nodes in the graph, and to be large enough that we can readily measure the leading order growth of Vr—and that this will be of the form rd. In addition, if the graph is not homogeneous we need to be averaging over a region X that is large compared to the size of inhomogeneities in the graph, but small compared to the values of r we will use in estimating the growth of Vr. And finally, as we have just seen, we may need to average over different ranges of r in estimating overall dimension. If we have something like a grid graph, all of this will work out fine. But there are certainly cases where we can immediately tell that it will not work. Consider, for example, first the case of a complete graph, and second of a tree: For a complete graph there is no way to have a range of r values “smaller than the radius of graph” from which to estimate a growth rate for Vr. For a tree, Vr grows exponentially rather than as a power of r, so our estimate of dimension Δ(r) will just continually increase with r: But notwithstanding these issues, we can try applying our approach to the objects generated by our models. As constructed, these objects correspond to directed graphs or hypergraphs. But for our current purposes, we will ignore directedness in determining distance, effectively taking all elements in a particular k-ary relation—regardless of their ordering—to be at unit distance from each other. As a first example, consider the 23 -> 33 rule we discussed above that “knits” a simple grid: As we run the rule, the structure it produces gets larger, so it becomes easier to estimate the growth rate of Vr. The picture below shows Δ(r) (starting at the center point) computed after successively more steps. And we see that, as expected, the dimension estimate appears to converge to value 2: It is worth mentioning that if we did not compute Vr(X) by starting at the center point, but instead averaged over all points, we would get a less useful result, dominated by edge effects: As a second example, consider the 23 -> 33 rule that slowly generates a somewhat complex kind of surface: As we run this longer, we see what appears to be increasingly close approximation to dimension 2, reflecting the fact that even though we can best draw this object embedded in 3D space, its intrinsic surface is two-dimensional (though, as we will discuss later, it also shows the effects of curvature): The successive dimension estimates shown above are spaced by 500 steps in the evolution of the rule. As another example, consider the 2312 -> 4342 rule, in which geometry emerges rapidly through a process of subdivision: These are dimension estimates for all of the first 10 steps in the evolution of this rule: We can also validate our approach by looking at rules that generate obviously nested structures. An example is the 22 -> 42 rule that produces: The results for each of the first 15 steps show good correspondence to dimension log2(3)≈1.58:
Limiting Behavior and Emergent Geometry 4.6 Dimension-Related Characterizations : Having seen how our notion of dimension works in cases where we can readily recognize emergent geometry, we now turn to using it to study the more general limiting behavior of our models. As a first example, consider the 22 -> 42 rule {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} which generates results such as (with about 1.84t relations at step t): If we attempt to reconstruct a surface from successive steps in the evolution of this rule, no clearly recognizable geometry emerges: But instead we can try to characterize the results using Vr(X) and our notion of dimension. We compute Vr(X) as we do elsewhere: by starting at a point in the structure and constructing successively larger balls: Computing the Δ(r) for all points over the first 16 steps of evolution gives: The most important feature of this plot is that it suggests Δ(r) might approach a definite limit as the number of steps increases. And from the increasing region of flatness there is some evidence that perhaps Vr might approach a stable rd form, with d ≈ 2.7, suggesting that in the limit this rule might produce some kind of emergent geometry with dimension around 2.7. What about other rules? Here are some examples for rules we have discussed above: Some rules do not show convergence, at least over the number of steps sampled here. Other rules show quite stable limiting forms, often with a flat region which suggests a structure with definite dimension. Sometimes this dimension is an integer, like 1 or 2; often it is not. Still other rules seem to show linear increase in log differences of Vr, implying an exponential form for Vr itself, characteristic of tree-like behavior.
Limiting Behavior and Emergent Geometry 4.7 Curvature : In ordinary plane geometry, the area of a circle is πr2. But if the circle is drawn on the surface of a sphere of radius a, the area of the spherical region enclosed by the circle is instead: In other words, curvature in the underlying space introduces a correction to the growth rate for the area of the circle as a function of radius. And in general there is a similar correction for the volume of a d-dimensional ball in a curved space (e.g. [24][1:p1050]) where here R is the Ricci scalar curvature of the space [25][26][27]. (For example, for the d-dimensional surface of a (d+1)-dimensional sphere of radius a, .) Now consider the sequence of “sphere” graphs: We can compute Vr for each of these graphs. Here are the log differences Δ(r) (the error bars come from the different neighborhoods associated with hexagonal and pentagonal “faces” in the graph): We immediately see the effect of curvature: even though in the limit the graphs effectively define 2D surfaces, the presence of curvature introduces a negative correction to pure r2 growth in Vr. (Somewhat confusingly, there is only one scale defined for the kind of “pure sphere” graphs shown here, so they all have the same curvature, independent of size.) A torus, unlike a sphere, has no intrinsic surface curvature. So torus graphs of the form give flat log differences for Vr: A graph based on a tiling in hyperbolic space has negative curvature, so leads to a positive correction to Vr: (One can imagine getting other examples by taking 3D objects and putting meshes on their surfaces. And indeed if the meshes are sufficiently faithful to the intrinsic geometry of the surfaces—say based on their geodesics—then the Vr(X) for the connectivity graphs of these meshes [28] will reflect the intrinsic curvatures of the surfaces. In practical computational geometry, though, meshes tend to be based on things like coordinate parametrizations, and so do not reflect intrinsic geometry.) Many structures produced by our models exhibit curvature. There are cases of negative curvature: As well as positive curvature: The most obvious examples of nested structures have fractional dimension, but no curvature: But even though it is not well characterized using ideas from traditional calculus, there is every reason to expect that the limits of our models can exhibit a combination of fractional dimension and curvature. In general, though, there is no obvious constraint on the possible limiting form of Vr. Curvature can be thought of as associated with the O(r2) term in a Taylor expansion of Vr about r = 0, after factoring out rd. But there is nothing to say that the leading behavior of Vr should match a form like rd. In addition to exponentials like λr it could show an infinite collection of intermediate asymptotic scales, like 2r log(r) or [29][30].
Limiting Behavior and Emergent Geometry 4.8 Homogeneity and Local Graph Neighborhoods : In studying Vr we are looking at the total size of the neighborhood up to distance r around a point in a graph. But what about the actual local structure of the neighborhood? In general, it can be different for every point on the graph. Thus, for example, in obtained from 10 steps of the rule {{x,y},{x,z}}->{{x,z},{x,w},{y,w},{z,w}} the collection of distinct range-1 neighborhoods (with their counts) is: The corresponding result after 12 steps is: And it seems that for this rule the distribution of different forms for a given range of neighborhood generally stabilizes as the number of steps increases. (It may be possible to characterize it as limiting to an invariant measure in the space of possible hypergraphs, perhaps with some related entropy (cf. [1:p958][31]).) One sees the same kind of stabilization for most rules, though, for example, in a case like from the rule {{x,y}}->{{x,y},{y,z},{z,x}} one always gets some neighborhoods with new forms at each step: In general, the presence of many identical neighborhoods reflects a certain kind of approximate symmetry or isometry of the emergent geometry of the system. In a torus graph, for example, the symmetry is exact, and all local neighborhoods of a given range are the same: The same is true for a 3D torus graph: For a sphere graph not every point has the exact same local neighborhood, but there are a limited number of neighborhoods of a given range: And from the dual graph it becomes clear that these are associated with hexagonal and pentagonal “faces”: For a (spherical) Sierpiński graph, there are also a limited number of neighborhoods of a given range: Whenever every local neighborhood is essentially identical, Vr(X) will have the same form for every point X in a graph or hypergraph. But in general Vr(X) (and the log differences Δr(X)) will depend on X. The picture below shows the relative values of Δr(X) at each point in the structure we showed above: We can also compute the distribution of values for Δr(X) across the structure, as a function of r: Both these pictures indicate a certain statistical uniformity in Vr(X). This is also seen if we look at the evolution of the distribution of Δr(X), here shown for the specific value r = 6, for steps 8 through 16:
Limiting Behavior and Emergent Geometry 4.9 Adjacency Matrices and Age Distributions : We have made explicit visualizations of the connectivity structures of the graphs (and hypergraphs) generated by our models. But an alternative approach is to look at adjacency matrices (or tensors). In our models, there is a natural way to index the nodes in the graph: the order in which they were created. Here are the adjacency matrices for the first 14 steps in the evolution of the rule {{x,y},{x,z}}->{{x,z},{x,w},{y,w},{z,w}} discussed above: It is notable that even though these adjacency matrices grow by roughly a factor of 1.84 at each step, they maintain many consistent features—and something similar is seen in many other rules. Our models evolve by continually adding new relations, and for example in the rule we are currently considering, there are roughly exponentially more relations at each step. The result, as shown below for step 14, is that at a given step the relations that exist will almost all be from the most recent step (shown in red): Other rules can show quite different age distributions. Here are age distributions for a few rules that “knit” their structures one relation at a time:
Limiting Behavior and Emergent Geometry 4.10 Other Graph Properties : There are many graph and hypergraph properties that can be studied for the output of our models. Here we primarily give examples for the rule {{x,y},{x,z}}->{{x,z},{x,w},{y,w},{z,w}} discussed above.A basic question is how the numbers of vertices and edges (elements and relations) grow with successive steps. Plotting on a logarithmic scale suggests eventually roughly exponential growth in this case: We can also compute the growth of the graph diameter (greatest distance between vertices) and graph radius: If one assumes that the total vertex count V is related to diameter D by V = Dd, then plotting d gives (to be compared to dimension approaching ≈2.68 computed from the growth of Vr): There are many measures of graph structure which basically support the expectation that after many steps, the outputs from the model somehow converge to a kind of statistically invariant “equilibrium” state: Some centrality measures [32][33] start (here at step 10) somewhat concentrated, but rapidly diffuse to be much more broadly distributed: There are local features of the graph that are closely related to V1(X) and V2(X): Another feature of our graphs to study is their cycle structure. At the outset, our graphs give us only connectivity information. But one way to imagine identifying “faces” that could be used to infer emergent topology is to look at the fundamental cycles in the graph: In this particular graph, there are altogether 320 fundamental cycles, with the longest one being of length 24. The distribution of cycle lengths on successive steps once again seems to approach an “equilibrium” form: One way to probe overall properties of a graph is to consider the evolution of some dynamical process on the graph. For example, one could run a totalistic cellular automaton with values at nodes of the graph. Another possibility is to solve a discretized PDE. For example, having computed a graph Laplacian [34] (or its higher order analogs) one can determine the distribution of eigenvalues, or the eigenmodes, for a particular graph [35]. The density of eigenvalues is then closely related to Vr and our estimates of dimension and curvature.
Limiting Behavior and Emergent Geometry 4.11 Graph Properties Conserved by Rules : Many rules (at least when they exhibit complex behavior) seem to lead to statistically similar behavior, independent of their initial conditions. But there could still be disjoint families of states that can be reached from different initial conditions, perhaps characterized by different graph or hypergraph invariants. As one example, we can ask whether there are rules that preserve the planarity of graphs. All rules with signature 12 -> 22 inevitably do this. A rule like {{x, y}} -> {{x, y}, {y, z}, {z, x}} might not at first appear to: But a different graph layout shows that actually all these graphs are planar [36]: Among larger rules, many still preserve planarity. But for example, {{x,y},{x,z}}->{{x,z},{x,w},{y,w},{z,w}} does not, since it transforms the planar graph to the nonplanar one: In general, a graph is planar so long as it does not contain as a subgraph either of [37] so a rule preserves planarity if (and only if) it never generates either of these subgraphs. Planarity is one of a class of properties of graphs that are preserved under deletion of vertices and edges, and contraction of edges. Another such property is whether a graph can be drawn without crossings on a 2D surface of any specific genus g [38]. It turns out [38] that for any such property it is known that there are in principle only a finite number of subgraphs that can “obstruct” the property—so if a rule never generates any of these, it must preserve the property.
Limiting Behavior and Emergent Geometry 4.12 Apparent Randomness and Growth Rates : The phenomenon of intrinsic randomness generation is an important and ubiquitous feature of computational systems [1:7.5][39]—the rule 30 cellular automaton [1:2.1] being a quintessential example. In our models the phenomenon definitely often occurs, but two issues make it slightly more difficult to identify. First, there is considerable arbitrariness in the way we choose to present or visualize graphs or hypergraphs—so it is more difficult to tell whether apparent randomness we see is a genuine feature of our system, or just a reflection of some aspect of our presentation or visualization method. And second, there may be many possible choices of updating orders, and the specific results we get may depend on the order we choose. Later we will discuss the phenomenon of causal invariance, and we will see that there are causal graphs that can be independent of updating order. But for now, we can consider our updating process to just be another deterministic procedure added to the rules of our system, and we can ask about apparent randomness for this combined system. And to avoid the arbitrariness of different graph or hypergraph presentations, we can look at graph or hypergraph invariants, which are the same for all isomorphic graphs or hypergraphs, independent of their presentation or visualization. The most obvious invariants to start with are the total numbers of elements and relations (nodes and edges) in the system. For rules that involve only a single relation on the left-hand side, it is inevitable that these numbers must be determined by a linear recurrence (cf. [1:p890]). (For 1k -> nk rules, up to a k-term linear recurrence may be involved.) For rules that involve more than one relation more complicated behavior is common. Consider for example the rule: {{x, y}, {x, z}} -> {{y, w}, {w, x}, {z, w}} The number of relations generated over the first 50 steps (using our standard updating order) is: Taking third differences yields: One can consider many other invariants, including counts of in and out degrees of elements, and counts of cycles. In general, one can construct invariant “fingerprints” of hypergraphs—and the typical observation is that for rules whose behavior seems complex, almost all of these will exhibit extensive apparent randomness.
Limiting Behavior and Emergent Geometry 4.13 Statistical Mechanics : Features like dimension and curvature can be used to probe the consistent large-scale structure of the limiting behavior of our models. But particularly insofar as our models generate apparent randomness, it also makes sense to study their statistical features. We discussed above the overall distribution of values of Vr(X) and Δr(X). But we can also consider fluctuations and correlations. For example, we can look at a 2-point correlation function Sr(s) = (〈Vr(X)Vr(Y)〉 – 〈Vr(X)〉2)/〈Vr(X)〉2 for points X and Y separated by graph distance s. For a uniform graph such as the torus graph, Sr(s) always vanishes. For the buckyball approximation to the sphere that we used above, Sr(s) shows peaks at the distances between “pentagons” in the graph. For the rule {{x,y},{x,z}}->{{x,z},{x,w},{y,w},{z,w}}, Sr(s) steadily expands the region of s over which it shows positive correlations, and perhaps (at least for larger r, indicated by redder curves) approaches a limiting form: CorrelationFunctions/2242rule.wxf"] It is conceivable that for this or other rules there might be systematic rescalings of distance and number of steps that would lead to fixed limiting forms. In statistical mechanics, it is common to think about the ensemble of all possible states of a system—and for example to discuss evolution from all possible initial conditions. But typical systems in statistical mechanics can basically be discussed in terms of a fixed number of degrees of freedom (either coordinates or values). For our models, there is no obvious way to apply the rules but, for example, to limit the total number of relations—making it difficult to do analysis in terms of ensembles of states. One can certainly imagine the set of all possible hypergraphs (and even have Ramsey-theory-style results about it), but this set does not appear to have the kind of geometry or structure that has typically been necessary for results in statistical mechanics or dynamical systems theory. (One could however potentially think in terms of a distribution of adjacency matrices, limiting to graphon-like functions [40] for infinite graphs.)
Limiting Behavior and Emergent Geometry 4.14 The Effect of Perturbations : Imagine that at some step in the evolution of a rule one reverses a single relation. What effect will it have? Here is an example for the rule {{x,y},{x,z}}->{{x,z},{x,w},{y,w},{z,w}}. The first row is the original evolution; the second is the evolution after reversing the relation: We can illustrate the effect by coloring edges in the first row of graphs that are different in the second one (taking account of graph isomorphism) [41]: Visualizing the second and third graphs in 3D makes it more obvious that the changed edges are mostly connected: It takes only a few steps before the effect of the change has spread to essentially all parts of the system. (In this particular case, with the updating order used, about 20% of edges are still unaffected after 5 steps, with the fraction slowly decreasing, even as the number of new edges increases.) In rules with fairly simple behavior, it is common for changes to remain localized: However, when complex behavior occurs, changes tend to spread. This is analogous to what is seen, for example, in the much simpler case of class 2 versus class 3 cellular automata [31][1:6.3]: Cellular automata are also known [31] to exhibit the important phenomenon of class 4 behavior—in which there is a discrete set of localized “particle-like” structures through which changes typically propagate: In cellular automata, there is a fixed lattice on which local rules operate, making it straightforward [1:6.3] to identify the region that can in principle be affected by a change in initial conditions. In the models here, however, everything is dynamic, and so even the question of what parts can in principle be affected by a change in initial conditions is nontrivial. As we will discuss at length later, however, it is always possible to trace which updating events in a particular evolution depend on which others, and which relations are associated with these. The result will always be a superset of the actual effect of a change in the initial condition: We discussed above the quantity Vr(X) obtained by “statically” looking at the number of nodes in a hypergraph reached by going graph distance r—in effect computing the volume of a ball of radius r in the hypergraph. By looking at the dependence of updating events in t successive steps of evolution, we can define another quantity Ct(X) which in effect measures the volume of a cone of dependencies in the evolution of the system. Vr(X) is in a sense a quantity that is “applied” to the system from outside; Ct(X) is in a sense intrinsic. But as we will discuss later, Vr(X) is in some sense an approximation to Ct(X)—and particularly when we can reasonably consider the evolution of a model to have reached some kind of “equilibrium”, Vr(X) will provide a useful characterization of the “state” of a model.
Limiting Behavior and Emergent Geometry 4.15 Geodesics : Given any two points in a graph or hypergraph one can find a (not necessarily unique) shortest path (or “geodesic”) between them, as measured by the number of edges or hyperedges traversed to go from one point to the other. Here are a few examples of such geodesics: A geodesic in effect defines the analog of a straight line in a graph or hypergraph, and by analogy with the way geodesics work in continuous spaces, we can use them to probe emergent geometry. For example, in the case of positive curvature, we can expect that nearby geodesics diverge, while in the case of negative curvature they converge: One can see the same effect in sufficiently large graphs (although it can be obscured by regularities in graphs which lead to large numbers of “degenerate” geodesics, all of the same length): We saw before that the growth rate of the volume Vr(X) of a ball centered at some point X in a graph could be identified as giving a measure of the Ricci scalar curvature R at X. But we can now consider tubes formed from balls centered at each point on a geodesic. And from the growth rates of volumes of these tubes we will be able to measure what can be identified as different components of curvature associated with the Ricci tensor (cf. [1:p1048][27][42]). In a continuous space (or, more precisely, on a Riemannian manifold) the infinitesimal volume element at a point X is given in terms of the metric tensor g by . If we look at a nearby point X + δx we can expand in a power series in δx (e.g. [43]) where Rij is the Ricci tensor and the δi (contravariant vectors) are orthogonal components of δx (say along axes defined by some coordinate system). If we integrate over a ball of radius r in d dimensions, we recover our previous formula for the volume of a ball where R = ∑Rii is the Ricci scalar curvature. But now let us consider integrating over a tube of radius r that goes a distance δ along a geodesic starting at X. Then we get a formula for the volume of the tube (cf. [44]) where the  are components of unit vectors along the geodesic. There is now a direct analog in our hypergraphs: just as we measured the growth rates of geodesic balls to find Ricci scalar curvature, we can now measure growth rates of geodesic tubes to probe full Ricci curvature. To construct an example, consider a graph formed from a mesh on the surface of an ellipsoid. (It is important that this mesh is intrinsic to the surface, with each mesh element corresponding to about the same surface area—and that the mesh does not just come from, say, a standard θ, ϕ coordinate grid.) As a first step, consider balls of progressively larger radii at different points on the ellipsoid mesh graph: In the region of higher curvature near the tip, the area of the ball for a given radius is smaller, reflecting higher values of the Ricci scalar curvature R there: But now consider tubes around geodesics on the ellipsoid mesh graph. Instead of measuring the scalar curvature R, these instead in effect measure components of the Ricci tensor along these geodesics. To measure all the components of the Ricci tensor, we could consider not just a tube but a bundle of geodesics, and we could look at the sectional curvature associated with deformations of the shape of this bundle. Or, as an alternative, we could consider tubes along not just one, but two geodesics through a given point. But in both cases, the analogy with the continuous case is easiest if we can identify something that we can consider an orthogonal direction. One way to do this on a graph is to start from a particular geodesic through a given point, then to look at all other geodesics through that point, and work out which ones are the largest graph distance away. These show sequences of progressively more distant geodesics (as measured by the graph distance to the original geodesic of their endpoints): In general there may be many choices of these geodesics—and in a sense these correspond to different local choices of coordinates. But given particular choices of geodesics we can imagine using them to form a grid. Looking at growth rates of volumes on this grid then gives us results not just about the Ricci tensor, but also about the Riemann tensor, about parallel transport and about covariant derivatives (cf. [27]). The examples we have shown so far all involve graphs that have a straightforward correspondence with familiar geometry. But exactly the same methods can be used on the kinds of graphs and hypergraphs that arise from our models. This shows tubes of successively larger radii along two different geodesics: In the limit of a large number of steps, we can measure the volumes of tubes like these to compute approximations to projections of the Ricci tensor—and for example determine the level of isotropy of the emergent geometry of our models.
Limiting Behavior and Emergent Geometry 4.16 Functions on Graphs : Traditional Riemannian manifolds are full of structure that our hypergraphs do not have. Nevertheless, we are beginning to see that there are analogs of many ideas from geometry and calculus on manifolds that can be applied to our hypergraphs—at least in some appropriate limit as they become sufficiently large. To continue the analogy, consider trying to define a function on a hypergraph. For a scalar function, we might just assign a value to each node of the hypergraph. And if we want the function to be somehow smooth, we should make sure that nearby nodes are assigned similar values. But what about a vector function? An obvious approach is just to assign values to each directed edge of the hypergraph. And given this, we can find the component in a direction corresponding to a particular geodesic just by averaging over all edges of the hypergraph along that geodesic. (To recover results for continuous spaces, we must take all sorts of potentially intricate limits.) (At a slightly more formal mathematical level, to define vectors in our system, we need some analog of a tangent space. On manifolds, the tangent space at a point can be defined in terms of the equivalence class of geodesics passing through that point. In our systems, the obvious analog is to look at the edges around a point, which are exactly what any geodesic through that point must traverse.) For a rank-p tensor function, we can assign values to p edges associated either with a single node, or with a neighborhood of nearby nodes. And, once again, we can compute “projections” of the tensor in particular “directions” by averaging values along p geodesics. The gradient of a scalar function ∇f at a particular point X can be defined by starting at X and seeing along what geodesic the (suitably averaged) values decrease fastest, and at what rate. The results of this can then be assigned to the edges along the geodesic so as to specify a vector function. The divergence of a vector function  can be defined by looking at a ball in the hypergraph, and asking for the total of the values of the function on all hyperedges in the ball. The analog of Gauss’s theorem then becomes a fairly straightforward “continuity equation” statement about sums of values on edges inside and at the surface of part of a hypergraph.
Limiting Behavior and Emergent Geometry 4.17 Manifolds and Model Spaces : We saw above a rule which generates a sequence of hypergraphs like this: We can think of this after an infinite number of steps as giving an infinitely fine mesh—or in effect a structure which limits to a two-dimensional manifold. In standard mathematics, the defining feature of a manifold is that it is locally like Euclidean space (in some number of dimensions d) [45]. By using Euclidean space as a model many things can be defined and computed about manifolds (e.g. [26]). Some of our models here yield emergent geometry whose limit is an ordinary manifold. But the question arises of what mathematical structures might be appropriate for describing the limiting behavior of other cases. Is there perhaps some other kind of model space whose properties can be transferred? It is tempting to try to start from Euclidean space (or n), and define some subset such as a Cantor set. But it seems more likely to be fruitful to start from convenient discrete structures, and see how their limits might correspond to what we have. One important feature of Euclidean space is its uniformity: every point is in a sense like every other, even if different points can be labeled by different coordinates. So this suggests that by analogy we could consider graphs (and hypergraphs) whose vertices all have the same graph neighborhood (vertex transitive graphs). Several obvious infinite examples are the limits of: Any uniform tessellation or regular tree provides an example. One might think that another example would be graphs formed by uniform application of a vertex substitution rule—such as “spherical Sierpiński graphs” starting from a tetrahedron, dodecahedron or buckyball graph: But (as mentioned in 4.8) in these graphs not every vertex has exactly the same neighborhood, at least if one goes beyond geodesic distance 2. The number of distinct neighborhoods does, however, grow fairly slowly, suggesting that it may be possible to consider such graphs “quasi vertex transitive” (in rough analogy to quasiconformal).But one important class of graphs that are precisely vertex transitive are Cayley graphs of groups—and indeed the infinite tessellation and tree graphs above are all examples of these. (Note that not all vertex-transitive graphs are Cayley graphs; the Petersen graph is an example [46]. It is also known that there are infinite vertex-transitive graphs that are not Cayley graphs [47], and are not even “close” to any such graphs [48].) In a Cayley graph for a group, each node represents an element of the group, and each edge is labeled with a generator of the group. (Different presentations of the group—with different choices of generators and relations—can have slightly different Cayley graphs, but their infinite limits can be considered the same.) Each point in the Cayley graph can then be labeled (typically not uniquely) by a word in the group, specified as a product of generators of the group. One can imagine progressively building up a Cayley graph by looking at longer and longer words. In the case of a free group with two generators A and B, this yields: If one adds the relation AB = BA, defining an Abelian group, the Cayley graph is instead a grid, with “coordinates” given by the numbers of As and Bs (or their inverses): Here are the Cayley graphs for the first few symmetric and alternating (finite) groups: For a given infinite Cayley graph, one can compute a limiting Vr just as we have for hypergraphs. If one picks a finite number of generators and relations at random, one will usually get a Cayley graph that has a basically tree-like structure, with a Vr that grows exponentially. For nilpotent groups, however, Vr always has polynomial growth—an example being the Heisenberg group H3 whose Cayley graph is the limit of [49]: There are also groups known that yield growth intermediate between polynomial and exponential [50]. There do not, however, appear to be groups that yield fractional-power growth, corresponding to finite but fractional dimension. It is possible that one could view the evolution of one of our models as being directly analogous to the growth of the Cayley graph for a group—or at least somehow approximated by it. As we discussed above, the hypergraphs generated by most of our systems are not, however, uniform, in the sense that the structures of the neighborhoods around different points in the hypergraph can be different. But this does not mean that a Cayley graph could not provide a good (at least approximate) local model for a part of the hypergraph. And if this connection could be made, there might be useful results from modern geometric group theory that could be applied, for example in classifying different kinds of limiting behaviors of our systems. On a sufficiently small scale, any manifold is defined to be like Euclidean space. But if one goes to a slightly larger scale, one needs to represent deviations from Euclidean space. And a convenient way to do this is again to consider model spaces. The most obvious is a sphere (or in general a hyperellipsoid)—and this is what gives the notion of curvature. Quite what the appropriate analog even of this is in fractional dimensional space is not clear, but it would potentially be useful in studying our systems. And when there is a possibility for change in dimension as well as change in curvature, the situation is even less clear.
The Updating Process for String Substitution Systems 5.1 String Substitution Systems : The basic concept of our models is to define rules for updating collections of relations. But for a particular collection of relations, there are often multiple ways in which a given rule can be applied, and there is considerable subtlety in the question of what effects different choices can have. To begin exploring this, we will first consider in this section the somewhat simpler case of string substitution systems (e.g. [1:3.5][51]). String substitution systems have arisen in many different settings under many different names [1:p893], but in all cases they involve strings whose elements are repeatedly replaced according to fixed substitution rules. As a simple example, consider the string substitution system with rules {A -> AB, B -> BA}. Starting with A and repeatedly applying these rules wherever possible gives a sequence of results beginning with: The application of these particular rules is simple and unambiguous. At each step, every occurrence of A or B is independently replaced, in a way that does not depend on its neighbors. One can visualize the process as a tree: At step n, there are 2n elements, and the kth element is determined by whether the number of 1s in the base-2 decomposition of k is even or odd. (This particular case corresponds to the Thue–Morse sequence.) The evolution of the string substitution system can still be represented by a tree even if the replacements are not the same length. The rules {A -> B, B -> AB} yield the “Fibonacci tree”: In this case, the number of elements on the tth step is the tth Fibonacci number. For any neighbor-independent substitution system, the number of elements on the nth step is determined by a linear recurrence, and usually, but not always, grows exponentially. (Equivalently, the number of elements of each type on the nth step can be determined from the nth power of the transition matrix.) But consider now a substitution system with rules {A -> BBB, BB -> A}. If one starts with A, the first step is unambiguous: just replace A by BBB. But now there are two possible replacements for BBB: either replace the first BB to get AB, or replace the second one to get BA. We can represent all the different possibilities as a multiway system [1:5.6] in which there can be multiple outcomes at each step, and there are multiple possible paths of evolution (here shown over the course of 5 steps): We can represent the possible paths of 5 steps of evolution between states of the system by a graph: In effect each path through this graph represents a different possible history for the system, based on applying a different sequence of possible updates. By adding something extra to our model, we can of course force a particular history. For example, we could consider sequential substitution systems (analogous to search-and-replace in a text editor) in which we always do only the first possible replacement in a left-to-right scan of the state reached at each step. With this setup we get the following history for the system shown above: An alternative strategy (analogous, for example, to the operation of StringReplace in the Wolfram Language) is to scan from left to right, but rather than just doing the first possible replacement at each step, instead keep scanning after the first replacement, and also carry out every subsequent replacement that can independently be done. With this “maximum scan” strategy, the sequence of states reached in the example above becomes: (The first deviation occurs at BBBB. After replacing the first BB, the maximum scan strategy can continue and replace the second BB as well, thereby in effect “skipping a step” in the multiway evolution graph shown above.) Note that in both the strategies just described, the evolution obtained can depend on the order in which different replacements are stated in the rule. With the rule {BB -> A, A -> BBB} instead of {A -> BBB, BB -> A}, the sequential substitution system updating scheme yields: instead of: Given a particular multiway system, one can ask whether it can ever generate a given string. In other words, does there exist any sequence of replacements that leads to a given string? In the case of {A -> BBB, BB -> A}, starting from A, the strings B and BB cannot be generated, though with this particular rule, all other strings eventually can be generated (it takes 5(k – 1) steps to get all 2k strings of length k). One of the applications of multiway systems is as an idealization of derivations in equational logic [1:p777], in which the rules of the multiway system correspond to axioms that define transformations between equivalent expressions in the logical system. Starting from a state corresponding to a particular expression, the states generated by the multiway system are expressions that are ultimately equivalent to the original expression. The paths in the multiway system are then chains of transformations that represent proofs of equivalences between expressions—and the problem of whether a particular equivalence between expression holds is reduced to the (still potentially very difficult) problem of determining whether there is a path in the multiway system that connects the states corresponding to these expressions. Thus, for example, with the transformations {A -> BBB, BB -> A}, it is possible to see that A can be transformed to AAA, but the path required is 10 steps long: In general, there is no upper bound on how long a path may be required to reach a particular string in a multiway system, and the question of whether a given string can ever be reached is in general undecidable [52][53][1:p778].
The Updating Process for String Substitution Systems 5.2 The Phenomenon of Causal Invariance : Consider the rule {BA -> AB}, starting from BABABA: As before, there are different possible paths through this graph, corresponding to different possible histories for the system. But now all these paths converge to a single final state. And in this particular case, there is a simple interpretation: the rule is effectively sorting As in front of Bs by repeatedly doing the transposition BA -> AB. And while there are multiple different possible sequences of transpositions that can be used, all of them eventually lead to the same answer: the sorted state AAABBB. There are many practical examples of systems that behave in this kind of way, allowing operations to be carried out in different orders, generating different intermediate states, while always leading to the same final answer. Evaluation or simplification of (parenthesized) arithmetic (e.g. [54]), algebraic or Boolean expressions are examples, as is lambda function evaluation [55]. Many substitution systems, however, do not have this property. For example, consider the rule {AB -> AA, AB -> BA}, again starting from BABABA. Like the sorting rule, after a limited number of steps this rule gets into a final state that no longer changes. But unlike the sorting rule, it does not have a unique final state. Depending on what path is taken, it goes in this case to one of three possible final states: But what about systems that do not “terminate” at particular final states? Is there some way to define a notion of “path independence” [55]—or what we will call “causal invariance” [1:9.10]—for these? Consider again the rule {A -> BBB, BB -> A} that we discussed above: The state BBB at step 2 has two possible successors: AB and BA. But after another step, AB and BA converge again to the state BBBB. And in fact the same kind of thing happens throughout the graph: every time two paths diverge, they always reconverge after just one more step. This means that the graph in effect consists of a collection of “diamonds” of edges [56]: There is no need, however, for reconvergence to happen in just one step. Consider for example the rule {A -> AA, AA -> AB}: As the picture indicates, there are paths from AAA leading to both ABA and AAB—but these only reconverge again (to ABAB) after two more steps. (In general, it can take an arbitrary number of steps for reconvergence to occur.) Whether a system is causal invariant may depend on its initial conditions. Consider, for example, the rule AA -> AAB. With initial condition AABAA the rule is causal invariant, but with initial condition AAA it is not: When a system is causal invariant for all possible initial conditions, we will say that it is totally causally invariant. (This is essentially the confluence property discussed in the theory of term-rewriting systems.) Later, we will discuss how to systematically test for causal invariance—and we will see that it is often easier to test for total causal invariance than for causal invariance for specific initial conditions. Causal invariance may at first seem like a rather obscure property. But in the context of our models, we will see in what follows that it may in fact be the key to a remarkable range of fundamental features of physics, including relativistic invariance, general covariance, and local gauge invariance, as well as the possibility of objective reality in quantum mechanics.
The Updating Process for String Substitution Systems 5.3 States Graphs : If there are multiple successors to a particular state in a substitution system one thing to do would be just to assume that each of these successors is a new, unique state. The result of this will always be to produce a tree of states, here shown for the rule {A -> BBB, BB -> A}: But in our construction of multiway systems, we assume that actually the states produced are not all unique, and instead that states at a given step consisting of the same string can be merged, thereby reducing the tree above to the directed graph: But if we are going to merge identical states, why do so only at a given step? Why not merge identical states whenever they occur in the evolution of the system? After all, given the setup, a particular state—wherever it occurs—will always evolve in the same way, so in some sense it is redundant to show it multiple times. The particular rule {A -> BBB, BB -> A} that we have just used as an example has the special feature that it always “makes progress” and never repeats itself—with the result that a given string only ever appears once in its evolution. Most rules, however, do not have this property. Consider for example the rule {AB -> BAB, BA -> A}. Starting from ABA, here is our normal “evolution graph”: Notice that in this evolution even the original state ABA appears again, both at step 3 and at step 5—and each time it appears, it necessarily makes a complete repeat of the same evolution graph. To remove this redundancy, we can make a graph in which we effectively merge all instances of a given state, so that we show each state only once, connecting it to whatever states it evolves to under the rule: But in this graph there are, for example, two connections from ABA to BABA, because this transformation happens twice in the 4 steps of evolution that we are considering. But in a sense this multiplicity again always gives redundant information, so in what we will call our “states graph” [1:p209], we only ever keep one connection between any given pair of states, so that in this case we get: There is one further subtlety in the construction of a states graph. The graph only records which state can be transformed into which other: it does not record how many different replacements could be applied to achieve this. In the rule we just showed, it is never possible to have different replacements on a single string yield the same result. Consider the rule {A -> AA, A -> B} starting with AA. There are, for example, two different ways that this rule can be applied to AA to get AAA: In our standard states graph, however, we show only that AA is transformed to AAA, and we do not record how many different possible replacements can achieve this: The degree of compression achieved in going from evolution graphs to states graphs can be quite dramatic. For example, for the rule {BA -> AB, AB -> BA} the evolution graph is: and the states graph is: or in our standard rendering: Note that causal invariance works the same in states graphs as it does in evolution graphs: if a rule is causal invariant, any two paths that diverge must eventually reconverge.
The Updating Process for String Substitution Systems 5.4 Typical Multiway Graph Structures : Before considering further features of the updating process, it is helpful to discuss the typical kinds of multiway graphs that are generated from string substitution systems. Much as for our primary models based on general relations (hypergraphs), we can assign signatures to string substitution system rules based on the lengths of the strings in each transformation. For example, we will say that the rule {A -> BBB, BB -> A} has signature 2: 1->3, 2->1, where the initial 2 indicates the number of distinct possible elements (here A and B) that occur in the rule. With a signature of the form k: n1->n2, n3->n4, … there are nominally k∑ni possible rules. However, many of these rules are equivalent under renaming of elements or reversal of strings. Taking this into account, the number of inequivalent possible rules for various cases is: Rules with signature k: 1->n must always be totally causal invariant. If they are started from strings of length 1, their states graphs can never branch. However, with strings of length more than 1, branching can (but may not) occur, as in this example of A -> AB started from three strings of length 2: With initial conditions AAA and AAAA, this rule produces the following states graphs: Even the rule A -> AA can produce states graphs like these if it has initial conditions that contain Bs as “separators”: And as a seemingly even more trivial example, the rule A -> B with an initial condition containing n As gives a states graph corresponding to an n-dimensional cube (with a total of 2n nodes): With multiple rules, one can get tree-like structures, with exponentially increasing numbers of states. The simplest case is the 2: 0->1, 0->1 rule { -> A, -> B} starting with the null string: With multiple rules, even with single-symbol left-hand sides, causal invariance is no longer guaranteed. Of the 8 inequivalent 2: 1->1, 1->1 rules, all are totally causal invariant, although the rule {A -> B, B -> A} achieves this through cyclic behavior: Among the 14 inequivalent 3: 1->1, 1->1 rules, all but two are totally causal invariant. The exceptions are the cyclic rule, and also the rule {A -> B, A -> C}, which in effect terminates before its B and C branches can reconverge: The 12 inequivalent 2: 1->2, 1->1 rules yield the following states graphs when run for 4 steps starting from all possible length-3 strings of As and Bs: All but three of these rules are totally causal invariant. Among causal invariant ones are: After more steps these yield: Examples of non-causal invariant 2: 1->2, 1->1 rules are: After more steps these yield: When we look at rules with larger signatures, the vast majority at least superficially show the same kinds of behavior that we have already seen. Like for the hypergraphs from our models that we considered in previous sections, we can study the limiting structure of states graphs generated in multiway systems, and see what emergent geometry they may have. And in analogy to Vr(X) for hypergraphs, we can define a quantity Mt(S) which specifies the total number of distinct states reached in the multiway system after t steps of evolution starting from a state S. For the rule {A -> AB} mentioned above, the geometry of the multiway graph obtained by starting from n As is effectively a regular n-dimensional grid: Some rules create states graphs that are trees, with Mt~mt. Other rules do not explicitly give trees, but still give exponentially increasing Mt. An example is {A -> AB, B -> A}, for which Mt is a sum of Fibonacci numbers, and successive states graphs can be rendered as: Note that rules with both polynomial and exponential growth in Mt can exhibit causal invariance. It is not uncommon to find rules with fairly long transients. A simple example is the totally causal invariant rule {A -> BBB, BBBB -> A}. When started from n As this stabilizes after 7n steps, with ~2.8n states, with a states graph like: Rules typically seem to have either asymptotically exponential or asymptotically polynomial Mt. (This may have some analogy with what is seen in the growth of groups [57][58][22].) Among rules with polynomial growth, it is typical to see fairly regular grid-like structures. An example is the rule {AB -> A, ABA -> BBAABB} which from initial condition ABAA gives states graph: With the slightly different initial condition ABAAA, the states graph has a more elaborate structure and the form of Mt is more complicated (though ~t2 and ultimately quasiperiodic); the second differences of Mt in this case are: Another example of a somewhat complex structure occurs in the rule {ABA -> BBAA, BAA -> AAB} that was discussed in [1:p205]. Starting from BABBAAB it gives the states graph:
The Updating Process for String Substitution Systems 5.5 Testing for Causal Invariance : Causal invariance implies that regardless of the order in which updates are made, it is always possible to get to the same outcome. One way this can happen is if different updates can never interfere with each other. Consider the rule {A -> AA, B -> BB}. Every time one sees an A, it can be “doubled”, and similarly with a B. But these doublings can happen independently, in any order. Looking at an evolution graph for this rule we see that at each state there can be an “A replacement” applied, or a “B replacement”. There are two branches of evolution depending on which replacement is chosen, but one can always eventually reach the same outcome, regardless of which choice was made—and as a result the rule is causal invariant: The replacements A -> AA and B -> BB trivially cannot interfere because their left-hand sides involve different elements. But a more general way to guarantee that interference cannot occur is for the left-hand sides of replacements not to be able to overlap with each other—or with themselves. In general, the set of strings of As and Bs up to length 5 that do not overlap themselves are (up to A,B interchange and reversal): [1:p1033][59] These can be formed into pairs in the following ways: The first triples with no overlaps are of length 6. An example is {AAABB, ABAABB, ABABB}. And whenever there is a set of strings with no overlaps being used as the left-hand side of replacements, one is guaranteed to have a system that is totally causal invariant. It is also perfectly possible for the right-hand sides of rules to be such that the system is totally causal invariant even though their left-hand sides overlap. An example is the simple rule {A -> AA, AA -> A}: At every branch point in a multiway system, one can identify all pairs of strings that can be generated. We will call such pairs of strings branch pairs (they are often called “critical pairs” [60]). And the question of whether a system is causal invariant is then equivalent to the question of whether all branch pairs that can be generated will eventually resolve, in the sense that there is a common successor for both members of the pair [61][62][63][64]. Consider for example the rule {A -> AB, B -> A}: After two steps, a branch pair appears: {AA, ABB}. But after just one more step it resolves. However, two more branch pairs are generated: {AAB, ABA} and {ABA, ABBB}. But after another step, these also resolve. And in fact in this system all branch pairs that are ever generated resolve (actually always in just one step), and so the system is causal invariant. In general, it can, however, take more than one step for a branch pair to resolve. The simplest case involving resolution after two steps involves the rule: {A -> B, AB -> AA} The state AB generates the branch pair {BB, AA}: In the evolution graph, we do not see a resolution of this branch pair. But looking at the states graph, we see that the branch pair does indeed resolve in two steps, though with BB being a terminating state: The same kind of thing can also happen without a terminating state. Consider for example {A -> AA, AB -> BA} where the branch pair {AAB, BA} takes 2 steps to resolve. In the rule {A -> AA, AAB -> BA} it takes 3 steps for the branch pair {AAAB, BA} to resolve: Things can get quite complicated even with simple rules. For example, in the rule {A -> AA, AA -> BAB} the branch pair {AAA, BAB} resolves after 4 steps. The following is essentially a proof of this: But finding this in the 67-node 4-step states graph is quite complicated: In the rule {A -> AAB, ABBA -> A} it takes 7 steps for the branch pair {A, AABBBA} to resolve to the common successor AAAABBBAAB. The 7-step states graph involved has 5869 nodes, and the proof that the branch pair resolves is: In general, there is no upper bound on how long it may take a branch pair to resolve, or for example how long its common successor—or intermediate strings involved in reaching it—may be. Here are the simplest rules with two distinct elements that take successively longer to resolve (the last column gives the size of the states graph when resolution is found): Note that—like the first case of a 2-step resolution that we showed above—quite a few of these longest-to-resolve rules actually terminate, with a member of the branch pair being their final output. Thus, for example, the last case listed is really just a reflection of the fact that with this rule, AAAA takes 16 steps to reach the termination state B: (With 3 distinct elements similar results are seen; the first time a shorter example is seen is {A -> BAC, A -> AABA} at resolution-length 6.) Despite the existence of long-to-resolve cases, most branch pairs in most rules in practice resolve quickly: the fraction that take τ steps seems to decrease roughly like 2–τ. But there is still in a sense an arbitrarily long tail—and the general problem of determining whether a branch pair will resolve is known to be formally undecidable (e.g. [65]). One interesting feature of causal invariance testing is that (while still in principle undecidable) it is in some ways easier to test for total causal invariance than to test for partial causal invariance for specific initial conditions. The reason is that if a rule is going to be totally causal invariant then there is a certain core set of branch pairs that must resolve, and if all these resolve then the rule is guaranteed to be totally causal invariant. This core set of branch pairs is derived from the possible overlaps between left-hand sides of replacements, and are the successors of the minimal “unifications” of these left-hand sides, formed by minimally overlapping the strings. Consider the rule: {AA -> A, AB -> BAA} The only possible overlap between the left-hand sides is A. This leads to the minimal unification AAB, which yields the branch pair {ABAA, AB}: From this branch pair we construct a states graph: And from this we see that the branch pair resolves in 3 steps. And because this is the only branch pair that can arise through overlaps between the left-hand sides, this resolution now establishes the total causal invariance of this rule. In the rule {AB -> BA, BAA -> A} there are two possible overlaps between the left-hand sides: A and B. These lead to two different minimal unifications: BAAB and ABAA. And these two unifications yield two branch pairs, {BABA, AB} and {BAAA, AA}. But now we can establish that both of these resolve, thus showing the total causal invariance of the original rule. In general, one might in principle have to continue for arbitrarily many steps to determine if a given branch pair resolves. But the crucial point here is that because the number of possible overlaps (and therefore unifications) is fine, there are only a finite number of branch pairs one needs to consider in order to determine if a rule is totally causal invariant. There is no need to look at branch pairs that arise from larger strings than the unifications; any additional elements are basically just “padding” that cannot affect the basic interference between replacements that leads to a breakdown of causal invariance. Looking at branch pairs from all possible unifications is a way to determine total causal invariance—and thus to determine whether a rule will be causal invariant from all possible initial conditions. But even if a rule is not totally causal invariant, it may still be causal invariant for particular initial conditions—effectively because whatever branch pairs might break causal invariance simply never occur in evolution from those initial conditions. In practice, it is fairly common to have rules that are causal invariant for some initial conditions, but not others. In effect, there is some “conservation law” that keeps the rule away from branch pairs that would break causal invariance—and that keeps the rule operating with some subset of its possible states that happen to yield causal invariance.
The Updating Process for String Substitution Systems 5.6 The Frequency of Causal Invariance : The plots below show the fractions of rules found to be totally causal invariant [66], as a function of the total number of elements they involve, for the cases of k = 2 (A, B) and k = 3 (A, B, C): The darker colors indicate larger numbers of steps to resolve branch pairs. (There is some uncertainty in these plots—conceivably as much as 9% for 10 total elements with k = 3—since in some cases the states graph became too big to compute before it could be determined whether all branch pairs resolved.) The dotted lines indicate rules are in a sense inevitably causal invariant because their left-hand sides involve strings that do not overlap themselves or each other, thereby guaranteeing total causal invariance. Rules such as AA -> AAA are causal invariant despite having overlapping left-hand sides because their right-hand sides in a sense give the same result whatever overlap occurs. Ignoring the structure of rules, one can just ask what fraction of strings are non-overlapping [67]. Out of the total of kn possible strings with length n containing k distinct elements the number that do not overlap themselves is given by [1:p1033]: This yields the following fractions (for the limit see e.g. [68][11:A003000]): One can also look at how many possible sets of s strings of length up to n allow no overlaps with themselves or each other [1:p1033]. The numbers and fractions for k = 2 are as follows: To get a sense of the distribution of non-overlapping strings, one can make an array that shows which pairs of strings (ordered lexicographically) do not allow overlaps. Here are the results for k = 2 and k = 3 for strings respectively up to length 6 and length 4, showing clear structure in the space of possible strings [51]:
The Updating Process for String Substitution Systems 5.7 Events and Their Causal Relationships : So far the nodes in our graphs have always been states generated by substitution systems. But we can also introduce nodes to represent the “updating events” associated with replacements performed on strings. Here is the result for evolution according to the rule {AB -> BAB, BA -> A} starting from ABA—with each event node indicating the string replacement to which it corresponds: We can also show this as a states graph, where we have merged instances of the same state that occur at different steps: States are connected through events. But how are events connected? Given two events the key question to ask is whether they are causally related. Does one event depend on the other—in the sense that all or part of its input comes from the output of the other event? Looking at the graph above, for example, the event  depends on  because  uses as input the A that arises as output from . On the other hand,  does not depend on  because the BA it consumes was not generated by . We can add this dependency information to the evolution graph by putting orange lines between events that are causally related: We can also do this in the states graph, in which we have merged instances of states from different steps: We can redraw this graph without the layering that puts the initial state at the top. We have added an “initialization event”  to indicate the creation of the initial condition: If we want to focus on causal relationships, we can now drop the state nodes altogether, and get a multiway causal graph that represents possible causal relationships between events: This causal graph is dual to our original evolution graph in the sense that edges in the original evolution graph correspond to events—which now become nodes in the causal graph. Similarly, each edge in the causal graph is associated with some state which appears as a node in the evolution graph. We can get a sense of “possible causal histories” of our system by arranging the multiway causal graph in layers: Just like every possible path through the multiway evolution graph gives a possible sequence of states that can occur in the evolution of a system, so also every possible path through the multiway causal graph gives a possible sequence of events that can occur in the evolution of the system. After 10 steps, the graph in our example has become quite complicated:
The Updating Process for String Substitution Systems 5.8 Causal Graphs for Particular Updating Sequences : The multiway causal graph that we have just constructed shows the causal relationships for all possible paths of evolution in a multiway system. But what if we just pick a single path of evolution? Instead of looking at the results for every possible updating order, let us pick a particular updating order. For example, for the rule above we could pick “sequential updating”, in which at each step, first for AB -> BAB and then for BA -> A, we scan the string from left to right, doing the first replacement we can [1:3.6]. This leads to a specific single path of evolution for the system (now drawn across the page): We can show the causal relationships between the events in this evolution: And we can generate a causal graph—which is very simple: But now let us consider a different updating scheme, in which now as we scan the string, we try at each position both AB -> BAB and BA -> A, then do the first replacement we can. This procedure again leads to a specific single path of evolution, but it is a different one from before: This particular path just involves an alternation between the states ABA and BABA, so the states graph is a cycle: Including causal relationships here we get: The resulting causal graph is then: This causal graph is quite different from the one we got for the previous updating scheme. But both individual causal graphs necessarily occur in the whole multiway causal graph: Given the multiway causal graph, we can explicitly find all possible individual causal graphs (not showing individual loop configurations separately): And what this shows is that at least with the particular rule we are looking at, there are many different classes of evolution paths, that can lead to distinctly different individual causal graphs—and therefore causal histories.
The Updating Process for String Substitution Systems 5.9 The Significance of Causal Invariance : One might think that all rules would work like the one we just studied, and would give different causal graphs depending on what specific path of evolution one chose, or what updating scheme one used. But a crucial fact that will be central to the potential application of our models to physics is this is not the case. Instead, whenever a rule is causal invariant, it does not produce many different causal graphs. Instead, whatever specific path of evolution one choses, the rule always yields an exactly equivalent causal graph. The rule we just studied is not causal invariant. But consider instead the simple causal invariant rule {BA -> AB} evolving from the state BBBAA: Adding in causal relationships this becomes: The corresponding multiway causal graph is: But now consider the individual causal graphs, corresponding to different possible paths of evolution. From the multiway causal graph we can extract all of these, and the result is: There are five different cases. But the remarkable fact is that they all correspond to isomorphic graphs. In other words. even though the specific sequence of states that the system visits is different in each case, the network of causal relationships is always exactly the same, regardless of what path is followed. And this is a general feature of any causal invariant system. The underlying evolution rules for the system may allow many different paths of evolution—and many different sequences of states. But when the rules for the system are causal invariant, it means that the network of relationships between updating events is always the same. Depending on the particular order in which updates are done, one can see different paths of evolution; but if one looks only at event updates and their relationships, there is just one thing the system does. Here is a slightly larger example for the rule {BA -> AB} starting from BBBBAAAA. In each case a different random updating order is used, leading to a different sequence of states. But the final causal graphs representing the causal relationships between events are exactly the same:
The Updating Process for String Substitution Systems 5.10 Causal Foliations and Causal Cones : We have discussed causal invariance in terms of path independence in multiway systems. But we can also explore it in terms of specific evolution histories for the underlying substitution system. Consider the rule {BA -> AB}. Here is a representation of one way it can act on a particular initial string: In a multiway system, each path represents a particular sequence of updates that occur one after another. But here in showing the action of {BA -> AB} we are choosing to draw several updates on the same row. If we want, we can think of these updates as being done in sequence, but since they are all independent, it is consistent to show them as we do. If we annotate the picture by showing causal connections between updates, we see the causal graph for the evolution—and we see that the updates we have drawn on the same row are indeed independent: they are not connected in the causal graph: The picture above in effect uses a particular updating order. The pictures below show three possible random choices of updating orders. In each case, the final result of the evolution is the same. The intermediate steps, however, are different. But because our rule is causal invariant, the causal graph of causal relationships always has exactly the same form: In picking updating orders there is always one constraint: no update can happen until the input for it is ready. In other words, if the input for update V comes from the output of update U, then U must already have happened before V can be done. But so long as this constraint is satisfied, we can pick whatever order of updates we want (cf. [1:9.10]). It is sometimes convenient, however, to think in terms of “complete steps of evolution” in which all updates that could yet be done have been done. And for the particular rule we are currently discussing, we can readily do this, separating each “complete step” in the pictures below with a red line: Where the red lines go depends on the update order we choose. But because of causal invariance it is always possible to draw them to delineate steps in which a collection of independent updates occur. Each choice of how to assign updates to steps in effect defines a foliation of the evolution. We will call foliations in which the updates at each step are causally independent “causal foliations”. Such causal foliations are in effect orthogonal to the connections defined by the causal graph. (In physics, the analogy is that the causal foliations are like foliations of spacetime defined by a sequence of spacelike hypersurfaces, with connections in the causal graph being timelike.) The fact that our underlying substitutions (in this case just BA -> AB) involve neighboring elements implies a certain locality to the process of evolution. The consequence of this is that we can meaningfully discuss the “spatial” spreading of causal effects. For example, consider tracing the causal connections from one event in our system: In effect there is a cone (here just two-dimensional) of elements in the system that can be affected. We will call this the causal cone for the evolution. (In physics, the analogy is a light cone.) If we pick a different updating order, the causal cone is distorted. But viewed in terms of the causal foliations, it is exactly the same: This is the result purely in terms of the causal graph: Now let us turn things around. Imagine we have a causal graph. Then we can ask how it relates to an actual sequence of states generated by a particular path of evolution. The pictures below show how we can arrange a causal graph so that its nodes—corresponding to events—appear at positions down the page that correspond to a particular causal foliation and thus a particular path of evolution: It is worth noticing that at least for the rule we are using here the intrinsic structure of the causal graph defines a convenient foliation in which successive events are simply arranged in layers: The rule BA -> AB that we are using has many simplifying features. But the concepts of causal foliations and causal cones are general, and will be important in much of what follows. As it happens, we have already implicitly seen both ideas. The “standard updating order” for our main models defines a foliation (similar, in fact, to the last one we showed here, in which in some sense “as much gets done as possible” at each step)—though the foliation is only a causal one if the rule used is causal invariant. In addition, in 4.14 we discussed how the effect of a small change in the state in one of our models can spread on subsequent steps, and this is just like the causal cone we are discussing here.
The Updating Process for String Substitution Systems 5.11 Causal Graphs for Infinite Evolutions : One of the simplifying features of the rule BA -> AB discussed in the previous subsection is that for any finite initial condition, it always evolves to a definite final state after a finite number of steps—so it is possible to construct a complete multiway causal graph for it, and for example to verify that all the causal graphs for specific paths of evolution are identical. But consider the rule {A -> BB, B -> A}: This rule is causal invariant, but never evolves to a definite state, and instead keeps growing forever. Including events in the evolution we get: The corresponding multiway causal graph is: And now if we extract possible individual causal graphs from this, we get: These look somewhat similar, but they are not directly equivalent. And the reason for this has to do with how we are “counting steps” in the evolution of our system. If we evolve for longer, the effect becomes progressively less important. Here are causal graphs generated by a few different randomly chosen specific sequences of updates (each corresponding to a specific path through the multiway system): Here are the corresponding results after a few more updates: This is a different rendering: And this is what happens after many more updates (with a somewhat more systematic ordering): If we continued for an infinite number of updates, all these would give the same result—and the same infinite causal graph, just as we expect from causal invariance. But in the particular cases we are showing, they are being cut off in different ways. And this is directly related to the causal foliations we discussed in the previous subsection. Here are examples of evolution with specific choices of updating orders: Adding causal graphs we get: Here is what happens if we continue these for longer: And here are the causal graphs that correspond to these evolutions:
The Updating Process for String Substitution Systems 5.12 Typical Causal Graphs : Any causal invariant system always ultimately has a unique causal graph. The graph can be found by analyzing any possible evolution for the system, with any updating scheme—though for visualization purposes, it is usually useful to use an updating scheme where as much happens as possible at each step. The trivial causal invariant rule A -> A starting from A has causal graph: Starting from a string of 10 As it has causal graph: A -> AA has a causal graph starting from A that is a binary tree which can also be rendered: The rule {A -> A, A -> AA} starting from A gives a “two-step binary tree” with  nodes at level t: One does not have to go beyond rules involving just a single element (all of which are causal invariant) to find a range of causal graph structures. For example, here are all the forms obtained by rules allowing up to 6 instances of a single element A, with initial condition AA: A notable case is the rule: {AA -> AAA} Shown in layered form, the first few steps give: After a few more steps, this can be rendered as: Running the underlying substitution system AA -> AAA updating as much as possible at each step (the StringReplace scheme), one gets strings with successive lengths which follow the recurrence: Other rules of the form Ap -> Aq for non-commensurate p and q give similar results, analogous to tessellations in hyperbolic space: Rules that involve multiple replacements can give similar behavior even starting from a single A: Rules just containing only As cannot progressively grow to produce ordinary tilings. One can get these with the “sorting rule” {BA -> AB} which when started with 20 BAs yields: There are also rules which “grow” grid-like tilings. For example, the rule {A -> AB, BB -> BB} starting from a single A produces which is equivalent to a square grid: There is also a simple rule that generates essentially a hexagonal grid: {A -> B, B -> AB, BA" -> A} Other forms of causal graphs produced by simple causal invariant substitution systems include (starting from A, AB or ABA): When rules terminate they yield finite causal graphs. But these can often be quite complicated. For example, the rule {A -> BBB, BBBB -> A} started from strings consisting of from 1 to 6 As yields the following finite causal graphs: With a string of 50 As, the rule gives the finite causal graph: Compared to the hypergraphs we studied in previous sections, or even the multiway graphs from earlier in this section, the causal graphs here may seem to have rather simple structures. But there is a good reason for this. While there can be many updating events in the evolution of a string substitution system, all of them are in a sense arranged on the same one-dimensional structure that is the underlying string. And since the updating rules we consider involve strings of limited length, there is inevitably a linear ordering to the events along the string. This greatly simplifies the possible forms of causal graphs that can occur, for example requiring them always to remain planar. In the next section, we will see that for our hypergraph-based models—which have no simplifying underlying structure—causal graphs can be considerably more complex.
The Updating Process for String Substitution Systems 5.13 Limits of Causal Graphs : Much as we did for hypergraphs in section 4, we can consider the limiting structures of causal graphs after a large number of steps. And much as for hypergraphs, we can potentially describe these limiting structures in terms of emergent geometry. But one difference from what we did for hypergraphs is that for causal graphs, it is essential to take account of the directedness of their edges. It is still perfectly possible to have a limit that is like a manifold, but now to measure its properties we must generate the analog of cones, rather than balls. Consider for example the simple directed grid graph: Now consider starting from a particular node, and constructing progressively larger “cones”: We can call the number of nodes in this cone after t steps Ct. In this case the result (for t below the diameter of the graph) is: And in the limit of large graphs, we will have: We can also set up a 3D directed grid graph and generate a similar cone for which now Ct~ t3. In general, we can think about the limits of these grid graphs as generating a d-dimensional “directed space”. There is also nothing to prevent having cyclic versions, such as and in general a family of graphs that are going to behave like d-dimensional directed space in the limit will have Ct~ td. In direct analogy to what we did with hypergraphs in section 4, we can compute Ct for causal graphs, and then estimate effective dimension by looking at its growth rate. (There are some additional subtleties, though, because whereas at any given step in the evolution of the system, Vr can be computed for any r for any point in a hypergraph, Ct can be computed only until t “reaches the edge of the causal graph” from that starting point—and later we will see that the cutoff can also depend on the foliation one uses.) Consider the substitution system: {A -> AB, BB -> BB} This generates the causal graph: The log differences of Ct averaged over all points for causal graphs obtained from 10 through 100 steps of evolution have the form (the larger error bars for larger t in each case are the result of fewer starting points being able to contribute): As expected, for t small compared to the number of steps, the limiting estimated dimension is 2. Most of the other causal graphs shown in the previous subsection do not have finite dimension, however. For example, for the rule AA -> AAA the causal graph has the form which increases exponentially, with . The limits of the grid graphs we showed above essentially correspond to flat d-dimensional directed space. But we can also consider d-dimensional directed space with curvature. Although we cannot construct a complete sphere graph that is consistently directed, we can construct a partial sphere graph: In a layered rendering, this is: Once again, we can compute the log differences of Ct (the similarity of sphere graphs means that using larger versions does not change the result): The systematic deviation from the d = 2 result is—like in the hypergraph case—a reflection of curvature.
The Updating Process for String Substitution Systems 5.14 Foliations and Coordinates on Causal Graphs : One way to describe a causal graph is to say that it defines the partial ordering of events in a system—or, in other words, it is a representation of a poset. (The actual graph is essentially the Hasse diagram of the poset (e.g. [69]).) Any particular sequence of updating events can then be thought of as a particular total ordering of the events. As a simple example, consider a grid causal graph (as generated, for example, by the rule BA -> AB): One total ordering of events consistent with all the causal relations in the graph is a “breadth-first scan” [70]: But another possible ordering is a “depth-first scan” [71]: Another conceivable ordering would be: And in general there are many orderings consistent with the relations defined by the causal graph. For the particular graph shown here, out of the n! conceivable orderings of nodes 1 through n, the orderings consistent with the causal relations correspond to possible Young tableaux, and the number of them is equal to the number of involutions (self-inverse permutations) of n elements (e.g. [11:A000085]) which is asymptotically a little larger than  times n!. Here are the possible causal orderings that visit nodes 1 through 6 above (out of all 6! = 720 orderings): And here are all 76 possible causal orderings that visit any six nodes starting with node 1: But while a great many total orderings are in principle possible, if one wants, for example, to think about large-scale limits, one usually wants to restrict oneself to orderings that can specified by (“reasonable”) foliations. The idea of a foliation is to define a sequence of slices with the property that events on successive slices must occur in the order of the slices, but that events within a slice can occur in any order. So, for example, an immediate possible foliation of the causal graph above is just: This foliation specifies that event 1 must happen first, but then events 2 and 3 can happen in any order, followed by events 4, 5 and 6 in any order, and so on. But another consistent foliation takes diagonal slices (with actual locations of events in the diagram being thought of as the centers of the boxes): And so long as the diagonals are not steeper than the connections in the causal graph, this foliation will again lead to orderings that are consistent with the partial order defined by the causal graph. (For example, here event 1 must occur first, followed by event 2, followed by events 3 and 4 in any order, and so on.) Particularly if the diagonals are steeper, multiple events will often happen in a single slice (as we see with events 4 and 7 here): Now let us consider how this relates to taking the limit of a large number of events. If it were not for the directedness of the graph, we could do as we did in 4.17, and just imagine a process of refinement that leads to a manifold. But the Euclidean space that is the model for the manifold does not immediately have a way to capture the directedness of the graph, and so we need to do a little more. But this is a place where foliations help. Because within a slice of a foliation we have events that can happen in any order. And at least for our string substitution system, the events can be thought of in the limit as being on a one-dimensional manifold, with a coordinate related to position on the string. And then there is just a second coordinate that is the index of the slices in the foliation. But if the limit of our causal graph is a continuous space, we should be able to have a consistent notion of “distance between events”. For events that are “out of order”, the distance should be undefined (or perhaps infinite). But for other events, we should be able to compute the distance in terms of the coordinate (say t) that indexes the slices in the foliation and the coordinate (say x) within the slices. Given the particular setup of diagonal slices on a causal graph that is a grid, there is a unique distance function that is independent of the angle of the slices, which can be expressed in terms of the coordinate differences Δt and Δx: This function is exactly the standard Minkowski metric for a Lorentzian manifold [72][73], and we will encounter it again in section 8 when we discuss potential connections to physics. But here the metric is simply an abstract way to express distance in the limit of our causal graphs for string substitution systems. What happens if we use a different foliation? For example, a foliation like the following also leads to orderings of events that are consistent with the partial ordering required by the causal graph: The process of limiting to a manifold is more complicated here. We can start by defining a “lapse function” α(t,x) (in analogy with the ADM formalism of general relativity [74][75]) which effectively says “how thick” each slice of the foliation is at each position. (If we also wanted to skew our foliations, we could include a “shift vector” as well.) And in the limit we can potentially define a distance by integrating along the shortest path from one point to another. In a sense, however, even by imagining that there is a reasonable function α(t,x) that depends on real variables t and x we are implicitly assuming that our foliation has a certain simplicity and structure—and is not trying to reproduce some of the more circuitous total orderings at the beginning of this subsection. But in a sense the question of what type of foliation we need to consider depends on what we want to use it for. And in making potential connections with physics, foliations will in effect be how we parametrize observers. And as soon as we assume that observers are limited in their computational capabilities, this puts constraints on the types of foliations we need to consider.
The Updating Process for String Substitution Systems 5.15 The Concept of Branchial Graphs : Causal graphs provide one kind of summary of the evolution of a system, based on capturing the causal relationships between events. What we call branchial graphs provide another kind of summary, based on capturing relationships between states on different branches of a multiway system. And whereas causal graphs capture relationships between events at different steps in the evolution of a system, branchial graphs capture relationships between states on different branches at a given step. And in a sense they define a map for exploring branchial space in a multiway system. One might perhaps have imagined that states on different branches of a multiway system would be completely independent. But when causal invariance is present they are definitely not—because for example whenever they split (to form a branch pair), they will always merge again. Consider the multiway evolution graph (for the rule {A -> AB, B -> A}): Look at the second-to-last step shown. This contains the following unresolved branch pairs: The two states in each of these branch pairs are related, in the sense that they diverged from a common ancestor—and will converge to a common successor. We form the branchial graph by connecting the states that appear in newly unresolved branch pairs at a given step. For the steps shown in the evolution graph above, the successive branchial graphs are: For the next few steps, the states’ branchial graphs are: For the rule shown here, the number of nodes at the tth step is the tth Fibonacci number, or ∼ϕt for large t. The graphs are highly connected, but far from completely so. The number of edges ~ 2t, while the diameter is . The degree of transitivity (i.e. whether X being connected to Y and Y being connected Z implies X being connected to Z) [76] gradually decreases with t. As a measure of uniformity, one can look at the local clustering coefficient [77] (which measures to what extent there are local complete graphs): The graphs have somewhat complex vertex degree distributions (here shown after 10 and 15 steps): For the rule we are showing here, the branchial graph turns out to have a particularly simple interpretation as a map of the level of common ancestry of states. By definition, if two states are directly connected on the branchial graph, it means they had an immediate common ancestor one step before. But what does it mean if two states are graph distance 2 apart? The particular rule shown here has the property that it is causal invariant, but also that all branch pairs resolve in just one step. And from this it follows that states that are distance 2 apart on the branchial graph must have a common ancestor 2 steps back. And in general the distance on the branchial graph is equal to the number of steps one must go back before one gets to a common ancestor. The following histograms show the distribution of graph distances between all pairs of states at steps 10 and 15—or alternatively, the distribution of how many steps it has been since the states had a common ancestor (for this rule the mean increases roughly like ): We are defining the branchial graph to be based on looking at branch pairs that are unresolved at a particular step, and are new at that step. But we could generalize to consider a “thickened” branchial graph that includes all branch pairs that have been new within the past m steps. By doing this we can capture common ancestry of states even when they are associated with branch pairs that take up to m steps to resolve—but when we do this it is at the cost of having many additions to the graph associated with branch pairs that have “come and gone” within our thickening depth. It should be noted that any possible interpretation of branchial graphs in terms of common ancestors depends on having causal invariance. Absent causal invariance, there is no guarantee that states with common ancestors will even be connected in the branchial graph. As an extreme example, consider the rule: {A -> AB, A -> AC} The multiway graph for this rule is a tree and its branchial graph on successive steps just consists of a collection of disconnected pieces: By thickening the branchial graph by m steps, one could capture m steps of common ancestry. And in general one could imagine infinitely thickening the graph, so that one looks all the way back to the initial conditions. But the branchial graph one would get in this way would essentially just be a copy of the whole multiway graph. When a rule has branch pairs that take several steps to resolve, it is possible for the branchial graph to be disconnected, even when the rule is causal invariant. Consider for example the rule {A -> AA, A -> BAB} in which the branch pair {BAAB, BBABB} takes 3 steps to resolve. The first few branchial graphs for this rule are: It is fairly common to get branchial graphs in which a few disconnected pieces are “thrown off” in the first few steps, and never recombine. Sometimes, however, disconnected pieces can recombine. The rule {A -> BB, BB -> AA} starting from initial condition A yields the sequence of branchial graphs: Sometimes the branchial graph can break into many components. This happens for the rule {AA -> AAAB} starting from initial condition AA: The causal graph for this rule reveals that there is also causal disconnection in this case:
The Updating Process for String Substitution Systems 5.16 Typical Forms of Branchial Graphs : As a first example, consider the (causal invariant) rule which effectively just creates either A or B from nothing: {"" -> A, "" -> B} At step t, this rule produces all 2t possible strings. Its multiway way graph is: The succession of branchial graphs is then: The graph on step t has 2t nodes and 2t–2 (t2 – t + 4) – 1 edges. The distance on the graph between two states is precisely the difference in the total number of As (or Bs) between them, plus 1—so combining states which differ only through ordering of A and B the last graph becomes: With the rule {"" -> A, "" -> B, "" -> C} the sequence of branchial graphs is and in the last case combining states which differ only in the order of elements, one gets: Note that no rule involving only As can have a nontrivial branchial graph, since all branch pairs immediately resolve. Consider now the rule: {A -> AB} As mentioned in 5.4, with initial condition AA this rule gives a multiway graph that corresponds to a 2D grid: The corresponding branchial graphs are 1D: With initial condition AAA, the multiway graph is effectively a 3D grid, and the branchial graph is a 2D grid: Some rules produce only finite sequences of branchial graphs. For example, the rule {A -> B} with initial condition AAAA yields what are effectively sections through a cube oriented on its corner: As another example producing a finite sequence of branchial graphs, consider the rule: {BA -> AB} Starting from BBBAAA it gives: Starting from BBBBBAAAAA it gives: One can think of this as showing the “shapes” of successive slices through the multiway system for the evolution of this rule. As another example of a rule yielding an infinite sequence of branchial graphs, consider: {A -> AAB} This yields the following branchial graphs: In a 3D rendering, the graph on the next step is: The following are the distinct forms of branchial graphs obtained from rules involving a total of up to 6 As and Bs (starting from a single A): We have seen that branchial graphs can form regular grids. But many branchial graphs have much higher levels of connectivity. No branchial graph can continue to be a complete graph (with all neighbors having distance 1) for more than a limited number of steps. However, the diameters of branchial graphs do tend to grow slowly, and on step t they can be no larger than t. Some branchial graphs show linear or polynomial growth with the number of steps in vertex and edge count, but many show exponential growth. In analogy to what we did for hypergraphs and causal graphs, we can define a quantity Bb which measures the number of nodes in the branchial graph reached by going out to graph distance b from a given node. Consider for example the rule: {"" -> A, "" -> B} The sequence of forms for Bb as a function of b on successive steps is: At step t, the diameter of the graph is just t, and Bb=t = 2t. For smaller b, the ratios of the Bb for given b at successive steps t steadily decrease, perhaps suggesting ultimately less-than-exponential growth: One can ask what the limit of a branchial graph after a large number of steps may be. As an initial possible model, consider graphs representing n-cubes in progressively higher dimensions: The graph distances between nodes in these graphs are exactly the same as the Euclidean distances between the 2n possible tuples of 0s and 1s (here shown in distance matrices arranged in lexicographic order): The values of Bb in this case can be found from [11:A008949]: This shows the ratios of Bb for given b for successive n: Much as one can consider progressively larger grid graphs as limiting to a manifold, so perhaps one may consider higher and higher “dimensional” cube graphs as limiting to a Hilbert space. It is also conceivable that limits of branchial graphs may be related to projective spaces [78]. As one potential connection, one can look at discrete models of incidence geometry [79]. For example, with integers representing points and triples representing lines, the Fano plane and representing both points and lines here as nodes, these correspond to the graphs: But for such graphs one finds that Bb has a very different form from typical branchial graphs: An alternative approach to connecting with discrete models of projective space is to think in terms of lattice theory [69][80][81]. A multiway graph can be interpreted as a lattice (in the algebraic sense), with its evolution defining the partial order in the lattice. The states in the multiway system are elements in the lattice, and the meet and join operations in the lattice correspond to finding the common ancestors and common successors of states. The analogy with projective geometry is based on thinking of states in the multiway system (which correspond to elements in the lattice) as points, connected by lines that correspond to their evolution in the multiway system. Points are considered collinear if they have the same common successor. But (assuming the multiway system starts from a single state), causal invariance is exactly the condition that any set of points will eventually have a common successor—or in other words, that all lines will eventually intersect, suggesting that the multiway graph is indeed in some sense a discrete model of projective space—so that branchial graphs may also be models of projective Hilbert spaces.
The Updating Process for String Substitution Systems 5.17 Foliations of the Multiway Graph and the Structure of Branchial Space : Just as states that occur at successive steps in the evolution of our underlying systems can be thought of as associated with successive slices in a foliation of the causal graph, so also branchial graphs can be thought of as being associated with successive slices in a foliation of the multiway graph. As we discussed above, different foliations of the causal graph define different relative orderings of updating events within our underlying system. But we can now think about this at a higher level and consider foliations of the multiway graph, that in effect define different relative orders of updating events on different branches of the multiway system. A foliation of the causal graph in effect defines how we should line up our notion of “time” for events in different parts of our underlying system; a foliation of the multiway graph now also defines how we should line up our notion of “time” for events on different branches of the multiway system. For example, with the rule {A -> AB} starting from AA, we can define the following foliation of the multiway graph: This yields the branchial graphs: Just as the causal graph defines a partial order for events, so now the multiway graph defines a partial order for states. And so long as it is consistent with this partial order, we can pick any total order for the states. And we can parametrize some of these total orders as foliations. For the particularly simple case shown here, an alternative foliation consistent with the partial order defined by the multiway system is: And if we use this foliation, we get a different sequence of branchial graphs, now no longer connected: This example is particularly obvious in its analogy with the causal graphs we discussed above. But what makes this work is a special feature of the branchial graphs in this case: the fact that the states that appear in them can in effect be assigned simple 1D “coordinates”. In the original foliation with unslanted (“one event per slice”) slices, the “coordinate” is effectively just the position of the second A in the string. And with respect to the “space” defined by this “coordinate”, the branchial graphs can readily be laid out in one dimension, and we can readily set up “slanted” foliations, just like we did for causal graphs. With the underlying systems we are discussing in this section being based on strings of elements, it is inevitable that there will be 1D coordinates that can be assigned to the events that occur in the causal graph. But nothing like this need be true of the branchial graph, and indeed most branchial graphs have a significantly more complex structure. Consider the same rule as above, but now started from AAA. The multiway graph in this case—with a foliation indicated—is then: The branchial graphs now have a two-dimensional structure—with the positions of the second and third As in each string providing potential “coordinates”: But consider now the rule {BA -> AB} started from BABABABA. The multiway graph in this case is: And the sequence of branchial graphs based on the foliation above is now: But here it is already much less clear how to assign “coordinates” in “branchial space”, or how to create a meaningful family of foliations of the multiway graph. In thinking about multiway graphs and their foliations there is another complication that can arise for some rules. Consider two versions of the multiway graph for the rule: {AB -> BAB, BA -> A} In the first version, every distinct state is shown only once. But in the second case, the evolution is “partially unrolled” to show separately different instances of the same state, produced after different numbers of updating events. With a foliation whose slices correspond to the layers in the renderings above, the first version of the multiway system yields the branchial graphs: The second version, however, yields different branchial graphs: To some extent this difference is just like taking a different foliation. But there is more to it, because the second version of the multiway graph actually defines different ordering constraints than the first one. In the second version, there is a true partial ordering, defined by the directed edges in the multiway graph. But in the first version, there can be loops, and so no strict partial order is defined. (We will discuss this phenomenon more in 6.9.)
The Updating Process for String Substitution Systems 5.18 The Relationship between Graphs, and the Multiway Causal Graph : In the course of this section, we have seen various ways of describing and relating the possible behaviors of systems. In many ways the most general is the combined multiway evolution and multiway causal graph. For the rule {A -> AB} starting from AA this graph has the form: Continuing for another step, we have: There are several different kinds of descriptions that we can derive from this graph. The standard multiway graph gives the evolution relationship between states: Each possible path through this graph corresponds to a possible evolution history for the system. The multiway causal graph gives the causal relationships between all events that can happen on any branch. The full multiway causal graph for the rule shown here is infinite. But truncating to show only the part contained in the graph above, one gets: Continuing for more steps one gets: From the multiway causal graph, one can project out the specific causal graph for each possible evolution history, corresponding to each possible branch in the multiway system. But for rules like the one shown here that have the property of causal invariance, every one of these specific causal graphs (at least if extended far enough) must have exactly the same structure. For the particular rule shown here, this structure is extremely simple: (In effect, the nodes here are “generic events” in the system, and could be labeled just by copies of the underlying local rule.) The multiway graph and multiway causal graph effectively give two different “vertical views” of the original graph—using respectively states as nodes and events as nodes. But an alternative is to view the graph in terms of “horizontal slices”. To get such slices we have to do foliations. But now if we look at horizontal slices associated with states, we get the branchial graphs, which for this rule with this initial condition are rather trivial: In principle we could also ask about horizontal slices associated with events. But by construction the events analog of the branchial graph must just consist of a collection of complete graphs. However, a particular sequence of slices through any particular causal graph defines an actual sequence of states for the underlying system, and thus a possible evolution history, such as: As a slightly cleaner example with similar behavior, consider the rule: {A -> AB, A -> BA} The combined multiway evolution and multiway causal graph in this case is and the individual multiway evolution and causal graphs are both regular 2D grids: The rules we have used as an example so far have behavior that is in a sense fairly trivial. But consider now the related rule with slightly less trivial behavior: {A -> AB, B -> A} For this rule, the combined multiway evolution and causal graph has the form: On their own, the state evolution graph and the event causal graph have the forms: The sequence of branchial graphs is: This rule is causal invariant, and so the multiway causal graph decomposes into many identical copies of causal graphs for all individual possible paths of evolution. In this case, these graphs all have the form: But even though the multiway causal graph can be decomposed into identical pieces, it still contains more information than any of them. Because in effect it describes not only “spatial” causal relationships between events happening in different places in the underlying string, but also “branchial” causal relationships between events happening on different branches of the multiway system. And just like for other graphs, we can study the large-scale structure of multiway causal graphs. We can define a quantity  which is the multiway analog of the cone volume Ct for individual causal graphs. For the rule shown here, the various graph growth rates (as computed with our standard foliation) have the forms: As another example, consider the “sorting” rule {BA -> AB} starting from BABABABA. The combined multiway evolution and causal graph has the (terminating) form: The multiway evolution and causal graphs on their own are: The branchial graphs are and the causal graph for a single (finite) path of evolution is: Earlier in this section we looked at the multiway evolution graphs generated by all 12 inequivalent 2: 1->2, 1->1 rules. The pictures below now compare these with the multiway causal graphs for the same rules (starting from all possible length-3 strings of As and Bs, and run for 4 steps of our standard multiway foliation): But even in a case like the rule {AA -> AAA} where the causal graph for a single evolution has the fairly regular form the full multiway causal graph is quite complex. This shows how it builds up over the first few steps (in our standard multiway foliation): And here are 3D renderings after 8 and 9 steps:
The Updating Process for String Substitution Systems 5.19 Weighted Multiway Graphs : In a multiway system, there are in general multiple paths that can produce the same state. But in our usual construction of the multiway graph, we record only what states are produced, not how many paths can do it. Consider the rule: {A -> AA, A -> A} The full form of its multiway graph—including an edge for every possible event—is: Here is the same graph, with a count included at each node for the number of distinct paths from the root that reach it: An alternative weighting scheme might be to start with weight 1 for the initial state, then at each state we reach, to distribute the weight to its successors, dividing it equally among possible events: This approach has the feature that it gives normalized weights (summing to 1) at each successive layer in a graph like this. But in general the approach is not robust, and if we even took a different foliation through the graph above, the weights on each slice would no longer be normalized. In addition, if we were to combine identical states from different steps, we would not know what weights to assign. Pure counting of paths, however, still works even in this case, although any normalization has to be done only after all the counts are known: Note that even the counting of paths becomes difficult to define if there is a loop in the multiway graph—though one can adopt the convention that one counts paths only to the first encounter with any given state: Weights on the multiway graph can also be inherited by branchial graphs. Consider for example the rule: {A -> AB, B -> BA} The multiway graph for this rule, weighted with path counts, is: The corresponding weighted branchial graphs are: The weights in effect define a measure on the branchial graph. A case with a particular straightforward limiting measure is the rule: {A -> AB} With initial condition A this gives weights that reproduce Pascal’s triangle, and yield a limiting Gaussian: With initial condition AAA, the weights in the branchial graph limit to a 2D Gaussian: In general, after sufficiently many steps one can expect that the weights will define an invariant measure, although a complexity is that the branchial graph will typically continue to grow. As one indication of the limiting measure, one can compute the distribution of values of the weights. The results for the rule {A -> B, B -> AB} above illustrate slow convergence to a limiting form: We discussed in a previous subsection probing the structure of the branchial graph by computing the number of nodes Bb at most graph distance b from a given point. We can now generalize this to computing a path-weighted quantity  (cf. [1:p959]). At least for simple multiway graphs, this may be related in the limit to the results of solving a PDE on the multiway graph.
The Updating Process for String Substitution Systems 5.20 Effective Causal Invariance : In any causal invariant rule, all branch pairs that are generated must eventually resolve. But by looking at how many branch pairs are resolved—and unresolved—at each step, we can get a sense of “how far” a rule is from causal invariance. Consider the causal invariant rule: {A -> AB, B -> A} With this rule, all branch pairs resolve in one step and the total number of branch pairs that have already resolved on successive steps is (roughly 2t): But now consider the slightly different—and non-causal-invariant—rule: {A -> AB, BA -> BB} The number of resolved branch pairs goes up on successive steps—in this case quadratically: But now there is a “residue” of new unresolved branch pairs at each step that reflect the lack of causal invariance: There are other rules in which the deviation from causal invariance is in a sense larger. Consider the rule: {AA -> AAB, AA -> B} The multiway graph for this rule shows various “dead ends” and now the number of resolved branch pairs is while the number of unresolved ones grows at a similar rate: When a system is not causal invariant, what it means is that in a sense the system can reach states from which it cannot ever “get back” to other states. But this suggests that by extending the rules for the system, one might be able to make it causal invariant. Consider the rule {A -> AA, A -> B} with multiway graph: With this rule, the branch pair {B, AA} never resolves. But now let us just extend the rule by adding B -> AA: The multiway graph becomes and the rule is now causal invariant. In general, given a rule that is not causal invariant, we can consider extending it by including transformations that relate strings in branch pairs that do not resolve. For the rule {AA -> AAB, AA -> B} discussed above it turns out that the minimal additions to achieve causal invariance are: {AB -> AAAB, BA -> AB} Having added these transformations, the multiway graph now begins: After more steps, the multiway graph is: This kind of “completion” procedure of adding “relations” in order to achieve what amounts to causal invariance is familiar in automated theorem proving [82] and related areas [60]. For our purposes we can think of it as a kind of “coarse graining” of our systems, in which the additional rules in effect define equivalences between states that would otherwise be distinct. If a particular multiway graph terminates after a finite number of steps, then it is always possible to add enough completion rules to the system to ensure causal invariance [63][64]. But if the multiway graph grows without bound, this may not be possible. Sometimes one may succeed in adding enough completions to achieve causal invariance for a certain number of steps, only to have it fail after more steps. And in general, like determining whether branch pairs will resolve, there is ultimately no upper bound on how long one may have to wait, making the problem of completion ultimately formally undecidable [83]. But if (as is often the case) one only has to add a small number of completion rules to make a system causal invariant, then one can take this to mean that the system is not far from causal invariance, so that it is likely that many of the large-scale properties of the completion of the system will be shared by the system itself.
The Updating Process for String Substitution Systems 5.21 Generational Evolution : Systems like cellular automata always update every element at every step in their evolution. But in string substitution systems (as well as in our hypergraph-based models), the presence of overlaps between possible updating events typically means that there is no single, consistent way to do this kind of parallel updating. Nevertheless, in studying our models in earlier sections, we often used “steps” of evolution in which we updated as many elements as we consistently could. And we can also apply this kind of “generation-based” updating to string substitution systems. Consider the rule: {A -> AB, B -> A} We can construct the multiway graph for this rule by considering how one state is produced from another by a single updating event, corresponding to a single application of one of the transformations in the rule: But we can also consider producing a “generational multiway graph” in which we do as many non-overlapping updates as possible on any given string. For this particular rule, doing this is straightforward, since every A and every B in the string can be transformed separately. But the result is now a radically simplified multiway graph, in which there is just a single path of evolution: The “generational steps” here involve an increasing number of update events, as we can see from this rendering of the evolution: All the states obtained at generational steps do appear somewhere in the full multiway graph, but the full graph also contains many additional states—that, among other things, can be thought of as representing all possible intermediate stages of generational steps: For a rule like {A -> AB} the generational steps correspond to a particularly simple trajectory through the full multiway graph: It is not always the case that the generational multiway graph involves just a single path. Consider the rule: {A -> AB, A -> B} The ordinary multiway graph for this rule starting from AA is: And the generational multiway graph is now: The generational multiway graph is always in a sense a compression of the full multiway graph. And one way to think of it is as being derived from the full multiway graph by combining sequences of edges when they correspond to updating events that do not overlap on the string. But there is also another view of generational evolution. Consider a branchial graph from the full multiway graph above (this branchial graph is derived from the layered foliation shown): The branch pairs in this branchial graph (shown as adjacent nodes) can be thought of as being of two kinds. The first are produced by applying different rules to a single part of a string (e.g. ABA -> {ABBA, BBA}). And the second (highlighted in the graph above) by applying rules to different parts of a string (e.g. ABA -> {ABBA, ABAB}). In the full multiway graph, no distinction is made between these two kinds of branch pairs, and the graph includes both of them. But in a generational multiway system, strings in the second kind of branch pairs can be combined. And indeed this provides another way to construct a generational multiway system: look at branchial graphs and take pairs of strings corresponding to “spatially” disjoint updating events, and then knit these together to form generational steps. And if there is only one way to do this for each branchial graph, one will get a single path of generational evolution. But if there are multiple ways, then the generational multiway graph will be more complicated. For the rule {A -> AB, B -> A} that we discussed above, the sequence of branchial graphs is and it is readily possible to assemble “string fragments” (such as those highlighted) to produce the states at successive generational steps: The branchial graphs are determined by the foliation of the multiway graph that one uses. But given a foliation, one can then assemble strings corresponding to generational steps using the procedure above. There is always an alternative, however: instead of combining strings from a branchial graph to produce the state for a generational step, one can always in a sense just wait, and eventually the full multiway system will have done the necessary sequence of updates to produce the complete state for the generational step. Whenever there are no possible overlaps in the application of rules, the generational multiway graph must always yield a single path of history. But there is also another feature of such rules: they are guaranteed to be causal invariant. There are, however, plenty of rules that are causal invariant even though they allow overlaps—in a sense because their right-hand sides also appropriately agree. And for such rules, the generational multiway graph may have multiple paths of history. A simple example is the rule: {A -> AB, A -> BA} This rule is causal invariant, and starting from A yields the full multiway graph: Its generational multiway graph is actually identical in this case—because all the rule ever does is to apply one transformation or the other to the single A that appears: Starting from AA, however, the rule yields the full multiway graph and the generational multiway graph: In an alternative rendering, these graphs after a few more steps become, respectively: Note that in both cases, the number of states reached after t steps grows like t2 (in the first case it is ; in the second case exactly t2). In the case of a rule like {A -> AB, A -> BB} the presence of many potential overlaps in where updates can be applied makes many of the possible states in the full multiway graph also appear in the generational multiway graph (in the limit about 64% of all possible states are generational results): Generational multiway graphs share many features with full multiway graphs. For example, generational multiway graphs can also show causal invariance—and indeed unless strings grow too fast, any deviation from causal invariance must also appear in the generational multiway graph. The basic construction of ordinary multiway graphs ensures that the number of states Mt after t steps can grow at most exponentially with t. In a generational multiway graph, there can be faster growth. Consider for example the rule: {A -> AA, A -> B} Its full multiway graph grows in a Fibonacci sequence ≈ϕt: But its generational multiway graph grows much faster. After 3 generational steps it has the form while after 4 steps (in a different rendering) it is: The number of states reached in successive steps is: {1, 2, 5, 24, 455, 128702} Although there is a distribution of lengths for the strings, say, at steps 4 and 5 the fact that the maximum string length at generational step t is 2t–1—combined with the lack of causal invariance for this rule—allows for double exponential growth in the number of possible states with generational steps. In fact, with this particular rule, by step t almost all sequences of up to 2t–1 Bs and AAs have appeared (the missing fractions on steps 3, 4, 5 are 0.13, 0.078, 0.017) so at step t the total number of states approaches technical introduction The Updating Process in Our Models 6.1 Updating Events and Causal Dependence Consider the rule: {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} When we discussed this rule previously, we showed the first few steps in its evolution as: But to understand the updating process in our models in more detail, it is helpful to “look inside” these steps, and see the individual updating events of which they are comprised: The 22 -> 42 signature of the rule means that in each updating event, two relations are destroyed, and four new ones are created. In the pictures above, new relations from each event are shown in red; the ones that will disappear in the next event are shown dotted. The elements are numbers in the sequence they are created. There are in general many possible sequences of updating events that are consistent with the rule. But in making the pictures above (and in much of our discussion in previous sections), we have used our “standard updating order”, in which each step in the overall evolution in effect includes as many non-overlapping updates as will “fit”. (In more detail, what is done is that in each overall step, relations are scanned from oldest to newest, in each case using them in an update event so long as this can be done without using any relation that has already been updated in this overall step.) Our models—and the hypergraphs on which they operate—are in many ways more difficult to handle than the string-based systems we discussed in the previous section. But one way in which they are simpler is that they more directly expose causal relationships between events. To see if an event B depends on an event A, all we need do is to see whether elements that were involved in A are also involved in B. Looking at the sequence of updates above, therefore, we can immediately construct a causal graph: Another feature of our models is that every element can be thought of as having a unique “lineage”, in that it was created by a particular updating event, which in turn was the result of some other updating event, and so on. When we introduced our models in section 2, we just said that any element created by applying a rule should be new and distinct from all others. If we were implementing the model, this might then make us imagine that the element would have a name based on some global counter, or a UUID. But there is another, more deterministic (as well as more local and distributed) alternative: think of each new element as being a kind of encapsulation of its lineage (analogous to a chain of pointers, or to a hash like in blockchains [84] or Git). In the evolution above, for example, we could describe element 10 by just saying it was created as part of the relation {2,10} from the relations {{2,4},{2,5}} (as the second part of the output of an update that uses them)—but then we could say that these relations were in turn created from earlier relations, and so on recursively, all the way back to the initial state of the system: The final expression here can also be written as: Roughly what this is doing is specifying an element (in this case the one we originally labeled simply as 10) by giving a symbolic representation of the path in the causal graph that led to its creation. And we can then use this to create a unique symbolic name for the element. But while this may be structurally interesting, when it comes to actually using an element as a node in a hypergraph, the name we choose to use for the element is irrelevant; all that matters is what elements are the same, and what are different.
The Updating Process in Our Models 6.1 Updating Events and Causal Dependence : Consider the rule: {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} When we discussed this rule previously, we showed the first few steps in its evolution as:{{x, y}, {x, z}} -> {{x, z}, {x, w}, {y, w}, {z,      w}}, {{1, 1}, {1, 1}}, 4 But to understand the updating process in our models in more detail, it is helpful to “look inside” these steps, and see the individual updating events of which they are comprised {{x, y}, {x, z}} -> {{x, z}, {x, w}, {y, w}, {z, w}}, {{0, 0}, {0, 0}}, 4]} The 22  42 signature of the rule means that in each updating event, two relations are destroyed, and four new ones are created. In the pictures above, new relations from each event are shown in red; the ones that will disappear in the next event are shown dotted. The elements are numbers in the sequence they are created.  There are in general many possible sequences of updating events that are consistent with the rule. But in making the pictures above (and in much of our discussion in previous sections), we have used our “standard updating order”, in which each step in the overall evolution in effect includes as many non-overlapping updates as will “fit”. (In more detail, what is done is that in each overall step, relations are scanned from oldest to newest, in each case using them in an update event so long as this can be done without using any relation that has already been updated in this overall step.)  Our models—and the hypergraphs on which they operate—are in many ways more difficult to handle than the string-based systems we discussed in the previous section. But one way in which they are simpler is that they more directly expose causal relationships between events. To see if an event B depends on an event A, all we need do is to see whether elements that were involved in A are also involved in B.  Looking at the sequence of updates above, therefore, we can immediately construct a causal graph: Another feature of our models is that every element can be thought of as having a unique “lineage”, in that it was created by a particular updating event, which in turn was the result of some other updating event, and so on. When we introduced our models in section 2, we just said that any element created by applying a rule should be new and distinct from all others. If we were implementing the model, this might then make us imagine that the element would have a name based on some global counter, or a UUID.  But there is another, more deterministic (as well as more local and distributed) alternative: think of each new element as being a kind of encapsulation of its lineage (analogous to a chain of pointers, or to a hash like in blockchains [84] or Git). In the evolution above, for example, we could describe element 10 by just saying it was created as part of the relation {2,10} from the relations {{2,4},{2,5}} (as the second part of the output of an update that uses them)—but then we could say that these relations were in turn created from earlier relations, and so on recursively, all the way back to the initial state of the system: The final expression here can also be written as: Roughly what this is doing is specifying an element (in this case the one we originally labeled simply as 10) by giving a symbolic representation of the path in the causal graph that led to its creation. And we can then use this to create a unique symbolic name for the element. But while this may be structurally interesting, when it comes to actually using an element as a node in a hypergraph, the name we choose to use for the element is irrelevant; all that matters is what elements are the same, and what are different.
The Updating Process in Our Models 6.2 Multiway Systems for Our Models : Just like for the string substitution systems of section 5, we can construct multiway systems [1:5.6] for our models, in which we include a separate path for every possible updating event that can occur: For string systems, it is straightforward to determine when states in the system should be merged: one just has see whether the strings corresponding to them are identical. For our systems, it is more complicated: we have to determine whether the hypergraphs associated with states are isomorphic [85], in the sense that they are structurally the same, independent of how their nodes might be labeled. Continuing one more step with our rule, we see some cases of merging: Here is an alternative rendering, now also showing the particular path obtained by following our “standard updating order”: In general, each path in the multiway system corresponds to a possible sequence of updating events—here shown along with the causal relationships that exist between them:
The Updating Process in Our Models 6.3 Causal Invariance : Like string substitution systems, our models can have the important feature of causal invariance [1:9.9]. In analogy with neighbor-independent string substitution systems, causal invariance is guaranteed if there is just a single relation on the left-hand side of a rule. Consider for example the rule: {{x, y}} -> {{x, y}, {y, z}} Starting from a single self-loop, this gives the multiway system: As implied by causal invariance, every pair of paths that diverge must reconverge. And looking at a few more steps, we can see that in fact with this particular rule, branches always recombine after just one step: The different paths here lead to hypergraphs that look fairly different. But causal invariance implies that every time there is divergence, there must always eventually be reconvergence. And for some rules, different paths give hypergraphs that do look very similar. An example is the rule {{x, y}} -> {{y, z}, {z, x}} where the hypergraphs produced on different paths differ only by the directions of their hyperedges: For rules that depend on more than one relation, causal invariance is not guaranteed, and in fact is fairly rare. Of the 4702 inequivalent 22 -> 32 rules, perhaps 5% are causal invariant. In some cases, the causal invariance is rather trivial. For example, the rule leads to a multiway graph that only allows one path of evolution: A less trivial example is the rule which yields the multiway system: With our standard updating order, this rule eventually produces forms like but the multiway system shows that other structures are also possible. As another example, consider the rule which with our standard updating order gives: The multiway system for this rule branches rapidly but every pair of branches still reconverges in one step. The rule provides an example of causal invariance in which branches can take 3 steps to converge: Among all possible rules, causal invariance is much more common for rules that generate disconnected hypergraphs. It is also perhaps slightly less common for rules with ternary relations instead of binary ones.
The Updating Process in Our Models 6.4 Testing for Causal Invariance : Testing for causal invariance in our models is similar in principle to the case of strings. Failure of causal invariance is again the result of branch pairs that do not resolve. And just like for strings, it is possible to test for total causal invariance by determining whether a certain finite set of core branch pairs resolve. (Again in analogy with strings, the finiteness of this set is a consequence of the finiteness of the hypergraphs involved in our rules.) The core branch pairs that we need to test represent the minimal cases of overlap between left-hand sides of rules—or, in a sense, the minimal unifications of the hypergraphs that appear. For the two hypergraphs there are two possible unifications (where the purple edge shows the overlap): For a single rule with left-hand side {{x, y}, {x, z}} the core branch pairs arise from unifications associated with the possible self-overlaps of this small hypergraph. Representing two copies of the hypergraph as the possible unifications are: In the case of strings, all that matters is what symbols appear within the unification. In the case of hypergraphs, one also has to know how the unification in effect “attaches”, and so one has to distinguish different labelings of the nodes. Starting from the unifications, one applies the rule to find what branch pairs can be produced. These branch pairs form the core set of branch pairs for the rule—and determining whether the rule is causal invariant then becomes a matter of finding out whether these branch pairs resolve. In the case of {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} the application of the rule to the unifications above yields the following 58 core branch pairs: Running the rule for one step yields resolutions for 6 of these branch pairs: Running for another step resolves no additional branch pairs. Will the rule turn out to be causal invariant in the end? As a comparison, consider the rule discussed in the previous subsection. This rule starts with 14 core branch pairs: After one step, 6 of them resolve: Then after another step the 8 remaining ones resolve, establishing that the rule is indeed causal invariant: But in general there is no upper bound on the number of steps it can take for core branch pairs to resolve. Perhaps the fact that so many additional branch pairs are generated at each step in the rule {{x,y},{x,z}}->{{x,z},{x,w},{y,w},{z,w}} makes it seem unlikely that they will all resolve, but ultimately this is not clear. And even if the rule does not show total causal invariance, it is still perfectly possible that it will be causal invariant for the particular set of states generated from a certain initial condition. However, determining this kind of partial causal invariance seems even more difficult than determining total causal invariance. Note that if one looks at all 4702 22 -> 32 rules, the largest number of core branch pairs for any rule is 554; the largest number that resolve in 1 step is 132, and the largest number that remain unresolved is 430.
The Updating Process in Our Models 6.5 Causal Graphs for Causal Invariant Rules : An important consequence of causal invariance is that it establishes that a rule produces the same causal graph independent of the particular order in which update events occurred. And so this means, for example, that we can generate causal graphs just by looking at evolution with our standard updating order. For rules that depend on only one relation, the causal graph is always just a tree regardless of whether the structure generated is also a tree or has a more compact form: But as soon as a rule depends on more than one relation, the causal graph can immediately be more complicated. For example, consider even the rule: {{x}, {x}} -> {{x}, {x}, {x}} The multiway system for this rule shows that only one path is possible (immediately demonstrating causal invariance): But the causal relationships between steps are not so straightforward and after 15 steps the causal graph has the form or in an alternative rendering: The fact that the multiway system is nontrivial does not mean that the causal graph for a particular rule evolution will be nontrivial. Consider for example a causal invariant rule that we discussed above: {{x, y}, {z, y}} -> {{x, w}, {y, w}, {z, w}} The multiway system for this rule, with causal connections shown, is: This yields the multiway causal graph: But the causal graph for any individual evolution is just: For the causal invariant rule (also discussed above) the multiway system after 5 steps has the form: After 20 steps of evolution with our standard updating order gives: The causal graph for this rule after 10 steps is and after 20 steps, in a different rendering, it becomes: As another example, consider the rule (also discussed above): {{x, y}, {x, z}} -> {{y, w}, {y, z}, {w, x}} The multiway system for this rule (with events included) has the form: After 20 steps, the causal graph is: After 100 steps it is: After 500 steps, in an alternative rendering, a grid-like structure emerges (the directed edges point outward from the center): After 5000 steps, rendering the graph in 3D with surface reconstruction reveals an elaborate effective geometry:
The Updating Process in Our Models 6.6 The Role of Causal Graphs : Even if we do not know that a rule is causal invariant, we can still construct a causal graph for it based on a particular updating order—and often different updating orders will give at least similar causal graphs. Thus, for example, for the rule {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} applying our standard updating order for 5 steps gives the causal graph: Continuing for 10 steps, we get: This can also be rendered as: After 15 steps, there are 10,346 nodes: In effect the successive steps in the evolution of the system correspond to successive slices through this causal graph. In the case of causal invariant rules, any possible updating order must correspond to a possible causal foliation of the graph. But here we can at least say that the foliation obtained by looking at successive layers starting from the root corresponds to successive steps of evolution with our standard updating order. For any system whose evolution continues infinitely, the causal graph will ultimately be infinite. But by slicing the graph as we have above, we are effectively showing the events that contribute to forming the state of the system after 15 steps of evolution (in this case, with our standard updating order): (Note that with this particular rule, the vast majority of relations that appear at step 15 were added specifically at that step, so in a sense most of the state at step 15 is associated just with the slice of the causal graph at layer 15.)
The Updating Process in Our Models 6.7 Typical Causal Graphs : As we discussed in 5.12, causal graphs for string substitution systems tend to have fairly simple structures. Causal graphs for our models tend to be considerably more complicated, and even among 22 -> 32 rules considerable diversity is observed. A typical random sample of different forms is: {{x, x}, {x, y}} -> {{y, y}, {z, y}, {x, z}} At each step, the self-loop just adds a relation, and effectively moves around the growing loop: The causal graph captures the causal connections created by the self-loop encountering the same relations again after it goes all the way around: The structure gets progressively more complicated: Re-rendering this gives or after 500 steps: As another example, consider the rule: {{x, y}, {x, z}} -> {{y, w}, {y, x}, {w, x}} Here are the first 25 steps in its evolution (using our standard updating order): After a few steps all that happens is that there is a small structure that successively moves around the loop creating new “hairs”. The causal graph (here shown after 25 steps) captures this process: An alternative rendering shows that a grid structure emerges: Here are the corresponding results after 100 steps: As a somewhat different example, consider the rule: After the same number of steps, one can effectively see the separate trees in the causal graph: Re-rendering the causal graph, it has a structure that is quite similar to the actual state of the system: Continuing for a few more steps, a definite tree structure emerges: It is not uncommon for a causal graph to “look like” the actual hypergraph generated by one of our models. For example, rules that produce globular structures tend to produce similar “globular” causal graphs (here shown for three 22 -> 42 rules from section 3): Rules that exhibit slow growth often yield either grid-like or “hyperbolic” causal graphs (here shown for some 23 -> 33 rules from section 3): A typical source of grid-like causal graphs [1:p489] is rules where in a sense only one thing ever happens at a time, or, in effect, the rules operate like a mobile automaton [1:3.3] or a Turing machine, with a single active element. As an example, consider the rule (see 3.10): {{x, y, y}, {z, x, u}} -> {{y, v, y}, {y, z, v}, {u, v, v}} Updates can only occur at the position of the self-loop, which progressively “moves around”, “knitting” a grid pattern. The causal graph captures the fact that “only one thing happens at a time”: But what is notable is that if we ask about the overall causal relationships between events, we realize that even events that happened many steps apart in the evolution as shown here are actually directly causally connected, because in a sense “nothing else happened in between”. Re-rendering the causal graph illustrates this, and shows how a grid is built up: Sometimes the actual growth process can be more complicated, as in the case of the rule After 200 steps this yields: And after 1000 steps it gives: But despite this elaborate structure, the causal graph is very simple: After 200 steps, the grid structure is clear: Sometimes the causal graph can locally be like a grid, while having a more complicated overall topological structure. Consider for example the rule: After 200 steps this gives: The corresponding causal graph is: After 1000 steps with surface reconstruction this gives: Rules (such as those with signature 22 -> 22) that cannot exhibit growth inevitably terminate or repeat, thus leading to causal graphs that are either finite or repetitive—but may still have fairly complex structure. Consider for example the rule (compare 3.15): {{x, y}, {y, z}} -> {{z, x}, {z, y}} Evolution from a chain of 9 relations leads to a 31-step transient, then a 9-step cycle: The first 30 layers in the causal graph are: In an alternative rendering, the graph is: After 50 more steps, the repetitive structure becomes clear: Sometimes the structure of the causal graph may be very much a reflection of the updating order used. Consider for example the rather trivial “identity” rule: {{x, y}, {y, z}} -> {{x, y}, {y, z}} Starting with a chain of 3 relations, this shows update events according to our standard updating order (note that the same relation can be both created and destroyed at a particular step): The corresponding causal graph is: For a chain of length of 21 the causal graph consists largely of independent regions—except for the connection created by updates fitting differently at different steps: Re-rendering this gives a seemingly elaborate structure: After 100 steps, though, its repetitive character becomes clear: Note that if the initial condition is a ring rather than a chain, one gets together with the tube-like structure:
The Updating Process in Our Models 6.8 Large-Scale Structure of Causal Graphs : In section 5 we used the cone volume Ct to probe the large-scale structure of causal graphs generated by string substitution systems. Now we use Ct to probe the large-scale structure of causal graphs generated by our models. Consider for example the rule {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} We found in section 4 that after a few steps, the volumes Vr of balls in the hypergraphs generated by this rule grow roughly like r2.6, suggesting that in the limit the hypergraphs behave like a finite-dimensional space, with dimension ≈2.6. The pictures below show the log differences in Vr  and Ct for this rule after 15 steps of evolution: The linear increase in this plot implies exponential growth in Ct and indeed we find that for this rule: This exponential growth—compared with the polynomial growth of Vr—implies that expansion according to this rule is in a sense sufficiently rapid that there is increasing causal disconnection between different parts of the system. The other three 22 -> 42 globular-hypergraph-generating rules shown in the previous subsection show similar exponential growth in Ct, at least over the number of steps of evolution tested. A rule such as {{x, y, y}, {x, z, u}} -> {{u, v, v}, {v, z, y}, {x, y, v}} whose hypergraph and causal graph (after 500 steps) are respectively gives the following for the log differences of Vr and Ct after 10,000 steps: This implies that for this rule the hypergraphs it generates and its causal graph both effectively limit to finite-dimensional spaces, with the hypergraphs having dimension perhaps slightly over 2, and the causal graph having dimension 2. Consider now the rule: {{x, y, x}, {x, z, u}} -> {{u, v, u}, {v, u, z}, {x, y, v}} The hypergraph and causal graph (after 1500 steps) for this rule are respectively: The log differences of Vr and Ct after 10,000 steps are then: Both suggest limiting spaces with dimension 2, but with a certain amount of (negative) curvature.
The Updating Process in Our Models 6.9 Foliations of Causal Graphs : At least in a causal invariant system, the structure of the causal graph is always the same, and it defines the causal relationships that exist between updating events. But in relating the causal graph to actual underlying evolution histories for the system, we need to specify how we want to foliate the causal graph—or, in effect, how we want to define “steps” in the evolution of the system. As an example, consider the rule: {{x, y}, {z, y}} -> {{x, z}, {y, z}, {w, z}} (This rule is probably not causal invariant, but this fact will not affect our discussion here.) The most obvious foliation for the causal graph basically follows our standard updating order: But this is not the only foliation we can use. In fact, we can divide the graph into slices in any way, so long as the slices respect the causal relationships defined by the graph, in the sense that within a slice the causal relationships allow the events to occur in any order, and between successive slices events must occur in the order of the slices. And with these criteria, for example, another possible foliation is: With the first foliation shown above, the hypergraphs from what we consider to be the first “few steps” in the evolution of the underlying rule are: But the second foliation in effect has a different (and coarser) definition of “steps”, and with this foliation the first few steps would be: When we discussed foliations in the context of string substitution systems, there were a number of simplifying features in our discussion. First, the underlying system fundamentally involved a linear string of elements. And second, the main causal graph we actually considered was a simple grid. With a rule like {{x, y, y}, {y, z}} -> {{x, y}, {y, z, z}} we can also get a simple grid causal graph (and this rule happens to be causal invariant). With the obvious foliation the steps in the evolution of the underlying system from a particular initial condition are: But given the grid structure of the causal graph, we can use the same diagonal slice method for generating foliations that we did in 5.14. And for example with the foliation there are more steps involved in the evolution of the system: But when the causal graph does not have such a simple structure, the definition of foliations can be much more complicated. When the causal graph at least in some statistical sense limits to a sufficiently uniform structure, it should be possible to set up foliations that are analogous to the diagonal slices. And even in other cases, it will often be possible to set up foliations that can be described, for example, by the kind of lapse functions we discussed in 5.14. But there is one issue that can make it impossible to set up any reasonable “progressive” foliation of a causal graph at all, and that is the issue of loops. This issue is actually already present even in the case of string substitution systems (and even causal invariant ones). Consider for example the rule: {AA -> A, A -> AA} Starting from AA the multiway causal graph for this rule is: But note here the presence of several loops. And looking at the states graph in this case one can see where these loops come from: they are reflections of the fact that in the evolution of the system, there are states that can repeat—and where in a sense a state can return to its past. Whenever this happens, there is no way to make a progressive foliation in which events in future slices systematically depend only on events in earlier slices. (In the continuum limit, the analog is failure of strong hyperbolicity [86]; loops are the analog of closed timelike curves (e.g. [75])) (Self-loops also cause trouble for progressive foliations by forcing events to happen repeatedly within a slice, rather than only affecting later slices.) The phenomenon of loops is quite common in string substitution systems, and already happens with the trivial rule A -> A. It also happens for example with a rule like: {AB -> BAB, BA -> "A} Starting with ABA, this gives the causal graph and has a states graph: Loops can also happen in our models. Consider for example the very simple rule: {{{x}, {x}} -> {{x}}, {{x}} -> {{x}, {x}}} The multiway graph for this rule is: (Note that the issue discussed in 6.1 of when we consider states “identical” as opposed to “equivalent” can again arise here. There are similar issues when we consider finite-size systems where the whole state inevitably repeats—and where in principle we can define a cyclic analog of our foliations.)
The Updating Process in Our Models 6.10 Causal Disconnection : In 2.9 we discussed the fact that some rules—even though the rules themselves are connected—can lead to hypergraphs that are disconnected. And—unless one is dealing with rules with disconnected left-hand sides—any hypergraphs that are disconnected must also be causally disconnected. As a simple example of what can happen, consider the rule: {{x, y}} -> {{y, z}, {y, z}} The evolution of this rule quickly leads to disconnected hypergraphs: The corresponding causal graph is a tree: In this particular case, the different branches happen to correspond to isomorphic hypergraphs, so that in our usual way of creating a multiway graph, this rule leads to a connected multiway graph, which even shows causal invariance: (Note that causal invariance is in a sense easier to achieve with disconnected hypergraphs, because there is no possibility of overlap, or of ambiguity in updates.) In the case of an extremely simple rule like {{x}} -> {{y}, {z}} the evolution is immediately disconnected the causal graph is a tree but the multiway graph consists of a simple “counting” sequence of states: In other rules, the disconnected pieces are not isomorphic, and the multiway graph can split. An example where this occurs is the rule: {{x, y}, {x, z}} -> {{x, x}, {y, u}, {u, v}} The multiway graph in this case is a tree: The multiway causal graph, however, does not have an exponential tree structure, but instead effectively just has one branch for each disconnected component in the hypergraph: As a result, for this rule the ordinary causal graph has a simple, sequential form: As a related example, consider the rule: {{x, y}, {y, z}} -> {{u, v}, {v, x}, {x, y}} In this case, the multiway graph has the two-branch form and the multiway causal graph has the similarly two-branch form though the ordinary causal graph is still just: Sometimes there can be multiple branches in both the multiway graph and the ordinary causal graph. An example occurs in the rule where the multiway graph is the multiway causal graph is and the ordinary causal graph is: Is it possible to have both an infinitely branching multiway graph, and an infinitely branching ordinary causal graph? One of the issues is that in general it can be undecidable whether this is ultimately infinite branching. Consider for example the rule: { {{x, x}, {x, y}} -> {{y, y}, {y, y}, {x, z}}} The ordinary causal graph for this rule has the form or in a different rendering after more steps: The multiway causal graph in this case is: But now the multiway graph is: Continuing for more steps yields: But while it is fairly clear that this multiway graph does not show causal invariance, it is not clear whether it will branch forever or not. As a similar example, consider the rule: {{x, y}, {y, z}} -> {{x, x}, {x, y}, {w, y}} This yields the same ordinary causal graph as the previous rule but now its multiway graph has a form that appears slightly more likely to branch forever: All the examples we have seen so far involve explicit disconnection of hypergraphs. However, it is also possible to have causal disconnection even without explicit disconnection of hypergraphs. As a very simple example, consider the rule: The causal graph in this case is although the multiway graph is just: For the rule the causal graph is again a tree but now the multiway graph is: The rule gives exactly the same causal graph, but now its hypergraph is a tree: Like some of the rules shown above, its multiway graph is somewhat complex: It is actually fairly common to have causal graphs that look like the corresponding hypergraphs in the case of rules where effectively only one update happens at a time. An example occurs in the case of the rule (see 3.10): So far, we have only considered fairly minimal initial conditions. But as soon as it is possible to have multiple independent events occur in the initial conditions, it is also possible to get completely disconnected causal graphs. (Note that if an “initial creation event” to create the initial conditions was added, then the causal graphs would again be connected.) As an example of disconnected causal graphs, consider the rule {{x}} -> {{x}, {x}} with an initial condition consisting of connected unary relations: This rule yields a disconnected causal graph: The multiway graph in this case is connected, and shows causal invariance: Sometimes the relationship between disconnection in the hypergraph and the existence of disconnected causal graphs can be somewhat complex. This shows results for the rule above with initial conditions consisting of increasing numbers of self-loops: Even with rather simple rules, the forms of branching in causal graphs can be quite complex—even when the actual hypergraphs remain simple. Here are a few examples:
The Updating Process in Our Models 6.11 Global Symmetries and Conservation Laws : Given the rule (stated here using numbers rather than our usual letters) {{1, 2, 3}, {3, 4, 5}} -> {{6, 7, 1}, {6, 3, 8}, {5, 7, 8}} if we reverse the elements in each relation we get: In graphical form, the rule and its transform are: Most rules would not be left invariant under a reversal of each relation. For example, the rule {{1, 2, 3}, {2, 4, 5}} -> {{5, 6, 4}, {6, 5, 3}, {7, 8, 5}} yields after reversal of each relation which is not the same as the original rule. If a rule is invariant under a symmetry operation such as reversing each relation, it implies that the rule commutes with the symmetry operation. So given a rule R and a symmetry operation Θ, this means that for any state S, R (Θ S) must be the same as Θ (R S). With the symmetric rule above, evolving from a particular initial state gives: But now reversing the relations in the initial state gives essentially the same evolution, but with states whose relations have been reversed: For the nonsymmetric rule above, evolution from a particular initial state gives: But if one now reverses the relations in the initial state, the evolution is completely different: For rules with binary relations, the only symmetry operation that can operate on relations is reversal, corresponding to the permutation {2,1}. Of the 73 distinct 12 -> 22 rules, 11 have this symmetry. Of the 4702 22 -> 32 rules, 92 have the symmetry. Of the 40,405 22 -> 42 rules, 363 have the symmetry. Those with the most complex behavior are: {{1, 2}, {2, 3}} -> {{1, 2}, {1, 4}, {2, 3}, {4, 3}} {{1, 2}, {2, 3}} -> {{1, 4}, {1, 3}, {4, 5}, {5, 3}} For rules with ternary relations, there are six distinct symmetry classes corresponding to the six subgroups of the symmetric group S3: no invariance, invariance under transposition of two elements (3 cases of S2) ({1,3,2}, {3,2,1} or {2,1,3} only), invariance under cyclic rotation (A3) ({2,3,1} and {3,1,2}), or invariance under any permutation (full S3). Here are the numbers of rules of various signatures with these different symmetries: Examples of rules with full S3 symmetry include (compare 7.2): {{1, 2, 3}, {2, 3, 1}} -> {{2, 2, 4}, {4, 3, 3}, {1, 4, 1}} {{1, 2, 3}, {2, 3, 1}} -> {{4, 4, 2}, {4, 1, 4}, {3, 4, 4}} An example of a rule with only cyclic (A3) symmetry is: {{1, 2, 3}, {2, 3, 1}} -> {{1, 1, 4}, {4, 2, 2}, {3, 4, 3}} The existence of symmetry in a rule has implications for its multiway graph, effectively breaking its state transition graph into pieces corresponding to different cosets (compare [1:p963]). For example, starting from all 102 distinct 2-element ternary hypergraphs, the first completely symmetric rule above gives multiway system: A somewhat simpler example of a completely symmetric rule is: {{1, 2, 3}, {2, 3, 1}} -> {{1, 2, 3}, {2, 3, 1}, {3, 1, 2}} This rule has a simple conservation law: it generates new relations but not new elements. And as a result its multiway graph breaks into multiple separate components. In general one can imagine many different kinds of conservation laws, some associated with identifiable symmetries, and some not. To get a sense of what can happen, let us consider the simpler case of string substitution systems. The rule (which has reversal symmetry) {BA -> AB, AB -> BA} gives a multiway graph which consists of separate components distinguished by their total numbers of As and Bs: The rule {AA -> BB, BB -> AA} gives the same basic structure, but now what distinguishes the components is the difference in the number of ABs vs. BAs that occur in each string. In both these examples, the number of distinct components increases linearly with the length of the strings. The rule {AA -> BB, AB -> BA} already gives exactly two components, one with an even number of Bs, and one with an odd number: The rule {AB -> AA, BB -> BA} also gives two components, but now these just correspond to strings that start with A or start with B.
The Updating Process in Our Models 6.12 Local Symmetries : In the previous subsection, we considered symmetries associated with global transformations made on all relations in a system. Here we will consider symmetries associated with local transformations on relations involved in particular rule applications. Every time one does an update with a given rule, say {{x, y}, {z, y}} -> {{x, x}, {y, y}, {z, w}} one needs to match the “variables” that appear on the left-hand side with actual elements in the hypergraph. But in general there may be multiple ways to do this. For example, with the hypergraph {{1, 2}, {3, 2}} one could either match {x -> 1, y -> 2, z -> 3} {z -> 1, y -> 2, x -> 3} The possible permutations of matches correspond to the automorphism group of the hypergraph that represents the left-hand side of the rule. For 22 hypergraphs of which {{x,y},{y,z}} is an example, there are only two possible automorphism groups: the trivial group (i.e. no invariances), and the group S2 (i.e. permutations {2,3}, {1,3} or {1,2}). Here are automorphism groups for binary and ternary hypergraphs with various signatures. In each case the group order is included, as are a couple of sample hypergraphs: If the right-hand side of a rule has at least as high a symmetry as the left-hand side, then any possible permutation of matches of elements will lead to the same result—which means that the same update will occur, and the only consequence will be a potential change in path weightings in the multiway graph. But if the right-hand side of the rule has lower symmetry than the left-hand side (i.e. its automorphism group is a proper subgroup), then different permutations of matches can lead to different outcomes, on different branches of the multiway system. It may still nevertheless be the case that some permutations will lead to identical outcomes—and this will happen whenever the canonical form of the rule is the same after a permutation of the elements on the left-hand side (cf. [87]). Thus for example the rule {{x, y}, {z, y}} -> {{x, x}, {y, y}, {z, w}} is invariant under any of the permutations of the elements corresponding to {x, y, z}. Note that the permutations that appear here do not form a group. To compose multiple such transformations one must take account of relabeling on the right-hand side as well as the left-hand side. For the 3138 22 -> 32 rules that involve 3 elements on the left, the following lists the 10 of 64 subsets of the 6 possible permutations that occur: In a sense, we can characterize the local symmetry of a rule by determining what permutations of inputs it leaves invariant. But we can do this not only for a single update, but for a sequence of multiple updates. In effect, all we have to do is to form a power of the rule, and then apply the same procedure as above. There are several ways to define a notion of powers (or in general, products) of rules. As one example, we can consider situations in which a rule is applied repeatedly to an overlapping set of elements—so that in effect the successive rule applications are causally connected. In this case—much as we did for testing total causal invariance—we need to work out the unifications of the possible initial conditions. Then we effectively just need to trace the multiway evolution from each of these unified initial conditions. Consider for example the rule: {{x, y}} -> {{x, y}, {y, z}} The “square” of this rule is: And its cube is: The multiway graph for the original rule after 4 updates is: The “square” of the rule generates the same states in only 2 updates: We can use the same approach to find the “square” of a rule like: Using this for actual evolution gives the result: And now that we can compute the power of a rule, we have a way to compute the effective symmetry for multiple updates according to a rule. In general, after t updates we will end up with a collection of permutations of variables that leave the effective “power rule” invariant. But if we now consider increasingly large values of t, we can ask whether the collections of permutations we get somehow converge to a definite limit. In direct analogy to the way that our hypergraphs can limit to manifolds, we may wonder whether these collections of permutations could limit to a Lie group (cf. [88]). As a simple example, say that the permutations are of length n, but of the n! possibilities, we only have the n cyclic permutations, say for n = 4: It is not so clear [89][90] how to deal in more generality with collections of permutations, although one could imagine an analog of a manifold reconstruction procedure. To get an idea of how this might work, consider the inverse problem of approximating a Lie group by permutations. (Note that things would be much more straightforward if we could build up matrix representations, but this is not the setup we have.) In some cases, there are definite known finite subgroups of Lie groups—such as the icosahedral group A5 as a subset of the 3D rotation group SO(3). In such cases one can then explicitly consider the permutation representation of the finite group. It is also possible to imagine just taking a lattice (or perhaps some more general structure of the kind that might be used in symbolic dynamics [91][92]) and applying random elements of a particular Lie group to it, then in each case recording the transformation of lattice points that this yields. Typically these transformations will not be permutations, but it may be possible to approximate them as such. By inverting this kind of procedure, one can imagine potentially being able to go from a collection of permutations to an approximating Lie group.
The Updating Process in Our Models 6.13 Branchial Graphs and Multiway Causal Graphs : Consider the rule: {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} If we pick a foliation for the first few steps in the multiway graph for this rule then just as in 5.15 for string substitution systems, we can generate branchial graphs that represent the connections defined by branch pairs between the states at each slice in the foliation: Branchial graphs provide one form of summary of the multiway evolution. Another summary is provided by the multiway causal graph, which includes causal connections between parts of hypergraphs both within a branch of the multiway system, and across different branches: The multiway causal graph is in many respects the richest summary of the behavior of our models, and it will be important in our discussion of possible connections to physics. In a case like the rule shown, the structure of branchial and multiway causal graphs is quite complex. As a simpler example, consider the causal invariant rule: {{x, y}} -> {{x, y}, {y, z}} With this rule, the multiway graph has the form: After more steps, and with a different rendering, the multiway graph is: (In this case, the size of the multiway graph as measured by Σt increases slightly faster than 2t.) The branchial graphs with the standard layered foliation in this case are: The volumes Bt in the branchial graph grow on successive steps like: The multiway causal graph in this case is or with more steps and a different layout: Note that in this case the ordinary multiway graph is simply: As a slightly more complicated example, consider the causal invariant 22 -> 32 rule: {{x, y}, {x, z}} -> {{y, w}, {y, z}, {w, x}} The multiway system in this case has the form: The sequence of branchial graphs in this case are: The causal graph for this rule is: The multiway causal graph has many repeated edges: Here it is in a different rendering: Note that in our models, even when the hypergraphs are disconnected, the branchial graphs can still be connected, as in the case of the rule: {{x, y}} -> {{y, z}, {z, w}}
Equivalence and Computation in Our Models 7.1 Correspondence with Other Systems : Our goal with the models introduced here is to have systems that are intrinsically as structureless as possible, and are therefore in a sense as flexible and general as possible. And one way to see how successful we have been is to look at what is involved in reproducing other systems using our models. As a first example, consider the case of string substitution systems (e.g [1:3.5]). An obvious way to represent a string is to use a sequence of relations to set up what amounts to a linked list. The “payload” at each node of the linked list is an element of the string. If one has two kinds of string elements A and B, these can for example be represented respectively by 3-ary and 4-ary relations. Thus, for example, the string ABBAB could be (where the Bs can be identified from the “inner self-loops” in their 4-ary relations): Note that the labels of the elements and the order of relations are not significant, so equivalent forms are or, with our standard canonicalization: A rule like {A -> BA, B -> A} can then be translated to Starting with A, the original rule gives: In terms of our translation, this is now: The causal graph for the rule in effect shows the dependence of the string elements corresponding to the evolution graph (see 5.1): We can also take our translation of the string substitution system, and use it in a multiway system: The result is a direct translation of what we could get with the underlying string system: Having seen how our models can reproduce string substitution systems, we consider next the slightly more complex case of reproducing Turing machines [93]. As an example, consider the simplest universal Turing machine [1:p709][94][95] [96], which has the 2-state 3-color rule: Given a tape with the Turing machine head at a certain position a possible encoding uses different-arity hyperedges to represent different values on the tape, and different states for the head, then attaches the head to a certain position on the tape, and uses special (in this case 6-ary) hyperedges to provide “extensible end caps” to the tape: With this setup, the rule can be encoded as: Starting from a representation of a blank tape, the first few steps of evolution are (note that the tape is extended as needed) which corresponds to the first few steps of the Turing machine evolution: The causal graph for our model directly reflects the motion of the Turing machine head, as well as the causal connections generated by symbols “remembered” on the tape between head traversals: Re-rendering this causal graph, we see that it begins to form a grid: It is notable that even though in the underlying Turing machine only one action happens at each step, the causal graph still connects many events in parallel (cf. [1:p489]). After 1000 steps the graph has become a closer approximation to a flat 2D manifold, with the specific Turing machine evolution reflected in the detailed “knitting” of connections on its surface: The rule we have set up allows only one thread of history, so the multiway system is trivial: But with our model the underlying setup is general enough that it can handle not only ordinary deterministic Turing machines in which each possible case leads to a specific outcome, but also non-deterministic ones (as used in formulating NP problems) (e.g. [97]), in which there are multiple outcomes for some cases: Continuing this, we see that the non-deterministic Turing machine shows a fairly complex pattern of branching and merging in the multiway system (this particular example is not causal invariant): After a few more steps, and using a different rendering, the multiway system has the form: (Note that in actually using a non-deterministic Turing machine, say to solve an NP-complete problem, one needs to check the results on each branch of the multiway system—with different search strategies corresponding to using different foliations in exploring the multiway system.) As a final example, consider using our models to reproduce cellular automata. Our models are in a sense intended to be as flexible as possible, while cellular automata have a simple but rigid structure. In particular, a cellular automaton consists of a rigid array of cells, with specific, discrete values that are updated in parallel at each step. In our models, on the other hand, there is no intrinsic geometry, no built-in notion of “values”, and different updating events are treated as independent and “asynchronous”, subject only to the partial ordering imposed by causal relations. In reproducing a Turing machine using our models, we already needed a definite tape that encodes values, but we only had to deal with one action happening at a time, so there was no issue of synchronization. For a cellular automaton, however, we have to arrange for synchronization of updates across all cells. But as we will see, even though our models ultimately work quite differently, there is no fundamental problem in doing this with the models. For example, given the rule 30 cellular automaton we encode a state like in the form where the 6-ary self-loops represent black cells. Note that there is a quite complex structure that in effect maintains the cellular automaton array, complete with “extensible end caps” that allow it to grow. Given this structure, the rule corresponding to the rule 30 cellular automaton becomes where the first two transformations relate to the end caps, and the remaining 8 actually implement the various cases of the cellular automata rule. Applying the rule for our model for a few steps to an initial condition consisting of a single black cell, we get: Each of these steps has information on certain cells in the cellular automaton at a certain step in the cellular automaton evolution. “Decoding” each of the steps in our model shown above, we get the following, in which the “front” of cellular automaton cells whose values are present at that step in our model are highlighted: The particular foliation we have used to determine the steps in the evolution of our model corresponds to a particular foliation of the evolution of the cellular automaton: The final “spacetime” cellular automaton pattern is the same, but the foliation defines a specific order for building it up. We can visualize the way the data flows in the computation by looking at the causal graph (with events forming cells with different colors indicated): Here is the foliation of the causal graph that corresponds to each step in a traditional synchronized parallel cellular automaton updating:
Equivalence and Computation in Our Models 7.2 Alternative Formulations : We have formulated our models in terms of the rewriting of collections of relations between elements. And in this formulation, we might represent a state in one of our models as a list of (here 3-ary) relations {{1, 2, 2}, {3, 1, 4}, {3, 5, 1}, {6, 5, 4}, {2, 7, 6}, {8, 7, 4}} and the rule for the model as {{x, y, z}, {z, u, v}} -> {{w, z, v}, {z, x, w}, {w, y, u}} where x, y, ... are taken to be pattern or quantified variables, suggesting notations like [98] {{x_, y_, z_}, {z_, u_, v_}} -> {{w, z, v}, {z, x, w}, {w, y, u}} or [99]: An alternative to these kinds of symbolic representations is to think—as we have often done here—in terms of transformations of directed hypergraphs. The state of one of our models might then be represented by a directed hypergraph such as while the rule would be: But in an effort to understand the generality of our models—as well as to see how best to enumerate instances of them—it is worthwhile to consider alternative formulations. One possibility to consider is ordinary graphs. If we are dealing only with binary relations, then our models are immediately equivalent to transformations of directed graphs. But if we have general k-ary relations in our models, there is no immediate equivalence to ordinary graphs. In principle we can represent a k-ary hyperedge (at least for k > 0) by a sequence of ordinary graph edges: For the hypergraph above, this then yields: The rule above can be stated in terms of ordinary directed graphs as: In terms of hypergraphs, the result of 5 and 10 steps of evolution according to this rule is and the corresponding result in terms of ordinary directed graphs is: In thinking about ordinary graphs, it is natural also to consider the undirected case. And indeed—as was done extensively in [1:9]—it is possible to study many of the same things we do here with our models also in the context of undirected graphs. However, transformations of undirected graphs lack some of the flexibility and generality that exist in our models based on directed hypergraphs. It is straightforward to convert from a system described in terms of undirected graphs to one described using our models: just represent each edge in the undirected graph as a pair of directed binary hyperedges, as in: Transformations of undirected graphs work the same—though with paired edges. So, for example, the rule which yields becomes which yields: In dealing with undirected graphs—as in [1:9]—it is natural to make the further simplification that all graphs are trivalent (or “cubic”). In the context of ordinary graphs, nothing is lost by this assumption: any higher-valence node can always be represented directly as a combination of trivalent nodes. But the point about restricting to trivalent graphs is that it makes the set of possible rules better defined—because without this restriction, one can easily end up having to specify an infinite family of rules to cover graphs of arbitrary valence that are generated. (In our models based on transformations for arbitrary relations, no analogous issue comes up.) It is particularly easy to get intricate nested structures from rules based on undirected trivalent graphs; it is considerably more difficult to get more complex behavior: Another issue in models based on undirected graphs has to do with the fact that the objects that appear in their transformation rules do not have exactly the same character as the objects on which they act. In our hypergraph-based models, both sides of a transformation are collections of relations (that can be represented by hypergraphs)—just like what appears in the states on which these transformations act. But in models based on undirected graphs, what appears in a transformation is not an ordinary graph: instead it is a subgraph with “dangling connections” (or “half-edges”) that must be matched up with part of the graph on which the transformation acts. Given this setup, it is then unclear, for example, whether or not the rule above—stated in terms of undirected graphs—should be considered to match the graph: (In a sense, the issue is that while our models are based on applying rules to collections of complete hyperedges, models based on undirected graphs effectively apply rules to collections of nodes, requiring “dangling connections” to be treated separately.) Another apparent problem with undirected trivalent graphs is that if the right-hand side of a transformation has lower symmetry than the left-hand side, as in then it can seem “undefined” how the right-hand side should be inserted into the final graph. Having seen our models here, however, it is now clear that this is just one of many examples where multiple different updates can be applied, as represented by multiway systems. A further issue with systems based on undirected trivalent graphs has to do with the enumeration of possible states and possible rules. If a graph is represented by pairs of vertices corresponding to edges, as in the fact that the graph is trivalent in a sense corresponds to a global constraint that each vertex must appear exactly three times. The alternate “vertex-based” representation does not overcome this issue. In our models based on collections of relations, however, there are no such global constraints, and enumeration of possible states—and rules—is straightforward. (In our models, as in trivalent undirected graphs, there is, however, still the issue of canonicalization.) In the end, though, it is still perfectly possible to enumerate distinct trivalent undirected graphs (here dropping cases with self-loops and multiple edges) as well as rules for transforming them, and indeed to build up a rich analysis of their behavior [1:9.12]. Notions such as causal invariance are also immediately applicable, and for example one finds that the simplest subgraphs that do not overlap themselves, and so guarantee causal invariance, are [1:p515][87]: Directed graphs define an ordering for every edge. But it is also possible to have ordered graphs in which the individual edges are undirected, but an order is defined for the edges at any given vertex [87]. Trivalent such ordered graphs can be represented by collections of ordered triples, where each triple corresponds to a vertex, and each number in each triple specifies the destination in the whole list of a particular edge: {{2, 1, 6}, {5, 4, 3}} For visualization purposes one can “name” each element of each triple by a color and then the ordered graph can be rendered as: In the context of our models, an ordered trivalent graph can immediately be represented as a hypergraph with ternary hyperedges corresponding to the trivalent nodes, and binary hyperedges corresponding to the edges that connect these nodes: To give rules for ordered trivalent graphs, one must specify how to transform subgraphs with “dangling connections”. Given the rule (where letters represent dangling connections) {{4, a, b}, {1, c, d}} -> {{4, 8, a}, {1, 11, b}, {10, 2, c}, {7, 5, d}} the evolution of the system is: The corresponding rule for hypergraphs would be and the corresponding evolution is: The rule just shown is example of a rule with 2 -> 4 internal nodes and 4 dangling connections—which is the smallest class that supports growth from minimal initial conditions. There are altogether 264 rules of this type, with rules of the following forms (up to vertex orderings) [87]: These rules produce the following distinct outcomes: Even though there is a direct translation between ordered trivalent graphs and our models, what is considered a simple rule (for example for purposes of enumeration) is different in the two cases. And while it is more difficult to find valid rules with ordered trivalent graphs, it is notable that even some of the very simplest such rules generate structures with limiting manifold features that we see only after exploring thousands of rules in our models: For unordered hypergraphs one can still use a representation like {{1, 2, 3}, {1, 2, 4}, {3, 4, 5}} but now there are no arrows needed within each hyperedge: There is a translation between unordered hypergraphs and ordered ones, or specifically between unordered hypergraphs and directed graphs. Essentially one creates an incidence graph in which each node and each hyperedge in the unordered hypergraph becomes a node in the directed graph—so that the unordered hypergraph above becomes: But despite this equivalence, just as in the case of ordered graphs, the sequence of rules will be different in an enumeration based on unordered hypergraphs from one based on ordered hypergraphs. There are many fewer rules with a given signature for unordered hypergraphs than for ordered ones: Here is an example of a 23 -> 33 rule for unordered hypergraphs: {{x, y, z}, {u, v, z}} -> {{x, x, w}, {u, v, x}, {y, z, y}} Starting from an unordered double ternary self-loop, this evolves as: In general the behavior seen for unordered rules with a given signature is considerably simpler than for ordered rules with the same signature. For example, here is typical behavior seen with a random set of unordered 23 -> 33 rules: In ordered 23 -> 33 rules, globular structures are quite common; in the unordered case they are not. Once one reaches 23 -> 43 rules, however, globular structures become common even for unordered hypergraph rules: {{x, y, z}, {u, y, v}} -> {{x, w, w}, {x, s, z}, {z, s, u}, {y, v, w}} It is worth noting that the concept of unordered hypergraphs can also be applied for binary hyperedges, in which case it corresponds to undirected ordinary graphs. We discussed above the specific case of trivalent undirected graphs, but one can also consider enumerating rules that allow any valence. An example is {{x, y}, {x, z}} -> {{x, w}, {y, z}, {y, w}, {z, w}} which evolves from an undirected double self-loop according to: This rule is similar, but not identical, to a rule we have often used as an example: {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} Interpreting this rule as referring to undirected graphs, it evolves according to: In general, rules for undirected graphs of a given signature yield significantly simpler behavior than rules of the same signature for directed graphs. And, for example, even among all the 7992 distinct 22 -> 52 rules for undirected graphs, no globular structures are seen. Hypergraphs provide a convenient approach to representing our models. But there are other approaches that focus more on the symbolic structure of the models. For example, we can think of a rule such as {{x, y, z}, {z, u, v}} -> {{w, z, v}, {z, x, w}, {w, y, u}} as defining a transformation for expressions involving a ternary operator f together with a commutative and associative (n-ary) operator ∘: In this formulation, the ∘ operator can effectively be arbitrarily nested. But in the usual setup of our models, f cannot be nested. One could certainly imagine a generalization in which one considers (much as in [98]) transformations on symbolic expressions with arbitrary structures, represented by pattern rules like or even: And much as in the previous subsection, it is always possible to represent such transformations in our models, for example by having fixed subhypergraphs that act as “markers” to distinguish different functional heads or different “types”. (Similar methods can be used to have literals in addition to pattern variables in the transformations, as well as “named slots” [100].) Our models can be thought of as abstract rewriting (or reduction) systems that operate on hypergraphs, or general collections of relations. Frameworks such as lambda calculus [101][102] and combinatory logic [103][104] have some similarities, but focus on defining reductions for tree structures, rather than general graphs or hypergraphs. One can ask how our models relate to traditional mathematical systems, for example from universal algebra [105][106]. One major difference is that our models focus on transformations, whereas traditional axiomatic systems tend to focus on equalities. However, it is always possible to define two-way rules or pairs of rules X->Y, Y->X which in effect represent equalities, and on which a variety of methods from logic and mathematics can be used. The general case of our models seems to be somewhat out of the scope of traditional mathematical systems. However, particularly if one considers the simpler case of string substitution systems, it is possible to see a variety of connections [1:p938]. For example, two-way string rewrites can be thought of as defining the relations for a semigroup (or, more specifically, a monoid). If one adds inverse elements, then one has a group. One thinks of the strings as corresponding to words in the group. Then the multiway evolution of the system corresponds to starting with particular words and repeatedly applying relations to them—to produce other words which for the purposes of the group are considered equivalent. This is in a sense a dual operation to what happens in constructing the Cayley graph of a group, where one repeatedly adds generators to words, always reducing by using the relations in the group (see 4.17). For example, consider the multiway system defined by the rule: {AB -> BA, BA -> AB} The first part of the multiway (states) graph associated with this rule is: Ignoring inverse elements (which in this case just make double edges) the first part of the infinite Cayley graph for the group with relations ABBA has the form: One can think of the Cayley graph as being created by starting with a tree, corresponding to the Cayley graph for a free group, then identifying nodes that are related by relations. The edges in the multiway graph (which correspond to updating events) thus have a correspondence to cycles in the Cayley graph. As one further example, consider the (finite) group S3 which can be thought of as being specified by the relations: {"" <-> AA, AA <-> BB, BB <-> ABABAB} The Cayley graph in this case is simply: The multiway graph in this case begins: Continuing for a few more steps gives: On successive steps, the volumes Σt in these multiway graphs grow like: There does not appear to be any direct correspondence to quantities such as growth rates of Cayley graphs (cf. [22]).
Equivalence and Computation in Our Models 7.3 Computational Capabilities of Our Models : An important way to characterize our models is in terms of their computational capabilities. We can always think of the evolution of one of our models as corresponding to a computation: the system starts from an initial state, then follows its rules, in effect carrying out a computation to generate a sequence of results. The Principle of Computational Equivalence [1:12] suggests that when the behavior of our models is not obviously simple it will typically correspond to a computation of effectively maximal sophistication. And an important piece of evidence for this is that our models are capable of universal computation. We saw above that our models can emulate a variety of other kinds of systems. Among these are Turing machines and cellular automata. And in fact we already saw above how our models can emulate what is known to be the simplest universal Turing machine [1:p709][94][95] [96]. We also showed how our models can emulate the rule 30 cellular automaton, and we can use the same construction to emulate the rule 110 cellular automaton, which is known to be computation universal [1:11.8]. So what this means is that we can set up one of our models and then “program” it, by giving appropriate initial conditions, to make it do any computation, or emulate any other computational system. We have seen that our models can produce all sorts of behavior; what this shows is that at least in principle our models can produce any behavior that any computational system can produce. But showing that we can set up one of our models to emulate a universal Turing machine is one thing; it is something different to ask what computations a random one of our models typically performs. To establish this for certain is difficult, but experience with the Principle of Computational Equivalence [1:12] in a wide range of other kinds of systems with simple underlying rules strongly suggests that not only is sophisticated computation possible to achieve in our models, it is also ubiquitous, and will occur basically whenever the behavior we see is not obviously simple. This notion has many consequences, but a particularly important one is computational irreducibility [1:12.6]. Given the simplicity of the underlying rules for our models, we might imagine that it would always be possible—by using some appropriately sophisticated mathematical or computational technique—to predict what the model would do after any number of steps. But in fact what the Principle of Computational Equivalence implies is that more or less whenever it is not obviously straightforward to do, making this prediction will actually take an irreducible amount of computational work—and that in effect we will not be able to compute what the system will do much more efficiently than by just following the steps of the actual evolution of the system itself. Much of what we have done in studying our models here has been based on just explicitly running the models and seeing what they do. Computational irreducibility implies that this is not just something that is convenient in practice; instead it is something that cannot theoretically be avoided, at least in general. Having said this, however, it is an inevitable feature of computational irreducibility that there is always an endless sequence of “pockets” of computational reducibility: specific features or questions that are amenable to computation or prediction without irreducible amounts of computational work. But another consequence of computational irreducibility is the appearance of undecidability [107][108]. If we want to know what will happen in one of our models after a certain number of steps, then in the worst case we can just run the model for that many steps and see what it does. But if we want to know if the model will ever do some particular thing—even after an arbitrarily long time—then there can be no way to determine that with any guaranteed finite amount of effort, and therefore we must consider the question formally undecidable. Will a particular rule ever terminate when running from a particular initial state? Will the hypergraphs it generates ever become disconnected? Will some branch pair generated in a multiway system ever resolve? These are all questions that are in general undecidable in our models. And what the Principle of Computational Equivalence implies is that not only is this the case in principle; it is something ubiquitous, that can be expected to be encountered in studying any of our models that do not show obviously simple behavior. It is worth pointing out that undecidability and computational irreducibility apply both to specific paths of evolution in our models, and to multiway systems. Multiway systems correspond to what are traditionally called non-deterministic computations [109]. And just as a single path of evolution in one of our models can reproduce the behavior of any ordinary deterministic Turing machine, so also the multiway evolution of our models can reproduce any non-deterministic Turing machine. The fact that our models show computation universality means that if some system—like our universe—can be represented using computation of the kind done, for example, by a Turing machine, then it is inevitable that in principle our models will be able to reproduce it. But the important issue is not whether some behavior can in principle be programmed, but whether we can find a model that faithfully and efficiently reflects what the system we are modeling does. Put another way: we do not want to have to set up some elaborate program in the initial conditions for the model we use; we want there to be a direct way to get the initial conditions for the model from the system we are modeling. There is another important point, particularly relevant, for example, in the effort to use our models in a search for a fundamental theory of physics. The presence of computation universality implies that any given model can in principle encode any other. But in practice this encoding can be arbitrarily complicated, and if one is going to make an enumeration of possible models, different choices of encoding can in effect produce arbitrarily large changes in the enumeration. One can think of different classes of models as corresponding to different languages for describing systems. It is always in principle possible to translate between them, but the translation may be arbitrarily difficult, and if one wants a description that is going to be useful in practice, one needs to have a suitable language for it.
Potential Relation to Physics 8.1 Introduction : Having explored our models and some of their behavior, we are now in a position to discuss their potential for application to physics. We shall see that the models generically show remarkable correspondence with a surprisingly wide range of known features of physics, inspiring the hope that perhaps a specific model can be found that precisely reproduces all details of physics. It should be emphasized at the outset that there is much left to explore in the potential correspondence between our models and physics, and what will be said here is merely an indication—and sometimes a speculative one—of how this might turn out. (See also Notes & Further References.)
Potential Relation to Physics 8.2 Basic Concepts : The basic concept of applying our models to physics is to imagine that the complete structure and content of the universe is represented by an evolving hypergraph. There is no intrinsic notion of space; space and its apparent continuum character are merely an emergent large-scale feature of the hypergraph. There is also no intrinsic notion of matter: everything in the universe just corresponds to features of the hypergraph. There is also no intrinsic notion of time. The rule specifies possible updates in the hypergraph, and the passage of time essentially corresponds to these update events occurring. There are, however, many choices for the sequences in which the events can occur, and the idea is that all possible branches in some sense do occur. But the concept is then that there is a crucial simplifying feature: the phenomenon of causal invariance. Causal invariance is a property (or perhaps effective property) of certain underlying rules that implies that when it comes to causal relationships between events, all possible branches give the same ultimate results. As we will discuss, this equivalence seems to yield several core known features of physics, notably Lorentz invariance in special relativity, general covariance in general relativity, as well as local gauge invariance, and the perception of objective reality in quantum mechanics. Our models ultimately just consist of rules about elements and relations. But we have seen that even with very simple such rules, highly complex structures can be produced. In particular, it is possible for the models to generate hypergraphs that can be considered to approximate flat or curved d-dimensional space. The dimension is not intrinsic to the model; it must emerge from the behavior of the model, and can be variable. The evolving hypergraphs in our models must represent not just space, but also everything in it. At a bulk level, energy and momentum potentially correspond to certain specific measures of the local density of evolution in the hypergraph. Particles potentially correspond to evolution-stable local features of the hypergraph. The multiway branching of possible updating events is potentially closely related to quantum mechanics, and much as large-scale limits of our hypergraphs may correspond to physical space, so large-scale limits of relations between branches may correspond to Hilbert spaces of states in quantum mechanics. In the case of physical space, one can view different choices of updating orders as corresponding to different reference frames—with causal invariance implying equivalence between them. In multiway space, one can view different updating orders as different sequences of applications of quantum operators—with causal invariance implying equivalence between them that lead different observers to experience the same reality. In attempting to apply our models to fundamental physics, it is notable how many features that are effectively implicitly assumed in the traditional formalism of physics can now potentially be explicitly derived. It is inevitable that our models will show computational irreducibility, in the sense that irreducible amounts of computational work will in general be needed to determine the outcome of their behavior. But a surprising discovery is that many important features of physics seem to emerge quite generically in our models, and can be analyzed without explicitly running particular models. It is to be expected, however, that specific aspects of our universe—such as the dimensionality of space and the masses and charges of particles—will require tracing the detailed behavior of models with particular rules. It is already clear that modern mathematical methods can provide significant insight into certain aspects of the behavior of our models. One complication in the application of these methods is that in attempting to make correspondence between our models and physics, many levels of limits effectively have to be taken, and the mathematical definitions of these limits are likely to be subtle and complex. In traditional approaches to physics, it is common to study some aspect of the physical world, but ignore or idealize away other parts. In our models, there are inevitably close connections between essentially all aspects of physics, making this kind of factored approach—as well as idealized partial models—much more difficult. Even if the general structure of our models provides an effective framework for representing our physical universe at the lowest level, there does not seem to be any way to know within a wide margin just how simple or complex the specific rule—or class of equivalent rules—for our particular universe might be. But assuming a certain degree of simplicity, it is likely that fitting even a modest number of details of our universe will completely determine the rule. The result of this would almost certainly be a large number of specific predictions about the universe that could be made even without irreducibly large amounts of computation. But even absent the determination of a specific rule, it seems increasingly likely that experimentally accessible predictions will be possible just from general features of our models.
Potential Relation to Physics 8.3 Potential Basic Translations : As a guide to the potential application of our models to physics, we list here some current expectations about possible translations between features of physics and features of our models. This should be considered a rough summary, with every item requiring significant explanation and qualification. In addition, it should be noted that in an effort to clarify presentation, many highly abstract concepts have been indicated here by more mechanistic analogies.
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: space: : general limiting structure of basic hypergraph
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: time: : index of causal foliations of hypergraph rewriting
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: matter (in bulk): : local fluctuations of features of basic hypergraph
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: energy: : flux of edges in the multiway causal graph through spacelike (or branchlike) hypersurfaces
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: momentum: : flux of edges in the multiway causal graph through timelike hypersurfaces
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: (rest) mass: : numbers of nodes in the hypergraph being reused in updating events
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: motion: : possible because of causal invariance; associated with change of causal foliations
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: particles: : locally stable configurations in the hypergraph
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: charge, spin, etc.: : associated with local configurations of hyperedges
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: quantum indeterminacy: : different foliations (of branchlike hypersurfaces) in the multiway graph
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: quantum effects: : associated with locally unresolved branching in the multiway graph
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: quantum states: : (instantaneously) nodes in the branchial graph
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: quantum entanglement: : shared ancestry in the multiway graph / distance in branchial graph
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: quantum amplitudes: : path counting and branchial directions in the multiway graph
Potential Relation to Physics 8.3 Potential Basic Translations: Basic Physics Concepts: quantum action density (Lagrangian): : total flux (divergence) of multiway causal graph edges
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: special relativity: : global consequence of causal invariance in hypergraph rewriting
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: general relativity / general covariance: : effect of causal invariance in the causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: locality / causality: : consequence of locality of hypergraph rewriting and causal invariance
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: rotational invariance: : limiting homogeneity of the hypergraph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: Lorentz invariance: : consequence of causal invariance in the causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: time dilation: : effect of different foliations of the causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: relativistic mass increase: : effect of different foliations of the causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: local gauge invariance: : consequence of causal invariance in the multiway graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: lack of quantum cosmological constant: : space is effectively created by quantum fluctuations
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: cosmological homogeneity: : early universe can have higher effective spatial dimension
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: expansion of universe: : growth of hypergraph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: conservation of energy: : equilibrium in the causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: conservation of momentum: : balance of different hyperedges during rewritings
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: principle of equivalence: : gravitational and inertial mass both arise from features of the hypergraph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: discrete conservation laws: : features of the ways local hypergraph structures can combine
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: microscopic reversibility: : limiting equilibrium of hypergraph rewriting processes
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: quantum mechanics: : consequence of branching in the multiway system
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: observer in quantum mechanics: : branchlike hypersurface foliation
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: quantum objective reality: : equivalence of quantum observation frames in the multiway graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: quantum measurements: : updating events with choice of outcomes, that can be frozen by a foliation
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: quantum eigenstates: : branches in multiway system
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: quantum linear superposition: : additivity of path counts in the multiway graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: uncertainty principle: : non-commutation of update events in the multiway graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: wave-particle duality: : relation between spacelike and branchlike projections of the multiway causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: operator-state correspondence: : states in the multiway graph are generated by events (operators)
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: path integral: : turning of paths in the multiway graph is proportional to causal edge density
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: violation of Bell’s inequalities, etc.: : existence of causal connections in the multiway graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: quantum numbers: : associated with discrete local properties of the hypergraph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: quantization of charge, etc.: : consequence of the discrete hypergraph structure
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: black holes / singularities: : causal disconnection in the causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: dark matter: : (possibly) relic oligons / dimension changes in of space
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: virtual particles: : local structures continually generated in the spatial and multiway graphs
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: black hole radiation / information: : causal disconnection of branch pairs
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Theories & Principles: holographic principle: : correspondence between spatial and branchial structure
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: dimension of space: : growth rate exponent in hypergraph / causal cones
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: curvature of space: : polynomial part of growth rate in hypergraph / causal cones
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: local gauge group: : limiting automorphisms of local hypergraph configurations
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: speed of light (c): : measure of edges in spatial graph vs. causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: light cones: : causal cones in the causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: unit of energy: : count of edges in the causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: momentum space: : limiting structure of causal graph in terms of edges
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: gravitational constant: : proportionality between node counts and spatial volume
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: quantum parameter (ℏ): : measure of edges in the branchial graph (maximum speed of measurement)
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: elementary unit of entanglement: : branching of single branch pair
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: electric/gauge charges: : counts of local hyperedge configurations
Potential Relation to Physics 8.3 Potential Basic Translations: Physical Quantities & Constructs: spectrum of particles: : spectrum of locally stable configurations in the hypergraph
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: inertial frame: : parallel foliation of causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: rest frame of universe: : geodesically layered foliation of causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: flat space: : uniform hypergraph (typically not maintained by rules)
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: Minkowski space: : effectively uniform causal graph
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: cosmological constant: : uniform curvature in the hypergraph
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: de Sitter space: : cyclically connected hypergraph
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: closed timelike curves: : loops in the causal graph (only possible in some rules)
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: point particle: : a persistent structure in the hypergraph involving comparatively few nodes
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: purely empty space: : not possible in our models (space is maintained by rule evolution)
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: vacuum: : statistically uniform regions of the spatial hypergraph
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: vacuum energy: : causal connections attributed purely to establishing the structure of space
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: isolated quantum system: : disconnected part of the branchial/multiway graph
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: collapse of the wave function: : degenerate foliation that infinitely retards branchlike entanglement
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: non-interacting observer in quantum mechanics: : “parallel” foliation of multiway graph
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: free field theory: : e.g. pure branching in the multiway system
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: quantum computation: : following multiple branches in multiway system (limited by causal invariance)
Potential Relation to Physics 8.3 Potential Basic Translations: Idealizations, etc. Used in Physics: string field theory: : (potentially) continuous analog of the multiway causal graph for string substitutions
Potential Relation to Physics 8.4 The Structure of Space : In our models, the structure of spacetime is defined by the structure of the evolving hypergraph. Causal foliations of the evolution can be used to define spacelike hypersurfaces. The instantaneous structure of space (on a particular spacelike hypersurface) corresponds to a particular state of the hypergraph. A position in space is defined by a node in the hypergraph. A geometrical distance between positions can be defined as the number of hyperedges on the shortest path in the hypergraph between them. Although the underlying rules for hypergraph rewriting in our models depend on the ordering of elements in hyperedges, this is ignored in computing geometrical distance. (The geometrical distance discussed here is basically just a proxy for a true physical distance measured from dynamic information transmission between positions.) A shortest path on the hypergraph between two positions defines a geodesic between them, and can be considered to define a straight line. The only information available to define the structure of space is the connectivity of the hypergraph; there is no predefined embedding or topological information. The continuum character of space assumed in traditional physics must emerge as a large-scale limit of the hypergraph (somewhat analogously to the way the continuum character of fluids emerges as a large-scale limit of discrete molecular dynamics (e.g. [1:p378][110]). Although our models follow definite rules, they can intrinsically generate effective randomness (much like the rule 30 cellular automaton, or the computation of the digits of π). This effective randomness makes large-scale behavior typically approximate statistical averages of small-scale dynamics. In our models, space has no intrinsic dimension defined; its effective dimension must emerge from the large-scale structure of the hypergraph. Around every node at position X consider a geodesic ball consisting of all nodes that are a hypergraph distance not more than r away. Let Vr(X) be the total number of nodes in this ball. Then the hypergraph can be considered to approximate d-dimensional space if for a suitable range of values of r. Here we encounter the first of many limits that must be taken. We want to consider the limit of a large hypergraph (say as generated by a large number of steps of evolution), and we want r to be large compared to 1, but small compared to the overall diameter of the hypergraph. As a simple example, consider the hypergraph created by the rule {{x, y}, {x, z}} -> {{x, y}, {x, w}, {y, w}, {z, w}} Starting from a minimal initial condition of two self-loops, the first few steps of evolution with our standard updating order are: The hypergraph obtained after 12 steps has 1651 nodes and can be rendered as: This plots the effective “dimension exponent” of r in Vr as a function of r, averaged over all nodes in the hypergraph, for a succession of steps in the evolution: A constant limiting value d indicates approximation to a “flat” d-dimensional space. For integer d, this corresponds to ordinary d-dimensional Euclidean space, but in our models d often does not end up being integer valued, nor does it need to be constant at different positions, or through the course of evolution. It is also important to note that only some rules give Vr~ rd; exponential or more complex behavior is common. Even when to leading order Vr~ rd, there are corrections. For small r (measured, say, relative to the diameter of the hypergraph) one can consider a power series expansion in r. By comparison to ordinary manifolds one can then write (e.g. [24][1:p1050]) where R can be identified as the (Ricci) scalar curvature [25][26] of the limiting space. The value of this curvature is again purely determined by the (limiting) structure of the hypergraph. (Note that particularly if one goes beyond a pure power series, there is the potential for subtle interplay between change in dimension and what one might attribute to curvature.) It is also possible to identify other limiting features of the hypergraph. For example, consider a small stretch of geodesic (where by “small” we mean still large compared to individual connections in the hypergraph, but small compared to the scale on which statistical features of the hypergraph change). Now create a tube of radius r by including every node with distance up to r from any node on the geodesic. The growth rate of the number of nodes in this tube can then be approximated as [44] where now Rijδxiδxj is the projection of the Ricci tensor along the direction of the geodesic. (The Ricci tensor measures the change in cross-sectional area for a bundle of geodesics, associated with their respective convergence and divergence for positive and negative curvature.) In a suitable limit, the nodes in the hypergraph correspond to points in a space. A tangent bundle at each point can be defined in terms of the equivalence class of geodesics through that point, or in our case the equivalence class of sequences of hyperedges that pass through the corresponding node in the hypergraph. One can set up what in the limit can be viewed as a rank-p tensor field on the hypergraph by associating values with p hyperedges at each node. When these values correspond to intrinsic features of the hypergraph (such as Vr), their limits give intrinsic properties of the space associated with the hypergraph. And for example the Riemann tensor can be seen as emerging from essentially measuring areas of “rectangles” defined by loops in the hypergraph, though in this case multiple limits need to be taken.
Potential Relation to Physics 8.5 Time and Spacetime : In our models the passage of time basically corresponds to the progressive updating of the hypergraph. Time is therefore fundamentally computational: its passage reflects the performance of a computation—and typically one that is computationally irreducible. It is notable that in a sense the progression of time is necessary even to maintain the structure of space. And this effectively forces the entropic arrow of time (reflected in the effective randomization associated with irreducible computation) to be aligned with the cosmological arrow of time (defined by the overall evolution of the structure of space). At the outset, time in our models has a very different character from space. The phenomenon of causal invariance, however, implies a link which leads to relativistic invariance. To see this, we can begin much as in the traditional development of special relativity [111] by considering what constitutes a physically realizable observer. In our model, everything is represented by the evolving hypergraph, including all of the internal state of any observer. One consequence of this is that the only way an observer can “sense” anything about the universe is by some updating event happening within the observer. And indeed in the end all that any observer can ultimately be sensitive to is the causal relationships between different updating events that occur. From a particular evolution history of a hypergraph, we can construct a causal graph whose nodes correspond to updating events, and whose directed edges represent the causal relations between these events—in the sense that there is an edge between events A and B if the input to B involves output from A. For the evolution shown above, the beginning of the causal graph is: We can think of this causal graph as representing the evolution of our system in spacetime. The analog of a light cone is then the set of nodes that can be reached from a given node in the graph. Every edge in the graph represents a timelike relationship between events, and can be thought of as corresponding to a timelike direction in spacetime. Nodes that cannot be reached from each other by following edges of the graph can be thought of as spacelike separated. Just as for space with the “spatial hypergraphs” we discussed above, there is nothing in the abstract that defines the geometry of spacetime associated with the causal graph; everything must emerge from the pattern of connections in the graph, which in turn are generated by the operation of the underlying rules for our models. In its original construction, a causal graph is in a sense a causal summary of a particular evolution history for a given rule, with a particular sequence of updating events. But when the underlying rule has the property of causal invariance, this has the important consequence that in the appropriate limit the causal graph obtained always has the same form, independent of the particular sequence of updating events. In other words, when there is causal invariance, the system in a sense always has a unique causal history. The interpretation of this causal history in terms of a spacetime history, however, depends on what amount to definitions made by an observer. In particular, to define what can be interpreted as a time coordinate, one must set up a foliation of the causal graph, with successive slices corresponding to successive steps in time. There are many such foliations that can be set up. The only fundamental constraint is that events in a given slice cannot be directly connected by an edge in the causal graph—or, in other words, they must be spacelike separated. The possible foliations thus correspond to possible sequences of spacelike hypersurfaces, analogous to those in standard discussions of spacetime. (Note that the causal graph ultimately just defines a partial order on the set of events, and one could in principle imagine having arbitrarily complex foliations set up to imply any given total order of events. But such foliations are not realistic for macroscopic observers with bounded computational resources, and in our analysis of observable continuum limits we can ignore them.) When one reaches a particular spacelike hypersurface, it represents a particular set of events having occurred, and thus a particular state of the underlying system having been reached, represented by a particular hypergraph. Different sequences of spacelike hypersurfaces thus correspond to different sequences of “instantaneous states” having been reached—corresponding to different evolution histories. But the crucial point is that causal invariance implies that even though the sequences of instantaneous states are different, the causal graphs representing the causal relationships between events that occur in them are always the same. And this is the essence of how the phenomena of relativistic invariance—and general covariance—are achieved.
Potential Relation to Physics 8.6 Motion and Special Relativity : In the traditional formalism of physics, the principles of special relativity are in a sense introduced as axioms, and then their consequences are derived. In our models, what amount to these principles can in effect emerge directly from the models themselves, without having to be introduced from outside. To see how this works, consider the phenomenon of motion. In standard physics, one thinks of different states of uniform motion as corresponding to different inertial reference frames (e.g. [111][112]). These different reference frames in turn correspond to different choices of sequences of spacelike hypersurfaces, or, in our setup, different foliations of the causal graph. As a simple example, consider the string substitution system BA -> AB, starting from ...BABABA... The causal graph for the evolution of this system can be drawn as a grid: A simple foliation is just to form successive layers: With this foliation, the sequence of states in the underlying string substitution system is: In drawing our foliation of the causal graph, we can think of time as being vertical, and space horizontal. Now imagine we want to represent uniform motion. We can do this by making our foliation use slices with a slope proportional to velocity: But imagine we want to show time vertically, while not destroying the partial order in our causal network. The unique way to do it (if we want to preserve straight lines) is to transform a point {t, x} to : But this is precisely the usual Lorentz transformation of special relativity. And time dilation is then, for example, associated with the fact that to reach what corresponds to an event at slice t in the original foliation, one now has to go through a sequence of events that is longer by a factor of . Normally one would argue for these results on the basis of principles supplied by special relativity. But the crucial point here is that in our models the results can be derived purely from the behavior of the models, without introducing additional principles. Imagine simply using the transformed causal graph to determine the order of updating events in the underlying substitution system: If we look vertically down the picture we see a different sequence of states of the system. But the crucial point is that the final outcome of the evolution is exactly the same as it was with the original foliation. In some sense the “physics” is the same, independent of the reference frame. And this is the essence of relativistic invariance (and here we immediately see some of its consequences, like time dilation). But in the context of the string substitution system, we can now see its origin of the invariance. It is the fact that the underlying rule we have used is causal invariant, so that regardless of the specific order in which updating events occur, the same causal graph is obtained, with the same final output. In our actual models based on infinitely evolving hypergraphs, the details are considerably more complicated. But the principles are exactly the same: if the underlying rule has causal invariance, its limiting behavior will show relativistic invariance, and (so long as it has limiting geometry corresponding to flat d-dimensional space) all the usual phenomena of special relativity. (Note that the concept of a finite speed of light, leading effectively to locality in the causal graph, is related to the fact that the underlying rules involve rewriting hypergraphs only of bounded size.)
Potential Relation to Physics 8.7 The Vacuum Einstein Equations : In discussing the structure of space, we considered how the volumes of geodesic balls grow with radius. In discussing spacetime, we want to consider the analogous question of how the volumes of light cones [1:p1052]) grow with time. But to do this, we have to say what we mean by time, since—as we saw in the previous subsection—different foliations can lead to different identifications. Any particular foliation—with its sequence of spacelike hypersurfaces—provides at every point a timelike vector that defines a time direction in spacetime. So if we start at any point in the causal graph, we can look at the forward light cone from this point, and follow the connections in the causal graph until we have gone a proper time t in the time direction we have defined. Then we can ask how many nodes we have reached in the causal graph. The result will depend on the underlying rule for the system. But if in the limit it is going to correspond to flat (d + 1)-dimensional spacetime, at any spacetime position X it must grow like: If we include the possibility of curvature, we get to first order where Rμν  is the spacetime Ricci tensor, and  δtμ δtνRμν is effectively its projection along the infinitesimal timelike vector δtμ. For any particular underlying rule, Ct(X) will take on a definite form. But in making connections with traditional continuum spacetime, we are interested in its limiting behavior. Assume, to begin, that we have scaled t to be measured relative to the size of the whole causal graph. Then for small t we can expand Ct(X) to get the expression involving curvature above. But now imagine scaling up t. Eventually it is inevitable that the curvature term has the potential to affect the overall t dependence, and potentially change the effective exponent of t. But if the overall continuum limit is going to correspond to a (d + 1)-dimensional spacetime, this cannot happen. And what this means is that at least a suitably averaged version of the curvature term must not in fact grow [1:9.15]. The details are slightly complicated [113], but suffice it to say here that the constraint on Rμν is obtained by averaging over directions, then averaging over positions with a weighting determined by the volume element  associated with the metric gμν defined by our choice of hypersurfaces. The requirement that this average not grow when t is scaled up can then be expressed as the vanishing of the variation of , which is precisely the usual Einstein–Hilbert action—thereby leading to the conclusion that Rμν must satisfy exactly the usual vacuum Einstein equations [114][115][75][116]: A full derivation of this is given in [113]. Causal invariance plays a crucial role, ensuring for example that timelike directions ti associated with different foliations give invariant results. Much like in the derivation of continuum fluid behavior from microscopic molecular dynamics (e.g. [110]), one also needs to take a variety of fairly subtle limits, and one needs sufficient intrinsic generation of effective randomness [1:7.5] to justify the use of certain statistical averages. But there is a fairly simple interpretation of the result above. Imagine all the geodesics that start at a particular point in the causal graph. The further we go, the more possible geodesic paths there will be in the graph. To achieve a power law corresponding to a definite dimension, the geodesics must in a sense just “stream outwards”, evenly distributed in direction. But the Ricci tensor specifically measures the rate at which bundles of geodesics change their cross-sectional area. And as soon as this change is nonzero, it will inevitably change the local density of geodesics and eventually grow to disrupt the power law. And so the only way a fixed limiting dimension can be achieved is for the Ricci curvature to vanish, just as it does according to the vacuum Einstein equations. (Note that higher-order terms, involving for example the Weyl tensor and other components of the Riemann tensor, yield changes in the shape of bundles of geodesics, but not in their cross-sectional area, and are therefore not constrained by the requirement of fixed limiting dimension.)
Potential Relation to Physics 8.8 Matter, Energy and Gravitation : In our models, not only space, but also everything “in space”, must be represented by features of our evolving hypergraphs. There is no notion of “empty space”, with “matter” in it. Instead, space itself is a dynamic construct created and maintained by ongoing updating events in the hypergraph. And what we call “matter”—as well as things like energy—must just correspond to features of the evolving hypergraph that somehow deviate from the background activity that we call “space”. Anything we directly observe must ultimately have a signature in the causal graph. And a potential hypothesis about energy and momentum is that they may simply correspond to excess “fluxes” of causal edges in time and space. Consider a simple causal graph in which we have marked spacelike and timelike hypersurfaces: The basic idea is that the number of causal edges that cross spacelike hypersurfaces would correspond to energy, and the number that cross timelike hypersurfaces would correspond to momentum (in the spatial direction defined by a given hypersurface). Inevitably the results one gets would depend on the hypersurfaces one chooses, and so would differ from one observer to another. And one important feature of this identification of energy and momentum is that it would explain why they follow the same relativistic transformations as time and space. In effect space and time are probing distances between nodes in the causal graph (as measured relative to a particular foliation), while momentum and energy are probing a directly dual property: the density of edges. There is additional subtlety here, though, because causal edges are needed just to maintain the structure of spacetime—and whatever we measure as energy and momentum must just be some excess in the density of causal edges over the “background” corresponding to space. But even to know what we mean by density we have to have some notion of volume, but this is also itself defined in terms of edges in the causal graph. But as a rough idealized picture, we might imagine that we have a causal graph that maintains the same overall structure, but adds some extra connections: In our actual models, the causal graphs one gets are considerably more complicated. But one can still identify some features from the simple idealization. The basic concept is that energy and momentum add “extra causal connections” that are not “necessary” to define the basic structure of spacetime. In a sense the core thing that defines the structure of spacetime is the way that “elementary light cones” are knitted together. Consider a causal graph like: One can think of a set of edges like the ones indicated as in effect “outlining” the causal graph. But then there are other edges that add “extra connections”. The edges that “outline the graph” in effect maximally connect spatially separated regions—or in a sense transmit causal information at a maximum speed. The other edges one can think of as having slower speeds—so they are typically drawn closer to vertical in a rendering like the one above. But now let us return to our simple grid idealization of the causal graph—with additional vertical edges added. Now do foliations like the ones we used above to represent inertial frames, parametrized by a velocity ratio β relative to the maximum speed (taken to be 1). Define E(β) to be the density of causal edge crossing of the spacelike hypersurfaces, and p(β) the corresponding quantity for timelike hypersurfaces. Then for speed 1 edges, we have (up to an overall multiplier) (cf. [111][112]): But in general for edges with speed α we have which means that for any β thus showing that our crossing densities transform like energy and momentum for a particle with mass . In other words, we can potentially identify edges that are not maximum speed in the causal graph as corresponding to “matter” with nonzero rest mass. Perhaps not surprisingly, this whole setup is quite analogous to thinking about world lines of massive particles in elementary treatments of relativity. But in our context, all of this must emerge from underlying features of the evolving hypergraph. Causal connections that transfer information at maximum speed can be thought of as arising from updating events that involve maximally separate nodes, and that are somehow always entraining “fresh” nodes. But causal connections that transfer information more slowly are associated with sequences of updating events that in effect reuse nodes. So in other words, rest mass can be thought of as being associated with local collections of nodes in the hypergraph that allow repeated updating events to occur without the involvement of other nodes. Given this setup, it is possible to derive other features of energy, momentum and mass by methods similar to those used in typical discussions of relativity. It is first helpful to include units in the quantities we have introduced. If an elementary light cone has timeline extent T then we can consider its spacelike extent to be c T, where c is the speed of light. Within the light cone let us say that there are effectively μ causal edges oriented in the timelike direction. With the inertial frame foliations used above, the contribution of these causal edges to energy and momentum will be (the factor c in the energy case comes from the spacelike extent of the light cone): But if we define the mass m as  and substitute , we get the standard formulas of special relativity [111][112], or to first order establishing in our model the relation E = m c2 between energy and rest mass. We should note that with our identification for energy and momentum, the conservation of energy becomes essentially the statement that the overall density of events in the causal network does not change as we progress through successive spacelike surfaces. And, as we will discuss later, if in effect the whole hypergraph is in some kind of dynamic equilibrium, then we can reasonably expect that this will be the case. Expansion (or, more specifically, non-uniform expansion) will lead to effective violations of energy conservation, much as it does for an expanding universe in the traditional formalism of general relativity [117][75]. In the previous subsection, we discussed the overall structure of spacetime, and we used the growth rate of the spacetime volume Ct(X) as a way to assess this. But now let us ask about specific values of Ct(X), complete with their “constant” multipliers. We can think of these multipliers as probing the local density of the causal graph. But deviations in this are what we have now identified as being associated with matter. To compute Ct(X) we ultimately need to be able to precisely count events in the causal graph. If the causal graph is somehow “uniform”, then it cannot contain what can be considered to be “matter”. In the setup we have defined, the presence of matter is effectively associated with “fluxes” of causal edges that reflect the non-uniform “arrangement” of nodes in the causal graph. To represent this, take ρ(X) to be the “local density” of nodes in the causal graph. We can make a series expansion to probe deviations from uniformity in ρ(X). And formally we can write where the tμ are timelike vectors used in the definition of Ct and now Tμν is effectively a tensor that represents “fluxes of edges” in the causal graph. But these fluxes are what we have identified as energy and momentum, and when we think about how causal edges traverse spacelike and timelike hypersurfaces, Tμν turns out to correspond exactly to the standard energy-momentum tensor of general relativity. So now we can combine our formula for the effect of local density with our formula for the effect of curvature from the previous section to get: But if we apply the same argument as in the previous subsection, then to maintain limiting fixed dimension we get the condition which has exactly the form of Einstein’s equations in the presence of matter [114][115][75][116]. Just as we interpreted the curvature part of these equations in the previous subsection in terms of the change in area of geodesic bundles, we can interpret the “matter” part in terms of the change of geodesics associated with additional local connections. As an example, consider starting with a 2D hexagonal grid. Now imagine adding edges at each node. Doing this creates additional connections and additional geodesics, eventually producing something like the hyperbolic space examples in 4.2. So what the equation says is that any such effect, which would lead to negative curvature, must be compensated by positive curvature in the “background” spacetime—just as general relativity suggests.
Potential Relation to Physics 8.9 Elementary Particles : Elementary particles are entities that—at least for some period—preserve their identity through space and time. In the context of our models, one can imagine that particles would correspond to structures in the hypergraph that are locally stable under the application of rules. As an idealized example, consider rules that operate on an ordinary graph, and have the property of preserving planarity. Such rules can never remove non-planarity from a graph. But it is a basic result of graph theory [37][118] that any non-planarity can always be attributed to one of the two specific subgraphs: If one inserts such subgraphs into an otherwise planar graph, they behave very much as “particle-like” structures. They can move around, but unless they meet and annihilate, they are preserved: There are presumably analogs of this in hypergraph-rewriting rules of the kind that appear in our models. Given a particular set of rules, the expectation would be that a certain set of local sub-hypergraphs would be preserved by the rules. Existing results in graph theory do not go very far in elucidating the details. However, there are analogs in other systems that provide some insight. Cellular automata provide a particularly good example. Consider the rule 110 cellular automaton [1:p32]. Starting from a random initial condition, the picture below shows how the system evolves to a collection of localized structures: The form of these structures is hard to determine directly from the rule. (They are a little like hard-to-predict solutions to a Diophantine equation.) But by explicit computation one can determine for example that rule 110 supports the following types of localized structures [1:p292][119] as well as the growing structure: There is a complex web of possible interactions between localized structures, that can at least in some cases be interpreted in terms of conservation laws: As in cellular automata, it is likely that not every one of our models will yield localized structures, although there is reason to think that some form of conserved structure will be more common in hypergraph rewriting than in cellular automata. But as in cellular automata, one can expect that with a given underlying rule, there will be a discrete set of possible localized structures, with hard-to-predict sizes and properties. The particular set of localized structures will probably be quite specific to particular rules. But as we will discuss in the next subsection, there will often be symmetries that cause collections of similar structures to exist—or in fact force certain structures to exist. In the previous subsection, we discussed the interpretation of energy and momentum in terms of additional edges in a causal graph. For particles, the expectation would be that there is a certain “core” structure that defines the core properties of a particle (like spin, charge, etc.), but that this structure is spread across a region of the hypergraph that maintains the “activity” associated with energy and momentum. It is worth noting that even in an example like non-planarity, it is perfectly possible for topological-like features to effectively be spread out across many nodes, while still maintaining their discrete character. In the previous subsection, we discussed the potential origin of rest mass in terms of “reuse” of nodes in the hypergraph. Once again, this seems to fit in well with our notion of the nature of particles—and to make it perfectly possible to imagine both “massive” and “massless” particles, associated with different kinds of structures in the evolving hypergraph. In a system like the rule 110 cellular automaton, there is a clear “background structure” on which it is possible to identify localized structures. In some very simple cases, similar things happen in our models. For example, consider the rule: {{x, y}, {y, z, u, v}} -> {{x, y, z, u}, {u, v}} The evolution of this rule yields behavior like in which there is a circular “background”, with a localized “particle-like” deformation. The causal graph (here generated for a larger case) also shows evidence of a particle-like structure on a simple grid-like background: But in most of our models the “background” tends to be much more complicated, and so such direct methods for identifying particles cannot be used. But as an alternative, one can consider exploring the effect of perturbations, as in 4.14. In effect, one starts the system with a perturbation, then sees whether the perturbation somehow decomposes into quantized elements that one can identify as “particles”. (The process is quite analogous to producing particles in a high-energy collision.) Such quantized effects are at best rare in class 3 cellular automata, but they are a defining feature of class 4 cellular automata, and there is reason to believe that they will be at least fairly common in our models. The defining feature of a localized “particle-like” structure is that it is capable of long-range propagation in the system. But the presence of even short-lived instances of particle-like structures will also potentially be identifiable—though with a certain margin of error—from detailed properties of the hypergraph in small regions. And in the “background” evolution of our models, one can expect that short-lived instances of particle-like structures will continually be being created and destroyed. The process that in a sense “creates the structure of space” in our models can thus also be thought of as producing a “vacuum” full of particle-like activity. And particularly when this is combined with the phenomenon (to be discussed in a later subsection) that pairs of particle-like structures can be produced and subsequently merged in the multiway system, there is some definite similarity with the ubiquitous virtual particles that appear in traditional treatments of quantum field theory.
Potential Relation to Physics 8.10 Reversibility and Irreversibility : One feature of the traditional formalism for fundamental physics is that it is reversible, in the sense that it implies that individual states of closed systems can be uniquely evolved both forward and backward in time. (Time reversal violation in things like Ko particle decays show that the rule for going forward and backward in time can be slightly different. In addition, the cosmological expansion of the universe defines an overall arrow of time.) One can certainly set up manifestly reversible rewriting rules (like A->B, B->A) in models like ours. And indeed the example of cellular automata [1:9.2] tends to suggest that most kinds of behavior seen in irreversible rules can also be seen—though perhaps more rarely—in reversible rules. But it is important to realize that even when the underlying rules for a system are not reversible, the system can still evolve to a situation where there is effective reversibility. One way for this to happen is for the evolution of the system to lead to a particular set of “attractor” states, on which the evolution is reversible. Another possibility is that there is no such well-defined attractor, but that the system nevertheless evolves to some kind of “equilibrium” in which measurable effects show effective reversibility. In our models, there is an additional complication: the fact that different possible updating orders lead to following different branches of the multiway system. In most kinds of systems, irreversible rules tend to be associated with the phenomenon of multiple initial states merging to produce a single final state in which the information about the initial state is lost. But when there is a branch in a multiway system, this is reversed: information is effectively created by the branch, and lost if one goes backwards. When there is causal invariance, however, yet something different happens. Because now in a sense every branching will eventually merge. And what this means is that in the multiway system there is a kind of reversibility: any information created by a branching will always be destroyed again when the branches merge—even though temporarily the “information content” may change. It is important to note that this kind of microscopic reversibility is quite unrelated to the more macroscopic irreversibility implied by the Second Law of thermodynamics. As discussed in [1:9.3] the Second Law seems to first and foremost be a consequence of computational irreducibility. Even when the underlying rules for a system are reversible, the actual evolution of the system can so “encrypt” the initial conditions that no computationally feasible measurement process will succeed in reconstructing them. (The idea of considering computational feasibility clarifies past uncertainty about what might count as a reasonable “coarse graining procedure”.) In any nontrivial example of one of our models, computational irreducibility is essentially inevitable. And this means that the model will tend to intrinsically generate effective randomness, or in other words, the computation it does will obscure whatever simplicity might have existed in its initial conditions. There can still be large-scale features—or particle-like structures—that persist. But the presence of computational irreducibility implies that even at a level as low as the basic structure of space we can expect our models to show the kind of irreversibility associated with the Second Law. And in a sense we can view this as the reason that things like a robust structure for space can exist: because of computational irreducibility, our models show a kind of equilibrium in which the details are effectively random, and the only features that are computationally feasible to measure are the statistical regularities.
Potential Relation to Physics 8.11 Cosmology, Expansion & Singularities : In our models the evolving hypergraph represents the whole universe, and the expansion of the universe is potentially a consequence of the growth of the hypergraph. In the minimal case of a model involving a single transformation rule, the growth of the hypergraph must be monotonic, although the rate can vary depending on the local structure of the hypergraph. If there are multiple transformation rules, there can be both increase and decrease in hypergraph size. (Even with a single rule, there is also still the possibility—discussed below—of effective size decrease as a result of pieces of the hypergraph becoming disconnected.) In the case of uniform growth, measurable quantities such as length and energy would essentially all continually scale as the universe evolves. The core structure of particles—embodied for example in topological-like features of the hypergraph—could potentially persist even as the number of nodes “within them” increases. Since the rate of increase in size in the hypergraph would undoubtedly greatly exceed the measurable growth rate of the universe, uniform growth implies a kind of progressive refinement in which the length scale of the discrete structure of the hypergraph becomes ever more distant from any given measured length scale—so that in effect the universe is becoming an ever closer approximation to continuous. In traditional cosmology, one thinks of the universe as effectively having exactly three dimensions of space (cf. [120]). In our models, dimension is in effect a dynamical variable. Possibly some of what is normally attributed to curvature in space can instead be reformulated as dimension change. But even beyond this, there is the potential for new phenomena associated, for example, with local change of dimension. In general, a change of dimension—like curvature—affects the density of geodesics. Changes of dimension generated by an underlying rule may potentially lead to effects that for example mimic the presence of mass, or positive or negative energy density. (There could also be dimension-change “waves”, perhaps with some rather unusual features.) In our models, the universe starts from some initial configuration. It could be something like a single self-loop hypergraph. Or in the multiway system it could be multiple initial hypergraphs. (Note that we can always “put the initial conditions into the rule” by adding a rule that says “from nothing, create the initial conditions”.) An obvious question is whether any traces of the initial conditions might persist, perhaps even through the whole evolution of the system. The effective randomness associated with computational irreducibility in the evolution will inevitably tend to “encrypt” most features of the initial conditions [1:9.3] to the point where they are unrecognizable. But it is still conceivable that, for example, some global symmetry breaking associated with the first few hypergraph updating events could survive—and the remote possibility exists that this could be visible today in the large-scale structure of the universe, say as a pattern of density fluctuations in the cosmic microwave background. Our models have potentially important implications for the early universe. If, for example, the effective dimension of the universe was initially much higher than 3 (as is basically inevitable if the initial conditions are small), there will have been a much higher level of causal contact between different parts of the universe than we have deduced by extrapolating the 3D expansion of the universe today [1:p1055]. (In effect this happens because the volume of the past light cone will grow like td—or perhaps exponentially with t—and not just like t3.) As we discussed in 2.9, it is perfectly possible in our models for parts of the hypergraph to become disconnected as a result of the operation of the rule. But assuming that the rule is local (in the sense that its left-hand side is a connected hypergraph), pieces of the hypergraph that become disconnected can never interact again. Even independent of outright disconnection of the spatial graph, it is also possible for the causal graph to “tear” into disconnected parts that can never interact again (see 6.10): A disconnection in the causal graph corresponds to an event horizon in our system—that cannot be crossed by any timelike curve. (And indeed our causal graphs—consisting as they do of “elementary light cones knitted together”—are like microscopic analogs of the causal diagrams often used in studying general relativity. Note that in our models, as in ordinary general relativity, there are two kinds of event horizons: “cosmic”—like the one above—in which effectively two distinct subuniverses are created, and “black hole”, in which there is a region of the causal graph with incoming, but not outgoing, edges.) We can also ask about other extreme phenomena in spacetime. Closed timelike curves correspond to loops in the causal graph, and with some rules they can occur. But they do not represent any real form of “time travel”; they just correspond to the presence of states that are precisely repeated as a result of the evolution of the system. (Note that in our models, time effectively corresponds to the progression of computation, and has a very different underlying character from something like space.) Wormholes and effective faster-than-light travel are not specifically excluded by the structure of our models, especially insofar as there can potentially be deviations in the effective local dimensionality of space. But insofar as the conditions to get general relativity as a limiting effective theory are satisfied, these will occur only in the circumstances where they do in that theory.
Potential Relation to Physics 8.12 Basic Concepts of Quantum Mechanics : Quantum mechanics is a key known feature of physics, and also, it seems, a natural and inevitable feature of our models. In classical physics—or in a system like a cellular automaton—one basically has rules that specify a unique path of history for the evolution of a system. But our models are not set up to define any such unique path of history. Instead, the models just give possible rewrites that can be performed on hypergraphs—but they do not say when or where these rewrites should be applied. So this means that—like the formalism of quantum mechanics—our models in a sense allow many different paths of history. There is, however, ultimately nothing non-deterministic about our models. Although they allow many different sequences of updating events—each of which can be viewed as a different path of history—the models still completely determine the overall set of possible sequences of updating events. And indeed at a global level, everything about the model can be captured in a multiway graph [1:5.6]—like the one below—with nodes in the graph corresponding to states of the system (here, for simplicity, a string substitution system), and every possible path through the graph corresponding to a possible history. In the standard formalism of quantum mechanics, one usually just imagines that all one can determine are probabilities for different histories or different outcomes. But this has made it something of a mystery why we have the impression that a definite objective reality seems to exist. One possible explanation would be that at some level a branch of reality exists for every possible behavior, and that we just experience the branch that our thread of consciousness has happened to follow. But our models immediately suggest another, more complete, and arguably much more scientifically satisfying, possibility. In essence, they suggest that there is ultimately a global objective reality, defined by the multiway system, and it is merely the locality of our experience that causes us to describe things in terms of probabilities, and all the various detailed features of the standard formalism of quantum mechanics. We will proceed in two stages. First, we will discuss the notion of an observer in the context of multiway systems, and the relation of this to questions about objective reality. And having done this, we will be in a position to discuss ideas like quantum measurement, and the role that causal invariance turns out to play in allowing observers to experience definite, seemingly classical results. So how might we represent a quantum observer in our models? The first key point is that the observer—being part of the universe—must themselves be a multiway system. And in addition, everything the observer does—and experiences—must correspond to events that occur in the model. This latter point also came up when we discussed spacetime—and we concluded there that it meant we only needed to consider the graph of causal relationships between events. To characterize any given observer, we then just had to say how the observer would sample this causal graph. A typical example in studying spacetime is to consider an observer in an inertial reference frame—which corresponds to a particular foliation of the causal graph. But in general to characterize what any observer will experience in the course of time, we need some sequence of spacelike hypersurfaces that form a foliation which respects the causal relationships—and thus the ordering relations between events—defined by the causal graph. But now we can see an analog of this in the quantum mechanical case. However, instead of considering foliations of the causal graph, what we need to consider now are foliations of the multiway graph: In the course of time, the observer progresses through such a foliation, in effect at each step observing some collection of states, with certain relationships between them. A different observer, however, might want to sample the states differently, and might effectively define a different foliation. One can potentially think of a different foliation as being a different “quantum observation frame” or “quantum frame”, analogous to the different reference frames one considers in studying spacetime. In the case of something like an inertial frame, one is effectively defining how an observer will sample different parts of space over the course of time. In a quantum observation frame one might have a more elaborate specification, involving sampling particular states of relevance to some measurement or another. But the key point is that a quantum observer can in principle use any quantum observation frame that corresponds to a foliation that respects the relationships between states defined by the multiway graph (and thus has a meaningful notion of time). In both the spacetime case and the quantum case, the slices in the foliation are indexed by time. But while in the spacetime case, where each slice corresponds to a spacelike hypersurface that spans ordinary space, in the quantum case, each slice corresponds to what we can call a branchlike hypersurface that spans not ordinary space, but instead the space of states, or the space of branches in the multiway system. But even without knowing the details of this space, we can already come to some conclusions. In particular, we can ask what observers with different quantum observation frames—and thus different choices of branchlike hypersurfaces—will conclude about relationships between states. And the point is that so long as the foliations that are used respect the orderings defined by the multiway graph, all observers must inevitably come to the same conclusions about the structure of the multiway graph—and therefore, for example, the relationships between states. Different observers may sample the multiway graph differently, and experience different histories, but they are always ultimately sampling the same graph. One feature of traditional quantum formalism is its concept of making measurements that effectively reduce collections of states—as exist in a multiway system—to what is basically a single state analogous to what would be seen in a classical single path of evolution. From the point of view of quantum observation frames, one can think of such a measurement as being achieved by sculpting the quantum observation frame to effectively pick out a single state in the multiway system: We will discuss this in more detail below. But the basic idea is as follows. Imagine that our universe is based on a simple string substitution system such as {A -> AB}. If we start from a state AA, as in the picture above, the multiway evolution from this state immediately leads to multiple outcomes, associated with different updating events. But let us say that we just wanted some kind of “classical” summary of the evolution, ignoring all these different branches. One thing we might do is not trace individual updates, but instead just look at “generational states” (5.21) in which all updates that can consistently be applied together have been applied. And with the particular rule shown here, we then get the unique sequence of states highlighted above. And as we will discuss below, we can indeed consider these generational states as corresponding to definite (“classical-like”) states of the system, that can consistently be thought of as potential results of measurements. But now let us imagine how this might work in something closer to a complete experiment. We are running the multiway system shown above. Multiple states are being generated. But at some moment we as observers notice that actually several states that have been produced (say ABBA and AABB) can be combined together to form a consistent generational state (ABBABB). But even though these states ultimately had a common ancestor, they now seem to be on different “branches of history”. But now causal invariance makes a crucial contribution. Because it implies that all such different branches must eventually converge. And indeed after a couple of steps, the fully assembled generational state ABBABB appears in the multiway system. To us as observers this is in a sense the state we were looking for (it is the “result of our measurement”), and as far as possible, we want to use it as our description of the system. And by setting up an appropriate quantum observation frame, that is exactly what we can do. For example, as illustrated in the picture above, we can make the foliation we choose effectively freeze the generational state, so that in the description we use of the system, the state stays the same in successive slices. The structure of the multiway system puts constraints on what foliations we can consistently set up. In the case shown here, it does allow us to freeze this particular state forever, but to do this consistently, it effectively forces us to freeze more and more states over time. And as we will see later, this kind of spreading of effects in the multiway graph is closely related to decoherence in the standard formalism of quantum mechanics. In what we just discussed, causal invariance is what guarantees that states the observer notices can consistently be assembled to form a generational (“classical-like”) state that will always actually converge in the multiway system to form that state. But it is worth pointing out that (as discussed in [121]) strict causal invariance is not ultimately needed for a picture like this to work. Recall that the observer themselves is also a multiway system. So “within their consciousness” there will usually be many “simultaneous” states. Looked at formally from the outside, the observer can be seen to involve many distinct states. But one could imagine that the internal experience of the observer would be in effect to conflate these states. Causal invariance ensures that branches in the multiway system will actually merge—just as a result of the evolution of the multiway system. But if the observer “experientially” conflates states, this in effect represents an additional way in which different branches in the multiway system will at least appear to merge [121]. Formally, one can think of this—in analogy to the operation of automated theorem-proving systems—as like the observer “adding lemmas” that assert the equivalence of branches, thereby allowing the system to be “completed” to the point where relevant branches converge. (For a given system, there is still the question of whether only a sufficiently bounded number of lemmas is needed to achieve the convergence one wants.) Independent of whether there is strict causal invariance or not, there is also the question of what kinds of quantum observation frames are possible. In the end—just like in the spacetime case—such frames reflect the description one is choosing to make of the world. And setting up different “coordinates”, one is effectively changing one’s description, and picking out different aspects of a system. And ultimately the restrictions on frames are computational ones. Something like an inertial frame in spacetime is simple to describe, and its coordinates are simple to compute. But a frame that tries to pick out some very particular aspect of a quantum system may run into issues of computational irreducibility. And as a result, much as happens in connection with the Second Law of thermodynamics [1:9.3], there can still for example be elaborate correlations that exist between different parts of a quantum system, but no realistic measurement—defined by a computationally feasible quantum observation frame—will succeed in picking them out.
Potential Relation to Physics 8.13 Quantum Formalism : To continue understanding how our models might relate to quantum mechanics, it is useful to describe a little more of the potential correspondence with standard quantum formalism. We consider—quite directly—each state in the multiway system as some quantum basis state |S>. An important feature of quantum states is the phenomenon of entanglement—which is effectively a phenomenon of connection or correlation between states. In our setup (as we will see more formally soon), entanglement is basically a reflection of common ancestry of states in the multiway graph. (“Interference” can then be seen as a reflection of merging—and therefore common successors—in the multiway graph.) Consider the following multiway graph for a string substitution system: Each pair of states generated by a branching in this graph are considered to be entangled. And when the graph is viewed as defining a rewrite system, these pairs of states can also be said to form a branch pair. Given a particular foliation of the multiway graph, we can now capture the entanglement of states in each slice of the foliation by forming a branchial graph in which we connect the states in each branch pair. For the string substitution system above, the sequence of branchial graphs is then: In physical terms, the nodes of the branchial graph are quantum states, and the graph itself forms a kind of map of entanglements between states. In general terms, we expect states that are closer on the branchial graph to be more correlated, and have more entanglement, than ones further away. As we discussed in 5.17, the geometry of branchial space is not expected to be like the geometry of ordinary space. For example, it will not typically correspond to a finite-dimensional manifold. We can still think of it as a space of some kind that is reached in the limit of a sufficiently large multiway system, with a sufficiently large number of states. And in particular we can imagine—for any given foliation—defining coordinates of some kind on it, that we will denote . So this means that within a foliation, any state that appears in the multiway system can be assigned a position  in “multiway space”. In the standard formalism of quantum mechanics, states are thought of as vectors in a Hilbert space, and now these vectors can be made explicit as corresponding to positions in multiway space. But now there is an additional issue. The multiway system should represent not just all possible states, but also all possible paths leading to states. And this means that we must assign to states a weight that reflects the number of possible paths that can lead to them: In effect, therefore, each branchlike hypersurface can be thought of as exposing some linear combination of basic states, each one with a certain weight: Let us say that we want to track what happens to some part of this branchlike hypersurface. Each state undergoes updating events that are represented by edges in the multiway graph. And in general the paths followed in the multiway graph can be thought of as geodesics in multiway space. And to determine what happens to some part of the branchlike hypersurface, we must then follow a bundle of geodesics. A notable feature of the multiway graph is the presence of branching and merging, and this will cause our bundle of geodesics to diverge and converge. Often in standard quantum formalism we are interested in the projection of one quantum state on another < | >. In our setup, the only truly meaningful computation is of the propagation of a geodesic bundle. But as an approximation to this that should be satisfactory in an appropriate limit, we can use distance between states in multiway space, and computing this in terms of the vectors  the expected Hilbert space norm [122][123] appears: . Time evolution in our system is effectively the propagation of geodesics through the multiway graph. And to work out a transition amplitude <i | S | f> between initial and final states we need to see what happens to a bundle of geodesics that correspond to the initial state as they propagate through the multiway graph. And in particular we want to know the measure (or essentially cross-sectional area) of the geodesic bundle when it intersects the branchlike hypersurface defined by a certain quantum observation frame to detect the final state. To analyze this, consider a single path in the multiway system, corresponding to a single geodesic. The critical observation is that this path is effectively “turned” in multiway space every time a branching event occurs, essentially just like in the simple example below: If we think of the turns as being through an angle θ, the way the trajectory projects onto the final branchlike hypersurface can then be represented by ei θ. But to work out the angle θ for a given path, we need to know how much branching there will be in the region of the multiway graph through which it passes. But now recall that in discussing spacetime we identified the flux of edges through spacelike hypersurfaces in the causal graph as potentially corresponding to energy. The spacetime causal graph, however, is just a projection of the full multiway causal graph, in which branchlike directions have been reduced out. (In a causal invariant system, it does not matter what “direction” this projection is done in; the reduced causal graph is always the same.) But now suppose that in the full multiway causal graph, the flux of edges across spacelike hypersurfaces can still be considered to correspond to energy. Now note that every node in the multiway causal graph represents some event in the multiway graph. But events are what produce branching—and “turns”—of paths in the multiway graph. So what this suggests is that the amount of turning of a path in the multiway graph should be proportional to energy, multiplied by the number of steps, or effectively the time. In standard quantum formalism, energy is identified with the Hamiltonian H, so what this says is that in our models, we can expect transition amplitudes to have the basic form ei H t—in agreement with the result from quantum mechanics. To think about this in more detail, we need not just a single energy quantity—corresponding to an overall rate of events—but rather we want a local measure of event rate as a function of location in multiway space. In addition, if we want to compute in a relativistically invariant way, we do not just want the flux of causal edges through spacelike hypersurfaces in some specific foliation. But now we can make a potential identification with standard quantum formalism: we suppose that the Lagrangian density ℒ corresponds to the total flux in all directions (or, in other words, the divergence) of causal edges at each point in multiway space. But now consider a path in the multiway system going through multiway space. To know how much “turning” to expect in the path, we need in effect to integrate the Lagrangian density along the path (together with the appropriate volume element). And this will give us something of the form ei S, where S is the action. But this is exactly what we see in the standard path integral formulation of quantum mechanics [124]. There are many additional details (see [121]). But the correspondence between our models and the results of standard quantum formalism is notable. It is worth pointing out that in our models, something like the Lagrangian is ultimately not something that is just inserted from the outside; instead it must emerge from actual rules operating on hypergraphs. In the standard formalism of quantum field theory, the Lagrangian is stated in terms of quantum field operators. And the implication is therefore that the structure of the Lagrangian must somehow emerge as a kind of limit of the underlying discrete system, perhaps a bit like how fluid mechanics can emerge from discrete underlying molecular dynamics (or cellular automata) [110]. One notable feature of standard quantum formalism is the appearance of complex numbers for amplitudes. Here the core concept is the turning of a path in multiway space; the complex numbers arise only as a convenient way to represent the path and understand its projections. But there is an additional way complex numbers can arise. Imagine that we want to put a metric on the full  space of the multiway causal graph. The normal convention for  space is to have real-number coordinates and a norm based on t2 – x2—but an alternative is use i t for time. In extending to  space, one might imagine that a natural norm which allows the contributions of t, x and b components to be appropriately distinguished would be t2 – x2 + i b2.
Potential Relation to Physics 8.14 Quantum Measurement : Above we gave a brief summary of how quantum measurement can work in the context of our models. Here we give some more detail. In a sense the key to quantum measurement is reconciling our notion that “definite things happen in the universe” with the formalism of quantum mechanics—or the branching structure of a multiway system. But if definite things are going to happen, what might they be? Here we will again consider the example of a string substitution system, although the core of what we say also applies to the full hypergraph case. Consider the rule {A -> AB, B -> A} We could imagine a simple “classical” procedure for evolving according to this rule, in which we just do all updates we can (say, based on a left-to-right scan) at each step: But in fact we know that there are many other possibilities, that can be represented by the multiway system: Most of the states that appear in the multiway system are, however, “unfinished”, in the sense that there are additional “independent” updates that can consistently be done on them. For example, with the rule {A -> BA} there are 4 separate updates that can be applied to AAAA: But none of these depend on the others, so they can in effect all be done together, giving the result BABABABA. Put another way, all of these updates involve “spacelike separated” parts of the string, so they are all causally independent, and can all consistently be carried out at the same time. As discussed in 5.21, doing all updates across a state together can be thought of as evolving a system in “generational steps” to produce “generational states”. In some multiway cases, there may be a single sequence of generational states: In other cases, there can be several branches of generational states: The presence of multiple branches is a consequence of having a mixture of spacelike and branchlike separated events that can be applied to a single state. For example, with the rule {A -> AB, A -> BB} the first and second updates here are spacelike separated, but the first and third are branchlike separated: A view of quantum measurement is that it is an attempt to describe multiway systems using generational states. Sometimes there may be a unique “classical path”; sometimes there may be several outcomes for measurements, corresponding to several generational states. But now let us consider the actual process of doing an experiment on a multiway system—or a quantum system. Our basic goal is—as much as possible—to describe the multiway system in terms of a limited number of generational states, without having to track all the different branches in the multiway system. At some point in the evolution of a string substitution system we might see a large number of different strings. But we can view them all as part of a single generational state if they in effect yield only spacelike separated events. In other words, if the strings can be assembled without “branchlike ambiguity” they can be thought of as forming a consistent generational state. In the standard formalism of quantum mechanics, we can think of the states in the multiway system as being quantum states. The construct we form by “assembling” these states can be thought of as a superposition of the states. Causal invariance then implies that through the evolution of the multiway system any such superposition will then actually become a single quantum state. In some sense the observer “did nothing”: they just notionally identified a collection of states. It was the actual evolution of the system that produced the specific combined state. In describing a quantum system—or a multiway system—one must in effect define coordinates, and in particular one must specify what foliation one is going to use to represent the progress of time. And this freedom to pick a “quantum observation frame” is critical in being able to maintain a view in which one imagines “definite things to happen” in the system. With a foliation like the following, at any given time there is a mixture of different states, and no attempt has been made to find a way to “summarize” what the system is doing: Consider, however, a foliation like the following: In this picture, generational states have been highlighted, and a foliation has been selected that essentially “freezes time” around a particular generational state. In effect, the observer is choosing a quantum observation frame in which there is a definite classical outcome for the behavior of the system. “Freezing time” around a particular state is something an observer can choose to do in their description of the system. But the crucial point is that the actual dynamics of the evolution of the multiway system cause this choice to have implications. In particular, in the case shown, the region of the multiway system in which “time is frozen” progressively expands. The choice the observer has made to freeze a particular state is causing more and more states to have to be considered as similarly frozen. In the physics of quantum measurement, one is used to the idea that for a quantum measurement to be considered to have a definite result, it must involve more and more quantum degrees of freedom. What we see here is effectively a manifestation of this phenomenon. In freezing time in something like the foliation in the picture above what we are effectively doing is creating a coordinate singularity in defining our quantum observation frame. And there is an analogy to this in general relativity and the physics of spacetime. Just as we freeze time in our quantum frame, so also we can freeze time in a relativistic reference frame. For example, as an object approaches the event horizon of a black hole, its time as described by a typical coordinate system set up by an observer far from the black hole will become frozen—and just like in our quantum case, we will consider the state to stay fixed. But there is a complicated issue here. To what extent is the singularity—and the freezing of time—a feature of our description, and to what extent is it something that “really happens”? This depends in a sense on the relationship one has to the system. In traditional thinking about quantum measurement, one is most interested in the “impressions” of observers who are in effect embedded in the system. And for them, the coordinate system they chose in effect defines their reality. But one can also imagine being somehow “outside the system”. For example, one might try to set up a quantum experiment (or a quantum computer) in which the construction of the system somehow makes it natural to maintain a “frozen time” foliation. The picture below shows a toy example in which the multiway system by its very construction has a terminal state for which time does not advance: But now the question arises of what can be achieved in the multiway system corresponding to the actual physical universe. And here we can expect that one will not be able to set up truly isolated states, and that instead there will be continual inevitable entanglement. What one might have imagined could be maintained as a separate state will always become entangled with other states. The picture below shows a slightly more realistic multiway system, with an attempt to construct a foliation that freezes time: And what we see here is that in a sense the structure of the multiway graph limits the extent to which we can freeze time. In effect, the multiway system forces decoherence—or entanglement—just by its very structure. We should note that it is not necessarily the case that there is just a single possible sequence of generational states, corresponding in a sense to a single possible “classical path”. Here is an example where there are four generational states that occur at a particular generational step. And now we can for example construct a foliation that—at least for a while—“freezes time” for all of these generational states: It is worth pointing out that if we try to freeze time for something that is not a proper generational state, there will be an immediate issue. A proper generational state contains the results of all spacelike separated events at a particular point in the evolution of a system. So when we freeze time for it, we are basically allowing other branchlike separated events to occur, but not other spacelike separated ones. However, if we tried to freeze time for a state that did not include all spacelike separated events, there would quickly be a mismatch with the progress of time for the excluded events—or in effect the singularity of quantum observation frame would “spill over” into a singularity in the causal graph, leading to a singularity in spacetime. In other words, the fact that the states that appear in quantum measurement are generational states is not just a convenience but a necessity. Or, put another way, in doing a quantum measurement we are effectively setting up a singularity in branchial space, and only if the states we measure are in effect “complete in spacetime” will this singularity be kept only in branchial space; otherwise it will also become a singularity in physical spacetime. In general, when we talk about quantum measurement, we are talking about how an observer manages to construct a description of a system that in effect allows the observer to “make a conclusion” about what has happened in the system. And what we have seen is that appropriate “time-freezing foliations” allow us to do this. And while there may be some restrictions, it is usually in principle possible to construct such foliations in a multiway system, and to have them last as long as we want. But in practice, as the pictures above begin to suggest, after a while the foliations we have to construct can get increasingly complicated. In effect, what we are having to do in constructing the foliation is to “reverse engineer” the actual evolution of the multiway system, so that with our elaborate description we are still managing to maintain time as frozen for a particular state, carefully avoiding complicated entanglements that have built up with other states. But there is a problem here. Because in effect we are asking the observer to “outcompute” the system itself. Yet we can expect that the evolution of the multiway system, say for one of our models, will usually correspond to an irreducible computation. And so we will be asking the observer to do a more and more elaborate computation to maintain the description they are using. And as soon as the computation required exceeds the capability of the observer, the observer will no longer be able to maintain the description, and so decoherence will be inevitable. It is worthwhile to compare this situation with what happens in thermodynamic processes, and in particular with apparent entropy increase. In a reversible system, it is always in principle possible to recognize, say, that the initial conditions for the systems were simple (and “low entropy”). But in practice the actual configurations of the system usually become complicated enough that this is increasingly difficult to do. In traditional statistical mechanics one talks of “coarse-grained” measurements as a way to characterize what an observer can actually analyze about a system. In computational terms we talk about the computational capabilities of the observer, and how computational irreducibility in the evolution of the system will eventually overwhelm the computational capabilities of the observer, making apparent entropy increase inevitable [1:9.3]. In the quantum case, we now see how something directly analogous happens. The analog of coarse graining is the effort to create a foliation with a particular apparent outcome. But eventually this becomes infeasible, and—just like in the thermodynamic case—we in effect see “thermalization”, which we can now attribute to the effects of computational irreducibility.
Potential Relation to Physics 8.15 Operators in Quantum Mechanics : In standard quantum formalism, there are states, and there are operators (e.g. [125]). In our models, updating events are what correspond to operators. In the standard evolution of the multiway system, all applicable operators are in effect “automatically applied” to every state to generate the actual evolution of the system. But to understand the correspondence with standard quantum formalism, we can imagine just applying particular operators by doing only particular updating events. Consider the string substitution system: {AB -> ABA, BA -> BAB} In this system we effectively have two operators O1 and O2, corresponding to these two possible updating rules. We can think about building up an operator algebra by considering the relations between different sequences of applications of these operators. In particular, we can study the commutator: [O1,O2] = O1O2 – O2O1 In terms of the underlying rules, this commutator corresponds to: At the first step, the results of applying O1 and O2 to the initial state are different, and we can say that the states generated form a branch pair. But then at the second step, the branch pair resolves, and the branches merge to the same state. In effect, we can represent this by saying that O1 and O2 commute, or that: [O1,O2] = O1O2 – O2O1 = 0 In general, there is a close relationship between causal invariance—and its implication for the resolution of all branch pairs—and the commuting of operators. And given our discussion above this should not be considered surprising: as we discussed, when there is causal invariance, it means that all branches can resolve to a single (“classical”) state, just like in standard quantum formalism the commuting of operators is associated with seemingly classical behavior. But there is a key point here: even if causal invariance implies that branch pairs (and similarly commutators) will eventually resolve, they may take time to do so. And it is this delay in resolution that is the core of what leads to what we normally think of as quantum effects. Once a branch pair has resolved, there are no longer multiple branches, and a single state has emerged. But before the branch pair has resolved, there are multiple states, and therefore what one might think of as “quantum indeterminacy”. In the case where a branch pair has not yet resolved, the corresponding commutator will be nonzero—and in a sense the value of the commutator measures the branchlike distance between the states reached by applying the two different updates (corresponding to the two different operators). In our model for spacetime, if a single event in the causal graph is connected in the causal graph to two different events we can ask what the spacelike separation of these events might be, and we might suppose that this spatial distance is determined by the speed of light c (say multiplied by the elementary time corresponding to traversal of the causal edge). In thinking now about the multiway system, we can ask what the branchlike separation of states in a branch pair might be. This will now be a distance on a branchial graph—or effectively a distance in state space—and we can suppose that this distance is determined by ℏ. And depending on our conventions for measuring branchial distance, we might introduce an i, yielding a setup very much aligned with traditional quantum formalism. Another interpretation of the non-commuting of operators is connected to the entanglement of quantum states. And here we now have a very direct picture of entanglement: two states are entangled if they are part of the same unresolved branch pair, and thus have a common ancestor. The multiway graph gives a full map of all entanglements. But at any particular time (corresponding to a particular slice of a foliation defined by a quantum observation frame), the branchial graph gives a snapshot that captures the “instantaneous” configuration of entanglements. States closer on the branchial graph are more entangled; those further apart are less entangled. It is important to note that distance on the branchial graph is not necessarily correlated with distance on the spatial graph. If we look at events, we can use the multiway causal graph to give a complete map of all connections, involving both branchlike and spacelike (as well as timelike) separations. Ultimately, the underlying rule determines what connections will exist in the multiway causal graph. But just as in the standard formalism of quantum mechanics, it is perfectly possible for there to be entanglement of spacelike-separated events.
Potential Relation to Physics 8.16 Wave-Particle Duality, Uncertainty Relations, Etc. : Wave-particle duality was an early but important concept in standard quantum mechanics, and turns out to be a core feature of our models, independent even of the details of particles. The key idea is to look at the correspondence between spacelike and branchlike projections of the multiway causal graph. Let us consider some piece of “matter”, ultimately represented as features of our hypergraphs. A complete description of what the matter does must include what happens on every branch of the multiway graph. But we can get a picture of this by looking at the multiway causal graph—which in effect has the most complete representation of all meaningful spatial and branchial features of our models. Fundamentally what we will see is a bundle of geodesics that represent the matter, propagating through the multiway causal graph. Looked at in terms of spacelike coordinates, the bundle will seem to be following a definite path—characteristic of particle-like behavior. But inevitably the bundle will also be extended in the branchlike direction—and this is what leads to wave-like behavior. Recall that we identified energy in spacetime as corresponding to the flux of causal edges through spacelike hypersurfaces. But as mentioned above, whenever causal edges are present, they correspond to events, which are associated with branching in the multiway graph and the multiway causal graph. And so when we look at geodesics in the bundle, the rate at which they turn in multiway space will be proportional to the rate at which events happen, or in other words, to energy—yielding the standard E ∝ ω proportionality between particle energy and wave frequency. Another fundamental phenomenon in quantum mechanics is the uncertainty principle. To understand this principle in our framework, we must think operationally about the process of, for example, first measuring position, then measuring momentum. It is best to think in terms of the multiway causal graph. If we want to measure position to a certain precision Δ x we effectively need to set up our detector (or arrange our quantum observation frame) so that there are O(1/Δ x) elements laid out in a spacelike array. But once we have made our position measurement, we must reconfigure our detector (or rearrange our quantum observation frame) to measure momentum instead. But now recall that we identified momentum as corresponding to the flux of causal edges across timelike hypersurfaces. So to do our momentum measurement we effectively need to have the elements of our detector (or the pieces of our quantum observation frame) laid out on a timelike hypersurface. But inevitably it will take at least O(1/Δ x) updating events to rearrange the elements we need. But each of these updating events will typically generate a branch in the multiway system (and thus the multiway causal graph). And the result of this will be to produce an O(1/Δ x) spread in the multiway causal graph, which then leads to an O(1/Δ x) uncertainty in the measurement of momentum. (Another ultimately equivalent approach is to consider different foliations, and to note for example that with a finer foliation in time, one is less able to determine the “true direction” of causal edges in the multiway graph, and thus to determine how many of them will cross a spacelike hypersurface.) To make our discussion of the uncertainty principle more precise, we should consider operators—represented by sequences of updating events. In the  space of the multiway causal graph, the operators corresponding to position and momentum must generate events that correspond to moving at different angles; as a result the operators do not commute. And with this setup we can see why position and momentum, as well as energy and time, form canonically conjugate pairs for which uncertainty relations hold: it is because these quantities are associated with features of the multiway causal graph that probe distinct (and effectively orthogonal) directions in multiway causal space.
Potential Relation to Physics 8.17 Correspondence between Relativity and Quantum Mechanics : One of the surprising consequences of the potential application of our models to physics is their implications around deep relationships between relativity and quantum mechanics. These are particularly evident in thinking about the multiway causal graph. As a toy model, consider the graph: Timelike edges go down, but then in each slice there are spacelike and branchlike edges. A more realistic example of the very beginning of such a graph is: The multiway causal graph in a sense captures in one graph both relativity and quantum mechanics. Time is involved in both of them, and in our models it is an essentially computational concept, involving progressive application of the underlying rules of the system. But then relativity is associated with the structure formed by spacelike and timelike edges, while quantum mechanics is primarily associated with the structure formed by branchlike and timelike edges. The spacelike direction corresponds to ordinary physical space; the branchlike direction is effectively the space of quantum states. Distance in the spacelike direction is ordinary spacetime distance. Distance in the branchlike direction reflects the level of quantum entanglement between states. When we form foliations in time, spacelike hypersurfaces represent in a sense the instantaneous configuration of space, while branchlike hypersurfaces represent the instantaneous entanglements between quantum states. It should be emphasized that (unlike in the idealization of our first picture above) the detailed structure of the spacelike+timelike component of the multiway causal graph will in practice be very different from that of the branchlike+timelike one. The spacelike+timelike component is expected to limit to something like a finite-dimensional manifold, reflecting the characteristics of physical spacetime. The branchlike+timelike one potentially limits to an infinite dimensional space (that is perhaps a projective Hilbert space), reflecting the characteristics of the space of quantum states. But despite these substantial geometrical differences, one can expect many structural aspects and consequences to be basically the same. We are used to the idea of motion in space. In the context of our models—and of the multiway causal graph—motion in space in effect corresponds to progressively sampling more spacelike edges in the graph. But now we can see a quantum analog: we can also have motion in the branchlike direction, in which, in effect, we progressively sample more branchlike edges, reaching more quantum states. Velocity in space is thus the analog of the rate at which additional states are sampled (and thus entangled). In relativity there is a fairly well-developed notion of an idealized observer. The observer is typically represented by some some causal foliation of spacetime—like an inertial reference frame that moves without forces acting on it. One can also define an observer in quantum mechanics, and in the context of our models it makes sense—as we have done above—to parametrize the observer in terms of a quantum observation frame that consists not of a sequence of spacelike hypersurfaces, but instead of a series of branchlike ones. A quantum observation frame in a sense defines a plan for how an observer will sample possible quantum states—and the analog of an inertial frame in spacetime is presumably a quantum observation frame that corresponds to a fixed plan that cannot be affected by anything outside. And in general, the analog in quantum mechanics of a world line in relativity is presumably a measurement plan. In special relativity a key idea is to think about comparing the perceptions of observers in different inertial frames. But in the context of our models we can now do the exact same thing for quantum observers. And the analog of relativistic invariance then becomes a statement of perception or measurement invariance: that in the end different quantum observers (despite the branching of states) in a sense perceive the same things to happen, or, in other words, that there is at some level an objective reality even in quantum mechanics. Our analogy between relativity and quantum mechanics suggests asking about quantum analogs of standard relativistic phenomena. One example is relativistic time dilation, in which, in effect, sampling spacelike edges faster reduces the rate of traversing timelike edges. The analog in quantum mechanics is presumably the quantum Zeno effect [126][127], in which more rapid measurement—corresponding to faster sampling of branchlike edges—slows the time evolution of a quantum system. A key concept in relativity is the light cone, which characterizes the maximum rate at which causal effects spread in spacelike directions. In our models, spacetime causal edges in effect define elementary light cones, which are then knitted together by the structure of the (spacetime) causal graph. But now in our models there is a direct analog for quantum mechanics, visible in the full multiway causal graph. In the multiway causal graph, every event effectively has a cone of causal influence. Some of that influence may be in spacelike directions (corresponding to ordinary relativistic light cone effects), but some of it may be in branchlike directions. And indeed, whenever there are branches in the multiway graph, these correspond to branchlike edges in the multiway causal graph. So what this means is that in addition to a light cone of effects in spacetime, there is also what we may call an entanglement cone, which defines the region affected in branchial space by some event. In the light cone case, the spacelike extent of the light cone is set by the speed of light (c). In the entanglement cone case (as we will discuss below) the branchlike extent of the entanglement cone is essentially set by ℏ. As we have mentioned, the definition of time is shared between spacelike and branchlike components of the multiway causal graph. Another shared concept appears to be energy (or in general, energy-momentum, or action). Time is effectively defined by displacement in the timelike direction; energy appears to be defined by the flux of causal edges in the timelike direction. In the relativistic setting, energy can be thought of as flux of causal edges through spacelike hypersurfaces; in the quantum mechanical setting, it can be thought of as a flux of causal edges through branchlike hypersurfaces. An important feature of the spacetime causal graph is that it can potentially describe curved space, and reproduce general relativity. And here again we can now see that in our models there are analogs in quantum mechanics. One issue, though, is that whereas ordinary space is—at least on a large scale—finite-dimensional, comparatively flat, and well modeled by a simple Lorentzian manifold, branchial space is much more complicated, probably in the limit infinite–dimensional, and not at all flat. At a mathematical level, we are in quantum mechanics used to forming commutators of operators, and in many cases finding that they do not commute, with their “deviation” being measured by ℏ. In general relativity, one can also form commutators, and indeed the Riemann tensor for measuring curvature is precisely the result of computing the commutator of two covariant derivatives. And perhaps even more analogously the Ricci scalar curvature gives the angle deficit for transport around a loop in spacetime. In our context, therefore, the non-flatness of space is directly analogous to a core phenomenon of quantum mechanics: the non-commuting of operators. In the general relativity case, we are used to thinking about the propagation of bundles of geodesics in spacetime, and the fact that the Ricci scalar curvature determines the local cross-section of the bundle. Now we can also consider the more general propagation of bundles of geodesics in the multiway causal graph. But when we look along branchlike directions, the limiting space we see tends to be highly connected, and effectively of high negative curvature. And what this means is that a bundle of geodesics can be expected to spread out rapidly in branchlike directions. But this has an immediate interpretation in quantum mechanics: it is the phenomenon of decoherence, whereby quantum effects get spread (and entangled) across large numbers of quantum degrees of freedom. In relativity, the speed of light c sets a maximum speed for the propagation of effects in space. In quantum mechanics, our entanglement cones in essence also set a maximum speed for the propagation of effects in branchial space. In special relativity, there is then a maximum speed defined for any observer—or, in other words, a maximum speed for motion. In quantum mechanics, we can now expect that there will also be a maximum speed for entanglement, or for measurement: it is not possible to set up a quantum observation frame that achieves a higher speed while still respecting the causal relations in the multiway causal graph. We will call this maximum speed ζ, and in 8.20 we will discuss its possible magnitude. One may ask to what extent the correspondences between relativity and quantum mechanics that we have been discussing rely on our models. In principle, for example, one could imagine a kind of “multicausal continuum” that is a mathematical structure (conceivably related to twistor spaces [128]) corresponding to a continuum limit of our multiway causal graph. But while there are challenges in understanding the limits associated with our models, this seems likely to be even more difficult to construct and handle—and has the great disadvantage that it cannot be connected to explicit models that are readily amenable, for example, to enumeration.
Potential Relation to Physics 8.18 Event Horizons and Singularities in Spacetime and Quantum Mechanics : Having discussed the general correspondence between relativity and quantum mechanics suggested by our models, we can now consider the extreme situation of event horizons and singularities. As we discussed above, an event horizon in spacetime corresponds in our models to disconnection in the causal graph: after some slice in our foliation in time, there is no longer causal connection between different parts of the system. As a result, even if the system is locally causal invariant, branch pairs whose products go on different sides of the disconnection can never resolve. The only way to make a foliation in which this does not happen is then effectively to freeze time before the disconnection occurs. When there is a true disconnection in the causal graph, there is no choice about this. But it is also perfectly possible just to imagine setting up a coordinate system that freezes time in a particular region of space—although it will typically take more and more effort (and energy) to consistently maintain such a coordinate singularity as other parts of the system evolve. But now there is an interesting correspondence with quantum measurement. As we discussed in 8.14, in the context of our models, one can view a quantum measurement (or a “collapse of the wave function”) as being associated with a foliation that freezes time for the state that is the outcome of the measurement. In essence, therefore, quantum measurement corresponds to having a coordinate singularity in a particular region of branchial space. What about an event horizon? As we saw above, one way in which an event horizon can occur is if some branch of the multiway system simply terminates, so that in a sense time stops for it. Another possibility is that—at least temporarily—there can be a disconnected piece in the branchial graph. Consider for example the (causal invariant) string substitution system: {A -> BB, BBB -> AA} The multiway system for this rule is and the branchial graph shows temporary disconnections although the “spacetime” causal graph stays connected: One can think of these temporary disconnections in the branchial graphs as corresponding to isolated regions of branchial space where entanglement at least temporarily cannot occur—and where some pure quantum state (such as qubits) can be maintained, at least for some period of time. In some sense, one can potentially view such disconnections as being like black holes in branchial space. But the continued generation of branch pairs (in a potential analog to Hawking radiation [129]) causes the “black hole” to dissipate. A different situation can occur when there is also disconnection in the causal graph—leading in our models to disconnection in the spatial hypergraph—and thus a spacetime event horizon. As a simple example, consider the string substitution system (starting from AA): {AA -> AAAB} The causal graph in this case is and the sequence of branchial graphs (with the standard foliation) is: What has happened here is that there are event horizons both in physical space and in branchial space. We can expect similar phenomena in our full models, and extrapolating this to a physical black hole what this represents is the presence of both a causal event horizon (associated with motion in space, propagation of light, etc.) and an entanglement event horizon (associated with quantum entanglement). The causal event horizon will be localized in physical space (say at the Schwarzschild radius [130]); the entanglement event horizon can be considered instead to be localized in branchial space. It should be noted that these horizons are in a sense linked through the multiway causal graph, which in the example above initially has the form and after more steps builds up the structure: In this graph, there are both spacelike and branchlike connections, and here both of them exhibit disconnection, and therefore event horizons. And even though the geometrical structure of branchial space is very different from physical space, there are potentially further correspondences to be made between them. For example, while the speed of light c governs the maximum spacelike speed, the maximum entanglement rate ζ that we introduced above governs the maximum “branchlike speed”, or entanglement rate. When a disconnection occurs in the spacetime causal graph (and thus the spatial hypergraph), we can think of this as implying that geodesics in spacetime would have to exceed c in order not to be trapped. When a disconnection occurs in the branchial graph, we can think of geodesics having to “exceed speed ζ” in order not to be trapped. It is worth pointing out that the analog of a true singularity—and not just an event horizon—can occur in our models if there are paths in the multiway system that simply terminate, as for B, BB, etc. in: When this happens, there are many geodesics that in effect converge to a single point, like in spacetime singularities in general relativity. Here, however, we see that this can happen not only in physical space, but also in the multiway system, or, in other words, in branchial space. (In our systems, it is probably the case that singularities must be enclosed in event horizons, in the analog of the cosmic censorship hypothesis.) Many results from general relativity can presumably be translated to our models, and can apply both to physical space and branchial space (see [121]). In the case of a black hole, our models suggest that not only may a causal event horizon form in physical space; also an entanglement horizon may form in branchial space. One may then imagine that quantum information is trapped inside the entanglement horizon, even without crossing the causal event horizon—with implications perhaps similar to recent discussions of resolutions to the black hole quantum information problem [131][132][133]. There is a simple physical picture that emerges from this setup. As we have discussed, quantum measurement can be thought of as a choice of coordinates that “freeze time” for some region in branchial space. For an observer close to the entanglement horizon, it will not be possible to do this. Much like an observer at a causal event horizon will be stretched in physical space, so also an observer at an entanglement horizon will be stretched in branchial space. And the result is that in a sense the observer will not be able to “form a classical thought”: they will not successfully be able to do a measurement that definitively picks the branch of the multiway system in which something fell into the black hole, or the one in which it did not.
Potential Relation to Physics 8.19 Local Gauge Invariance : An important phenomenon discussed especially in the context of quantum field theories is local gauge invariance (e.g. [134]). In our models this phenomenon can potentially arise as a result of local symmetries associated with underlying rules (see 6.12). The basic idea is that these symmetries allow different local configurations of rule applications—that can be thought of as different local “gauge” coordinate systems. But the collection of all such possible configurations appears in the multiway graph (and the multiway causal graph)—so that a local choice of gauge can then be represented by a particular foliation in the multiway graph. But causal invariance then implies the equivalence of foliations—and establishes local gauge invariance. As a very simple example, consider the rule: Starting from a square, this rule can be applied in two different ways: There is similar freedom if one applies the rule twice to a larger region: In both cases one can think of the freedom to apply the rule in different ways as being like a symmetry, for example characterized by the list of possible permutations of input elements. But now imagine taking the limit of a large number of steps. Then one can expect to apply the resulting aggregate rule in a large number of ways. And much as we expect the limit of our spatial hypergraphs to be able to be represented—at least in certain cases—as a continuous manifold, we can expect something similar here. In particular, we can think of ourselves as winding up with a very large number of permutations corresponding to equivalent rule applications, which in the limit can potentially correspond to a Lie group. Each different possible choice of how to apply the rule corresponds to a different event that is represented in the multiway graph, and the multiway causal graph: But the important point is that local choices of how the rule is repeatedly applied must always correspond to purely branchlike connections in the multiway causal graph. The picture is analogous to the one in traditional mathematical physics. The spatial hypergraph can be thought of as a base space for a fiber bundle, then the different choices of which branchlike paths to follow correspond to different choices of coordinate systems (or gauges) in the fibers of the fiber bundle (cf. [135][136]). The connection between fibers is defined by the foliation that is chosen. There is an analog when one sets up foliations in the spacetime causal graph—in which case, as we have argued, causal invariance leads to general covariance and general relativity. But here we are dealing with branchlike paths, and instead of getting general relativity, we potentially get gauge theories. In traditional physics, local gauge invariance already occurs in classical theories (such as electromagnetism), and it is notable that for us it appears to arise from considering multiway systems. Yet although multiway systems appear to be deeply connected to quantum mechanics, the aggregate symmetry phenomenon that leads to gauge theories in effect makes slightly different use of the structure of the multiway causal graph. But much as in other cases, we can think about geodesics—now in the multiway causal graph—and can study the properties of the effective space that emerges, with local phenomena (including things like commutators) potentially reflecting features of the Lie algebra. In traditional physics an important consequence of local gauge invariance is its implication of the existence of fields, and gauge bosons such as the photon and gluon. In our models the mathematical derivations that lead to this implication should be similar. But by looking at the evolution of our models, it is possible to get a more explicit sense of how this works. Consider a particular sequence of updates with the rule shown above: At the beginning, symmetry effectively allows many equivalent updates to be made. But once a particular update has been made, this has consequences for which of the possible updates—each independently equivalent on their own—can be made subsequently. These “consequences” are captured in the causal relationships encoded in the multiway causal graph—which have not only branchlike but also spacelike extent, corresponding in essence to the propagation of effects in what can be described as a gauge field.
Potential Relation to Physics 8.20 Units and Scales : Most of our discussion so far has focused on how the structure of our models might correspond to the structure of our physical universe. But to make direct contact between our models and known physics, we need to fill in actual units and scales for the constructs in our models. In this section we give some indication of how this might work. In our models, there is a fundamental unit of time (that we will call ) that represents the interval of time corresponding to a single updating event. This interval of time in a sense defines the scale for everything in our models. Given , there is an elementary length , determined by the speed of light c according to: The elementary length defines the spatial separation of neighboring elements in the spatial hypergraph. Another fundamental scale is the elementary energy : the contribution of a single causal edge to the energy of a system. The energy scale ultimately has both relativistic and quantum consequences. In general relativity, it relates to how much curvature a single causal edge can produce, and in quantum mechanics, it relates to how much change in angle in an edge in the multiway graph a single causal edge can produce. The speed of light c determines the elementary length in ordinary space, specifying in effect how far one can go in a single event, or in a single elementary time. To fill in scales for our models, we also need to know the elementary length in branchial space—or in effect how far in state space one can go in a single event, or a single elementary time (or, in effect, how far apart in branchial space two members of a branch pair are). And it is an obvious supposition that somehow the scale for this must be related to ℏ. An important point about scales is that there is no reason to think that elementary quantities measured with respect to our current system of units need be constant in the history of the universe. For example, if the universe effectively just splits every spatial graph edge in two, the number of elementary lengths in what we call 1 meter will double, and so the elementary length measured in meters will halve. Given the structure of our models, there are two key relationships that determine scales. The first—corresponding to the Einstein equations—relates energy density to spacetime curvature, or, more specifically, gives the contribution of a single causal edge (with one elementary unit of energy) to the change of Vr and the corresponding Ricci curvature: (Here we have dropped numerical factors, and G is the gravitational constant, which, we may note, is defined with its standard units only when the dimension of space d = 3.) The second key relationship that determines scales comes from quantum mechanics. The most obvious assumption might be that quantum mechanics would imply that the elementary energy should be related to the elementary time by . And if this were the case, then our various elementary quantities would be equal to their corresponding Planck units [137], as obtained with G = c = ℏ = 1 (yielding elementary length ≈ 10–35 m, elementary time ≈ 10–43 s, etc.) But the setup of our models suggests something different—and instead suggests a relationship that in effect also depends on the size of the multiway graph. In our models, when we make a measurement in a quantum system, we are at a complete quantum observation frame—or in effect aggregating across all the states in the multiway graph that exist in the current slice of the foliation that we have defined with our quantum frame. There are many individual causal edges in the multiway causal graph, each associated with a certain elementary energy . But when we measure an energy, it will be the aggregate of contributions from all the individual causal edges that we have combined in our quantum frame. A single causal edge, associated with a single event which takes a single elementary time, has the effect of displacing a geodesic in the multiway graph by a certain unit distance in branchial space. (The result is a change of angle of the geodesic—with the formation of a single branch pair perhaps being considered to involve angle .) Standard quantum mechanics in effect defines ℏ through E = ℏ ω. But in this relationship E is a measured energy, not the energy associated with a single causal edge. And to convert between these we need to know in effect the number of states in the branchial graph associated with our quantum frame, or the number of nodes in our current slice through the multiway system. We will call this number Ξ. And finally now we can give a relation between elementary energy and elementary time: In effect, ℏ sets a scale for measured energies, but ℏ/Ξ sets a scale for energies of individual causal edges in the multiway causal graph. This is now sufficient to determine our elementary units. The elementary length is given in dimension d = 3 by where lP, tP, EP are the Planck length, time and energy. To go further, however, we must estimate Ξ. Ultimately, Ξ is determined by the actual evolution of the multiway system for a particular rule, together with whatever foliation and other features define the way we describe our experience of the universe. As a simple model, we might then characterize what we observe as being “generational states” in the evolution of a multiway system, as we discussed in 5.21. But now we can use what we have seen in studying actual multiway systems, and assume that in one generational step of at least a causal invariant rule each generational state generates on average some number κ of new states, where κ is related to the number of new elements produced by a single updating event. In a generation of evolution, therefore, the total number of states in the multiway system will be multiplied by a factor κ. But to relate this to observed quantities, we must ask what time an observer would perceive has elapsed in one generational step of evolution. From our discussion above, we expect that the typical time an observer will be able to coherently maintain the impression of a definite “classical-like” state will be roughly the elementary time  multiplied by the number of nodes in the branchlike hypersurface. The number of nodes will change as the multiway graph grows. But in the current universe we have defined it to be Ξ. Thus we have the relation where tH is the current age of the universe, and for this estimate we have ignored the change of generation time at different points in the evolution of the multiway system. Substituting our previous result for  we then get: There is a rough upper limit on κ from the signature for the underlying rule, or effectively the ratio in the size of the hypergraphs between the right and left-hand sides of a rule. (For most of the rules we have discussed here, for example, κ ≲ 2.) The lower limit on κ is related to the “efficiency” of causal invariance in the underlying rule, or, in effect, how long it takes branch pairs to resolve relative to how fast new ones are created. But inevitably κ > 1. Given the transcendental equation we can solve for Ξ to get where W is the product log function [138] that solves w ew = z. But for large σ log(κ) (and we imagine that σ ≈ 1061), we have the asymptotic result [30]: Plotting the actual estimate for Ξ as a function of κ we get the almost identical result: If κ = 1, then we would have Ξ = 1, and for κ extremely close to 1, Ξ ≈ 1 + σ (κ – 1) + ... But even for κ = 1.01 we already have Ξ ≈ 10112, while for κ = 1.1 we have Ξ ≈ 10115, for κ = 2 we have Ξ ≈ 4 × 10116 and for κ = 10 we have Ξ ≈ 5 × 10117. To get an accurate value for κ we would have to know the underlying rule and the statistics of the multiway system it generates. But particularly at the level of the estimates we are giving, our results are quite insensitive to the value of κ, and we will assume simply: In other words, for the universe today, we are assuming that the number of distinct instantaneous complete quantum states of the universe being represented by the multiway system (and thus appearing in the branchial graph) is about 10116. But now we can estimate other quantities: The fact that our estimate for the elementary length  is considerably smaller than the Planck length indicates that our models suggest that space may be more closely approximated by a continuum than one might expect. The fact that the elementary energy  is much smaller than the surprisingly macroscopic Planck energy (≈ 1019 GeV ≈ 2 GJ, or roughly the energy of a lightning bolt) is a reflection of the fact the Planck energy is related to measurable energy, not the individual energy associated with an updating event in the multiway causal graph. Given the estimates above, we can use the rest mass of the electron to make some additional very rough estimates—subject to many assumptions—about the possible structure of the electron: In quantum electrodynamics and other current physics, electrons are assumed to have zero intrinsic size. Experiments suggest that any intrinsic size must be less than about 10–22 m [139][140]—nearly 1060 times our estimate. Even despite the comparatively large number of elements suggested to be within an electron, it is notable that the total number of elements in the spatial hypergraph is estimated to be more than 10200 times the number of elements in all known particles of matter in the universe—suggesting that in a sense most of the “computational effort” in the universe is expended on the creation of space rather than on the dynamics of matter as we know it. The structure of our models implies that not only length and time but also energy and mass must ultimately be quantized. Our estimates indicate that the mass of the electron is > 1036 times the quantized unit of mass—far too large to expect to see “numerological relations” between particle masses. But with our model of particles as localized structures in the spatial hypergraph, there seems no reason to think that structures much smaller than the electron might not exist—corresponding to particles with masses much smaller than the electron. Such “oligon” particles involving comparatively few hypergraph elements could have masses that are fairly small multiples of 10–30 eV. One can expect that their cross-sections for interaction will be extremely small, causing them to drop out of thermal equilibrium extremely early in the history of the universe (e.g. [141][142]), and potentially leading to large numbers of cold, relic oligons in the current universe—making it possible that oligons could play a role in dark matter. (Relic oligons would behave as a more-or-less-perfect ideal gas; current data indicates only that particles constituting dark matter probably have masses ≳ 10–22 eV [143].) As we discussed in the previous subsection, the structure of our models—and specifically the multiway causal graph—indicates that just as the speed of light c determines the maximum spacelike speed (or the maximum rate at which an observer can sample new parts of the spatial hypergraph), there should also be a maximum branchlike speed that we call ζ that determines the maximum rate at which an observer can sample new parts of the branchial graph, or, in effect, the maximum speed at which an observer can become entangled with new “quantum degrees of freedom” or new “quantum information”. Based on our estimates above, we can now give an estimate for the maximum entanglement speed. We could quote it in terms of the rate of sampling quantum states (or branches in the multiway system) but in connecting to observable features of the universe, it seems better to quote it in terms of the energy associated with edges in the causal graph, in which case the result based on our estimates is: This seems large compared to typical astrophysical processes, but one could imagine it being relevant for example in mergers of galactic black holes. 
Potential Relation to Physics 8.21 Specific Models of the Universe : If we pick a particular one of our models, with a particular set of underlying rules and initial conditions, we might think we could just run it to find out everything about the universe it generates. But any model that is plausibly similar to our universe will inevitably show computational irreducibility. And this means that we cannot in general expect to shortcut the computational work necessary to find out what it does. In other words, if the actual universe follows our model and takes a certain number of computational steps to get to a certain point, we will not be in a position to reproduce in much less than this number of steps. And in practice, particularly with the numbers in the previous subsection, it will therefore be monumentally infeasible for us to find out much about our universe by pure, explicit simulation. So how, then, can we expect to compare one of our models with the actual universe? A major surprise of this section is how many known features of fundamental physics seem in a sense to be generic to many of our models. It seems, for example, that both general relativity and quantum mechanics arise with great generality in models of our type—and do not depend on the specifics of underlying rules. One may suspect, however, that there are still plenty of aspects of our universe that are specific to particular underlying rules. A few examples are the effective dimension of space, the local gauge group, and the specific masses and couplings of particles. The extent to which finding these for a particular rule will run into computational irreducibility is not clear. It is, however, to be expected that parameters like the ones just mentioned will put strong constraints on the underlying rule, and that if the rule is simple, they will likely determine it uniquely. Of all the detailed things one can predict from a rule, it is inevitable that most will involve computational irreducibility. But it could well be that those features that we have identified and measured as part of the development of physics are ones that correspond to computationally reducible aspects of our universe. Yet if the ultimate rule is in fact simple, it is likely that just these aspects will be sufficient to determine it. In section 7 we discussed some of the many different representations that can be used for our models. And in different representations, there will inevitably be a different ranking of simplicity among models. In setting up a particular representation for a model, we are in effect defining a language—presumably suitable for interpretation by both humans and our current computer systems. Then the question of whether the rule for the universe is simple in this language is in effect just the question of how suitable the language is for describing physics. Of course, there is no guarantee that there exists a language in which, with our current concepts, there is a simple way to describe the rule for our physical universe. The results of this section are encouraging, but not definitive. For they at least suggest that in the representation we are using, known features of our universe generically emerge: we do not have to define some thin and complicated subset to achieve this.
Potential Relation to Physics 8.22 Multiway Systems in the Space of All Possible Rules : We have discussed the possibility that our physical universe might be described as following a model of the type we have introduced here, with a particular rule. And to find such a rule would be a great achievement, and might perhaps be considered a final answer to the core question of fundamental physics. But if such a rule is found, one might then go on and ask why—out of the infinite number of possibilities—it is this particular rule, or, for example, a simple rule at all. And here the paradigm we have developed makes a potential additional suggestion: perhaps there is not just one rule being used after all, but instead in a sense all possible rules are simultaneously used. In the multiway systems we have discussed so far, there is a single underlying rule, but separate branches for all possible sequences of updating events. But one can imagine a rule-space multiway system, that includes branches not only for every sequence of updating events, but also for every possible rule used to do the updating. Somewhat like with updating events, there will be many states reached to which many of the possible rules cannot apply. (For example, a rule that involves only ternary edges cannot apply to a state with only binary edges.) And like with updating events, branches with different sequences of rules applied may reach equivalent states, and thus merge. Operationally, it is not so difficult to see how to set up a rule-space multiway system. All it really involves is listing not just one or a few possible rules that can be used for each updating event, but in a sense listing all possible rules. In principle there are an infinite number of such rules, but any rule that involves rewriting a hypergraph that is larger than the hypergraph that represents the whole universe can never apply, so at least at any given point in the evolution of the system, the number of rules to consider is finite. But like with the many other kinds of limits we have discussed, we can still imagine taking the limit of all infinitely many possible rules. As a toy example of a rule-space multiway system, consider all inequivalent 2 -> 2 rules on strings of As and Bs: We can immediately construct the rule-space multiway graph for these rules (here starting from all possible length-4 sequences of As and Bs): Different branches of the rule-space multiway system use different rules: One can include causal connections: But even removing multiedges, the full rule-space multiway causal graph is complicated: The “branchial graph” of the rule-space multiway system, though, is fairly simple, at least after one step (though it is now really more of a “rule-space graph”): At least in this toy example, we already see something important: the rule-space multiway system is causal invariant: given branching even associated with using different rules, there is always corresponding merging—so the graph of causal relationships between updating events, even with different rules, is always the same. Scaling up to unbounded evolutions and unbounded collections of rules involves many issues. But it seems likely that causal invariance will survive. And ultimately one may anticipate that across all possible rules it will emerge as a consequence of the Principle of Computational Equivalence [1:12]. Because this principle implies that in the space of all possible rules, all but those with simple behavior are equivalent in their computational capabilities. And that means that across all the different possible sequences of rules that can be applied in the rule-space multiway system there is fundamental equivalence—with the result that one can expect causal invariance. But now consider the role of the observer, who is inevitably embedded in the system, as part of the same rule-space multiway graph as everything else. Just as we did for ordinary multiway graphs above, we can imagine foliating the rule-space multiway graph, with the role of space or branchial space now being taken by rule space. And one can think of exploring rule space as effectively corresponding to sampling different possible descriptions of how the universe works, based on different underlying rules. But if each event in the rule-space multiway graph is just a single update, based on a particular (finite) rule, there is immediately a consequence. Just like with light cones in ordinary space, or entanglement cones in branchial space, there will be a new kind of cone that defines a limit on how fast it is possible to “travel” in rule space. For an observer, traveling in rule space involves ascribing different rules to the universe, or in effect changing one’s “reference frame” for interpreting how the universe operates. (An “inertial frame” in rule space would probably correspond to continuing to use a particular rule.) But from the Principle of Computational Equivalence [1:12] (and specifically from the idea of computation universality (e.g. [1:11])) it is always possible to set up a computation that will translate between interpretations. But in a sense the further one goes in rule space, the more difficult the translation may become—and the more computation it will require. But now remember that the observer is also embedded in the same system, so the fundamental rate at which it can do computation is defined by the structure of the system. And this is where what one might call the “translation cone” comes from: to go a certain “distance” in rule space, the observer must do a certain irreducible amount of computational work, which takes a certain amount of time. The maximum rate of translation is effectively a ratio of “rule distance” to “translation effort” (measured in units of computational time). In a sense it probes something that has been difficult to quantify: just how “far apart” are different description languages, that involve different computational primitives? One can get some ideas by thinking about program size [144][145][146], or running time, but in the end new measures that take account of things, like the construction of sequences of abstractions, seem to be needed [147]. For our current discussion, however, the main point is the existence of a kind of “rule-space relativity”. Depending on how an observer chooses to describe our universe, they may consider a different rule—or rather a different branch in the rule-space multiway system—to account for what they see. But if they change their “description frame”, causal invariance (based on the Principle of Computational Equivalence) implies that they will still find a rule (or a branch in the rule-space multiway system) that accounts for what they see, but it will be a different one. In the previous section, we discussed equivalences between our models and other formulations. The fact that we base our models on hypergraph rewriting (or any of its many equivalent descriptions) is in a sense like a choice of coordinate system in rule space—and there are presumably infinitely many others we could use. But the fact that there are many different possible parametrizations does not mean that there are not definite things that can be said. It is just that there is potentially a higher level of abstraction that can be reached. And indeed, in our models, not only have we abstracted away notions of space, time, matter and measurement; now in the rule-space multiway system we are in a sense also abstracting away the very notion of abstraction itself (see also [2]).
Additional Material 9.1 Notes & Further References Additional Material: Structure of Models & Methodology : The class of models studied here represent a simplification and generalization of the trivalent graph models introduced in [1:9] and [87] (see also [148]). The methodology of computational exploration used here has been developed particularly in [5][31][1]. Some exposition of the methodology has been given in [149]. The class of models studied here can be viewed as generalizing or being related to a great many kinds of abstract systems. One class is graph rewriting systems, also known as graph transformation systems or graph grammars (e.g. [150]). The models here are generalizations of both the double-pushout and single-pushout approaches. Note that the unlabeled graphs and hypergraphs studied here are different from the typical cases usually considered in graph rewriting systems and their applications. Multiway systems as used here were explicitly introduced and studied in [1:p204] (see also [1:p938]). Versions of them have been invented many times, most often for strings, under names such as semi-Thue systems [151], string rewriting systems [152], term rewriting systems [65], production systems [153], associative calculi [154] and canonical systems [153][155]. Connections to Physics Theories An outline of applying models of a type very similar to those considered here was given in [1:9]. Some additional exposition was given in [156][157][158]. The discussion here contains many new ideas and developments, explored in [159]. For a survey of ultimate models of physics, see [1:p1024]. The possibility of discreteness in space has been considered since antiquity [160][161][162][163]. Other approaches that have aspects potentially similar to what is discussed here include: causal dynamical triangulation [164][165][166], causal set theory [167][168][169], loop quantum gravity [170][171], pregeometry [172][173][174], quantum holography [175][176][177], quantum relativity [178], Regge calculus [179], spin networks [180][181][182][183][184], tensor networks [185], superrelativity [186], topochronology [187], topos theory [188], twistor theory [128]. Other discrete and computational approaches to fundamental physics include: [189][190][191][192][193][194][195][196]. The precise relationships among these approaches and references and the current work are not known. In some cases it is expected that conceptual motivations may be aligned; in others specific mathematical structures may have direct relevance. The latter may also be the case for such areas as conformal field theory [197], higher-order category theory [198], non-commutative geometry [199], string theory [200].
Additional Material 9.2 Appendix: Implementation Tools Created for This Project : A variety of new functions have been added to the Wolfram Function Repository to directly implement, visualize and analyze the models defined here [201].
Additional Material 9.2 Appendix: Basic Direct Symbolic Transformation: : The class of models defined here can be implemented very directly just using symbolic transformation rules of the kind on which the Wolfram Language [98] is based. It is convenient to represent relations as Wolfram Language lists, such as {1,2}. One way to represent collections is to introduce a symbolic operator σ that is defined to be flat (associative) and orderless (commutative): Thus we have, for example: We can then write a rule such as {{x, y}} -> {{x, y}, {y, z}} more explicitly as: This rule can then be applied using standard Wolfram Language pattern matching: The Module causes a globally unique new symbol to be created for the new node z every time it is used: But in applying the rule to a collection with more than one relation, there is immediately an issue with the updating process. By default, the Wolfram Language performs only a single update in each collection: As discussed in the main text, there are many possible updating orders one can use. A convenient way to get a whole “generation” of update events is to define an inert form of collection σ1 then repeatedly replace collections σ until a fixed point is reached: By replacing σ1 with σ at the end, one gets the result for a complete generation update: NestList applies this whole process repeatedly, here for 4 steps: Replacing σ by a Graph operator, one can render the results as graphs: IndexGraph creates a graph in which nodes are renamed sequentially: Here is the result with a different graph layout: Exactly the same approach works for rules that involve multiple relations. For example, consider the rule: {{x, y}, {x, z}} -> {{x, z}, {x, w}, {y, w}, {z, w}} This can be run for 2 steps using: Here is the result after 10 steps, rendered as a graph:
Additional Material 9.2 Appendix: Alternative Syntactic Representation: : As an alternative to introducing an explicit head such as σ, one can use a system-defined matchfix operator such as AngleBracket (entered as <, >) that does not have a built-in meaning. With the definition one immediately has for example and one can set up rules such as
Additional Material 9.2 Appendix: Pattern Sequences: : Instead of having an explicit “collection operator” that is defined to be flat and orderless, one can just use lists to represent collections, but then apply rules that are defined using OrderlessPatternSequence: Note that even though the pattern appears twice, /. applies the rule only once:
Additional Material 9.2 Appendix: Subset Replacement: : Yet another alternative is to use the function SubsetReplace (built into the Wolfram Language as of Version 12.1). SubsetReplace replaces subsets of elements in a list, regardless of where they occur: Unlike ReplaceAll (/.) it keeps scanning for possible replacements even after it has done one: One can find out what replacements SubsetReplace would perform using SubsetCases: This uses SubsetReplace to apply a rule for one of our models; note that the rule is applied twice to this state (Splice is used to make the sequence of lists be spliced into the collection): This gives the result of 10 applications of SubsetReplace: This turns each list in the collection into a directed edge, and renders the result as a graph: IndexGraph can then for example be used to relabel all elements in the graph to be sequential integers. Note that SubsetReplace does not typically apply rules in exactly our “standard updating order”.
Additional Material 9.2 Appendix: Parallelization: : Our models do not intrinsically define updating order (see section 6), and thus allow for asynchronous implementation with immediate parallelization, subject only to the local partial ordering defined by the graph of causal relationships (or, equivalently, of data flows). However, as soon as a particular sequence of foliations—or a particular updating order—is defined, its implementation may require global coordination across the system.
Additional Material 9.3 Appendix: Graph Types: Single-Evolution-History Graphs: : Graphs obtained from particular evolution histories, with particular sequences of updating events.  For rules with causal invariance, the ultimate causal graph is independent of the sequence of updating events.
Additional Material 9.3 Appendix: Graph Types: Spatial Graph: : Hypergraph whose nodes and hyperedges represent the elements and relations in our models. Update events locally rewrite this hypergraph. In the large-scale limit, the hypergraph can show features of continuous space. The hypergraph potentially represents the “instantaneous” configuration of the universe on a spacelike hypersurface. Graph distances in the hypergraph potentially approximate distances in physical space.
Additional Material 9.3 Appendix: Graph Types: Causal Graph (“Spacetime Causal Graph”): : Graph with nodes representing updating events and edges representing their causal relationships. In causal invariant systems, the same ultimate causal graph is obtained regardless of the particular sequence of updating events. The causal graph potentially represents the causal history of the universe. Causal foliations correspond to sequences of spacelike hypersurfaces. The effect of an update event is represented by a causal cone, which potentially corresponds to a physical light cone. The translation from time units in the causal graph to lengths in the spatial graph is potentially given by the speed of light c.
Additional Material 9.3 Appendix: Graph Types: Multiway-Evolution-Related Graphs: : Graphs obtained from all possible evolution histories, following every possible sequence of updating events.  For rules with causal invariance, different paths in the multiway system lead to the same causal graph.
Additional Material 9.3 Appendix: Graph Types: Multiway States Graph  (Multiway Graph): : Graph representing all possible branches of evolution for the system. Each node represents a possible complete state of the system at a particular step. Each connection corresponds to the evolution of one state to another as a result of an updating event. The multiway graph potentially represents all possible paths of evolution in quantum mechanics. In a causal invariant system, every branching in the multiway system must ultimately reconverge.
Additional Material 9.3 Appendix: Graph Types: Multiway States+Causal Graph: : Graph representing both all possible branches of evolution for states, and all causal relationships between updating events. Each node representing a state connects to other states via nodes representing updating events. The updating events are connected to indicate their causal relationships. The multiway states+causal graph in effect gives complete, causally annotated information on the multiway evolution.
Additional Material 9.3 Appendix: Graph Types: Multiway Causal Graph: : Graph representing causal connections among all possible updating events that can occur in all possible paths of evolution for the system. Each node represents a possible updating event in the system. Each edge represents the causal relationship between two possible updating events. In a causal invariant system, the part of the multiway causal graph corresponding to a particular path of evolution has the same structure for all possible paths of evolution. The multiway causal graph provides the ultimate description of potentially observable behavior of our models. Its edges represent both spacelike and branchlike relationships, and can potentially represent causal relations both in spacetime and through quantum entanglement.
Additional Material 9.3 Appendix: Graph Types: Branchial Graph: : Graph representing the common ancestry of states in the multiway system. Each node represents a state of the system, and two nodes are joined if they are obtained on different branches of evolution from the same state. To define a branchial graph requires specifying a foliation of the multiway graph. The branchial graph potentially represents entanglement in the “branchial space” of quantum states.
Has the project been peer reviewed?: The project is undergoing active, open post-publication peer review, and is continually being refined and developed based on community input. Stephen Wolfram’s core paper is available on arXiv, and is intended for publication in an academic journal, as are other project papers.
What is the quickest way to understand the project?: It depends on your background. For a general reader, check out Stephen Wolfram’s announcement. For a more formal introduction, see Stephen Wolfram’s technical introduction. Professional physicists may find it easiest to look at Jonathan Gorard’s papers on the derivations of relativity and quantum mechanics. There’s also the Visual Summary, which gives a high-level overview. In addition, people who want to start doing their own computer experiments can check out the Hands-On Introduction.
What is the history and background to the project?: It’s an outgrowth of Stephen Wolfram’s work in the 1990s, that led to the Fundamental Physics section in his 2002 book A New Kind of Science. The project that it defined was long hibernated, but restarted in late 2019. The full story—going back to Stephen’s early life as a physicist—is in Stephen Wolfram’s post, “How We Got Here: The Backstory of the Wolfram Physics Project”.
What existing approaches is the project closest to?: First and foremost (though it’s really an extension of the same project), Stephen Wolfram’s work from 2002 in A New Kind of Science. Other approaches that have definite similarities to certain aspects of our formalism include causal dynamical triangulation (which can be thought of as corresponding to a special case of our more topologically generic description of spacetime in terms of hypergraphs—namely the case in which spacetime is triangulated topologically into a simplicial complex of pentachora), loop quantum gravity (since Wolfram model evolutions can be thought of as corresponding to generalizations of spin foams in which spatial hyperedges are purely abstract, and, unlike in spin networks, do not have a compact Lie group structure explicitly defined), and twistor theory (the twistor correspondence space can be thought of as being a continuous model of the multiway causal graph, at least in certain idealized cases). The Relations to Other Approaches section of this Q&A gives more details. See also the Notes & Further References in Stephen Wolfram’s Technical Introduction.
Does the project invalidate existing physics?: No. But it gives a coherent foundation for what previously appeared to be disparate ideas and results. In doing this, it introduces new concepts that are different from those in existing physics. For example, it suggests that space is fundamentally discrete, rather than continuous. It also suggests that time is fundamentally different from space rather than being just part of a combined spacetime. Despite such differences—which are primarily relevant on extremely small scales—existing physics emerges. In addition, our model appears to dovetail very elegantly with many recent formal directions in physics.
Have you found The Rule for the universe?: We believe we have found the class of rules, but (so far as we know) we have not yet found the specific rule. The phenomenon of computational irreducibility makes it difficult to determine the complete consequences of any given rule. However, a major finding of the project so far is that the core theories of current physics can be derived generically for models of the class we have identified, without needing to know the specific underlying rule. One of the more abstract findings of the project is that it is also the case that the specific rule to use to describe the universe depends on one’s language for description, and so with different description languages different rules will be considered The Rule.
What is left to do in the project?: A lot! We think we have identified the correct class of models and approach, but it remains to find specific rules and to connect them to all known aspects of physics, and to derive detailed experimental predictions, etc. In addition, we expect connections to many existing directions in physics and mathematics, and these remain to be elucidated in detail. Watch the livestreams to see the process in action!
How can I get involved in your project?: We plan to make this a very open project, and look forward to contributions from many people. Core research contributions will typically require research-level education in theoretical physics, mathematics or certain areas of theoretical computer science, or sophisticated algorithmic programming. However, we will be livestreaming our working sessions, and anyone can join the livestream and participate in the text chat. We will be organizing a variety of educational programs around the project, notably our Summer School. We hope to expand our core team of researchers, as well as to have academic and other affiliates associated with the project.
Does your theory make predictions?: Ultimately it should predict everything about the universe, although many of these predictions will be computational irreducibly difficult to work out in detail. However, even at this stage, there are a variety of surprisingly concrete directions for predictions. One issue is that we do not yet know the overall quantitative scale of the core phenomena (since we do not know for sure the elementary length for discrete distances in space). However, we can already see that there are general predictions about cosmology, astrophysics and quantum processes, especially related to black holes.
Can I sponsor or donate to your project?: This project has so far been funded by Stephen Wolfram and Wolfram Research, and has used internal Wolfram Research compute resources. As the project scales up, there will be opportunities for additional sponsorship to support research fellows, educational activities, outreach, computation, etc. There are also opportunities to provide large-scale computation resources. We will be setting up programs for individual donations, both financial and computational. In addition, there will be many opportunities for volunteers interested in the project.
What background do I need to understand what you’re doing?: We’ve tried to make the general outline of what we’re doing as broadly accessible as possible. But since we are connecting with existing theoretical physics, understanding the technical details requires understanding technical details of existing theoretical physics, often at a research or advanced graduate level.  Some aspects of the project do not specifically require physics knowledge, but often instead require knowledge of advanced mathematics or theoretical computer science. Nevertheless, as part of the project we hope to provide educational programs and resources which will allow a wide range of people to learn what is needed to understand many aspects of our project.
I have my own theory; will you look at it?: We’ve got our hands full studying one theory, so unless your theory is directly connected to what we’re working on, we’re not realistically going to be able to look at it.
Can I get the code to run your models?: Yes! It’s set up to run immediately in any Wolfram Language environment, including the cloud (i.e. through a web browser). See the Software Tools page.
Are you saying that the universe is a computer?: “Computational”, yes. Our model implies that the universe operates at the lowest level according to definite rules of the kind one could readily program on a computer. But when one says “is a computer” one often means that one imagines that something has been constructed for the purpose of being a computer. All our model does is to say that the operation of the universe can be described computationally, not that the universe was in any way “built to be a computer”.
If the universe is computational, what computer is it running on?: None. It is just following certain rules that we can think of as computational. There is no underlying “substrate”. The universe is just doing what it does, and we are describing it in terms of computation. When we think about Newton’s laws describing the motion of the Earth using equations, we are also imagining the equations describe what the Earth is doing, not that the Earth itself has a mechanism inside that is solving the equations as Mathematica might.
How could your model be proved wrong?: Any particular rule could be proved wrong by disagreeing with observations, for example predicting particles that do not exist. But the overall framework of our models is something more general, and not as directly amenable to experimental falsification. Asking how to falsify our framework is similar to asking how one would prove that calculus could not be a model for physics. An obvious answer would be another model successfully providing a fundamental theory of physics, and being proved incompatible. There are, however, some structural features of our models which are unavoidable. One of them is the assertion that hypercomputation is not physically realizable. But quite how we—with human perception and actuation capabilities—could explicitly set up and interpret hypercomputation is quite unclear.
Are there other universes?: In our model, many possible rules yield causally disconnected regions of spacetime, often corresponding to disconnecting parts of the spatial hypergraph. In addition, there can be disconnected regions of branchial space, corresponding to causally disconnected branches of quantum evolution. These kinds of non-communicating regions are still operating according to the same underlying rule. But rule-space relativity in our models suggests that even universes with different rules are equivalent to observers embedded within the universe, implying that there cannot meaningfully be “other universes” that somehow run in parallel to ours with different rules. There can be different forms of description for our universe that are incoherent with each other, but they are still describing the same universe. The only possibility in our model for “other universes” with fundamentally different rules is for these universes to perform computations at a higher level in the arithmetic hierarchy, i.e. hypercomputations. However, such universes would inevitably be disconnected from us, effectively separated by the analog of a cosmological event horizon.
What does your model say about the simulation argument?: The model implies that there is a definite computational rule that determines every aspect of what happens in our universe. If the universe is to be considered a “simulation” this would suggest that the rule is being determined by something outside the system, and presumably in an “intentional” way. It is difficult enough to extend the notion of intentionality far beyond the specifics of what humans do, making it unrealistic to attribute it to something beyond even the universe. In addition, the concept of rule-space relativity implies that in a sense all possible rules are equivalent, at least to an appropriate observer, and therefore there would be nothing for an entity setting up the simulation to “intentionally decide”—since any rule they could choose would appear to be the same universe to observers embedded within it.
How will you know if you have the correct rule?: Computational irreducibility means that it may be irreducibly difficult to determine any particular consequence of a rule. However, there is reason to hope that certain properties will be identifiable. If the rule is simple, then it is to be expected that just getting a few specifics of our universe exactly correct will be sufficient to determine the particular rule. For example, knowing that the universe has (at least roughly) three spatial dimensions, or knowing local gauge symmetry groups, will presumably already go far in homing in on the correct rule. Finding specific masses and properties of elementary particles will be a critical validation for any particular rule.
Could there be more than one correct rule?: Yes, the concept of rule space relativity suggests that many rules can be equivalent, but each different rule will be appropriate for a different “observer”, or, more specifically, will be the rule suitable for an observer using a certain language to describe the universe. Our particular description language—based on our sensory experience, and the formal descriptions of existing physics—may well have enough freedom that it allows several possible consistent rules.
Could the universe just stop?: Yes, in principle the rules for the universe could simply no longer apply to any part of the spatial hypergraph. If this situation occurred, it would mean that time would no longer progress, and the universe would reach a final state, or fixed point, in effect giving the final result of the computation that corresponded to its evolution. Given that the universe probably has 10300 or more elements, behaving in a computationally irreducible way, it seems absolutely inconceivable that everything could effectively “line up” to reach a fixed point. If this were to happen, however, then it would imply that the evolution of the universe could be summarized with much less computational effort than actually doing the evolution, or, in other words, that the universe is not ultimately capable of full universal computation.
Are you saying that the universe is a cellular automaton?: No. Cellular automata are very useful models for many things, and provided the intuition that led to the development of this model. But cellular automata as such have rigid predefined notions of space and time, and a critical feature of our models is that space and time are instead dynamic and emergent. Structurally, our models involve rewrite rules for collections or relations, or equivalently, for hypergraphs, rather than updates of values in pre-existing arrays of cells.
Is this a theory of everything?: It is intended to be an underlying theory of the whole universe, in perfect detail. If one could run the model long enough, then it is intended to reproduce everything about the universe, including the writing of this answer. However, the amount of computation required to do this would be immense—and the phenomenon of computational irreducibility implies that there cannot in general be shortcuts.
How do your models relate to string theory?: The precise correspondence is not yet clear, but we have several ideas. One possible point of connection lies in the evolution of what we refer to colloquially as “snake states”—sets of global states in the multiway evolution graph produced by maximally consistent sets of spacelike-separated updating events. The evolution of such a snake state corresponds to a purely relativistic evolution of the global state of the universe (since all states within the snake were produced via strictly spacelike-separated updating events, their linear superposition itself corresponds to a valid global state), with distinct snake states thus being purely branchlike-separated. The worldsheet (or, more generally, worldvolume) defined by the trajectory of a snake state through the multiway system is therefore some higher-dimensional analog of the worldline defined by the trajectory of an individual multiway eigenstate. The convergence and divergence of branch pairs in the multiway system can hence be described purely in terms of splitting and joining operations on these worldsheets, which we conjecture may be related to the splitting and joining vertices for propagators in, for instance, light-cone string field theory.
How do your models relate to causal set theory and causal dynamical triangulation?: Very directly. Indeed, the causal graphs that one investigates in the context of Wolfram model systems, as a plausible candidate for the discrete structure of spacetime, are ultimately just concrete representations of causal sets: the graph itself may be thought of as being the Hasse diagram for a partial order relation between spacetime events that satisfies reflexivity, antisymmetry, transitivity (by virtue of the analog with the causal structure of a Lorentzian manifold), and local finiteness (by virtue of the discrete nature of the events), just as in a standard causal set. The formal structure of the Wolfram model may, in fact, be thought of as being an abstract generalization of a causal dynamical triangulation, in which spacetime is triangulated topologically into a simplicial complex of “pentachora” (4-simplices), which evolve in accordance with some deterministic dynamical law; the only difference in our case is that the choice of simplex is less constrained, because our formulation in terms of hypergraphs is more topologically generic.
How do your models relate to tensor networks?: Our formulation of branchlike hypersurfaces within multiway evolution graphs may be thought of as being a variant of the concept of a tensor network; in much the same way as the combinatorial structure of a hierarchical tensor network designates the entanglement structure of ground states in the context of entanglement renormalization methods, connections between branchlike states denote entanglements between global multiway states within our models. However, unlike with a tensor network, the model specifies explicitly how these branchlike hypersurfaces evolve, in accordance with some multiway analog of the Einstein field equations (with, for instance, the Fubini–Study metric tensor playing the role of the standard spacetime metric tensor).
How do your models relate to spin networks, spin foams and loop quantum gravity?: One can think of the Wolfram model as being a significant generalization of the concept of a spin network or spin foam in loop quantum gravity. In standard loop quantum gravity, a spin network is a combinatorial structure for representing the quantum state of a gravitational field on a three-dimensional spacelike hypersurface as a directed graph, whilst a spin foam is a higher-dimensional version of the same idea, with the directed graph now replaced with a topological 2-complex, representing the overall quantum state of the entire four-dimensional spacetime. In this way, spin networks and spin foams may be thought of as being directly analogous to spatial hypergraphs and (multiway) causal graphs for Wolfram model systems. However, the crucial distinction is that, in the case of spin networks, edges correspond explicitly to irreducible representations of some predefined compact Lie group (with vertices corresponding to the intertwiners of the adjacent representations), whereas the spatial hyperedges in the case of Wolfram model systems correspond to abstract relations between elementary elements, with no group structure explicitly defined (in other words, all salient algebraic and geometrical features of Wolfram model systems are purely emergent, as opposed to being “burned in” to the underlying combinatorial structure, as is the case in spin networks and spin foams).
How do your models relate to twistor theory?: Very intimately, at least so we believe; indeed, one of our current conjectures is that the most natural candidate for the limiting mathematical structure of the multiway causal graph is some generalization of the correspondence space that appears in twistor theory. The twistor correspondence, at least in Penrose’s original formulation, is a natural isomorphism between sheaf cohomology classes on a real hypersurface of complex projective 3-space (i.e. twistor space) and massless Yang–Mills fields on Minkowski space. Mathematically speaking, the twistor space is the Grassmannian of lines in complexified Minkowski space, and the massless Yang–Mills fields correspond to the Grassmannian of planes in the same space. The correspondence space is therefore the Grassmannian of lines in planes in complexified Minkowski space, and it somehow encodes both the quantum mechanical structure of the Yang–Mills fields, and the geometrical structure of the underlying spacetime. This is directly analogous to the definition of the multiway causal graph, whose causal edges between branchlike-separated updating events encode the quantum mechanical structure of the multiway evolution graph, and whose causal edges between spacelike-separated updating events encode the relativistic structure of a pure (spacetime) causal graph.
How do your models relate to homotopy theory, derived geometry and higher category theory?: The relationship between the Wolfram model and ordinary category theory is actually relatively straightforward. One can think of a given hypergraph substitution system as being a morphism of the category Set, mapping the category of possible hypergraphs onto a power set construction on the category of possible hypergraphs, where the power set construction is considered to be an endofunctor on the category Set. As such, a Wolfram model system is really just an F-coalgebra of the power set functor. The relationship to higher-order mathematics, specifically homotopy theory and higher category theory, and their geometrical incarnation in the form of derived geometry, is somewhat more speculative. A possible connection exists via the “snake states” of multiway evolution, as described in our answer to the question about string theory above. The essential idea here would be to use the so-called “cobordism hypothesis”—a theorem which implies that functors on monoidal (∞, n)-categories are entirely determined by their values on a single point, corresponding to an n-vector space of states—to deduce, for instance, that whilst the evolution of a single eigenstate through the multiway evolution graph may be described by a 1-category of spaces of states, the evolution of a maximally-consistent snake state may be described by a higher-order n-category of n-modules.
What do your models imply regarding the black hole information paradox?: The maximum rate of quantum entanglement (i.e. the natural propagation velocity of geodesics in the multiway evolution graph) is, in general, much higher than the speed of light (i.e. the natural propagation velocity of geodesics in the purely relativistic causal graph); however this ceases to be the case in the presence of a sufficiently high mass density in spacetime (i.e. in the presence of a sufficiently high density of causal edges in the multiway causal graph), such as near a black hole. Therefore, a black hole in the multiway causal graph may be characterized by the presence of two distinct horizons: a standard event horizon corresponding to regular causal disconnection, and an entanglement event horizon corresponding to multiway disconnection, which always lies strictly on the exterior of the causal event horizon. As such, from the point of view of an external observer in the multiway causal graph watching an infalling object to a black hole, the object will appear to “freeze” (due to quantum Zeno effects that are the multiway analog of time dilation) at the entanglement horizon, and will never get close to the true causal event horizon. Since Hawking radiation (which occurs as a consequence of non-convergent branch pairs in the multiway evolution graph) is emitted from the entanglement horizon and not the causal event horizon, the particles that get radiated from the black hole may be perfectly correlated with the information contained within the infalling object, without any apparent or actual violation of special relativity (since no information ever crossed a spacetime event horizon), thus resolving the black hole information paradox. This resolution is formally quite similar to the standard resolution to the black hole information paradox implied by the holographic principle and the AdS/CFT duality.
Are your models consistent with the holographic principle/AdS-CFT correspondence?: They certainly seem to be! Indeed, as discussed in the answer about implication for the black hole information paradox, the structure of the multiway causal graph seems to imply a form of the holographic principle in a very natural way. Recall that the multiway causal graph encodes both the structure of the (purely quantum mechanical) multiway evolution graph, as well as the structures of the (purely relativistic) causal graphs corresponding to each branch of multiway evolution. Therefore, one can imagine “walling off” a certain bundle of causal edges in the multiway causal graph corresponding to some particular branch of multiway evolution, such that all of the causal edges inside the boundary of the wall correspond to edges in a purely relativistic causal graph (i.e. they designate causal relations between events in spacetime), whilst all of the causal edges intersecting the boundary of the wall correspond to edges in a purely quantum mechanical multiway graph (i.e. they designate causal relations between events in branchtime). As such, one immediately obtains a duality between the bulk gravitational theory on the interior of the wall, and the boundary quantum mechanical theory on the surface of the wall, just as in AdS/CFT.
What does your formalism mean for computational complexity theory?: One very exciting side effect of our formalism is the ability to recast many open questions in theoretical computer science and computational complexity theory in terms of it; as an illustrative example of this, note that the P vs. NP problem can, at some level, be thought of as corresponding to a question about the relative rates of divergence and convergence of branch pairs in the multiway evolution graph for our universe. More precisely, we begin by noting that Turing machines compute by following a single multiway branch in a completely deterministic fashion. Nondeterministic Turing machines, on the other hand, compute by also following a single multiway branch, but with the choices of which successive branches to take made in accordance with some nondeterministic rule. Therefore, P vs. NP (i.e. the question of whether the set of all problems solvable in polynomial time by a deterministic Turing machine is identical to the set of problems solvable in polynomial time by a nondeterministic Turing machine) is ultimately a question about whether a multiway state obtained by following a predetermined path in the multiway system is always reachable by following a nondeterministic path of approximately the same length. This is trivially true in the case of highly causal-invariant universes (e.g. ones in which all multiway branches eventually converge to a normal form). For a more nontrivial, non-terminating multiway system, the relative rates of branch pair divergence/convergence place constraints on the degree to which P and NP can be related.
Are your models consistent with inflationary cosmology?: Absolutely! The structure of the Wolfram model allows for both local and global variation in spacetime dimensionality; indeed, one of the more subtle mathematical points regarding our derivation of the Einstein field equations is that, at least up to a certain level of granularity, it is not possible to distinguish between local spacetime curvature and a local change in effective spacetime dimension. Therefore, if the initial condition for the universe is a spatial hypergraph with abnormally high vertex connectivity (e.g. a complete graph), then we can interpret this as corresponding to an arbitrarily large number of initial spatial dimensions. If the update rules exhibit the property of being asymptotically dimensionality preserving, then they will eventually cause the effective dimensionality of space to converge to some fixed, finite value. Thinking about such a universe from the point of view of its causal structure, we can see that it therefore starts off with an arbitrarily large value for the speed of light (since the causal graph is arbitrarily densely connected, allowing information propagation at abnormally high speeds), which then converges down to a much lower value at late times. This makes such a universe compatible with a so-called “VSL” or “variable speed of light” cosmology; VSL cosmologies are known to yield similar observational consequences to standard inflationary models, and, in particular, allow for valid solutions to the horizon and flatness problems of ΛCDM cosmology.
Are your models consistent with the ER=EPR conjecture?: Rather as with the holographic principle, the ER=EPR conjecture appears to arise as a natural consequence of the structure of our formalism, since it is ultimately a statement of similarity between the combinatorial structure of the multiway evolution graph vs. spacetime causal graph, which emerges as a consequence of both objects being derived from the (more fundamental) multiway causal graph. In other words, there exists a natural duality between spacetime and branchtime, with connections between distinct points in the spacetime causal graph (which correspond to Einstein–Rosen bridges) behaving in a fundamentally similar way to connections between distinct points in the multiway evolution graph (which correspond to quantum entanglements). More precisely, a formal statement of the ER=EPR conjecture is that the Bekenstein–Hawking entropy of a pair of entangled black holes is equivalent to their entanglement entropy. If Hawking radiation effects occur as a result of branch pairs that fail to reconverge as a consequence of disconnections in the multiway causal graph, the ER=EPR conjecture is really just a rather elementary statement about the geometry of branchtime (in other words, it states that the natural distance metric in branchtime is the entanglement entropy of pairs of microstates, which one can prove directly from the properties of the Fubini–Study metric tensor).
What do your models imply regarding dark energy and the cosmological constant?: Our derivation of general relativity in the continuum limit of Wolfram model systems that satisfy causal invariance and asymptotic dimensionality preservation defines the Einstein field equations only up to an integration constant, thus implying that the model is compatible with both zero and non-zero values of the cosmological constant. Since the energy-momentum tensor for a Wolfram model evolution corresponds to a measure of the flux of causal edges through certain discrete hypersurfaces in the causal graph, we can conclude that those causal edges that are associated with the evolution of elementary particles (such as, for instance, causal edges corresponding to the evolution of nonplanar regions of the spatial hypergraph) will correspond to standard (baryonic) matter contributions, and all remaining causal edges, that are simply associated with the evolution of the background hypergraph, will correspond to vacuum energy contributions. As the latter quantity may vary dynamically as a function of position within the causal graph, we can conclude that the Wolfram model is also compatible with dynamical scalar field models of dark energy, such as quintessence and moduli fields.
Why do you get a Euclidean/Riemannian metric, as opposed to a taxicab metric, induced on your hypergraphs?: If one measures distances by considering lengths of single geodesics between pairs of points in a hypergraph, using (for instance) some variant of Dijkstra’s algorithm, then evidently the induced metric will be discrete, and akin to a generalized taxicab metric. However, our derivation of the Einstein field equations involves first defining the Ollivier-Ricci curvature, which works by considering finite “bundles” of such geodesics (where, for two points in the hypergraph, one constructs a pair of geodesic balls surrounding those points by considering collections of random walks, and then one determines the geodesic distances between corresponding points on those balls using the Wasserstein transportation metric between the balls, considered as probability measures). This has the consequence of effectively “softening” the natural combinatorial metric on the hypergraph into an appropriately discretized version of a Riemannian metric; the full details are given in our formal discussion of general relativity.
Do your models permit the possibility of time travel (i.e. the existence of closed timelike curves)?: The existence of closed timelike curves is forbidden by the requirement of causal invariance in our models (in much the same way as their existence is forbidden by the requirement of strong hyperbolicity in more conventional formulations of Hamiltonian general relativity). More specifically, a closed timelike curve manifests as a cycle in the multiway evolution graph, and since the condition of causal invariance corresponds to the requirement that all paths in the multiway evolution graph yield causal graphs that are ultimately isomorphic, the existence of such a cycle clearly violates this property (since one could always make a causal graph that is “arbitrarily different” from all of the others, by simply traversing that cycle an arbitrary number of times). However, since we are open to the possibility that causal invariance may be violated over sufficiently short timescales (indeed, this appears to be critical to our derivation of quantum mechanics), the existence of short-lived (and presumably microscopic) CTCs is not ruled out by our formalism.
How does the uncertainty principle work in your models?: One particularly exciting feature of the Wolfram model is that its basic structure allows us to prove many deep quantum mechanical results, such as the uncertainty principle, as pure theorems about abstract term rewriting systems. One begins by noting that a pair of abstract rewrite relations, R1 and R2, are said to “commute” if the state obtained by applying R1 and then R2 is identical to the state obtained by applying R2 and then R1. If a multiway Wolfram model evolution is not confluent, in the sense that there exist bifurcations in the multiway evolution graph that never re-converge, then this immediately implies the existence of non-commutative rewrite relations (since an abstract rewriting system is confluent if and only if it commutes with itself). Since each updating event in the multiway system can be thought of as being the application of an abstract rewrite relation, it follows that there must exist pairs of updating events that do not commute, in the sense that the final hypergraph obtained will depend upon the timelike-ordering of the application of those events. If we now interpret the multiway system as being the discrete analog of a (complex) projective Hilbert space, with the rewrite relations being linear operators acting on this space, then this statement immediately reduces to the statement of the standard uncertainty principle regarding the timelike-orderings of measurement operations for pairs of non-commuting observables in quantum mechanics.
How does quantum entanglement occur in your models?: Two global Wolfram model states are said to be “entangled” if they share a common ancestor in the multiway evolution graph. Since spacelike-locality is not a necessary condition for branchlike-locality, it is possible for these states to be causally connected (i.e. to be connected in the multiway causal graph) even if they are not spatially local. This is the essence of quantum entanglement as it occurs, for instance, in the context of Bell’s theorem.
How can your models be consistent with Bell’s theorem?: Despite the deterministic nature of the Wolfram model, consistency with Bell’s theorem is actually a very natural consequence of the combinatorial structure of the multiway causal graph. By allowing for the existence of causal connections not only between updating events on the same branch of evolutionary history, but also between updating events on distinct branches of evolution history, one immediately obtains an explicitly nonlocal theory of multiway evolution. More precisely, one extends the notion of causal locality beyond mere spatial locality, since events that are branchlike-local will not, in general, also be spacelike-local. Therefore, one is able to prove violation of the Bell-CHSH inequality in much the same way as one does for standard deterministic and nonlocal interpretations of quantum mechanics, such as the de Broglie–Bohm or causal interpretation.
How do your models relate to the many-worlds formulation of quantum mechanics?: The simplest variant of the (Everettian) many-worlds interpretation of quantum mechanics, in which there is no effective interference between distinct branches of history, may be thought of as corresponding to the special case of multiway evolution in which there is no resolution of branch pairs (i.e. there is only branch pair divergence), and in which the observer follows only a single multiway branch. The full version of our model, with branch pair convergence and measurement-imposed completion procedures (as detailed in the question on interference), may therefore be thought of as corresponding to a version of Deutsch’s variant of the many-worlds interpretation, in which branches of history are permitted to interfere.
How does quantum interference occur within your models?: Interference occurs as a natural byproduct of the Knuth–Bendix completion procedure for multiway evolution graphs. The simplest way this can work, in the case of the double slit experiment, is as follows: in one multiway branch, the photon goes through one slit, and in another multiway branch, the photon goes through the other slit. By applying a Knuth–Bendix completion, one introduces effective lemmas that equate the divergent branch pair states between these two branches; these lemmas are sufficiently general that they allow for new states in the multiway evolution graph to be reached, and these correspond exactly to the interference states in which the photon traveled through both slits and interfered with itself.
How do your models relate to the de Broglie–Bohm/pilot wave formulation of quantum mechanics?: Our proof of the violation of the CHSH inequality for our model works in much the same way as it does for other standard deterministic and nonlocal interpretations of quantum mechanics, such as the de Broglie–Bohm (otherwise known as the pilot wave or causal) interpretation. However, in our particular case, this nonlocality does not emerge from the propagation of a pilot wave or similar structure, but rather results from the structure of the multiway causal graph (since it allows for the existence of causal connections between branchlike-local microstates in the multiway system, which may or may not be spacelike-local).
How do black holes work within the context of your models?: Spacetime event horizons are characterized by the existence of localized disconnections in the causal graph; if one timelike path in the causal graph cannot be reached from another timelike path, even when allowing for the traversal of infinitely many intermediate causal edges, then we can say that the former region is “causally disconnected” from the latter region (analogous to the definition of an apparent horizon in spacetime as being the region from which light rays cannot escape to future null infinity). If the disconnection is symmetric (i.e. if region 1 and region 2 are mutually unreachable), then this is analogous to a cosmological event horizon. On the other hand, if the disconnection is asymmetric (i.e. if region 1 is reachable from region 2, but not vice versa, which occurs whenever all causal edges connecting the two regions are oriented strictly in the direction of region 1), then this is analogous to a black hole event horizon. The analog of a gravitational singularity is a spatially localized but temporally extended structure in the causal graph with an unusually high density of causal edges. For certain classes of rules, spatially localized structures with sufficiently high causal edge density necessarily break off into locally disconnected regions of the causal graph; as such, these rules can be thought of as being consistent with Penrose’s weak cosmic censorship hypothesis.
How does wave-particle duality work in your models?: Much like Bell’s theorem, the phenomenon of wave-particle duality follows immediately from the basic combinatorial properties of the multiway causal graph. A geodesic bundle propagating through an ordinary (i.e. purely relativistic) causal graph can be thought of as corresponding to the trajectory of a collection of test particles. On the other hand, a geodesic bundle propagating through a pure multiway evolution graph can be thought of as corresponding to the trajectory of a quantum wave packet (since each branch of the multiway evolution graph is analogous to the evolution history of a single quantum eigenstate, and therefore a collection of multiway branches corresponds to the evolution history of some linear superposition of those eigenstates). Since the multiway causal graph reflects both the structure of the multiway evolution graph and the structure of all of its associated causal graphs, any valid foliation of the multiway causal graph will consist of a time-ordered sequence of hypersurfaces, each of which will contain pairs of updating events that can be either spacelike-separated, branchlike-separated, or some combination of the two, with the type of separation depending upon the particular choice of foliation. Therefore, an observer who is embedded within a particular multiway causal foliation will, in general, find it impossible to determine whether a geodesic bundle propagating through the multiway causal graph corresponds to the evolution of a collection of test particles, a wave packet, or both (since the geodesics themselves will either appear to be purely spacelike-separated, purely branchlike-separated, or a combination, depending on the observer). Thus, wave-particle duality is just one of many immediate consequences of the principle of multiway relativity—the preservation of timelike-orderings of updating events in the multiway causal graph, independent of the choice of foliation.
How can your models be Lorentz invariant?: Lorentz covariance, as well as the far stronger condition of general covariance, is one of the many consequences of the principle of causal invariance, i.e. the requirement that all branches of the multiway system should yield causal networks that eventually become isomorphic as directed acyclic graphs. Since each possible foliation of a causal graph into discrete spacelike hypersurfaces corresponds to a possible relativistic observer (and therefore, in the case of spatially flat hypersurfaces, to a possible inertial reference frame), and because each such foliation also defines a particular updating order for the underlying spatial hypergraph, the condition of causal invariance necessarily ensures that the orderings of timelike-separated updating events are always preserved across all inertial reference frames, even though the orderings of spacelike-separated updating events are not. This is precisely the statement of Lorentz covariance.
How does quantum computation work within the context of your models?: In a surprisingly clean way! In short, one straightforward consequence of our interpretation of quantum mechanics in terms of multiway evolutions is the following, very concrete, interpretation of the relationship between Turing machines, non-deterministic Turing machines, and quantum Turing machines: classical Turing machines evolve along a single path of the multiway system (using a deterministic rule to select which branches to follow), non-deterministic Turing machines also evolve along single paths (but now using a non-deterministic rule to select the sequence of successive branches to take), and quantum Turing machines evolve across the entire multiway system itself (i.e. along a superposition of all possible paths).
How do your models relate to the anthropic principle?: The anthropic principle main thrust is to say that for life/intelligence/observers to exist, the universe must be a certain way. It is true that what our models suggest is that the universe looks the way it does to us, because we are a certain way. The anthropic principle would say that we couldn’t exist (in any way) unless the universe was a certain way. But our models say that we can exist in many ways, but the way we perceive the same underlying universe would depend on our details.
Is the project related to sacred geometry?: Not in any direct or formal sense. The specific geometric forms (such as the flower of life) commonly discussed in sacred geometry are overwhelmingly simpler than the forms that emerge even from extremely simple rules in our models. However, the notion (dating back to antiquity) that constructs can combine to reproduce nature has definite conceptual resonance with our approach.
accumulative system: A system in which states are rules and rules update rules. Successive steps in the evolution of such a system are collections of rules that can be applied to each other.
axiomatic level: The traditional foundational way to represent mathematics using axioms, viewed here as being intermediate between the raw ruliad and human-scale mathematics.
bisubstitution: The combination of substitution and cosubstitution that corresponds to the complete set of possible transformations to make on expressions containing patterns.
branchial space: Space corresponding to the limit of a branchial graph that provides a map of common ancestry (or entanglement) in a multiway graph.
cosubstitution: The dual operation to substitution, in which a pattern expression that is to be transformed is specialized to allow a given rule to match it.
eme: The smallest element of existence according to our framework. In physics it can be identified as an “atom of space”, but in general it is an entity whose only internal attribute is that it is distinct from others.
entailment cone: The expanding region of a multiway graph or token-event graph affected by a particular node. The entailment cone is the analog in metamathematical space of a light cone in physical space.
entailment fabric: A piece of metamathematical space constructed by knitting together many small entailment cones. An entailment fabric is a rough model for what a mathematical observer might effectively perceive.
entailment graph: A combination of entailment cones starting from a collection of initial nodes.
expression rewriting: The process of rewriting (tree-structured) symbolic expressions according to rules for symbolic patterns. (Called “operator systems” in A New Kind of Science. Combinators are a special case.)
mathematical observer: An entity sampling the ruliad as a mathematician might effectively do it. Mathematical observers are expected to have certain core human-derived characteristics in common with physical observers.
metamathematical space: The space in which mathematical expressions or mathematical statements can be considered to lie. The space can potentially acquire a geometry as a limit of its construction through a branchial graph.
multiway graph: A graph that represents an evolution process in which there are multiple outcomes from a given state at each step. Multiway graphs are central to our Physics Project and to the multicomputational paradigm in general.
paramathematics: Parallel analogs of mathematics corresponding to different samplings of the ruliad by putative aliens or others.
pattern expression: A symbolic expression that involves pattern variables (x_ etc. in Wolfram Language, or ∀ quantifiers in mathematical logic).
physicalization of metamathematics: The concept of treating metamathematical constructs like elements of the physical universe.
proof cone: Another term for the entailment cone.
proof graph: The subgraph in a token-event graph that leads from axioms to a given statement.
proof path: The path in a multiway graph that shows equivalence between expressions, or the subgraph in a token-event graph that shows the constructibility of a given statement.
ruliad: The entangled limit of all possible computational processes, that is posited to be the ultimate foundation of both physics and mathematics.
rulial space: The limit of rulelike slices taken from a foliation of the ruliad in time. The analog in the rulelike “direction” of branchial space or physical space.
shredding of observers: The process by which an observer who has aggregated statements in a localized region of metamathematical space is effectively pulled apart by trying to cover consequences of these statements.
statement: A symbolic expression, often containing a two-way rule, and often derivable from axioms, and thus representing a lemma or theorem.
substitution event: An update event in which a symbolic expression (which may be a rule) is transformed by substitution according to a given rule.
token-event graph: A graph indicating the transformation of expressions or statements (“tokens”) through updating events.
two-way rule: A transformation rule for pattern expressions that can be applied in both directions (indicated with ).
uniquification: The process of giving different names to variables generated through different events.
space: general limiting structure of basic hypergraph.
time: index of causal foliations of hypergraph rewriting.
matter (in bulk): local fluctuations of features of basic hypergraph.
energy: flux of edges in the multiway causal graph through spacelike (or branchlike) hypersurfaces.
momentum: flux of edges in the multiway causal graph through timelike hypersurfaces.
(rest) mass: numbers of nodes in the hypergraph being reused in updating events.
motion: possible because of causal invariance; associated with change of causal foliations.
particles: locally stable configurations in the hypergraph.
charge, spin, etc.: associated with local configurations of hyperedges.
quantum indeterminacy: different foliations (of branchlike hypersurfaces) in the multiway graph.
quantum effects: associated with locally unresolved branching in the multiway graph.
quantum states: (instantaneously) nodes in the branchial graph.
quantum entanglement: shared ancestry in the multiway graph / distance in branchial graph.
quantum amplitudes: path counting and branchial directions in the multiway graph.
quantum action density (Lagrangian): total flux (divergence) of multiway causal graph edges.
special relativity: global consequence of causal invariance in hypergraph rewriting.
general relativity / general covariance: effect of causal invariance in the causal graph.
locality / causality: consequence of locality of hypergraph rewriting and causal invariance.
rotational invariance: limiting homogeneity of the hypergraph.
Lorentz invariance: consequence of causal invariance in the causal graph.
time dilation: effect of different foliations of the causal graph.
relativistic mass increase: effect of different foliations of the causal graph.
local gauge invariance: consequence of causal invariance in the multiway graph.
lack of quantum cosmological constant: space is effectively created by quantum fluctuations.
cosmological homogeneity: early universe can have higher effective spatial dimension.
expansion of universe: growth of hypergraph.
conservation of energy: equilibrium in the causal graph.
conservation of momentum: balance of different hyperedges during rewritings.
principle of equivalence: gravitational and inertial mass both arise from features of the hypergraph.
discrete conservation laws: features of the ways local hypergraph structures can combine.
microscopic reversibility: limiting equilibrium of hypergraph rewriting processes.
quantum mechanics: consequence of branching in the multiway system.
observer in quantum mechanics: branchlike hypersurface foliation.
quantum objective reality: equivalence of quantum observation frames in the multiway graph.
quantum measurements: updating events with choice of outcomes, that can be frozen by a foliation.
quantum eigenstates: branches in multiway system.
quantum linear superposition: additivity of path counts in the multiway graph.
uncertainty principle: non-commutation of update events in the multiway graph.
wave-particle duality: relation between spacelike and branchlike projections of the multiway causal graph.
operator-state correspondence: states in the multiway graph are generated by events (operators).
path integral: turning of paths in the multiway graph is proportional to causal edge density.
violation of Bell’s inequalities, etc.: existence of causal connections in the multiway graph.
quantum numbers: associated with discrete local properties of the hypergraph.
quantization of charge, etc.: consequence of the discrete hypergraph structure.
black holes / singularities: causal disconnection in the causal graph.
dark matter: (possibly) relic oligons / dimension changes in of space.
virtual particles: local structures continually generated in the spatial and multiway graphs.
black hole radiation / information: causal disconnection of branch pairs.
holographic principle: correspondence between spatial and branchial structure.
dimension of space: growth rate exponent in hypergraph / causal cones.
curvature of space: polynomial part of growth rate in hypergraph / causal cones.
local gauge group: limiting automorphisms of local hypergraph configurations.
speed of light (c): measure of edges in spatial graph vs. causal graph.
light cones: causal cones in the causal graph.
unit of energy: count of edges in the causal graph.
momentum space: limiting structure of causal graph in terms of edges.
gravitational constant: proportionality between node counts and spatial volume.
quantum parameter: measure of edges in the branchial graph (maximum speed of measurement).
elementary unit of entanglement: branching of single branch pair.
electric/gauge charges: counts of local hyperedge configurations.
spectrum of particles: spectrum of locally stable configurations in the hypergraph.
inertial frame: parallel foliation of causal graph.
rest frame of universe: geodesically layered foliation of causal graph.
flat space: uniform hypergraph (typically not maintained by rules).
Minkowski space: effectively uniform causal graph.
cosmological constant: uniform curvature in the hypergraph.
de Sitter space: cyclically connected hypergraph.
closed timelike curves: loops in the causal graph (only possible in some rules).
point particle: a persistent structure in the hypergraph involving comparatively few nodes.
purely empty space: not possible in our models (space is maintained by rule evolution).
vacuum: statistically uniform regions of the spatial hypergraph.
vacuum energy: causal connections attributed purely to establishing the structure of space.
isolated quantum system: disconnected part of the branchial/multiway graph.
collapse of the wave function: degenerate foliation that infinitely retards branchlike entanglement.
non-interacting observer in quantum mechanics: “parallel” foliation of multiway graph.
free field theory: e.g. pure branching in the multiway system.
quantum computation: following multiple branches in multiway system (limited by causal invariance).
string field theory: (potentially) continuous analog of the multiway causal graph for string substitutions.
